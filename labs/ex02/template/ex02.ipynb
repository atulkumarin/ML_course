{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Computing the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `compute_loss` function below:\n",
    "<a id='compute_loss'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    \n",
    "    res = y - np.dot(tx,w)\n",
    "    mse = (np.dot(res.T,res)/(2*len(y)))\n",
    "    \n",
    "    return mse\n",
    "    \n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2792.2367127591669"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([0,0])\n",
    "compute_loss(y,tx,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Grid Search\n",
    "\n",
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    for i, x0 in enumerate(w0):\n",
    "        for j, x1  in enumerate(w1):\n",
    "            losses[i][j] = compute_loss(y, tx, np.array([x0,x1]))\n",
    "    # ***************************************************\n",
    "    return losses\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=18.79354101952324, w0*=71.42857142857142, w1*=15.306122448979579, execution time=0.092 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAAF5CAYAAAAbAcfLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRKROKZETERERqVP/H4LwFm5RtEXXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1142571d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient and loss\n",
    "    gradient = (-1/len(y))*np.dot(tx.T, y - np.dot(tx,w))\n",
    "    # ***************************************************\n",
    "    print(gradient)\n",
    "    return gradient\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-72.293922   -11.47971243]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-72.293922  , -11.47971243])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(y,tx,[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*gradient\n",
    "#         raise NotImplementedError\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "        \n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-73.293922   -13.47971243]\n",
      "Gradient Descent(0/499): loss=2792.236712759167, w0=51.305745401473644, w1=9.435798704492269\n",
      "[-21.9881766   -4.04391373]\n",
      "Gradient Descent(1/499): loss=265.3024621089598, w0=66.69746902191571, w1=12.266538315840005\n",
      "[-6.59645298 -1.21317412]\n",
      "Gradient Descent(2/499): loss=37.87837955044126, w0=71.31498610804834, w1=13.115760199244333\n",
      "[-1.97893589 -0.36395224]\n",
      "Gradient Descent(3/499): loss=17.41021212017447, w0=72.70024123388814, w1=13.370526764265632\n",
      "[-0.59368077 -0.10918567]\n",
      "Gradient Descent(4/499): loss=15.568077051450457, w0=73.11581777164007, w1=13.446956733772023\n",
      "[-0.17810423 -0.0327557 ]\n",
      "Gradient Descent(5/499): loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "[-0.05343127 -0.00982671]\n",
      "Gradient Descent(6/499): loss=15.387363601208632, w0=73.27789262136334, w1=13.476764421879516\n",
      "[-0.01602938 -0.00294801]\n",
      "Gradient Descent(7/499): loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "[-0.00480881 -0.0008844 ]\n",
      "Gradient Descent(8/499): loss=15.38589982226167, w0=73.29247935783842, w1=13.47944711380919\n",
      "[-0.00144264 -0.00026532]\n",
      "Gradient Descent(9/499): loss=15.385888944638305, w0=73.29348920882515, w1=13.47963283863509\n",
      "[ -4.32793280e-04  -7.95963540e-05]\n",
      "Gradient Descent(10/499): loss=15.3858879656522, w0=73.29379216412117, w1=13.479688556082861\n",
      "[ -1.29837984e-04  -2.38789062e-05]\n",
      "Gradient Descent(11/499): loss=15.385887877543452, w0=73.29388305070998, w1=13.479705271317192\n",
      "[ -3.89513952e-05  -7.16367186e-06]\n",
      "Gradient Descent(12/499): loss=15.385887869613665, w0=73.29391031668663, w1=13.479710285887492\n",
      "[ -1.16854186e-05  -2.14910156e-06]\n",
      "Gradient Descent(13/499): loss=15.38588786889998, w0=73.29391849647962, w1=13.479711790258582\n",
      "[ -3.50562557e-06  -6.44730466e-07]\n",
      "Gradient Descent(14/499): loss=15.385887868835754, w0=73.29392095041752, w1=13.479712241569908\n",
      "[ -1.05168767e-06  -1.93419140e-07]\n",
      "Gradient Descent(15/499): loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "[ -3.15506298e-07  -5.80257425e-08]\n",
      "Gradient Descent(16/499): loss=15.385887868829453, w0=73.2939219074533, w1=13.479712417581325\n",
      "[ -9.46518941e-08  -1.74077234e-08]\n",
      "Gradient Descent(17/499): loss=15.385887868829403, w0=73.29392197370962, w1=13.479712429766732\n",
      "[ -2.83955739e-08  -5.22231686e-09]\n",
      "Gradient Descent(18/499): loss=15.3858878688294, w0=73.29392199358652, w1=13.479712433422353\n",
      "[ -8.51866771e-09  -1.56669521e-09]\n",
      "Gradient Descent(19/499): loss=15.3858878688294, w0=73.2939219995496, w1=13.47971243451904\n",
      "[ -2.55559244e-09  -4.70008302e-10]\n",
      "Gradient Descent(20/499): loss=15.3858878688294, w0=73.29392200133852, w1=13.479712434848047\n",
      "[ -7.66675839e-10  -1.41001982e-10]\n",
      "Gradient Descent(21/499): loss=15.385887868829398, w0=73.29392200187519, w1=13.479712434946748\n",
      "[ -2.30001388e-10  -4.23007350e-11]\n",
      "Gradient Descent(22/499): loss=15.3858878688294, w0=73.29392200203618, w1=13.479712434976358\n",
      "[ -6.90057277e-11  -1.26902762e-11]\n",
      "Gradient Descent(23/499): loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "[ -2.07041921e-11  -3.80678102e-12]\n",
      "Gradient Descent(24/499): loss=15.385887868829398, w0=73.29392200209898, w1=13.479712434987906\n",
      "[ -6.20748324e-12  -1.14231398e-12]\n",
      "Gradient Descent(25/499): loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "[ -1.85864337e-12  -3.43004558e-13]\n",
      "Gradient Descent(26/499): loss=15.3858878688294, w0=73.29392200210464, w1=13.479712434988945\n",
      "[ -5.54064172e-13  -1.03244702e-13]\n",
      "Gradient Descent(27/499): loss=15.385887868829403, w0=73.29392200210502, w1=13.479712434989018\n",
      "[ -1.70530257e-13  -3.05135472e-14]\n",
      "Gradient Descent(28/499): loss=15.3858878688294, w0=73.29392200210513, w1=13.47971243498904\n",
      "[ -5.42968337e-14  -8.98694452e-15]\n",
      "Gradient Descent(29/499): loss=15.385887868829398, w0=73.29392200210518, w1=13.479712434989047\n",
      "[ -1.10958354e-14  -2.11457518e-15]\n",
      "Gradient Descent(30/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(31/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(32/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(33/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(34/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(35/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(36/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(37/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(38/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(39/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(40/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(41/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(42/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(43/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(44/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(45/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(46/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(47/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(48/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(49/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(50/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(51/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(52/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(53/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(54/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(55/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(56/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(57/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(58/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(59/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(60/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(61/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(62/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(63/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(64/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(65/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(66/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(67/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(68/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(69/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(70/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(71/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(72/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(73/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(74/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(75/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(76/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(77/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(78/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(79/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(80/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(81/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(82/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(83/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(84/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(85/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(86/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(87/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(88/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(89/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(90/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(91/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(92/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(93/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(94/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(95/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(96/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(97/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(98/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(99/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(100/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(101/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(102/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(103/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(104/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(105/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(106/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(107/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(108/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(109/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(110/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(111/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(112/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(113/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(114/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(115/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(116/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(117/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(118/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(119/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(120/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(121/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(122/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(123/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(124/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(125/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(126/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(127/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(128/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(129/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(130/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(131/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(132/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(133/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(134/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(135/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(136/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(137/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(138/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(139/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(140/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(141/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(142/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(143/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(144/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(145/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(146/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(147/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(148/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(149/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(150/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(151/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(152/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(153/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(154/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(155/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(156/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(157/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(158/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(159/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(160/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(161/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(162/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(163/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(164/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(165/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(166/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(167/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(168/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(169/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(170/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(171/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(172/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(173/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(174/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(175/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(176/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(177/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(178/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(179/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(180/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(181/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(182/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(183/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(184/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(185/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(186/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(187/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(188/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(189/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(190/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(191/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(192/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(193/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(194/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(195/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(196/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(197/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(198/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(199/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(200/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(201/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(202/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(203/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(204/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(205/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(206/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(207/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(208/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(209/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(210/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(211/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(212/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(213/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(214/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(215/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(216/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(217/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(218/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(219/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(220/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(221/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(222/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(223/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(224/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(225/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(226/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(227/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(228/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(229/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(230/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(231/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(232/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(233/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(234/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(235/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(236/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(237/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(238/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(239/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(240/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(241/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(242/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(243/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(244/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(245/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(246/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(247/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(248/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(249/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(250/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(251/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(252/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(253/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(254/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(255/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(256/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(257/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(258/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(259/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(260/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(261/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(262/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(263/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(264/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(265/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(266/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(267/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(268/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(269/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(270/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(271/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(272/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(273/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(274/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(275/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(276/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(277/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(278/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(279/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(280/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(281/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(282/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(283/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(284/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(285/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(286/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(287/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(288/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(289/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(290/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(291/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(292/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(293/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(294/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(295/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(296/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(297/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(298/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(299/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(300/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(301/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(302/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(303/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(304/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(305/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(306/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(307/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(308/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(309/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(310/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(311/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(312/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(313/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(314/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(315/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(316/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(317/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(318/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(319/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(320/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(321/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(322/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(323/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(324/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(325/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(326/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(327/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(328/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(329/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(330/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(331/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(332/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(333/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(334/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(335/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(336/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(337/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(338/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(339/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(340/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(341/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(342/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(343/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(344/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(345/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(346/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(347/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(348/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(349/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(350/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(351/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(352/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(353/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(354/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(355/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(356/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(357/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(358/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(359/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(360/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(361/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(362/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(363/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(364/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(365/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(366/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(367/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(368/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(369/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(370/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(371/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(372/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(373/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(374/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(375/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(376/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(377/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(378/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(379/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(380/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(381/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(382/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(383/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(384/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(385/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(386/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(387/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(388/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(389/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(390/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(391/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(392/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(393/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(394/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(395/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(396/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(397/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(398/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(399/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(400/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(401/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(402/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(403/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(404/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(405/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(406/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(407/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(408/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(409/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(410/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(411/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(412/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(413/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(414/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(415/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(416/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(417/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(418/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(419/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(420/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(421/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(422/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(423/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(424/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(425/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(426/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(427/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(428/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(429/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(430/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(431/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(432/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(433/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(434/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(435/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(436/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(437/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(438/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(439/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(440/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(441/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(442/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(443/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(444/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(445/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(446/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(447/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(448/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(449/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(450/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(451/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(452/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(453/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(454/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(455/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(456/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(457/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(458/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(459/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(460/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(461/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(462/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(463/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(464/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(465/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(466/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(467/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(468/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(469/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(470/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(471/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(472/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(473/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(474/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(475/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(476/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(477/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(478/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(479/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(480/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(481/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(482/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(483/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(484/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(485/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(486/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(487/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(488/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(489/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(490/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(491/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(492/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(493/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(494/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(495/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(496/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(497/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(498/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(499/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent: execution time=0.337 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cc47a31bd4435d906d4c595eb1767c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation.It's same as the gradient descent.\n",
    "    # ***************************************************\n",
    "    return compute_gradient(y, tx, w)\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "    # ***************************************************\n",
    "    \n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    n_iter=-1\n",
    "    \n",
    "    for i in range(0,max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y,tx,batch_size,int(np.floor(len(y)/batch_size))):\n",
    "            gradient = compute_gradient(y_batch, tx_batch,w)\n",
    "            loss = compute_loss(y, tx, w)\n",
    "\n",
    "            w = w - gamma*gradient\n",
    "\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            n_iter=n_iter+1\n",
    "            print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                  bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    \n",
    "    \n",
    "    return losses, ws\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ufunc 'floor'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-72.06192918 -12.25447362]\n",
      "Gradient Descent(0/9): loss=2792.236712759167, w0=50.44335042841476, w1=8.578131532398674\n",
      "[-23.03433979  -2.97947126]\n",
      "Gradient Descent(1/9): loss=288.4729461633232, w0=66.56738828110616, w1=10.663761417050145\n",
      "[-4.08353951 -2.99401876]\n",
      "Gradient Descent(2/9): loss=41.97380588641347, w0=69.425865940481, w1=12.7595745488649\n",
      "[-0.20125059 -0.39672637]\n",
      "Gradient Descent(3/9): loss=23.12611600427888, w0=69.56674135651403, w1=13.037283009924193\n",
      "[-3.47677108  2.19066158]\n",
      "Gradient Descent(4/9): loss=22.429697549345676, w0=72.00048111237734, w1=11.503819901664755\n",
      "[ 0.4525515  -1.14189266]\n",
      "Gradient Descent(5/9): loss=18.174458188062722, w0=71.683695064338, w1=12.303144763862429\n",
      "[-3.06854824  0.47923431]\n",
      "Gradient Descent(6/9): loss=17.374459006755114, w0=73.83167883080706, w1=11.967680744922777\n",
      "[ 0.43343753  0.22855849]\n",
      "Gradient Descent(7/9): loss=16.673598988119483, w0=73.5282725619203, w1=11.807689802285624\n",
      "[-2.05601833 -3.72916223]\n",
      "Gradient Descent(8/9): loss=16.811177803408473, w0=74.96748539175766, w1=14.418103363200927\n",
      "[ 2.50363178 -0.10552638]\n",
      "Gradient Descent(9/9): loss=17.226583845497114, w0=73.21494314651859, w1=14.491971828644932\n",
      "[-1.86846384  2.9769273 ]\n",
      "Gradient Descent(10/9): loss=15.901341238666573, w0=74.52286783304257, w1=12.408122722041854\n",
      "[ 2.36632345  0.90138885]\n",
      "Gradient Descent(11/9): loss=16.71519405296571, w0=72.86644142136721, w1=11.777150523800936\n",
      "[ 0.03086402 -1.75858242]\n",
      "Gradient Descent(12/9): loss=16.926616222997698, w0=72.84483660902097, w1=13.008158219646955\n",
      "[-2.512321   -2.03414959]\n",
      "Gradient Descent(13/9): loss=15.597908402973653, w0=74.60346130737027, w1=14.432062930868709\n",
      "[ 0.15716322  2.05110694]\n",
      "Gradient Descent(14/9): loss=16.696820198347588, w0=74.49344705278466, w1=12.99628807449945\n",
      "[-0.01407945 -2.24272024]\n",
      "Gradient Descent(15/9): loss=16.22216759859058, w0=74.50330266951396, w1=14.566192241602915\n",
      "[ 0.43848908 -0.6811767 ]\n",
      "Gradient Descent(16/9): loss=16.70740785327029, w0=74.19636031397063, w1=15.043015930587618\n",
      "[ 2.82443037  0.5403056 ]\n",
      "Gradient Descent(17/9): loss=17.015044231866028, w0=72.21925905482999, w1=14.66480201053818\n",
      "[-0.39044968  2.37959469]\n",
      "Gradient Descent(18/9): loss=16.665556744990123, w0=72.49257382887114, w1=12.999085728798399\n",
      "[-0.3163142   0.73357633]\n",
      "Gradient Descent(19/9): loss=15.822468331554008, w0=72.71399376699492, w1=12.485582294842692\n",
      "[-0.83697635 -3.17970263]\n",
      "Gradient Descent(20/9): loss=16.048193615542164, w0=73.29987721217181, w1=14.711374138816616\n",
      "[-1.9767095   2.44754958]\n",
      "Gradient Descent(21/9): loss=16.14440087743058, w0=74.68357386020313, w1=12.998089432538084\n",
      "[ 0.5328245   0.56884651]\n",
      "Gradient Descent(22/9): loss=16.467434370431864, w0=74.31059670812544, w1=12.599896874948342\n",
      "[ 4.25073274 -3.29754763]\n",
      "Gradient Descent(23/9): loss=16.28973930760496, w0=71.33508378995342, w1=14.908180215060003\n",
      "[ 0.39460034  1.85224726]\n",
      "Gradient Descent(24/9): loss=18.324671538872803, w0=71.05886355495903, w1=13.611607134152683\n",
      "[-1.0235233  3.4718989]\n",
      "Gradient Descent(25/9): loss=17.892329105742828, w0=71.77532986829672, w1=11.18127790712079\n",
      "[-0.39146969 -5.83553245]\n",
      "Gradient Descent(26/9): loss=19.180349542710367, w0=72.04935865233736, w1=15.26615062260652\n",
      "[-3.34287933  1.64906557]\n",
      "Gradient Descent(27/9): loss=17.75603753371106, w0=74.38937418145505, w1=14.11180472188646\n",
      "[ 1.43305661 -1.5498059 ]\n",
      "Gradient Descent(28/9): loss=16.185665937028183, w0=73.38623455442554, w1=15.196668849549269\n",
      "[-1.40554139  1.77309014]\n",
      "Gradient Descent(29/9): loss=16.86411833723709, w0=74.37011352412317, w1=13.955505750512177\n",
      "[ 0.16576126 -0.18036782]\n",
      "Gradient Descent(30/9): loss=16.078171604409334, w0=74.2540806432696, w1=14.08176322472899\n",
      "[ 2.55907674 -2.29432793]\n",
      "Gradient Descent(31/9): loss=16.02807275364399, w0=72.46272692233408, w1=15.687792773266974\n",
      "[ 0.55229777 -0.14813661]\n",
      "Gradient Descent(32/9): loss=18.16913988929203, w0=72.07611848073358, w1=15.791488401545857\n",
      "[-1.87824715  0.22182564]\n",
      "Gradient Descent(33/9): loss=18.79956463693678, w0=73.39089148512515, w1=15.636210455515554\n",
      "[ 0.08668104  8.23197382]\n",
      "Gradient Descent(34/9): loss=17.71583126541535, w0=73.33021475783889, w1=9.873828781866129\n",
      "[-1.76696198 -0.32726003]\n",
      "Gradient Descent(35/9): loss=21.88774491081832, w0=74.56708814371306, w1=10.102910805969982\n",
      "[ 2.4732204  -4.92808479]\n",
      "Gradient Descent(36/9): loss=21.897758501770657, w0=72.83583386320082, w1=13.552570160857586\n",
      "[ 1.05262056  0.29042409]\n",
      "Gradient Descent(37/9): loss=15.493464364441206, w0=72.09899947058649, w1=13.34927329960549\n",
      "[-1.29565347 -0.41249267]\n",
      "Gradient Descent(38/9): loss=16.108314981014733, w0=73.00595690027052, w1=13.638018170412504\n",
      "[-1.27536744  4.27543324]\n",
      "Gradient Descent(39/9): loss=15.439880171700707, w0=73.89871410682163, w1=10.645214904341206\n",
      "[ 1.76306692 -3.12413871]\n",
      "Gradient Descent(40/9): loss=19.585962739417432, w0=72.66456726221661, w1=12.832112000412527\n",
      "[-2.69713521  2.75775774]\n",
      "Gradient Descent(41/9): loss=15.79362472457136, w0=74.55256191197786, w1=10.901681582889665\n",
      "[-0.43552263 -7.32325355]\n",
      "Gradient Descent(42/9): loss=19.501096617379687, w0=74.85742775062737, w1=16.027959064610364\n",
      "[-0.39552028  0.4707879 ]\n",
      "Gradient Descent(43/9): loss=19.854943424348537, w0=75.13429194967242, w1=15.698407537540165\n",
      "[ 1.31148851  2.76318616]\n",
      "Gradient Descent(44/9): loss=19.54067261982585, w0=74.21624999523014, w1=13.76417722633683\n",
      "[ 1.13175166  0.57185799]\n",
      "Gradient Descent(45/9): loss=15.851692441038619, w0=73.42402383544265, w1=13.363876633864713\n",
      "[ 1.20160811  1.88066496]\n",
      "Gradient Descent(46/9): loss=15.40106007875934, w0=72.58289816050501, w1=12.047411159996257\n",
      "[-0.93302068 -3.83429112]\n",
      "Gradient Descent(47/9): loss=16.664408791664325, w0=73.23601263699021, w1=14.731414946783197\n",
      "[ 0.15216494  3.5187674 ]\n",
      "Gradient Descent(48/9): loss=16.1709442051293, w0=73.12949717738212, w1=12.268277768537514\n",
      "[-0.07675922 -4.1085987 ]\n",
      "Gradient Descent(49/9): loss=16.133192605862273, w0=73.18322863447837, w1=15.144296856822374\n",
      "[ 1.75863167  1.58140649]\n",
      "Gradient Descent(50/9): loss=16.777435028352777, w0=71.9521864636228, w1=14.037312310865548\n",
      "[-1.12970679  0.4422465 ]\n",
      "Gradient Descent(51/9): loss=16.441473807231464, w0=72.74298121383774, w1=13.727739762023097\n",
      "[-2.20559556  0.1863425 ]\n",
      "Gradient Descent(52/9): loss=15.568414522395608, w0=74.28689810508409, w1=13.597300013939337\n",
      "[-0.3684161  6.2465443]\n",
      "Gradient Descent(53/9): loss=15.885802058734678, w0=74.54478937273224, w1=9.224719004227282\n",
      "[ 1.25943184 -6.34184956]\n",
      "Gradient Descent(54/9): loss=25.220707006192015, w0=73.6631870844502, w1=13.66401369830163\n",
      "[ 0.68108948  0.27727307]\n",
      "Gradient Descent(55/9): loss=15.47104969717834, w0=73.1864244470837, w1=13.469922547803062\n",
      "[-2.44831417 -0.49768007]\n",
      "Gradient Descent(56/9): loss=15.391713651942755, w0=74.90024436287783, w1=13.818298595634058\n",
      "[ 0.56542727 -0.80494278]\n",
      "Gradient Descent(57/9): loss=16.733343926278653, w0=74.50444527611295, w1=14.381758540704363\n",
      "[ 2.25440209  3.4378413 ]\n",
      "Gradient Descent(58/9): loss=16.52541475570471, w0=72.92636381058014, w1=11.975269628048466\n",
      "[-0.65576922 -3.99025568]\n",
      "Gradient Descent(59/9): loss=16.585111460585612, w0=73.38540226499384, w1=14.76844860750072\n",
      "[-1.24270657 -2.18175603]\n",
      "Gradient Descent(60/9): loss=16.220492649248506, w0=74.25529686272624, w1=16.295677831091925\n",
      "[-0.43548435  2.68089498]\n",
      "Gradient Descent(61/9): loss=19.812839236170888, w0=74.56013590945503, w1=14.419051341909974\n",
      "[ 2.44248443  0.26842683]\n",
      "Gradient Descent(62/9): loss=16.62871548944017, w0=72.85039680756618, w1=14.231152559747057\n",
      "[-2.43375853  1.03701402]\n",
      "Gradient Descent(63/9): loss=15.766576298472998, w0=74.55402777567528, w1=13.505242746975433\n",
      "[-1.33114953  0.98712295]\n",
      "Gradient Descent(64/9): loss=16.1801470475368, w0=75.48583244923759, w1=12.81425668304845\n",
      "[ 1.54925957 -1.75173727]\n",
      "Gradient Descent(65/9): loss=18.0095392518489, w0=74.40135075317542, w1=14.040472769597107\n",
      "[-1.68266234  3.22753871]\n",
      "Gradient Descent(66/9): loss=16.156313164612754, w0=75.5792143944546, w1=11.781195669549952\n",
      "[ 1.70949102 -0.99525285]\n",
      "Gradient Descent(67/9): loss=19.439648129333285, w0=74.38257068286549, w1=12.477872664680966\n",
      "[-2.41613325  0.53787207]\n",
      "Gradient Descent(68/9): loss=16.480307306575448, w0=76.07386395459882, w1=12.101362217944848\n",
      "[-0.05049941 -0.89728709]\n",
      "Gradient Descent(69/9): loss=20.199851158859353, w0=76.10921353901571, w1=12.729463180171226\n",
      "[-0.01827814  0.2942973 ]\n",
      "Gradient Descent(70/9): loss=19.63025805990676, w0=76.1220082353012, w1=12.523455066895417\n",
      "[ 1.68571422 -0.88467937]\n",
      "Gradient Descent(71/9): loss=19.842137817042467, w0=74.9420082783639, w1=13.142730625372728\n",
      "[-0.03993618 -4.16498772]\n",
      "Gradient Descent(72/9): loss=16.800760425831697, w0=74.96996360374538, w1=16.058222030582865\n",
      "[ 4.15856081  2.81529543]\n",
      "Gradient Descent(73/9): loss=20.114801461328405, w0=72.05897103694161, w1=14.087515228772435\n",
      "[-1.50792473  1.51561812]\n",
      "Gradient Descent(74/9): loss=16.333151930074067, w0=73.11451834490613, w1=13.026582541331384\n",
      "[ 1.62306937  0.47147968]\n",
      "Gradient Descent(75/9): loss=15.5046440552007, w0=71.97836978714876, w1=12.696546765117944\n",
      "[-1.53376679 -1.76648667]\n",
      "Gradient Descent(76/9): loss=16.5579009172001, w0=73.0520065394206, w1=13.933087437171555\n",
      "[-1.20108223 -0.98872209]\n",
      "Gradient Descent(77/9): loss=15.517923860674346, w0=73.89276409745061, w1=14.625192897352504\n",
      "[-2.13637979  0.88053411]\n",
      "Gradient Descent(78/9): loss=16.221256541236443, w0=75.38822995212604, w1=14.008819020670824\n",
      "[ 2.207582    1.46738923]\n",
      "Gradient Descent(79/9): loss=17.71892765309558, w0=73.84292255091775, w1=12.981646560065922\n",
      "[-0.03706802  0.01532939]\n",
      "Gradient Descent(80/9): loss=15.660623478009116, w0=73.86887016411707, w1=12.970915989691983\n",
      "[ 2.60029048  2.00596912]\n",
      "Gradient Descent(81/9): loss=15.680607474703283, w0=72.04866682627788, w1=11.566737603834058\n",
      "[-3.05509136 -0.94066523]\n",
      "Gradient Descent(82/9): loss=17.990954447607983, w0=74.18723077665558, w1=12.225203267209784\n",
      "[-1.61442915 -0.78254075]\n",
      "Gradient Descent(83/9): loss=16.571784778194875, w0=75.31733118051584, w1=12.772981791743822\n",
      "[ 1.29730915 -0.79699119]\n",
      "Gradient Descent(84/9): loss=17.68271432151853, w0=74.40921477701379, w1=13.330875625579846\n",
      "[ 0.94249546 -1.58870962]\n",
      "Gradient Descent(85/9): loss=16.018903053628616, w0=73.74946795470203, w1=14.442972357400382\n",
      "[-0.5049328   2.62263367]\n",
      "Gradient Descent(86/9): loss=15.953583765355024, w0=74.10292091475883, w1=12.607128789995032\n",
      "[ 2.95598164  1.8153671 ]\n",
      "Gradient Descent(87/9): loss=16.093828597922304, w0=72.03373376796073, w1=11.33637182283373\n",
      "[-3.04506876 -5.41366272]\n",
      "Gradient Descent(88/9): loss=18.476879551424638, w0=74.16528189789611, w1=15.12593572692754\n",
      "[ 0.00824924 -1.1352713 ]\n",
      "Gradient Descent(89/9): loss=17.12054746628618, w0=74.15950743099536, w1=15.920625640286385\n",
      "[ 3.08301324  3.31881435]\n",
      "Gradient Descent(90/9): loss=18.739535574080346, w0=72.00139816352453, w1=13.597455593725709\n",
      "[-2.36582547 -0.66655461]\n",
      "Gradient Descent(91/9): loss=16.22812853119369, w0=73.6574759956475, w1=14.064043824202571\n",
      "[ 0.94332384  4.88842266]\n",
      "Gradient Descent(92/9): loss=15.622695208149784, w0=72.99714930635497, w1=10.642147965582325\n",
      "[-2.86366985 -0.70287102]\n",
      "Gradient Descent(93/9): loss=19.455810944320557, w0=75.0017182015111, w1=11.134157682374102\n",
      "[-0.0634584  -3.47542137]\n",
      "Gradient Descent(94/9): loss=19.594985346939314, w0=75.04613908471838, w1=13.566952638247466\n",
      "[ 0.48127899  0.23752714]\n",
      "Gradient Descent(95/9): loss=16.924825647662416, w0=74.7092437942551, w1=13.400683642936075\n",
      "[ 1.98051752  0.12079492]\n",
      "Gradient Descent(96/9): loss=16.390578531483293, w0=73.3228815325149, w1=13.316127201296979\n",
      "[ 0.8989006   0.12476414]\n",
      "Gradient Descent(97/9): loss=15.39968726037122, w0=72.69365111363777, w1=13.228792300558883\n",
      "[-0.33508114 -1.4443149 ]\n",
      "Gradient Descent(98/9): loss=15.597530895531358, w0=72.92820791134545, w1=14.239812728624603\n",
      "[ 0.8057923  1.0862481]\n",
      "Gradient Descent(99/9): loss=15.74163749511194, w0=72.36415330451986, w1=13.479439055951374\n",
      "[-0.62406139 -2.16108636]\n",
      "Gradient Descent(100/9): loss=15.81812282170221, w0=72.80099628050694, w1=14.992199510538509\n",
      "[-0.34509415  1.5658486 ]\n",
      "Gradient Descent(101/9): loss=16.65118432918806, w0=73.04256218414724, w1=13.896105491508957\n",
      "[-3.4763759   1.45816431]\n",
      "Gradient Descent(102/9): loss=15.504170336630324, w0=75.47602531601528, w1=12.8753904776777\n",
      "[-2.13887947 -0.4279582 ]\n",
      "Gradient Descent(103/9): loss=17.949277819162415, w0=76.97324094162548, w1=13.174961218212527\n",
      "[ 1.98549062 -0.89223336]\n",
      "Gradient Descent(104/9): loss=22.201018450249133, w0=75.58339750607111, w1=13.799524567118091\n",
      "[ 0.96232371  0.08334467]\n",
      "Gradient Descent(105/9): loss=18.05787681038786, w0=74.90977090850966, w1=13.741183297923554\n",
      "[ 2.1577239  -1.78366256]\n",
      "Gradient Descent(106/9): loss=16.725555219075513, w0=73.39936418091114, w1=14.989747092003416\n",
      "[ 0.06097724  1.18688665]\n",
      "Gradient Descent(107/9): loss=16.531549228057322, w0=73.35668011062857, w1=14.158926438128205\n",
      "[-2.5190582   4.55689633]\n",
      "Gradient Descent(108/9): loss=15.618522989952274, w0=75.12002085077619, w1=10.969099010155936\n",
      "[ 2.71200389  1.45449941]\n",
      "Gradient Descent(109/9): loss=20.204796255864302, w0=73.22161812525707, w1=9.950949426064309\n",
      "[ 2.69262678 -4.35156543]\n",
      "Gradient Descent(110/9): loss=21.614585980710828, w0=71.336779377727, w1=12.997045226628222\n",
      "[-3.34613088  0.2525476 ]\n",
      "Gradient Descent(111/9): loss=17.417575311921794, w0=73.67907099385238, w1=12.8202619046265\n",
      "[ 0.51961307 -0.23922671]\n",
      "Gradient Descent(112/9): loss=15.677495242749062, w0=73.315341843504, w1=12.987720602809793\n",
      "[ 0.50063447  0.05665959]\n",
      "Gradient Descent(113/9): loss=15.507145255097727, w0=72.9648977132577, w1=12.94805888851335\n",
      "[ 0.72867834 -0.95493928]\n",
      "Gradient Descent(114/9): loss=15.58134410689529, w0=72.45482287467597, w1=13.616516385012101\n",
      "[ 0.28791758  0.04220741]\n",
      "Gradient Descent(115/9): loss=15.747289202026591, w0=72.25328057085939, w1=13.586971195289145\n",
      "[-0.94956794 -0.16523566]\n",
      "Gradient Descent(116/9): loss=15.93310738387261, w0=72.91797812785958, w1=13.702636153917677\n",
      "[-0.59159878 -0.37560865]\n",
      "Gradient Descent(117/9): loss=15.481402259351285, w0=73.33209727152988, w1=13.965562207321273\n",
      "[ 1.33502655 -0.35034011]\n",
      "Gradient Descent(118/9): loss=15.50464154506486, w0=72.39757868956124, w1=14.210800286937618\n",
      "[-1.47536776 -1.05464615]\n",
      "Gradient Descent(119/9): loss=16.054848259433914, w0=73.4303361201555, w1=14.949052592395496\n",
      "[-2.17802     0.71413735]\n",
      "Gradient Descent(120/9): loss=16.474672523714723, w0=74.95495012177685, w1=14.449156444885848\n",
      "[ 3.76201145  0.04914042]\n",
      "Gradient Descent(121/9): loss=17.23530592016177, w0=72.32154210524321, w1=14.414758153838614\n",
      "[-1.48425901  2.41850647]\n",
      "Gradient Descent(122/9): loss=16.295804448909507, w0=73.36052341216376, w1=12.72180362575557\n",
      "[ 2.22332936 -0.63817393]\n",
      "Gradient Descent(123/9): loss=15.675318624297148, w0=71.8041928575195, w1=13.16852537645704\n",
      "[-5.08641588 -0.08663717]\n",
      "Gradient Descent(124/9): loss=16.543953023642313, w0=75.36468397171805, w1=13.229171398753598\n",
      "[-2.29725669 -1.27293398]\n",
      "Gradient Descent(125/9): loss=17.56130084164584, w0=76.9727636548265, w1=14.12022518198011\n",
      "[ 2.88191188 -1.53062198]\n",
      "Gradient Descent(126/9): loss=22.357954111257023, w0=74.95542533994819, w1=15.191660569269288\n",
      "[-0.83679902  0.80571719]\n",
      "Gradient Descent(127/9): loss=18.231567746893912, w0=75.54118465561697, w1=14.627658535503425\n",
      "[ 2.19505786  0.94562479]\n",
      "Gradient Descent(128/9): loss=18.56987271060689, w0=74.00464415585327, w1=13.965721183187771\n",
      "[-1.12394691  1.54806597]\n",
      "Gradient Descent(129/9): loss=15.756553110406399, w0=74.79140699113991, w1=12.882075006032586\n",
      "[ 3.51252191  0.98198141]\n",
      "Gradient Descent(130/9): loss=16.685703763266403, w0=72.3326416573789, w1=12.194688020808668\n",
      "[-0.79339196 -0.02506678]\n",
      "Gradient Descent(131/9): loss=16.67356169192776, w0=72.88801602620413, w1=12.212234764932731\n",
      "[ 0.61871615 -0.29510579]\n",
      "Gradient Descent(132/9): loss=16.271517521511193, w0=72.45491472102373, w1=12.4188088193659\n",
      "[-1.49913176 -1.63283938]\n",
      "Gradient Descent(133/9): loss=16.30061271850439, w0=73.5043069501085, w1=13.561796388169137\n",
      "[ 0.8565683  -0.45612845]\n",
      "Gradient Descent(134/9): loss=15.411387669687416, w0=72.90470913710212, w1=13.881086302436358\n",
      "[-0.35457725  0.16858041]\n",
      "Gradient Descent(135/9): loss=15.542181686706156, w0=73.15291321466509, w1=13.763080012756316\n",
      "[ 1.87605201 -2.12323275]\n",
      "Gradient Descent(136/9): loss=15.435978199961907, w0=71.83967680724197, w1=15.249342939091427\n",
      "[-1.77955725  0.13099208]\n",
      "Gradient Descent(137/9): loss=18.009098472745606, w0=73.0853668845507, w1=15.157648485422243\n",
      "[ 0.51695781  2.03311394]\n",
      "Gradient Descent(138/9): loss=16.815370182030158, w0=72.72349641464419, w1=13.734468730600126\n",
      "[-0.49270779 -0.16251761]\n",
      "Gradient Descent(139/9): loss=15.581030929321255, w0=73.06839186910871, w1=13.848231059918989\n",
      "[ 0.07238533 -0.24202911]\n",
      "Gradient Descent(140/9): loss=15.479222777734234, w0=73.0177221389578, w1=14.017651435311388\n",
      "[-2.18296548 -0.07425019]\n",
      "Gradient Descent(141/9): loss=15.56872023506462, w0=74.54579797276703, w1=14.069626565127928\n",
      "[-0.37725297 -0.89082165]\n",
      "Gradient Descent(142/9): loss=16.343483932258422, w0=74.80987505297098, w1=14.69320171873203\n",
      "[ 3.09624686  0.42041493]\n",
      "Gradient Descent(143/9): loss=17.271222815923576, w0=72.6425022508546, w1=14.398911269342907\n",
      "[-2.89291844 -1.39745972]\n",
      "Gradient Descent(144/9): loss=16.020524963527834, w0=74.66754515555822, w1=15.377133075712758\n",
      "[-4.29775858  4.84800135]\n",
      "Gradient Descent(145/9): loss=18.1294106966027, w0=77.67597616148244, w1=11.983532134154887\n",
      "[ 2.99100893 -4.71742328]\n",
      "Gradient Descent(146/9): loss=26.10636494298923, w0=75.58226990893417, w1=15.285728430430925\n",
      "[-0.38064505  0.83064568]\n",
      "Gradient Descent(147/9): loss=19.635002828069627, w0=75.84872144415112, w1=14.704276455428081\n",
      "[ 3.86252472  0.17785155]\n",
      "Gradient Descent(148/9): loss=19.399166483445406, w0=73.1449541411675, w1=14.57978037122005\n",
      "[ 3.52870042  1.84702066]\n",
      "Gradient Descent(149/9): loss=16.00205831278734, w0=70.67486384581942, w1=13.286865910232105\n",
      "[-1.91335307  0.06913811]\n",
      "Gradient Descent(150/9): loss=18.834215572888333, w0=72.01421099699023, w1=13.238469232193776\n",
      "[-1.22841352 -0.09262836]\n",
      "Gradient Descent(151/9): loss=16.23381713858303, w0=72.87410045980685, w1=13.303309082332367\n",
      "[ 0.21818479  0.68638219]\n",
      "Gradient Descent(152/9): loss=15.489572003932533, w0=72.72137110332295, w1=12.822841549138852\n",
      "[-1.09847711  0.63074739]\n",
      "Gradient Descent(153/9): loss=15.765534815016386, w0=73.49030508356813, w1=12.381318373095299\n",
      "[-0.78516242  1.09612121]\n",
      "Gradient Descent(154/9): loss=16.008405783773565, w0=74.03991877496188, w1=11.61403352738831\n",
      "[ 0.60154062 -3.12462362]\n",
      "Gradient Descent(155/9): loss=17.40452235451884, w0=73.61884034301615, w1=13.801270062074776\n",
      "[ 0.23667686 -1.18444631]\n",
      "Gradient Descent(156/9): loss=15.490373486728068, w0=73.45316654126, w1=14.630382479126874\n",
      "[ 1.68998901  5.82722687]\n",
      "Gradient Descent(157/9): loss=16.060588055692786, w0=72.27017423571243, w1=10.551323667347496\n",
      "[-2.51419713 -3.46640319]\n",
      "Gradient Descent(158/9): loss=20.19764800065108, w0=74.03011222881972, w1=12.977805900873243\n",
      "[ 0.35234098 -2.10001551]\n",
      "Gradient Descent(159/9): loss=15.782830978278467, w0=73.78347354051425, w1=14.447816759573692\n",
      "[ 1.76125009  2.78817774]\n",
      "Gradient Descent(160/9): loss=15.974331214848483, w0=72.55059847994222, w1=12.496092342092558\n",
      "[ 1.06596141 -0.66252453]\n",
      "Gradient Descent(161/9): loss=16.14590704170463, w0=71.8044254941039, w1=12.959859513273345\n",
      "[-2.70138629 -0.45572342]\n",
      "Gradient Descent(162/9): loss=16.6303113226116, w0=73.6953958969004, w1=13.27886590407529\n",
      "[ 4.50178222 -1.67606562]\n",
      "Gradient Descent(163/9): loss=15.486648177420461, w0=70.54414834405648, w1=14.452111834759807\n",
      "[-4.85338057  1.63317751]\n",
      "Gradient Descent(164/9): loss=19.639295750415954, w0=73.94151474072957, w1=13.308887576748385\n",
      "[ 2.86521966 -0.65767671]\n",
      "Gradient Descent(165/9): loss=15.610166612485386, w0=71.93586097607209, w1=13.769261270725362\n",
      "[-2.17667783 -3.23890825]\n",
      "Gradient Descent(166/9): loss=16.34997200818256, w0=73.45953545678275, w1=16.036497048779452\n",
      "[-0.04104315  2.00469213]\n",
      "Gradient Descent(167/9): loss=18.668175557672193, w0=73.48826566526391, w1=14.633212557563969\n",
      "[-3.47903548 -1.94366646]\n",
      "Gradient Descent(168/9): loss=16.070053864924553, w0=75.92359050020562, w1=15.993779077160031\n",
      "[ 1.62418538  1.29066752]\n",
      "Gradient Descent(169/9): loss=22.003731614418722, w0=74.78666073353291, w1=15.09031181488838\n",
      "[ 2.12924904  0.40831498]\n",
      "Gradient Descent(170/9): loss=17.79703751024768, w0=73.29618640447154, w1=14.804491327981054\n",
      "[ 2.42724532  1.02247571]\n",
      "Gradient Descent(171/9): loss=16.263409990247002, w0=71.59711468152385, w1=14.088758331634008\n",
      "[-1.02710173  3.06107252]\n",
      "Gradient Descent(172/9): loss=17.01093386252865, w0=72.31608589379522, w1=11.94600756610383\n",
      "[-2.39596173 -1.57180841]\n",
      "Gradient Descent(173/9): loss=17.040094908607905, w0=73.9932591081303, w1=13.046273449951132\n",
      "[ 1.17937578  0.26241494]\n",
      "Gradient Descent(174/9): loss=15.724358739636541, w0=73.16769606504113, w1=12.862582990459956\n",
      "[-0.85077132 -3.0764402 ]\n",
      "Gradient Descent(175/9): loss=15.584278738075643, w0=73.76323598980922, w1=15.016091128627354\n",
      "[ 0.90037221  1.23300174]\n",
      "Gradient Descent(176/9): loss=16.6762454234896, w0=73.13297544200894, w1=14.152989913985506\n",
      "[ 0.30190586 -0.64136537]\n",
      "Gradient Descent(177/9): loss=15.62549104829472, w0=72.92164133800782, w1=14.601945674597731\n",
      "[-2.58230152 -1.44899907]\n",
      "Gradient Descent(178/9): loss=16.08488803730109, w0=74.72925240440995, w1=15.616245020735876\n",
      "[ 2.31804616  1.16394394]\n",
      "Gradient Descent(179/9): loss=18.698360295698578, w0=73.10662009079506, w1=14.801484262388058\n",
      "[ 0.39170938  0.463999  ]\n",
      "Gradient Descent(180/9): loss=16.276969253672473, w0=72.83242352592265, w1=14.47668496571162\n",
      "[ 0.31845075  0.5976775 ]\n",
      "Gradient Descent(181/9): loss=15.989355404096488, w0=72.60950800083423, w1=14.058310712446257\n",
      "[-0.20840872  3.96513713]\n",
      "Gradient Descent(182/9): loss=15.787487114735493, w0=72.75539410191791, w1=11.282714724938426\n",
      "[ 2.54137456 -1.54945629]\n",
      "Gradient Descent(183/9): loss=17.944293487453294, w0=70.97643191099549, w1=12.36733413059236\n",
      "[-2.00901826  0.54173398]\n",
      "Gradient Descent(184/9): loss=18.68996077607145, w0=72.38274469267294, w1=11.988120344880356\n",
      "[ 1.13561636 -2.31206897]\n",
      "Gradient Descent(185/9): loss=16.9134333950789, w0=71.58781324168022, w1=13.606568626561806\n",
      "[-2.22607472  0.02148602]\n",
      "Gradient Descent(186/9): loss=16.849337666698982, w0=73.14606554344442, w1=13.591528414953023\n",
      "[ 0.26505207 -1.51758346]\n",
      "Gradient Descent(187/9): loss=15.403070041700904, w0=72.96052909125153, w1=14.65383683510899\n",
      "[-1.56605313  1.4051401 ]\n",
      "Gradient Descent(188/9): loss=16.130747338811645, w0=74.05676628295512, w1=13.67023876258956\n",
      "[ 2.9720856   0.74176364]\n",
      "Gradient Descent(189/9): loss=15.695003707996596, w0=71.97630636469151, w1=13.151004214614337\n",
      "[-0.34134168  0.58701332]\n",
      "Gradient Descent(190/9): loss=16.307967899878882, w0=72.21524554194829, w1=12.740094887848912\n",
      "[ 0.55219488  0.54596422]\n",
      "Gradient Descent(191/9): loss=16.241176379696505, w0=71.82870912864958, w1=12.3579199336975\n",
      "[-1.97244824 -2.89443024]\n",
      "Gradient Descent(192/9): loss=17.088521459076397, w0=73.20942289595699, w1=14.384021099372728\n",
      "[ 1.16749494 -0.59212184]\n",
      "Gradient Descent(193/9): loss=15.798344998539019, w0=72.39217644121535, w1=14.798506388510983\n",
      "[ 0.76706567  3.73412868]\n",
      "Gradient Descent(194/9): loss=16.66206914304467, w0=71.85523047391801, w1=12.184616314222431\n",
      "[ 2.51189016 -5.65536888]\n",
      "Gradient Descent(195/9): loss=17.25944150648054, w0=70.0969073634625, w1=16.143374528346524\n",
      "[-4.84154502  0.86380493]\n",
      "Gradient Descent(196/9): loss=24.04388704247198, w0=73.48598887833995, w1=15.538711074029429\n",
      "[-2.15568953 -0.83622663]\n",
      "Gradient Descent(197/9): loss=17.524070409087763, w0=74.99497154900406, w1=16.12406971565227\n",
      "[ 3.26965518  0.08275686]\n",
      "Gradient Descent(198/9): loss=20.328985363230114, w0=72.70621292339554, w1=16.06613991091587\n",
      "[-2.08937262  2.34158415]\n",
      "Gradient Descent(199/9): loss=18.903392393542866, w0=74.16877375859019, w1=14.427031006000034\n",
      "[-2.6722159   1.45372938]\n",
      "Gradient Descent(200/9): loss=16.217276904232993, w0=76.0393248853447, w1=13.409420441033014\n",
      "[ 2.72290372 -1.05896273]\n",
      "Gradient Descent(201/9): loss=19.156976846686458, w0=74.13329228175225, w1=14.150694354696121\n",
      "[ 3.5770774   0.49927043]\n",
      "Gradient Descent(202/9): loss=15.963267470293685, w0=71.62933810006727, w1=13.801205053350447\n",
      "[-2.39181145  4.17948346]\n",
      "Gradient Descent(203/9): loss=16.822986404121725, w0=73.30360611346386, w1=10.875566629874863\n",
      "[-0.5822696  -2.62435022]\n",
      "Gradient Descent(204/9): loss=18.77672244698271, w0=73.7111948340125, w1=12.712611783991834\n",
      "[-0.23504611 -1.60686303]\n",
      "Gradient Descent(205/9): loss=15.767167881333549, w0=73.87572711054841, w1=13.837415906244011\n",
      "[-0.71018787 -0.00680953]\n",
      "Gradient Descent(206/9): loss=15.619112347608642, w0=74.37285861983737, w1=13.842182574646719\n",
      "[ 2.30126429  1.52908923]\n",
      "Gradient Descent(207/9): loss=16.03363228244281, w0=72.76197361689466, w1=12.771820112623166\n",
      "[-1.72628514 -1.52919808]\n",
      "Gradient Descent(208/9): loss=15.777928181125723, w0=73.97037321811314, w1=13.842258766614021\n",
      "[ 2.47643277 -0.9977038 ]\n",
      "Gradient Descent(209/9): loss=15.680400913936078, w0=72.23687028225153, w1=14.540651424127176\n",
      "[ 1.21646923  0.78240146]\n",
      "Gradient Descent(210/9): loss=16.507362807388905, w0=71.38534181861952, w1=13.992970400674501\n",
      "[-0.13986804  3.28694485]\n",
      "Gradient Descent(211/9): loss=17.338943896896275, w0=71.48324944353799, w1=11.692109007781644\n",
      "[-1.40576353 -1.85446089]\n",
      "Gradient Descent(212/9): loss=18.622918432485378, w0=72.467283914226, w1=12.99023162778655\n",
      "[-6.02117369 -0.77542241]\n",
      "Gradient Descent(213/9): loss=15.847348863305486, w0=76.68210549608484, w1=13.533027314931738\n",
      "[ 1.26369079  0.84165362]\n",
      "Gradient Descent(214/9): loss=21.127202801479143, w0=75.7975219452639, w1=12.943869778331527\n",
      "[-0.03629625 -1.47100742]\n",
      "Gradient Descent(215/9): loss=18.663457882868435, w0=75.8229293208019, w1=13.973574969118532\n",
      "[ 1.68858347  0.44820867]\n",
      "Gradient Descent(216/9): loss=18.705776979148567, w0=74.64092089120155, w1=13.659828899999376\n",
      "[ 2.54715721  0.35767307]\n",
      "Gradient Descent(217/9): loss=16.30931184292672, w0=72.85791084327396, w1=13.409457749361168\n",
      "[-1.4940778  -1.82630484]\n",
      "Gradient Descent(218/9): loss=15.483408594568415, w0=73.90376530584534, w1=14.687871139078833\n",
      "[ 3.82346088  0.1233153 ]\n",
      "Gradient Descent(219/9): loss=16.301666023521705, w0=71.22734268921725, w1=14.601550431788924\n",
      "[-2.96289495  0.78702568]\n",
      "Gradient Descent(220/9): loss=18.15052314258957, w0=73.30136915233865, w1=14.050632453569474\n",
      "[ 0.12079282  2.20721038]\n",
      "Gradient Descent(221/9): loss=15.548890432660636, w0=73.21681417680963, w1=12.505585184933064\n",
      "[-0.95367903 -0.05547545]\n",
      "Gradient Descent(222/9): loss=15.863322626841121, w0=73.88438949473056, w1=12.544417998915398\n",
      "[ 1.67067202  0.1395043 ]\n",
      "Gradient Descent(223/9): loss=15.997601639828208, w0=72.7149190773433, w1=12.446764986778627\n",
      "[ 0.14559671 -1.69640529]\n",
      "Gradient Descent(224/9): loss=16.087000277653022, w0=72.61300138101298, w1=13.63424868954844\n",
      "[ 0.30130921  1.68518429]\n",
      "Gradient Descent(225/9): loss=15.629655041930324, w0=72.40208493605955, w1=12.454619687233922\n",
      "[-1.62331127 -0.14233081]\n",
      "Gradient Descent(226/9): loss=16.30898211576592, w0=73.5384028243881, w1=12.554251252899187\n",
      "[ 1.01194366 -1.57586074]\n",
      "Gradient Descent(227/9): loss=15.844012504839048, w0=72.83004226366828, w1=13.657353768863828\n",
      "[ 0.84720738  0.62322888]\n",
      "Gradient Descent(228/9): loss=15.509258296445955, w0=72.23699709652597, w1=13.221093553259697\n",
      "[-3.91769907 -1.11472695]\n",
      "Gradient Descent(229/9): loss=15.977874859839693, w0=74.97938644780116, w1=14.001402415315491\n",
      "[-0.763616   -0.14419293]\n",
      "Gradient Descent(230/9): loss=16.942363285468517, w0=75.51391764651137, w1=14.102337469684116\n",
      "[ 4.17961584  1.32661447]\n",
      "Gradient Descent(231/9): loss=18.04390916633513, w0=72.58818655528157, w1=13.173707343964626\n",
      "[-0.20062792  1.09606208]\n",
      "Gradient Descent(232/9): loss=15.681738687147503, w0=72.72862610034855, w1=12.406463889623316\n",
      "[ 1.92092414 -1.87680062]\n",
      "Gradient Descent(233/9): loss=16.121598817165655, w0=71.38397920141769, w1=13.720224320382766\n",
      "[-3.51509081  2.06280491]\n",
      "Gradient Descent(234/9): loss=17.23875160328623, w0=73.84454276884783, w1=12.27626088512206\n",
      "[-0.98724899 -1.84809471]\n",
      "Gradient Descent(235/9): loss=16.261627299652154, w0=74.53561706003515, w1=13.569927178868852\n",
      "[ 0.35888044 -1.59439356]\n",
      "Gradient Descent(236/9): loss=16.16086052727989, w0=74.28440074944712, w1=14.686002667915592\n",
      "[ 1.36805552 -1.24635608]\n",
      "Gradient Descent(237/9): loss=16.603980006324402, w0=73.32676188437128, w1=15.558451923292495\n",
      "[-2.60907432  1.95145535]\n",
      "Gradient Descent(238/9): loss=17.547006027879064, w0=75.15311391062573, w1=14.192433180257979\n",
      "[ 2.15217843  5.63957028]\n",
      "Gradient Descent(239/9): loss=17.36817057555188, w0=73.64658901220494, w1=10.244733983798636\n",
      "[ 0.49257449 -4.48959135]\n",
      "Gradient Descent(240/9): loss=20.680617668668916, w0=73.30178686813603, w1=13.387447931568722\n",
      "[ 0.12410993  1.00364516]\n",
      "Gradient Descent(241/9): loss=15.390175166183942, w0=73.21490991950037, w1=12.684896321346727\n",
      "[-1.16254529 -1.39701703]\n",
      "Gradient Descent(242/9): loss=15.704875650680915, w0=74.02869162282074, w1=13.662808239937341\n",
      "[ 4.35244707 -0.91932212]\n",
      "Gradient Descent(243/9): loss=15.672593103487472, w0=70.98197867525512, w1=14.306333723311054\n",
      "[-2.22694156  2.7198894 ]\n",
      "Gradient Descent(244/9): loss=18.40008021926625, w0=72.54083776774623, w1=12.402411142234769\n",
      "[-0.83341058 -1.03672858]\n",
      "Gradient Descent(245/9): loss=16.249744838534436, w0=73.12422517617436, w1=13.128121147514848\n",
      "[ 0.75493552 -0.19091733]\n",
      "Gradient Descent(246/9): loss=15.46209459190878, w0=72.59577031434598, w1=13.261763278126288\n",
      "[-0.68865439 -0.61930925]\n",
      "Gradient Descent(247/9): loss=15.653346675878508, w0=73.07782838526593, w1=13.695279755492637\n",
      "[-2.59523917 -1.7720036 ]\n",
      "Gradient Descent(248/9): loss=15.432470729283285, w0=74.89449580574409, w1=14.93568227600688\n",
      "[ 0.63301047  3.56953834]\n",
      "Gradient Descent(249/9): loss=17.726730208253684, w0=74.45138847472714, w1=12.437005434817351\n",
      "[ 1.00370039 -1.42344108]\n",
      "Gradient Descent(250/9): loss=16.59937113055488, w0=73.74879819836981, w1=13.433414187994263\n",
      "[ 0.71641026 -1.75599013]\n",
      "Gradient Descent(251/9): loss=15.490415809630882, w0=73.24731101506617, w1=14.662607276841877\n",
      "[ 1.51484692  1.11771634]\n",
      "Gradient Descent(252/9): loss=16.08659426432679, w0=72.18691817266871, w1=13.88020584142581\n",
      "[-2.64258549  0.09017239]\n",
      "Gradient Descent(253/9): loss=16.078814092322574, w0=74.03672801252446, w1=13.81708517122317\n",
      "[ 1.12254438  0.66517192]\n",
      "Gradient Descent(254/9): loss=15.718678434963946, w0=73.25094694373657, w1=13.351464828485536\n",
      "[ 0.2928327  -2.13028054]\n",
      "Gradient Descent(255/9): loss=15.395035020937236, w0=73.04596405628165, w1=14.84266120812247\n",
      "[ 0.17147881 -0.50340505]\n",
      "Gradient Descent(256/9): loss=16.345444119370864, w0=72.9259288917845, w1=15.195044744025582\n",
      "[-3.08524334  3.01386903]\n",
      "Gradient Descent(257/9): loss=16.924779798663454, w0=75.08559922962279, w1=13.085336421362708\n",
      "[ 0.50010487 -1.82854848]\n",
      "Gradient Descent(258/9): loss=17.068707732693873, w0=74.73552582268579, w1=14.365320359477204\n",
      "[ 3.68281555  4.09604932]\n",
      "Gradient Descent(259/9): loss=16.817149354543798, w0=72.15755494088208, w1=11.498085837783645\n",
      "[ 0.93530224  1.26461852]\n",
      "Gradient Descent(260/9): loss=17.994974903121758, w0=71.50284337012411, w1=10.612852873462913\n",
      "[ 0.4716629  -1.59597961]\n",
      "Gradient Descent(261/9): loss=21.099311074555917, w0=71.17267934273052, w1=11.73003859829106\n",
      "[-2.37449919  0.58565894]\n",
      "Gradient Descent(262/9): loss=19.166402346217588, w0=72.8348287751795, w1=11.320077338948062\n",
      "[ 0.86743281 -2.50576018]\n",
      "Gradient Descent(263/9): loss=17.823283038359897, w0=72.22762580965961, w1=13.074109465746549\n",
      "[-0.20281654 -2.41633646]\n",
      "Gradient Descent(264/9): loss=16.03663853817053, w0=72.36959738830792, w1=14.765544985091221\n",
      "[ 0.30156867  2.1536479 ]\n",
      "Gradient Descent(265/9): loss=16.63975853811626, w0=72.15849931774962, w1=13.257991456835299\n",
      "[-3.3106093   0.12992124]\n",
      "Gradient Descent(266/9): loss=16.055060300980735, w0=74.47592582920673, w1=13.16704658910638\n",
      "[ 2.10997393  0.76350751]\n",
      "Gradient Descent(267/9): loss=16.133334358061507, w0=72.99894407815799, w1=12.632591331556968\n",
      "[-2.78086136 -1.01819473]\n",
      "Gradient Descent(268/9): loss=15.78820093857749, w0=74.94554702869846, w1=13.345327645998783\n",
      "[ 5.82761592 -3.13300164]\n",
      "Gradient Descent(269/9): loss=16.758850118819986, w0=70.86621588220865, w1=15.53842879562636\n",
      "[-5.23675256  2.47040546]\n",
      "Gradient Descent(270/9): loss=20.45192289789883, w0=74.53194267594496, w1=13.809144975971904\n",
      "[ 1.50052624  3.76759836]\n",
      "Gradient Descent(271/9): loss=16.20649836278595, w0=73.48157430493309, w1=11.171826123011314\n",
      "[ 0.62083705 -3.21984414]\n",
      "Gradient Descent(272/9): loss=18.066664176714806, w0=73.04698837231281, w1=13.425717024286834\n",
      "[-3.73704634  1.23849864]\n",
      "Gradient Descent(273/9): loss=15.417833729779069, w0=75.66292081138035, w1=12.558767978508753\n",
      "[ 4.05549696 -3.19510993]\n",
      "Gradient Descent(274/9): loss=18.616034893963853, w0=72.82407294041803, w1=14.79534492909261\n",
      "[-0.13583604  3.91659059]\n",
      "Gradient Descent(275/9): loss=16.36171136898413, w0=72.91915816518315, w1=12.053731515722081\n",
      "[-1.97135222 -0.51737268]\n",
      "Gradient Descent(276/9): loss=16.472822626618395, w0=74.2991047224863, w1=12.415892391437186\n",
      "[-1.67672264 -3.25844097]\n",
      "Gradient Descent(277/9): loss=16.456940562037133, w0=75.47281057060887, w1=14.696801070171698\n",
      "[ 2.31562676  2.16968399]\n",
      "Gradient Descent(278/9): loss=18.500317938752794, w0=73.85187183534272, w1=13.178022280253584\n",
      "[ 0.77126586  0.74877409]\n",
      "Gradient Descent(279/9): loss=15.58705035176645, w0=73.31198573313844, w1=12.653880415882908\n",
      "[ 1.64386625 -0.18735153]\n",
      "Gradient Descent(280/9): loss=15.727050279909285, w0=72.16127935838097, w1=12.785026490019753\n",
      "[-0.98233672 -1.458901  ]\n",
      "Gradient Descent(281/9): loss=16.26862182908964, w0=72.84891505944326, w1=13.806257192942336\n",
      "[-0.07376784  1.54870917]\n",
      "Gradient Descent(282/9): loss=15.538219197811445, w0=72.90055254990024, w1=12.722160776746406\n",
      "[-3.22945318 -0.82984569]\n",
      "Gradient Descent(283/9): loss=15.750199889246499, w0=75.16116977937861, w1=13.303052759080794\n",
      "[ 0.81064095  0.85489853]\n",
      "Gradient Descent(284/9): loss=17.14479932024167, w0=74.5937211159968, w1=12.704623789386261\n",
      "[ 5.65014233 -4.72644607]\n",
      "Gradient Descent(285/9): loss=16.53100794133729, w0=70.63862148493719, w1=16.013136035031323\n",
      "[-2.66097057  1.2584709 ]\n",
      "Gradient Descent(286/9): loss=22.12031585569131, w0=72.50130088064549, w1=15.13220640686199\n",
      "[-5.86394243 -1.28571844]\n",
      "Gradient Descent(287/9): loss=17.065380153459625, w0=76.60606058479334, w1=16.03220931270337\n",
      "[ 5.44105846  3.42632026]\n",
      "Gradient Descent(288/9): loss=24.128639019665805, w0=72.79731966331583, w1=13.633785129584995\n",
      "[ 2.61813043  3.95029736]\n",
      "Gradient Descent(289/9): loss=15.521064007884958, w0=70.96462835887364, w1=10.86857697523449\n",
      "[ 0.37992139 -3.46506939]\n",
      "Gradient Descent(290/9): loss=21.507706501622675, w0=70.69868338348785, w1=13.294125545539824\n",
      "[-3.35906554 -1.54591107]\n",
      "Gradient Descent(291/9): loss=18.77074085937855, w0=73.05002926434874, w1=14.376263293829329\n",
      "[-0.11885431 -0.37730943]\n",
      "Gradient Descent(292/9): loss=15.817531423838192, w0=73.13322728338801, w1=14.640379891425148\n",
      "[ 2.56237893  1.92831081]\n",
      "Gradient Descent(293/9): loss=16.07237373735612, w0=71.33956203175208, w1=13.290562323940957\n",
      "[-3.33801197  0.46594737]\n",
      "Gradient Descent(294/9): loss=17.313538197943455, w0=73.67617041381021, w1=12.96439916601045\n",
      "[-0.47768541  2.01953822]\n",
      "Gradient Descent(295/9): loss=15.591718675547611, w0=74.01055020343694, w1=11.550722415322623\n",
      "[ 0.97679002 -3.16474319]\n",
      "Gradient Descent(296/9): loss=17.50316710628773, w0=73.32679719271282, w1=13.7660426450594\n",
      "[ 6.54436691  3.9358135 ]\n",
      "Gradient Descent(297/9): loss=15.427420752507611, w0=68.74574035322821, w1=11.010973191656165\n",
      "[-6.52594412 -2.56953348]\n",
      "Gradient Descent(298/9): loss=28.776202750205936, w0=73.3139012354204, w1=12.809646630236015\n",
      "[-1.34881393 -2.85896972]\n",
      "Gradient Descent(299/9): loss=15.610581545060995, w0=74.25807098648939, w1=14.810925434894326\n",
      "[ 5.60593457  2.13419074]\n",
      "Gradient Descent(300/9): loss=16.736743526432342, w0=70.3339167894499, w1=13.316991919121891\n",
      "[-4.86539421 -0.98191244]\n",
      "Gradient Descent(301/9): loss=19.779942281444672, w0=73.73969273701235, w1=14.004330624165691\n",
      "[ 0.10660414  0.64379875]\n",
      "Gradient Descent(302/9): loss=15.622855765086724, w0=73.66506984134921, w1=13.553671499938499\n",
      "[ 2.62870396  6.67693144]\n",
      "Gradient Descent(303/9): loss=15.457498199761252, w0=71.82497707264001, w1=8.879819493597743\n",
      "[-3.49911987 -4.76817146]\n",
      "Gradient Descent(304/9): loss=27.0442950078609, w0=74.27436098010688, w1=12.217539513009282\n",
      "[ 3.32394761 -2.92116826]\n",
      "Gradient Descent(305/9): loss=16.66305840611137, w0=71.94759765344254, w1=14.262357292313215\n",
      "[ 0.26754937 -1.90233644]\n",
      "Gradient Descent(306/9): loss=16.598448981078334, w0=71.76031309697619, w1=15.593992802741745\n",
      "[-3.92096529  4.58025057]\n",
      "Gradient Descent(307/9): loss=18.796956742507128, w0=74.50498880150306, w1=12.387817403380874\n",
      "[ 4.11107207 -1.4635109 ]\n",
      "Gradient Descent(308/9): loss=16.71534664515661, w0=71.62723834932878, w1=13.412275032038576\n",
      "[-0.9711917   0.10332747]\n",
      "Gradient Descent(309/9): loss=16.777078969703812, w0=72.30707253589512, w1=13.33994580208319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.19611289 -0.56521038]\n",
      "Gradient Descent(310/9): loss=15.882591159145875, w0=72.44435155677385, w1=13.735593070892497\n",
      "[-1.7168855   0.21434265]\n",
      "Gradient Descent(311/9): loss=15.779510289534821, w0=73.6461714061623, w1=13.58555321631206\n",
      "[-0.80547443 -1.52877213]\n",
      "Gradient Descent(312/9): loss=15.45352882565423, w0=74.21000350728717, w1=14.655693708347835\n",
      "[ 0.48985037  1.30409134]\n",
      "Gradient Descent(313/9): loss=16.49695650854292, w0=73.86710824500955, w1=13.742829771210122\n",
      "[ 0.82977767  0.18625427]\n",
      "Gradient Descent(314/9): loss=15.584774469666844, w0=73.28626387832848, w1=13.61245177949505\n",
      "[-1.97809449  1.22103998]\n",
      "Gradient Descent(315/9): loss=15.394727059049233, w0=74.67093001826771, w1=12.75772379195046\n",
      "[ 1.6185184  -0.06561264]\n",
      "Gradient Descent(316/9): loss=16.594597207455674, w0=73.53796714170628, w1=12.803652640579005\n",
      "[-3.16306869 -1.2357315 ]\n",
      "Gradient Descent(317/9): loss=15.644195306719732, w0=75.75211522144515, w1=13.668664692446933\n",
      "[ 0.80572285  1.59766568]\n",
      "Gradient Descent(318/9): loss=18.4250962984331, w0=75.18810922792105, w1=12.550298717944386\n",
      "[ 0.19903233 -1.17623738]\n",
      "Gradient Descent(319/9): loss=17.611765420766773, w0=75.04878659352576, w1=13.37366488591411\n",
      "[ 1.95854076 -2.61632962]\n",
      "Gradient Descent(320/9): loss=16.93128577727265, w0=73.67780805807466, w1=15.205095619772404\n",
      "[ 3.27609357  2.66715163]\n",
      "Gradient Descent(321/9): loss=16.948045687979874, w0=71.38454256009223, w1=13.338089481289527\n",
      "[-2.35872059 -0.43521707]\n",
      "Gradient Descent(322/9): loss=17.21878132612755, w0=73.03564697383241, w1=13.642741427401894\n",
      "[ 0.78671749 -0.01322889]\n",
      "Gradient Descent(323/9): loss=15.432530090127626, w0=72.4849447295346, w1=13.652001650239654\n",
      "[-1.93048637  1.20048171]\n",
      "Gradient Descent(324/9): loss=15.727951769443107, w0=73.83628518942842, w1=12.811664452708563\n",
      "[-0.07584309 -1.00383882]\n",
      "Gradient Descent(325/9): loss=15.756110835625622, w0=73.88937535435794, w1=13.514351629366296\n",
      "[ 4.0643728  -1.00467757]\n",
      "Gradient Descent(326/9): loss=15.56377015307747, w0=71.04431439240969, w1=14.217625927346454\n",
      "[-3.62919713  1.47331369]\n",
      "Gradient Descent(327/9): loss=18.18851322873091, w0=73.58475238383426, w1=13.186306342561585\n",
      "[ 1.68083584  0.21126659]\n",
      "Gradient Descent(328/9): loss=15.471222591834515, w0=72.40816729513678, w1=13.038419726852178\n",
      "[-1.78247934  0.64633664]\n",
      "Gradient Descent(329/9): loss=15.875538196415134, w0=73.6559028360821, w1=12.585984077812284\n",
      "[-3.11762824 -0.30392766]\n",
      "Gradient Descent(330/9): loss=15.850778119123651, w0=75.83824260353774, w1=12.7987334428831\n",
      "[-0.27554214 -6.04707039]\n",
      "Gradient Descent(331/9): loss=18.854537724111257, w0=76.03112210415455, w1=17.03168271504513\n",
      "[ 1.52793885  4.6529386 ]\n",
      "Gradient Descent(332/9): loss=25.440266503359734, w0=74.9615649096987, w1=13.774625696533235\n",
      "[ 3.09260113  4.04309849]\n",
      "Gradient Descent(333/9): loss=16.819891218370174, w0=72.79674411523544, w1=10.944456756879676\n",
      "[ 0.38950536 -0.95728949]\n",
      "Gradient Descent(334/9): loss=18.72324147111844, w0=72.52409036137307, w1=11.614559402147277\n",
      "[ 1.49246112  1.0942873 ]\n",
      "Gradient Descent(335/9): loss=17.42160616432498, w0=71.47936758060281, w1=10.848558289065567\n",
      "[-5.40419505 -3.41458314]\n",
      "Gradient Descent(336/9): loss=20.493677812931473, w0=75.26230411767416, w1=13.238766490034013\n",
      "[ 2.93985141 -3.59722448]\n",
      "Gradient Descent(337/9): loss=17.35217941947043, w0=73.20440813212078, w1=15.756823626699191\n",
      "[ 3.87456464 -2.70583736]\n",
      "Gradient Descent(338/9): loss=17.982511924994988, w0=70.49221288602018, w1=17.65090978062237\n",
      "[-1.6519729   4.79337153]\n",
      "Gradient Descent(339/9): loss=28.01011850251556, w0=71.64859391437108, w1=14.295549708164467\n",
      "[ 0.21789387  0.45652055]\n",
      "Gradient Descent(340/9): loss=17.072235355123947, w0=71.49606820722158, w1=13.975985326552076\n",
      "[-0.43254253  0.1959772 ]\n",
      "Gradient Descent(341/9): loss=17.125170394168258, w0=71.79884798051168, w1=13.838801289045438\n",
      "[-1.02742325 -0.03007565]\n",
      "Gradient Descent(342/9): loss=16.567983436405058, w0=72.51804425462537, w1=13.859854245824208\n",
      "[ 0.06952918 -1.2161288 ]\n",
      "Gradient Descent(343/9): loss=15.7591349065191, w0=72.46937383067593, w1=14.711144408897663\n",
      "[-0.46397716 -0.40042606]\n",
      "Gradient Descent(344/9): loss=16.4840400655153, w0=72.79415784473382, w1=14.991442649764725\n",
      "[-0.58245816  1.66231874]\n",
      "Gradient Descent(345/9): loss=16.653434096458817, w0=73.20187855698603, w1=13.827819528824907\n",
      "[ 0.23884141 -1.03347171]\n",
      "Gradient Descent(346/9): loss=15.450713141113525, w0=73.03468957161586, w1=14.551249723968905\n",
      "[-1.90058091  1.23389832]\n",
      "Gradient Descent(347/9): loss=15.993584676175253, w0=74.36509620804945, w1=13.687520900649922\n",
      "[ 0.84596708  0.13358462]\n",
      "Gradient Descent(348/9): loss=15.981187137769721, w0=73.77291925264782, w1=13.59401166690985\n",
      "[ 1.70747173  0.31155721]\n",
      "Gradient Descent(349/9): loss=15.507139209051942, w0=72.57768904237908, w1=13.375921620724938\n",
      "[ 0.62439717  0.42637677]\n",
      "Gradient Descent(350/9): loss=15.647768961691217, w0=72.14061102036332, w1=13.077457878657016\n",
      "[ 0.64144026  0.69705973]\n",
      "Gradient Descent(351/9): loss=16.131855343177534, w0=71.69160283748501, w1=12.589516066055829\n",
      "[-4.79828792 -0.135596  ]\n",
      "Gradient Descent(352/9): loss=17.0658260091149, w0=75.05040437977648, w1=12.684433264337576\n",
      "[-0.76311227 -0.5988096 ]\n",
      "Gradient Descent(353/9): loss=17.244737520000346, w0=75.5845829710996, w1=13.103599983091282\n",
      "[ 4.61716136  2.33860463]\n",
      "Gradient Descent(354/9): loss=18.080181994502862, w0=72.35257001963582, w1=11.466576741634602\n",
      "[-2.05205483 -2.14751031]\n",
      "Gradient Descent(355/9): loss=17.85531730620775, w0=73.78900839917017, w1=12.96983396181656\n",
      "[ 0.43253362  1.28485528]\n",
      "Gradient Descent(356/9): loss=15.638431167811147, w0=73.48623486818161, w1=12.070435268124271\n",
      "[-1.49246281 -2.78286891]\n",
      "Gradient Descent(357/9): loss=16.397411054581873, w0=74.53095883306834, w1=14.018443501910623\n",
      "[ 1.04315765 -0.58046442]\n",
      "Gradient Descent(358/9): loss=16.296133510642306, w0=73.80074847686839, w1=14.424768594715454\n",
      "[-2.46847393  1.12431798]\n",
      "Gradient Descent(359/9): loss=15.96088997910826, w0=75.52868022513816, w1=13.637746007440915\n",
      "[ 1.45292111  1.06774997]\n",
      "Gradient Descent(360/9): loss=17.895447331547093, w0=74.51163544927479, w1=12.890321027070181\n",
      "[ 0.79823763  0.41782643]\n",
      "Gradient Descent(361/9): loss=16.30099200440253, w0=73.95286910487269, w1=12.597842529257749\n",
      "[ 3.06732287 -3.75301958]\n",
      "Gradient Descent(362/9): loss=15.99184077626951, w0=71.80574309536631, w1=15.224956236645435\n",
      "[-3.87083514  3.58205674]\n",
      "Gradient Descent(363/9): loss=18.016164061670683, w0=74.51532769685409, w1=12.717516516981785\n",
      "[ 1.77141586 -0.55155125]\n",
      "Gradient Descent(364/9): loss=16.422275113125387, w0=73.2753365947498, w1=13.10360238895393\n",
      "[ 0.762129    0.09465982]\n",
      "Gradient Descent(365/9): loss=15.456789960876952, w0=72.74184629380188, w1=13.037340512090331\n",
      "[-1.24153722 -1.70674433]\n",
      "Gradient Descent(366/9): loss=15.636128121763253, w0=73.6109223505503, w1=14.232061545477219\n",
      "[ 2.29085245 -0.69027979]\n",
      "Gradient Descent(367/9): loss=15.719147071312731, w0=72.00732563481246, w1=14.715257401719668\n",
      "[ 2.38729034  2.86085842]\n",
      "Gradient Descent(368/9): loss=16.97683865740151, w0=70.33622239338223, w1=12.712656508569438\n",
      "[-1.74711509  1.38975046]\n",
      "Gradient Descent(369/9): loss=20.0540687536771, w0=71.55920295467891, w1=11.739831187550646\n",
      "[-3.23255998 -1.65769362]\n",
      "Gradient Descent(370/9): loss=18.404106333175076, w0=73.82199494063886, w1=12.9002167225171\n",
      "[-1.36826331  1.96863642]\n",
      "Gradient Descent(371/9): loss=15.693226023421879, w0=74.77977925441353, w1=11.52217122932149\n",
      "[ 0.85704013 -0.5854982 ]\n",
      "Gradient Descent(372/9): loss=18.405757541891244, w0=74.1798511603873, w1=11.932019971033903\n",
      "[ 1.5380391  -2.13599509]\n",
      "Gradient Descent(373/9): loss=16.975999087068402, w0=73.1032237926385, w1=13.427216535482682\n",
      "[-0.7011207   0.77764828]\n",
      "Gradient Descent(374/9): loss=15.405448682108796, w0=73.59400828566048, w1=12.88286273968324\n",
      "[ 0.15243943 -2.16721278]\n",
      "Gradient Descent(375/9): loss=15.609028537011731, w0=73.48730068806928, w1=14.399911686791185\n",
      "[ 1.46795102  1.11728034]\n",
      "Gradient Descent(376/9): loss=15.827968858430605, w0=72.45973497466744, w1=13.61781545171124\n",
      "[-0.50132377 -1.52245335]\n",
      "Gradient Descent(377/9): loss=15.743358088815995, w0=72.81066161602794, w1=14.683532795782618\n",
      "[ 0.77052017 -1.06387545]\n",
      "Gradient Descent(378/9): loss=16.227249899735742, w0=72.27129749642803, w1=15.428245612680357\n",
      "[ 2.29587686  1.73492858]\n",
      "Gradient Descent(379/9): loss=17.807159080917017, w0=70.6641836936986, w1=14.213795606547517\n",
      "[-0.39571614  2.49982625]\n",
      "Gradient Descent(380/9): loss=19.113088705562635, w0=70.94118499227689, w1=12.463917228546702\n",
      "[-2.816282   -0.12824361]\n",
      "Gradient Descent(381/9): loss=18.66949353825293, w0=72.91258239162991, w1=12.553687758614075\n",
      "[-1.14489403 -1.41254601]\n",
      "Gradient Descent(382/9): loss=15.887358668715807, w0=73.71400821505009, w1=13.542469966559178\n",
      "[-0.48811206 -1.95797214]\n",
      "Gradient Descent(383/9): loss=15.476093335866981, w0=74.05568665728975, w1=14.913050462396434\n",
      "[ 0.68753618 -0.09261304]\n",
      "Gradient Descent(384/9): loss=16.703259514179678, w0=73.57441133058455, w1=14.977879593323989\n",
      "[ 3.97573477  0.56148484]\n",
      "Gradient Descent(385/9): loss=16.5474774176815, w0=70.79139699483389, w1=14.584840203423248\n",
      "[-4.6567048   1.56324789]\n",
      "Gradient Descent(386/9): loss=19.12785726712069, w0=74.05109035465185, w1=13.49056668202831\n",
      "[ 3.20580312  0.37128333]\n",
      "Gradient Descent(387/9): loss=15.672598733217903, w0=71.8070281698717, w1=13.230668352057577\n",
      "[-3.79559815  2.75407251]\n",
      "Gradient Descent(388/9): loss=16.522325980617985, w0=74.46394687291269, w1=11.302817593680983\n",
      "[ 0.05621633 -1.80053374]\n",
      "Gradient Descent(389/9): loss=18.43980254304029, w0=74.42459544418205, w1=12.563191211284384\n",
      "[ 4.91817282 -0.2663558 ]\n",
      "Gradient Descent(390/9): loss=16.445104661888912, w0=70.98187446794903, w1=12.749640270157096\n",
      "[ 0.59514357 -3.48729905]\n",
      "Gradient Descent(391/9): loss=18.3251724518594, w0=70.56527396722811, w1=15.190749608272542\n",
      "[-2.29005926  2.56480065]\n",
      "Gradient Descent(392/9): loss=20.57247202212767, w0=72.16831544639784, w1=13.395389150840076\n",
      "[-1.54916334  0.10632751]\n",
      "Gradient Descent(393/9): loss=16.02293813607991, w0=73.25272978521015, w1=13.320959892081198\n",
      "[ 1.47947619  1.51683035]\n",
      "Gradient Descent(394/9): loss=15.39933745313562, w0=72.21709645260908, w1=12.259178649659992\n",
      "[-3.14998793  0.09372047]\n",
      "Gradient Descent(395/9): loss=16.71051586141803, w0=74.42208800158761, w1=12.193574324010639\n",
      "[ 2.30693322  1.37167891]\n",
      "Gradient Descent(396/9): loss=16.849342750279042, w0=72.80723474528408, w1=11.233399090426335\n",
      "[ 2.36408147 -0.77230467]\n",
      "Gradient Descent(397/9): loss=18.02728193278569, w0=71.15237771393203, w1=11.774012358555307\n",
      "[-2.86210117 -1.91373263]\n",
      "Gradient Descent(398/9): loss=19.133700213305982, w0=73.15584853331087, w1=13.113625198168242\n",
      "[ 0.32568085 -2.62666669]\n",
      "Gradient Descent(399/9): loss=15.462429942703393, w0=72.92787193780948, w1=14.952291878858961\n",
      "[-1.05728935  1.85709531]\n",
      "Gradient Descent(400/9): loss=16.537129302868955, w0=73.66797448069003, w1=13.652325162049678\n",
      "[ 1.8980337   0.19829091]\n",
      "Gradient Descent(401/9): loss=15.470743073968787, w0=72.33935089350709, w1=13.51352152521748\n",
      "[ 0.39145094  1.50401284]\n",
      "Gradient Descent(402/9): loss=15.842062396805542, w0=72.06533523383463, w1=12.460712533857711\n",
      "[-0.89517217  0.58530738]\n",
      "Gradient Descent(403/9): loss=16.659780991666985, w0=72.69195575321426, w1=12.05099736549945\n",
      "[-2.05357701 -1.29935104]\n",
      "Gradient Descent(404/9): loss=16.587682926124643, w0=74.12945965899262, w1=12.960543095331685\n",
      "[-0.78720585 -0.56132151]\n",
      "Gradient Descent(405/9): loss=15.869717858487997, w0=74.68050375149623, w1=13.35346815204376\n",
      "[ 5.36900229 -4.67294898]\n",
      "Gradient Descent(406/9): loss=16.355161152189744, w0=70.92220214703154, w1=16.624532434752396\n",
      "[-3.1586292   5.90613446]\n",
      "Gradient Descent(407/9): loss=23.143361819760468, w0=73.13324258411325, w1=12.490238315876464\n",
      "[ 0.01261727 -1.37552763]\n",
      "Gradient Descent(408/9): loss=15.888326322709327, w0=73.12441049599151, w1=13.45310765815805\n",
      "[ 1.13018217  0.07397177]\n",
      "Gradient Descent(409/9): loss=15.400608851256978, w0=72.3332829804392, w1=13.40132742085788\n",
      "[ 0.1436512   1.21229521]\n",
      "Gradient Descent(410/9): loss=15.850373639023273, w0=72.23272714288578, w1=12.552720772547076\n",
      "[-1.12401808  0.44141352]\n",
      "Gradient Descent(411/9): loss=16.378611904564718, w0=73.01953979703998, w1=12.243731311706691\n",
      "[ 2.82737918  0.46168496]\n",
      "Gradient Descent(412/9): loss=16.187355334612782, w0=71.04037436894188, w1=11.92055183839235\n",
      "[-2.46853282 -0.64895617]\n",
      "Gradient Descent(413/9): loss=19.140617219287254, w0=72.76834734597331, w1=12.374821158346666\n",
      "[ 1.21241631  0.24671104]\n",
      "Gradient Descent(414/9): loss=16.13439459501369, w0=71.91965593080096, w1=12.20212343246281\n",
      "[-0.49823319 -2.78205145]\n",
      "Gradient Descent(415/9): loss=17.14630831588638, w0=72.26841916550524, w1=14.149559446587286\n",
      "[-0.45007011  0.41887004]\n",
      "Gradient Descent(416/9): loss=16.136063412240222, w0=72.5834682424378, w1=13.856350420532696\n",
      "[ 0.89419544  2.83295515]\n",
      "Gradient Descent(417/9): loss=15.709188227219354, w0=71.95753143276481, w1=11.873281816966566\n",
      "[-2.4276187  -0.85247227]\n",
      "Gradient Descent(418/9): loss=17.569167411000397, w0=73.65686452262398, w1=12.4700124037091\n",
      "[ 0.33086499 -0.64478029]\n",
      "Gradient Descent(419/9): loss=15.961498582013032, w0=73.42525902783578, w1=12.92135860941106\n",
      "[-1.96212357  7.207142  ]\n",
      "Gradient Descent(420/9): loss=15.550392073262064, w0=74.7987455254647, w1=7.876359212448933\n",
      "[ 1.36712817 -3.64757438]\n",
      "Gradient Descent(421/9): loss=32.216918455332824, w0=73.84175580418153, w1=10.429661281022243\n",
      "[ 4.08184366 -2.90265817]\n",
      "Gradient Descent(422/9): loss=20.18735482708523, w0=70.98446524323798, w1=12.46152200031852\n",
      "[-6.55368261  4.50646422]\n",
      "Gradient Descent(423/9): loss=18.5710390099954, w0=75.57204306680998, w1=9.306997047132395\n",
      "[ 3.68551422 -3.27342235]\n",
      "Gradient Descent(424/9): loss=26.68658251558312, w0=72.99218311232266, w1=11.598392691891814\n",
      "[ 2.25200382 -1.30346007]\n",
      "Gradient Descent(425/9): loss=17.201093035516717, w0=71.41578043583252, w1=12.510814742015135\n",
      "[-2.35451213 -2.44300052]\n",
      "Gradient Descent(426/9): loss=17.618977110035072, w0=73.06393892535901, w1=14.22091510561821\n",
      "[ 4.68015278 -0.81675173]\n",
      "Gradient Descent(427/9): loss=15.687024676098119, w0=69.78783197736632, w1=14.792641316331643\n",
      "[-7.12838479  0.91306522]\n",
      "Gradient Descent(428/9): loss=22.394112623347855, w0=74.77770133086791, w1=14.153495664280529\n",
      "[ 3.58262469 -0.95927173]\n",
      "Gradient Descent(429/9): loss=16.713680337098403, w0=72.26986404514614, w1=14.824985877631203\n",
      "[-1.26475272  2.14655101]\n",
      "Gradient Descent(430/9): loss=16.815115536174115, w0=73.15519095197035, w1=13.322400171150301\n",
      "[-2.64549939  0.35601801]\n",
      "Gradient Descent(431/9): loss=15.407884595142194, w0=75.00704052478856, w1=13.073187565383105\n",
      "[-0.08983648  0.93743107]\n",
      "Gradient Descent(432/9): loss=16.9359066400139, w0=75.06992606220956, w1=12.416985814123903\n",
      "[ 1.54714057 -2.05334218]\n",
      "Gradient Descent(433/9): loss=17.52767701493073, w0=73.98692766191301, w1=13.854325341943712\n",
      "[-2.60006506 -0.51950521]\n",
      "Gradient Descent(434/9): loss=15.69618370612075, w0=75.80697320097916, w1=14.21797899085153\n",
      "[ 5.08496059  2.33147796]\n",
      "Gradient Descent(435/9): loss=18.816119786662863, w0=72.2475007875854, w1=12.585944421181463\n",
      "[-4.47276559  0.5883083 ]\n",
      "Gradient Descent(436/9): loss=16.332797179180723, w0=75.37843670135473, w1=12.174128608020895\n",
      "[ 1.67826654 -0.65660194]\n",
      "Gradient Descent(437/9): loss=18.410763199143506, w0=74.20365012299722, w1=12.63374996726586\n",
      "[ 2.83431602 -0.35231954]\n",
      "Gradient Descent(438/9): loss=16.157516744198425, w0=72.21962891021397, w1=12.880373643198185\n",
      "[ 1.07878608 -0.02979561]\n",
      "Gradient Descent(439/9): loss=16.14254418614466, w0=71.46447865538897, w1=12.90123057268733\n",
      "[ 1.06136459 -2.11608015]\n",
      "Gradient Descent(440/9): loss=17.226639980757547, w0=70.72152344341339, w1=14.382486678345472\n",
      "[-3.81913873  0.09234433]\n",
      "Gradient Descent(441/9): loss=19.102005708443112, w0=73.39492055709974, w1=14.31784564828394\n",
      "[ 2.54065014  0.64606711]\n",
      "Gradient Descent(442/9): loss=15.742221864498905, w0=71.6164654565868, w1=13.86559867256478\n",
      "[  1.19932196e+00  -2.34944528e-04]\n",
      "Gradient Descent(443/9): loss=16.86727219405583, w0=70.77694008655251, w1=13.865763133734164\n",
      "[ -5.68358536e+00   5.53670937e-04]\n",
      "Gradient Descent(444/9): loss=18.628004421439808, w0=74.7554498367191, w1=13.865375564078118\n",
      "[-1.13880525  0.34587322]\n",
      "Gradient Descent(445/9): loss=16.52828769907439, w0=75.55261351050596, w1=13.623264311521575\n",
      "[ 3.71627085 -2.72777915]\n",
      "Gradient Descent(446/9): loss=17.947035104518285, w0=72.95122391782184, w1=15.532709713138487\n",
      "[-0.14363435 -0.18356004]\n",
      "Gradient Descent(447/9): loss=17.552007769359644, w0=73.0517679635293, w1=15.661201738257978\n",
      "[ 4.55716645  6.0955726 ]\n",
      "Gradient Descent(448/9): loss=17.794654948167086, w0=69.86175144602794, w1=11.394300921287641\n",
      "[-4.9316732   0.42956277]\n",
      "Gradient Descent(449/9): loss=23.450255822570405, w0=73.31392268844134, w1=11.093606981108131\n",
      "[-2.55991052 -4.11043991]\n",
      "Gradient Descent(450/9): loss=18.232837501076485, w0=75.10586005129113, w1=13.970914920566404\n",
      "[ 3.52915878 -0.11530012]\n",
      "Gradient Descent(451/9): loss=17.148087556791964, w0=72.63544890841585, w1=14.051625004466203\n",
      "[ 0.03767479  1.36642807]\n",
      "Gradient Descent(452/9): loss=15.766223269948785, w0=72.60907655506975, w1=13.095125356163404\n",
      "[-0.64513626  4.58248497]\n",
      "Gradient Descent(453/9): loss=15.694348122591808, w0=73.06067194001106, w1=9.88738587609243\n",
      "[ 1.22925994 -6.22323243]\n",
      "Gradient Descent(454/9): loss=21.865495717439863, w0=72.20018998050801, w1=14.243648577641238\n",
      "[-0.41254035 -1.0619154 ]\n",
      "Gradient Descent(455/9): loss=16.275811951388075, w0=72.48896822321883, w1=14.986989357858777\n",
      "[-1.42331114  0.45162078]\n",
      "Gradient Descent(456/9): loss=16.845805023008936, w0=73.4852860217544, w1=14.67085480968092\n",
      "[-0.21306386  0.11003437]\n",
      "Gradient Descent(457/9): loss=16.113608041230847, w0=73.6344307260422, w1=14.593830751300981\n",
      "[ 0.21334851  3.96683128]\n",
      "Gradient Descent(458/9): loss=16.06449077573887, w0=73.4850867720858, w1=11.81704885182147\n",
      "[ 0.78855315 -2.90629402]\n",
      "Gradient Descent(459/9): loss=16.786384948866097, w0=72.93309956922315, w1=13.851454662805102\n",
      "[ 0.63777314 -0.36964874]\n",
      "Gradient Descent(460/9): loss=15.520080424835681, w0=72.48665837216902, w1=14.11020878421845\n",
      "[-6.48422225  0.75447186]\n",
      "Gradient Descent(461/9): loss=15.910487976134064, w0=77.02561394626949, w1=13.582078479055024\n",
      "[ 3.56335379  0.88578641]\n",
      "Gradient Descent(462/9): loss=22.353889655388617, w0=74.53126629565895, w1=12.962027993716717\n",
      "[ 1.80045534 -2.26654146]\n",
      "Gradient Descent(463/9): loss=16.285396909592155, w0=73.27094755620695, w1=14.548607017214769\n",
      "[-1.39526452 -0.65116998]\n",
      "Gradient Descent(464/9): loss=15.957419595367316, w0=74.24763271764205, w1=15.004426001413593\n",
      "[-1.10257421  2.19495189]\n",
      "Gradient Descent(465/9): loss=17.003045663113838, w0=75.01943466141948, w1=13.467959678494852\n",
      "[ 0.62945456 -0.05789998]\n",
      "Gradient Descent(466/9): loss=16.874653901198947, w0=74.57881647251624, w1=13.50848966582658\n",
      "[ 0.92897196  0.80870859]\n",
      "Gradient Descent(467/9): loss=16.211778833383182, w0=73.92853610225909, w1=12.942393650010274\n",
      "[ 3.51600085 -0.29760227]\n",
      "Gradient Descent(468/9): loss=15.731611135232002, w0=71.46733550475982, w1=13.150715239940332\n",
      "[-1.99734662 -2.34946474]\n",
      "Gradient Descent(469/9): loss=17.10821656214658, w0=72.86547814043216, w1=14.7953405595429\n",
      "[ 0.73502492  1.17879811]\n",
      "Gradient Descent(470/9): loss=16.34310862119059, w0=72.35096069607555, w1=13.970181881358696\n",
      "[ 1.05283289 -0.18397515]\n",
      "Gradient Descent(471/9): loss=15.950756020075042, w0=71.61397767563116, w1=14.098964483615488\n",
      "[-0.19526962  0.46607045]\n",
      "Gradient Descent(472/9): loss=16.98873088871956, w0=71.75066641107527, w1=13.772715172001888\n",
      "[ 1.20151898 -0.87541051]\n",
      "Gradient Descent(473/9): loss=16.619632080400457, w0=70.90960312159406, w1=14.385502529089782\n",
      "[-2.5275301   0.33285703]\n",
      "Gradient Descent(474/9): loss=18.638603978095833, w0=72.67887418968789, w1=14.152502608378397\n",
      "[-1.6038215  -0.61573901]\n",
      "Gradient Descent(475/9): loss=15.801353083313687, w0=73.80154924076734, w1=14.583519913759767\n",
      "[-2.31878046 -2.04201352]\n",
      "Gradient Descent(476/9): loss=16.123926050640367, w0=75.42469555951773, w1=16.012929374778295\n",
      "[-0.54479079  0.71268592]\n",
      "Gradient Descent(477/9): loss=20.864579877331234, w0=75.80604911390996, w1=15.514049229419706\n",
      "[ 2.14821797  0.41502921]\n",
      "Gradient Descent(478/9): loss=20.610542278348895, w0=74.30229653391804, w1=15.223528779228394\n",
      "[ 3.21324909  4.10957982]\n",
      "Gradient Descent(479/9): loss=17.414745188251928, w0=72.05302217203872, w1=12.346822904111344\n",
      "[ 1.35185947 -1.31748437]\n",
      "Gradient Descent(480/9): loss=16.797523407545043, w0=71.10672054505343, w1=13.269061961820366\n",
      "[-4.02450247  4.58192747]\n",
      "Gradient Descent(481/9): loss=17.799999786617153, w0=73.92387227106339, w1=10.061712731076172\n",
      "[ 1.79441672 -5.00529784]\n",
      "Gradient Descent(482/9): loss=21.425667527483913, w0=72.66778056731457, w1=13.565421221289876\n",
      "[-2.13847297  2.93911422]\n",
      "Gradient Descent(483/9): loss=15.58558741503481, w0=74.16471164459898, w1=11.508041268274052\n",
      "[ 0.81210848 -2.57696013]\n",
      "Gradient Descent(484/9): loss=17.70876876439427, w0=73.59623570828096, w1=13.311913358868429\n",
      "[ 0.55523775  0.13936868]\n",
      "Gradient Descent(485/9): loss=15.445662922273733, w0=73.20756928546784, w1=13.214355282430027\n",
      "[ 0.36907958 -0.4322542 ]\n",
      "Gradient Descent(486/9): loss=15.42482347387184, w0=72.94921358005902, w1=13.516933225577294\n",
      "[-1.8096007   0.45492308]\n",
      "Gradient Descent(487/9): loss=15.445992510570187, w0=74.21593406985376, w1=13.198487069998853\n",
      "[-0.68346485  0.55661129]\n",
      "Gradient Descent(488/9): loss=15.850484848323331, w0=74.69435946298714, w1=12.808859170438243\n",
      "[ 1.66139252 -1.96859494]\n",
      "Gradient Descent(489/9): loss=16.591522461029374, w0=73.53138469895275, w1=14.186875627740696\n",
      "[ 1.4179035  -0.70055948]\n",
      "Gradient Descent(490/9): loss=15.664122025617809, w0=72.53885224985704, w1=14.67726726189734\n",
      "[ 0.27791747  0.47949917]\n",
      "Gradient Descent(491/9): loss=16.38802181593511, w0=72.34431002348052, w1=14.34161784253789\n",
      "[-0.89570639  1.22880852]\n",
      "Gradient Descent(492/9): loss=16.2082097895841, w0=72.97130449464152, w1=13.481451879547102\n",
      "[ 0.1628043   0.58683073]\n",
      "Gradient Descent(493/9): loss=15.437930409724123, w0=72.8573414825148, w1=13.070670369469232\n",
      "[-0.88706651 -1.40859849]\n",
      "Gradient Descent(494/9): loss=15.564846849554666, w0=73.47828804200734, w1=14.056689310038768\n",
      "[ 0.33757835  2.6509697 ]\n",
      "Gradient Descent(495/9): loss=15.569334444335068, w0=73.24198319650066, w1=12.201010518254023\n",
      "[-0.0647702  -1.59826763]\n",
      "Gradient Descent(496/9): loss=16.204775984524126, w0=73.28732233413976, w1=13.319797859192676\n",
      "[-2.89252246 -1.24577464]\n",
      "Gradient Descent(497/9): loss=15.398695982414093, w0=75.31208805705043, w1=14.191840104918674\n",
      "[ 2.41994344 -0.55356099]\n",
      "Gradient Descent(498/9): loss=17.675947890635715, w0=73.61812764681241, w1=14.579332799582192\n",
      "[ 0.47553831  2.10122331]\n",
      "Gradient Descent(499/9): loss=16.04302499197339, w0=73.28525083152448, w1=13.108476480970337\n",
      "[-0.73475576 -0.72490186]\n",
      "Gradient Descent(500/9): loss=15.454833530207113, w0=73.79957986263352, w1=13.615907780743099\n",
      "[ 1.74214559  1.63692805]\n",
      "Gradient Descent(501/9): loss=15.523007390888978, w0=72.58007795143989, w1=12.470058144290673\n",
      "[ 0.05651808 -1.91842498]\n",
      "Gradient Descent(502/9): loss=16.15037542652734, w0=72.5405152940684, w1=13.812955632519852\n",
      "[ 0.72589247 -0.6535862 ]\n",
      "Gradient Descent(503/9): loss=15.725224217037095, w0=72.03239056225644, w1=14.27046597207349\n",
      "[-2.71771844  0.60920045]\n",
      "Gradient Descent(504/9): loss=16.494264233898612, w0=73.9347934688583, w1=13.844025660442806\n",
      "[-0.34093903 -1.17921846]\n",
      "Gradient Descent(505/9): loss=15.657608050398805, w0=74.17345078892004, w1=14.669478584933946\n",
      "[ 1.21687795  0.62963301]\n",
      "Gradient Descent(506/9): loss=16.48044505802475, w0=73.3216362232713, w1=14.228735474490419\n",
      "[ 0.47162828  0.60537906]\n",
      "Gradient Descent(507/9): loss=15.666789664708759, w0=72.99149642820075, w1=13.804970135267071\n",
      "[ 0.98718535 -1.79569122]\n",
      "Gradient Descent(508/9): loss=15.484514768500189, w0=72.3004666852296, w1=15.061953987652831\n",
      "[-3.94638579 -1.48814393]\n",
      "Gradient Descent(509/9): loss=17.131108767631545, w0=75.06293674027643, w1=16.103654739759865\n",
      "[ 3.37586584  2.45597419]\n",
      "Gradient Descent(510/9): loss=20.39313105014591, w0=72.69983065535345, w1=14.384472809050408\n",
      "[-2.41130025  1.19604323]\n",
      "Gradient Descent(511/9): loss=15.971655800207873, w0=74.38774083301763, w1=13.547242546029006\n",
      "[ 1.609326    0.09134397]\n",
      "Gradient Descent(512/9): loss=15.986387844207266, w0=73.26121263378106, w1=13.483301767064722\n",
      "[-0.76846168  2.56127373]\n",
      "Gradient Descent(513/9): loss=15.386429261869855, w0=73.79913580950418, w1=11.69041015471128\n",
      "[ 1.4215407 -3.7834937]\n",
      "Gradient Descent(514/9): loss=17.114309689526305, w0=72.80405731705768, w1=14.338855745973913\n",
      "[ 0.14086423 -1.35994888]\n",
      "Gradient Descent(515/9): loss=15.874935188062766, w0=72.70545235711208, w1=15.290819959485672\n",
      "[-1.0363801   2.39475133]\n",
      "Gradient Descent(516/9): loss=17.199091363012702, w0=73.43091842800716, w1=13.614494026558955\n",
      "[ 0.57295162 -0.4058308 ]\n",
      "Gradient Descent(517/9): loss=15.404354917897415, w0=73.02985229670028, w1=13.898575584840062\n",
      "[-0.54585211  0.5996186 ]\n",
      "Gradient Descent(518/9): loss=15.508477442637272, w0=73.41194877080592, w1=13.478842563964127\n",
      "[ 1.3058759   3.31602574]\n",
      "Gradient Descent(519/9): loss=15.392853406232168, w0=72.49783563825268, w1=11.157624546674342\n",
      "[-2.73294549 -0.95551123]\n",
      "Gradient Descent(520/9): loss=18.398810698714282, w0=74.41089747961972, w1=11.826482409297755\n",
      "[ 1.6466149  -0.01692655]\n",
      "Gradient Descent(521/9): loss=17.37628973643743, w0=73.25826704826699, w1=11.83833099518546\n",
      "[ 2.69457877 -1.7179978 ]\n",
      "Gradient Descent(522/9): loss=16.733590022161852, w0=71.37206191127527, w1=13.040929454897771\n",
      "[ 1.12073755  2.0692923 ]\n",
      "Gradient Descent(523/9): loss=17.32892622500069, w0=70.58754562606657, w1=11.59242484557662\n",
      "[-2.83199795 -2.40489486]\n",
      "Gradient Descent(524/9): loss=20.829051635794446, w0=72.5699441939676, w1=13.275851250997803\n",
      "[ 1.86693081 -1.85917553]\n",
      "Gradient Descent(525/9): loss=15.668739493336409, w0=71.2630926293934, w1=14.577274123485793\n",
      "[ 0.98174118  3.0569128 ]\n",
      "Gradient Descent(526/9): loss=18.0503426693918, w0=70.57587380096588, w1=12.437435160147306\n",
      "[-1.79934574 -2.51258691]\n",
      "Gradient Descent(527/9): loss=19.622951839513483, w0=71.83541581936218, w1=14.196245996296671\n",
      "[-4.28461101 -0.69446254]\n",
      "Gradient Descent(528/9): loss=16.70621818361928, w0=74.8346435232813, w1=14.682369771476992\n",
      "[-1.71949844 -0.45768955]\n",
      "Gradient Descent(529/9): loss=17.29599160624115, w0=76.03829243181326, w1=15.00275245654083\n",
      "[ 3.22731707 -1.22497846]\n",
      "Gradient Descent(530/9): loss=20.311497850181652, w0=73.77917048146004, w1=15.860237380306751\n",
      "[-2.56543997  1.39155542]\n",
      "Gradient Descent(531/9): loss=18.337070419827423, w0=75.57497845696389, w1=14.886148589194063\n",
      "[ 1.49699048  0.01058748]\n",
      "Gradient Descent(532/9): loss=18.976528471883164, w0=74.52708511959537, w1=14.878737351824363\n",
      "[ 1.27399969  1.74016047]\n",
      "Gradient Descent(533/9): loss=17.124868864961474, w0=73.63528533928962, w1=13.66062502588709\n",
      "[-1.37394762  0.0819043 ]\n",
      "Gradient Descent(534/9): loss=15.460517015588968, w0=74.59704867036163, w1=13.603292012795869\n",
      "[-2.18725472 -1.26020068]\n",
      "Gradient Descent(535/9): loss=16.24259338161542, w0=76.12812697226552, w1=14.485432487989199\n",
      "[ 2.90506143  1.58275173]\n",
      "Gradient Descent(536/9): loss=19.907983187773468, w0=74.09458397124801, w1=13.377506279749014\n",
      "[-2.24651349 -0.45628466]\n",
      "Gradient Descent(537/9): loss=15.71164071232971, w0=75.6671434166987, w1=13.696905540348318\n",
      "[ 5.15047432 -0.93045351]\n",
      "Gradient Descent(538/9): loss=18.22556423267983, w0=72.06181139503889, w1=14.348222997641539\n",
      "[-1.31600026  0.50173194]\n",
      "Gradient Descent(539/9): loss=16.522091441571522, w0=72.98301157660067, w1=13.997010640943532\n",
      "[-1.29251395  3.43586807]\n",
      "Gradient Descent(540/9): loss=15.568019232114963, w0=73.88777134210021, w1=11.591902991098916\n",
      "[ 1.75677375  0.39859573]\n",
      "Gradient Descent(541/9): loss=17.344128636356047, w0=72.65802971783783, w1=11.312885977007028\n",
      "[ 2.15473618 -0.85433565]\n",
      "Gradient Descent(542/9): loss=17.935635816930233, w0=71.14971439508193, w1=11.910920931727462\n",
      "[-4.12259458 -1.64377851]\n",
      "Gradient Descent(543/9): loss=18.915254390190476, w0=74.03553060150693, w1=13.061565891790098\n",
      "[-0.99092462  1.08988208]\n",
      "Gradient Descent(544/9): loss=15.748302791977325, w0=74.72917783864834, w1=12.298648433610985\n",
      "[ 2.56592909  1.09894755]\n",
      "Gradient Descent(545/9): loss=17.113323614670573, w0=72.93302747402946, w1=11.52938514681098\n",
      "[ 0.00778715 -0.01022681]\n",
      "Gradient Descent(546/9): loss=17.35289856453291, w0=72.9275764698535, w1=11.536543914737026\n",
      "[ 0.31876286 -3.00137335]\n",
      "Gradient Descent(547/9): loss=17.340944342379, w0=72.70444246747762, w1=13.637505259632462\n",
      "[ 0.20302252 -0.59461492]\n",
      "Gradient Descent(548/9): loss=15.572080217456241, w0=72.56232670262855, w1=14.053735700133423\n",
      "[-1.6855096   1.11475808]\n",
      "Gradient Descent(549/9): loss=15.81825506440106, w0=73.74218342077491, w1=13.27340504740175\n",
      "[ 0.53896041  0.00268428]\n",
      "Gradient Descent(550/9): loss=15.507638387649845, w0=73.36491113441879, w1=13.271526054563232\n",
      "[ 0.55858693 -2.65080468]\n",
      "Gradient Descent(551/9): loss=15.410078381780123, w0=72.97390028176859, w1=15.127089328962262\n",
      "[ 0.82066855  1.44504303]\n",
      "Gradient Descent(552/9): loss=16.79402013497141, w0=72.39943229833641, w1=14.11555920887688\n",
      "[-1.13833954  0.66017612]\n",
      "Gradient Descent(553/9): loss=15.988094343835366, w0=73.19626997514108, w1=13.653435924844322\n",
      "[ 2.69741037 -0.37022672]\n",
      "Gradient Descent(554/9): loss=15.405745753478246, w0=71.30808271844914, w1=13.912594629356095\n",
      "[-1.452924   -0.56553514]\n",
      "Gradient Descent(555/9): loss=17.451360196185195, w0=72.32512951707976, w1=14.308469228091123\n",
      "[-0.35210335  1.47751988]\n",
      "Gradient Descent(556/9): loss=16.198586219406696, w0=72.57160186020786, w1=13.274205309237992\n",
      "[-1.41372895 -0.9458818 ]\n",
      "Gradient Descent(557/9): loss=15.667877651891915, w0=73.56121212233033, w1=13.936322568224362\n",
      "[-0.56937432 -0.84912557]\n",
      "Gradient Descent(558/9): loss=15.525856279900973, w0=73.9597741462207, w1=14.530710469059331\n",
      "[-0.60186465  1.84067731]\n",
      "Gradient Descent(559/9): loss=16.159865841550808, w0=74.38107940225814, w1=13.242236349770815\n",
      "[ 0.7852658   1.00023816]\n",
      "Gradient Descent(560/9): loss=16.005040920708346, w0=73.83139334176069, w1=12.542069637841173\n",
      "[-1.72056267 -0.21730945]\n",
      "Gradient Descent(561/9): loss=15.969912596826584, w0=75.03578721390618, w1=12.694186253276342\n",
      "[ 2.04567727 -0.90072252]\n",
      "Gradient Descent(562/9): loss=17.211460767948722, w0=73.60381312411513, w1=13.32469201459344\n",
      "[ 0.35546049  0.44628436]\n",
      "Gradient Descent(563/9): loss=15.445919787949506, w0=73.3549907805559, w1=13.01229296305694\n",
      "[ 1.88269362  0.71084843]\n",
      "Gradient Descent(564/9): loss=15.49699304805078, w0=72.03710524340097, w1=12.514699064373376\n",
      "[ 0.75225551 -1.47543   ]\n",
      "Gradient Descent(565/9): loss=16.641307454042796, w0=71.51052638816672, w1=13.547500066835438\n",
      "[-1.29587224  2.05200196]\n",
      "Gradient Descent(566/9): loss=16.978435408252547, w0=72.41763695501393, w1=12.111098692467653\n",
      "[ 0.58454441 -2.32083304]\n",
      "Gradient Descent(567/9): loss=16.70637739881647, w0=72.00845586831626, w1=13.735681819927791\n",
      "[-0.01903222  0.51913641]\n",
      "Gradient Descent(568/9): loss=16.24485962240149, w0=72.02177842526856, w1=13.372286332602986\n",
      "[-0.79706209 -1.7485284 ]\n",
      "Gradient Descent(569/9): loss=16.200832692609673, w0=72.57972188949043, w1=14.596256211947622\n",
      "[ 0.16890347  1.0412081 ]\n",
      "Gradient Descent(570/9): loss=16.264263772191327, w0=72.46148945818804, w1=13.867410543193236\n",
      "[ 1.99855075  1.41704063]\n",
      "Gradient Descent(571/9): loss=15.807514750468144, w0=71.06250393246044, w1=12.875482102330185\n",
      "[-2.56978576  1.21733501]\n",
      "Gradient Descent(572/9): loss=18.05804831705046, w0=72.86135396229348, w1=12.02334759276827\n",
      "[ 0.18848492  0.33330449]\n",
      "Gradient Descent(573/9): loss=16.53994470019105, w0=72.72941451657793, w1=11.790034451749777\n",
      "[ 0.04652339 -3.16862228]\n",
      "Gradient Descent(574/9): loss=16.97272806295932, w0=72.69684814675732, w1=14.008070051045117\n",
      "[ 3.10193684 -0.52188767]\n",
      "Gradient Descent(575/9): loss=15.703717348421609, w0=70.52549235912801, w1=14.373391422410984\n",
      "[-2.55293331  3.0295812 ]\n",
      "Gradient Descent(576/9): loss=19.617320279166528, w0=72.31254567849221, w1=12.25268458243319\n",
      "[-3.08632026 -2.85250248]\n",
      "Gradient Descent(577/9): loss=16.62023628857738, w0=74.47296985873832, w1=14.249436316787632\n",
      "[-2.12892624  4.73179627]\n",
      "Gradient Descent(578/9): loss=16.377202220050535, w0=75.9632182301387, w1=10.937178926265256\n",
      "[ 2.41733044 -2.66220161]\n",
      "Gradient Descent(579/9): loss=22.180697366818013, w0=74.27108692020138, w1=12.800720051939624\n",
      "[ 0.28867125 -0.16567453]\n",
      "Gradient Descent(580/9): loss=16.09382883552794, w0=74.06901704539702, w1=12.916692223908635\n",
      "[ 1.93238965 -1.84625753]\n",
      "Gradient Descent(581/9): loss=15.844769910939695, w0=72.71634429023217, w1=14.209072492053583\n",
      "[-0.82809305 -0.10756422]\n",
      "Gradient Descent(582/9): loss=15.818668921876224, w0=73.29600942514949, w1=14.284367445088312\n",
      "[-0.58084862 -0.11439089]\n",
      "Gradient Descent(583/9): loss=15.709624890135805, w0=73.70260345878394, w1=14.3644410679233\n",
      "[-2.33352778  0.45581852]\n",
      "Gradient Descent(584/9): loss=15.860770512312788, w0=75.33607290231176, w1=14.045368106587533\n",
      "[ 3.73819828 -2.45040262]\n",
      "Gradient Descent(585/9): loss=17.63106118784241, w0=72.71933410480554, w1=15.760649939969454\n",
      "[ 0.83186274  4.05942741]\n",
      "Gradient Descent(586/9): loss=18.15230144550414, w0=72.13703019029481, w1=12.919050750539835\n",
      "[-6.35010857  3.4265951 ]\n",
      "Gradient Descent(587/9): loss=16.212257963151064, w0=76.58210619213419, w1=10.52043418190756\n",
      "[ 6.16942256 -8.54478879]\n",
      "Gradient Descent(588/9): loss=25.170629392188264, w0=72.26351039984911, w1=16.50178633143306\n",
      "[ 0.63171927  4.08594901]\n",
      "Gradient Descent(589/9): loss=20.48322722164552, w0=71.82130690811731, w1=13.641622022631413\n",
      "[-3.16529889 -0.80716669]\n",
      "Gradient Descent(590/9): loss=16.48329283363513, w0=74.03701613105815, w1=14.206638706134202\n",
      "[-3.1300495  -0.21655431]\n",
      "Gradient Descent(591/9): loss=15.92619321291208, w0=76.22805078222646, w1=14.358226723465327\n",
      "[ 3.18176567  1.07865504]\n",
      "Gradient Descent(592/9): loss=20.076337395525858, w0=74.00081481434756, w1=13.603168198850273\n",
      "[ 1.33887047 -1.56301147]\n",
      "Gradient Descent(593/9): loss=15.643357255644645, w0=73.06360548301764, w1=14.69727622987396\n",
      "[-0.17369338 -1.24572146]\n",
      "Gradient Descent(594/9): loss=16.153641515618975, w0=73.1851908487184, w1=15.569281250246563\n",
      "[ 1.97198427  4.10678833]\n",
      "Gradient Descent(595/9): loss=17.574948017536155, w0=71.80480185951707, w1=12.694529421302798\n",
      "[-2.0782749   0.45069693]\n",
      "Gradient Descent(596/9): loss=16.80288345085094, w0=73.25959428911469, w1=12.379041572632431\n",
      "[-0.04731225 -3.55751559]\n",
      "Gradient Descent(597/9): loss=15.992215238389408, w0=73.29271286755746, w1=14.869302483117208\n",
      "[ 1.9071977   1.86337131]\n",
      "Gradient Descent(598/9): loss=16.351368850760988, w0=71.95767447997936, w1=13.56494256842814\n",
      "[-2.75410782  0.75840435]\n",
      "Gradient Descent(599/9): loss=16.282298676846132, w0=73.88554995150486, w1=13.034059520002664\n",
      "[-0.88650771  0.1405749 ]\n",
      "Gradient Descent(600/9): loss=15.660202944402757, w0=74.5061053468533, w1=12.93565709045709\n",
      "[-0.97575278  1.39397178]\n",
      "Gradient Descent(601/9): loss=16.26858020842866, w0=75.18913229034821, w1=11.959876847685189\n",
      "[ 1.00873872 -1.46529339]\n",
      "Gradient Descent(602/9): loss=18.336748993378137, w0=74.48301518402795, w1=12.985582219731148\n",
      "[ 0.68330588 -2.47853958]\n",
      "Gradient Descent(603/9): loss=16.214941501292405, w0=74.0047010669079, w1=14.72055992361071\n",
      "[-1.26193968 -0.7694313 ]\n",
      "Gradient Descent(604/9): loss=16.408342553319645, w0=74.8880588405181, w1=15.25916183282108\n",
      "[-0.53686746 -0.34096009]\n",
      "Gradient Descent(605/9): loss=18.23974407834434, w0=75.26386606281676, w1=15.497833896014914\n",
      "[ 1.14229221 -1.01844407]\n",
      "Gradient Descent(606/9): loss=19.362634785722378, w0=74.46426151251356, w1=16.210744747219884\n",
      "[-2.49725259 -0.18406838]\n",
      "Gradient Descent(607/9): loss=19.800003898865302, w0=76.21233832208702, w1=16.339592615054062\n",
      "[ 1.69454739  2.50747036]\n",
      "Gradient Descent(608/9): loss=23.733922099361887, w0=75.02615514922456, w1=14.58436336233079\n",
      "[ 1.65052743  1.29718779]\n",
      "Gradient Descent(609/9): loss=17.49633054245743, w0=73.8707859455959, w1=13.67633190663313\n",
      "[ 0.86448189 -1.58546065]\n",
      "Gradient Descent(610/9): loss=15.571603481794027, w0=73.2656486215429, w1=14.786154361138028\n",
      "[-0.8417765   0.33698501]\n",
      "Gradient Descent(611/9): loss=16.239682814053538, w0=73.8548921698864, w1=14.55026485664775\n",
      "[-0.52917677  1.25641141]\n",
      "Gradient Descent(612/9): loss=16.116272877159293, w0=74.22531590655443, w1=13.670776869429591\n",
      "[-1.68250878 -2.1978665 ]\n",
      "Gradient Descent(613/9): loss=15.83788798050604, w0=75.4030720507289, w1=15.209283418923759\n",
      "[ 3.92670824  0.29642262]\n",
      "Gradient Descent(614/9): loss=19.105852726868523, w0=72.65437628607394, w1=15.001787586637711\n",
      "[-1.90784848  1.01189993]\n",
      "Gradient Descent(615/9): loss=16.74875361390951, w0=73.98987021945898, w1=14.293457638736955\n",
      "[-0.08037276  0.9544262 ]\n",
      "Gradient Descent(616/9): loss=15.95915045775972, w0=74.04613114886142, w1=13.625359296345092\n",
      "[-0.69623247 -0.77148999]\n",
      "Gradient Descent(617/9): loss=15.679403673172702, w0=74.53349387809344, w1=14.165402286290732\n",
      "[ 1.09249775  0.46550971]\n",
      "Gradient Descent(618/9): loss=16.38924237278897, w0=73.76874545108278, w1=13.839545491541791\n",
      "[ 0.73947752  0.51182873]\n",
      "Gradient Descent(619/9): loss=15.563356436972931, w0=73.25111118725778, w1=13.481265381347969\n",
      "[-0.95569848 -1.21593904]\n",
      "Gradient Descent(620/9): loss=15.386805457584542, w0=73.92010011996038, w1=14.332422708812421\n",
      "[ 1.64441149  1.02018172]\n",
      "Gradient Descent(621/9): loss=15.9454947920117, w0=72.76901207823605, w1=13.618295505629677\n",
      "[-0.02377885 -0.5681249 ]\n",
      "Gradient Descent(622/9): loss=15.533255716651647, w0=72.78565727489192, w1=14.015982933913453\n",
      "[ 0.51129676  0.22694646]\n",
      "Gradient Descent(623/9): loss=15.658847409302306, w0=72.42774954614282, w1=13.857120413079246\n",
      "[-1.61976966  2.36046149]\n",
      "Gradient Descent(624/9): loss=15.832233621526404, w0=73.56158830641462, w1=12.204797369313622\n",
      "[-0.42404694  0.03807914]\n",
      "Gradient Descent(625/9): loss=16.234414706403822, w0=73.85842116629308, w1=12.178141973739917\n",
      "[-2.17377754 -2.44057407]\n",
      "Gradient Descent(626/9): loss=16.392260354811953, w0=75.38006544211424, w1=13.886543822367456\n",
      "[ 1.67018744  1.61695792]\n",
      "Gradient Descent(627/9): loss=17.644640983853925, w0=74.21093423093373, w1=12.754673278616597\n",
      "[-1.11434469 -0.30749866]\n",
      "Gradient Descent(628/9): loss=16.069184471876582, w0=74.99097551351228, w1=12.969922341586301\n",
      "[ 2.31390447  0.36112017]\n",
      "Gradient Descent(629/9): loss=16.95582614878476, w0=73.37124238287218, w1=12.717138224443381\n",
      "[-0.62861556 -1.83359013]\n",
      "Gradient Descent(630/9): loss=15.679636802765051, w0=73.81127327692086, w1=14.000651316149524\n",
      "[ 0.73092949  0.09976611]\n",
      "Gradient Descent(631/9): loss=15.655402698558465, w0=73.29962263191369, w1=13.930815040468998\n",
      "[-0.03770634  1.15076817]\n",
      "Gradient Descent(632/9): loss=15.487650897754909, w0=73.32601706905162, w1=13.12527732469911\n",
      "[-2.23613148 -0.52210816]\n",
      "Gradient Descent(633/9): loss=15.449215039193668, w0=74.8913091053926, w1=13.490753036660372\n",
      "[ 0.63520888 -0.44803724]\n",
      "Gradient Descent(634/9): loss=16.661771595146494, w0=74.44666289134682, w1=13.804379103939198\n",
      "[-3.46231135  3.37722252]\n",
      "Gradient Descent(635/9): loss=16.102997870657784, w0=76.87028083441731, w1=11.440323340110126\n",
      "[ 3.15241827  0.52799645]\n",
      "Gradient Descent(636/9): loss=23.860613057713422, w0=74.66358804291922, w1=11.07072582413335\n",
      "[ 0.11940371 -4.49149878]\n",
      "Gradient Descent(637/9): loss=19.225488646150005, w0=74.58000544341364, w1=14.214774972172979\n",
      "[ 3.09430634  2.22123444]\n",
      "Gradient Descent(638/9): loss=16.483051644618932, w0=72.41399100609755, w1=12.659910863828314\n",
      "[ 0.72893781 -1.78302734]\n",
      "Gradient Descent(639/9): loss=16.109064455735705, w0=71.90373453818502, w1=13.908030003974691\n",
      "[-2.69196324  1.41916825]\n",
      "Gradient Descent(640/9): loss=16.443926431200683, w0=73.78810880282735, w1=12.91461223143207\n",
      "[-1.87132936 -0.22840852]\n",
      "Gradient Descent(641/9): loss=15.667667285863473, w0=75.09803935422828, w1=13.074498198456531\n",
      "[ 2.35723143 -0.16929035]\n",
      "Gradient Descent(642/9): loss=17.09540686768953, w0=73.44797735163898, w1=13.193001443735735\n",
      "[ 1.52801891  0.57751131]\n",
      "Gradient Descent(643/9): loss=15.438855990442121, w0=72.37836411298983, w1=12.788743527532834\n",
      "[ 0.30635089  0.43462895]\n",
      "Gradient Descent(644/9): loss=16.043730008525703, w0=72.1639184890291, w1=12.48450326533668\n",
      "[-0.49205826 -3.22623127]\n",
      "Gradient Descent(645/9): loss=16.519562484291626, w0=72.5083592694029, w1=14.742865157613057\n",
      "[ 1.15782244  2.29507149]\n",
      "Gradient Descent(646/9): loss=16.492219672670966, w0=71.6978835601215, w1=13.136315116521288\n",
      "[-0.50820946  0.46482448]\n",
      "Gradient Descent(647/9): loss=16.718518082139692, w0=72.05363018314127, w1=12.810937980798524\n",
      "[-3.48570581 -0.07914978]\n",
      "Gradient Descent(648/9): loss=16.378679402212736, w0=74.49362425207546, w1=12.866342825427763\n",
      "[-0.94047312  2.80765337]\n",
      "Gradient Descent(649/9): loss=16.293641752087954, w0=75.15195543460013, w1=10.900985463280154\n",
      "[ 4.51281042 -6.06448586]\n",
      "Gradient Descent(650/9): loss=20.436948384273325, w0=71.99298813957336, w1=15.146125565134614\n",
      "[-1.05375033  1.58622879]\n",
      "Gradient Descent(651/9): loss=17.620568686331172, w0=72.73061337090705, w1=14.035765409944988\n",
      "[-2.88987837  1.81637882]\n",
      "Gradient Descent(652/9): loss=15.699143631299236, w0=74.7535282329512, w1=12.764300232523176\n",
      "[ 0.73460603 -0.36745493]\n",
      "Gradient Descent(653/9): loss=16.70702035311018, w0=74.23930400870424, w1=13.021518683452797\n",
      "[ 1.01644715 -1.63077074]\n",
      "Gradient Descent(654/9): loss=15.937732195003452, w0=73.52779100040632, w1=14.163058204241082\n",
      "[ 0.07896284  4.50078148]\n",
      "Gradient Descent(655/9): loss=15.646715943189914, w0=73.47251701395356, w1=11.01251116600726\n",
      "[ 1.21066745 -1.52911137]\n",
      "Gradient Descent(656/9): loss=18.445377008790633, w0=72.62504979788238, w1=12.082889124649707\n",
      "[-4.29809572 -1.9308113 ]\n",
      "Gradient Descent(657/9): loss=16.585140561774015, w0=75.63371680201351, w1=13.434457035769697\n",
      "[ 4.58739312  2.11773477]\n",
      "Gradient Descent(658/9): loss=18.124231747247663, w0=72.42254162138029, w1=11.952042694927671\n",
      "[-1.85540494 -2.07824608]\n",
      "Gradient Descent(659/9): loss=16.93242717013513, w0=73.72132507598069, w1=13.406814952020621\n",
      "[ 1.36820793  0.18061835]\n",
      "Gradient Descent(660/9): loss=15.479881584120076, w0=72.76357952216094, w1=13.280382107508633\n",
      "[ 0.33981608 -0.800483  ]\n",
      "Gradient Descent(661/9): loss=15.546385731572835, w0=72.52570826596047, w1=13.840720208190664\n",
      "[-0.90937653 -1.49126072]\n",
      "Gradient Descent(662/9): loss=15.746127347186103, w0=73.16227184014413, w1=14.884602714889267\n",
      "[ 0.0762547   1.84477471]\n",
      "Gradient Descent(663/9): loss=16.381412100680645, w0=73.10889354913998, w1=13.593260414637214\n",
      "[-0.70175553 -0.95367298]\n",
      "Gradient Descent(664/9): loss=15.40945220487384, w0=73.60012242247184, w1=14.260831504114847\n",
      "[-1.48996605  1.90270329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(665/9): loss=15.737840717621735, w0=74.6430986604385, w1=12.928939201015979\n",
      "[-1.1080925  -4.40587402]\n",
      "Gradient Descent(666/9): loss=16.44770227415569, w0=75.41876341242936, w1=16.01305101496301\n",
      "[ 3.81971843  5.51239806]\n",
      "Gradient Descent(667/9): loss=20.852265558735834, w0=72.74496051330115, w1=12.154372372592887\n",
      "[-2.40426225  0.51407261]\n",
      "Gradient Descent(668/9): loss=16.414830367420503, w0=74.42794408879239, w1=11.794521546291094\n",
      "[-1.16863515 -2.19701795]\n",
      "Gradient Descent(669/9): loss=17.448825081051897, w0=75.24598869394792, w1=13.33243410936773\n",
      "[ 3.87591413 -2.97809812]\n",
      "Gradient Descent(670/9): loss=17.302015506129223, w0=72.53284880051004, w1=15.417102795086016\n",
      "[-0.66160921 -0.41530473]\n",
      "Gradient Descent(671/9): loss=17.552244781620875, w0=72.9959752447866, w1=15.70781610397828\n",
      "[-2.39249507  4.65892848]\n",
      "Gradient Descent(672/9): loss=17.91249698380937, w0=74.67072179179135, w1=12.446566167415234\n",
      "[ 4.82308887 -3.25992648]\n",
      "Gradient Descent(673/9): loss=16.867372304370186, w0=71.29455958509233, w1=14.728514706022125\n",
      "[-4.18846047  0.65742503]\n",
      "Gradient Descent(674/9): loss=18.16436646217984, w0=74.22648191159449, w1=14.268317182113137\n",
      "[-1.04903811 -0.225475  ]\n",
      "Gradient Descent(675/9): loss=16.13167058481607, w0=74.96080859120943, w1=14.426149681607727\n",
      "[-0.16238947 -0.44838204]\n",
      "Gradient Descent(676/9): loss=17.22301505019075, w0=75.07448122304618, w1=14.74001711125463\n",
      "[ 2.73998304  2.31381049]\n",
      "Gradient Descent(677/9): loss=17.765267376976833, w0=73.1564930966945, w1=13.120349769675558\n",
      "[-1.76009885 -0.08409703]\n",
      "Gradient Descent(678/9): loss=15.459901983461197, w0=74.38856229127438, w1=13.179217693772582\n",
      "[ 0.61754173 -0.15802022]\n",
      "Gradient Descent(679/9): loss=16.03015509491498, w0=73.9562830770648, w1=13.289831849082006\n",
      "[ 0.88275996 -1.69884893]\n",
      "Gradient Descent(680/9): loss=15.623276284092427, w0=73.33835110591676, w1=14.479026097573055\n",
      "[-3.64388663 -1.5394721 ]\n",
      "Gradient Descent(681/9): loss=15.886188739575681, w0=75.8890717463113, w1=15.556656564702989\n",
      "[ 2.81879052  0.91468644]\n",
      "Gradient Descent(682/9): loss=20.910137425232517, w0=73.91591838197824, w1=14.916376054158258\n",
      "[-0.65939371  1.39442986]\n",
      "Gradient Descent(683/9): loss=16.611328794439178, w0=74.37749397597037, w1=13.94027514893437\n",
      "[-0.64164796  1.40570725]\n",
      "Gradient Descent(684/9): loss=16.07901098684078, w0=74.82664754898369, w1=12.956280075221938\n",
      "[ 0.78423999 -2.21860812]\n",
      "Gradient Descent(685/9): loss=16.697502387482075, w0=74.27767955878709, w1=14.509305762694494\n",
      "[ 3.69171338 -1.58295083]\n",
      "Gradient Descent(686/9): loss=16.39980854422155, w0=71.69348018940664, w1=15.617371344230893\n",
      "[-3.63269291  3.72730723]\n",
      "Gradient Descent(687/9): loss=18.95138767287683, w0=74.2363652245512, w1=13.008256283213122\n",
      "[ 0.94189893 -0.60255883]\n",
      "Gradient Descent(688/9): loss=15.941122934120289, w0=73.57703597352787, w1=13.430047461717614\n",
      "[ 0.30496619 -0.31667161]\n",
      "Gradient Descent(689/9): loss=15.42719793402179, w0=73.3635596392573, w1=13.651717592009737\n",
      "[-2.97611734 -1.89712386]\n",
      "Gradient Descent(690/9): loss=15.403105456104322, w0=75.44684177438876, w1=14.979704293918852\n",
      "[ 3.56680505  3.73673954]\n",
      "Gradient Descent(691/9): loss=18.828407430202006, w0=72.95007824244928, w1=12.363986613331129\n",
      "[-0.64125327 -2.50659299]\n",
      "Gradient Descent(692/9): loss=16.067424188913677, w0=73.39895552954813, w1=14.118601707290033\n",
      "[-1.34299624  2.38156774]\n",
      "Gradient Descent(693/9): loss=15.595493640903593, w0=74.33905289704867, w1=12.451504287594673\n",
      "[ 0.94615402 -0.15349798]\n",
      "Gradient Descent(694/9): loss=16.460643159796174, w0=73.67674508275252, w1=12.558952872382111\n",
      "[-0.34125102  0.38843464]\n",
      "Gradient Descent(695/9): loss=15.883063710433618, w0=73.9156207957444, w1=12.287048623928449\n",
      "[-0.07372625  1.28488774]\n",
      "Gradient Descent(696/9): loss=16.290366046942424, w0=73.96722917176457, w1=11.387627204193882\n",
      "[-0.00948953 -1.08018122]\n",
      "Gradient Descent(697/9): loss=17.8009694476424, w0=73.97387184171725, w1=12.14375405551558\n",
      "[-0.53068582  0.25892125]\n",
      "Gradient Descent(698/9): loss=16.509446156866325, w0=74.345351914839, w1=11.962509181027523\n",
      "[ 1.34497491 -4.52932849]\n",
      "Gradient Descent(699/9): loss=17.089593156440834, w0=73.40386947871198, w1=15.1330391244836\n",
      "[-0.01191455 -0.42264469]\n",
      "Gradient Descent(700/9): loss=16.75867666373301, w0=73.41220966057497, w1=15.428890408401164\n",
      "[ 2.24183181  0.06087024]\n",
      "Gradient Descent(701/9): loss=17.292531239920017, w0=71.8429273949809, w1=15.386281238565848\n",
      "[ 0.70881804  1.43385861]\n",
      "Gradient Descent(702/9): loss=18.25608284516742, w0=71.34675476978138, w1=14.38258021430839\n",
      "[-4.52474904  0.22990444]\n",
      "Gradient Descent(703/9): loss=17.689203097613696, w0=74.51407910108912, w1=14.221647105700828\n",
      "[ 6.458252    4.80555966]\n",
      "Gradient Descent(704/9): loss=16.40551306973194, w0=69.99330269949235, w1=10.857755346418386\n",
      "[-0.42675474 -3.46611577]\n",
      "Gradient Descent(705/9): loss=24.270261246372606, w0=70.29203101525609, w1=13.284036383289807\n",
      "[-1.99028675 -0.76591338]\n",
      "Gradient Descent(706/9): loss=19.910707175896643, w0=71.68523173992135, w1=13.820175751584324\n",
      "[ 0.2441405   2.06351514]\n",
      "Gradient Descent(707/9): loss=16.737787683625484, w0=71.51433339272235, w1=12.375715154918897\n",
      "[-3.43302867 -1.65479865]\n",
      "Gradient Descent(708/9): loss=17.57876067535312, w0=73.9174534601231, w1=13.534074206505544\n",
      "[ 0.56687536  1.15421636]\n",
      "Gradient Descent(709/9): loss=15.581761209499573, w0=73.52064070499208, w1=12.72612275373022\n",
      "[ 0.72414082 -1.51692122]\n",
      "Gradient Descent(710/9): loss=15.695537257798645, w0=73.0137421334866, w1=13.787967606057418\n",
      "[ 0.41122202  0.17416719]\n",
      "Gradient Descent(711/9): loss=15.472648873464157, w0=72.72588671938509, w1=13.666050573891903\n",
      "[-1.1172533   0.85724917]\n",
      "Gradient Descent(712/9): loss=15.564580861041744, w0=73.50796403060686, w1=13.065976152934299\n",
      "[-1.67196129  0.23661836]\n",
      "Gradient Descent(713/9): loss=15.494383719356199, w0=74.67833693281835, w1=12.900343298680532\n",
      "[ 0.35164604  0.79567097]\n",
      "Gradient Descent(714/9): loss=16.512024517073606, w0=74.43218470644852, w1=12.343373616517594\n",
      "[-0.0625486  -3.65488549]\n",
      "Gradient Descent(715/9): loss=16.679341816061452, w0=74.47596872483717, w1=14.901793461256172\n",
      "[ 3.23705804  2.80903092]\n",
      "Gradient Descent(716/9): loss=17.09566231882459, w0=72.21002809643599, w1=12.935471817660106\n",
      "[-0.42752225  0.77172797]\n",
      "Gradient Descent(717/9): loss=16.12139979297811, w0=72.50929367119438, w1=12.39526223876067\n",
      "[ 0.90243401 -1.28577278]\n",
      "Gradient Descent(718/9): loss=16.281724791713224, w0=71.87758986308636, w1=13.29530318817903\n",
      "[-3.98708072 -0.69704707]\n",
      "Gradient Descent(719/9): loss=16.405889617992745, w0=74.66854636853382, w1=13.783236137719472\n",
      "[ 2.57929567  1.29256808]\n",
      "Gradient Descent(720/9): loss=16.376747262278656, w0=72.86303940021452, w1=12.878438483360526\n",
      "[-0.25570362  0.08535071]\n",
      "Gradient Descent(721/9): loss=15.659482959588926, w0=73.04203193092502, w1=12.818692983013793\n",
      "[-2.79573918  0.11899511]\n",
      "Gradient Descent(722/9): loss=15.636085530753807, w0=74.99904935504473, w1=12.735396406350764\n",
      "[ 4.64859843 -3.42393962]\n",
      "Gradient Descent(723/9): loss=17.116620688944643, w0=71.74503045358145, w1=15.132154142637319\n",
      "[ 0.16826902 -0.95484335]\n",
      "Gradient Descent(724/9): loss=17.950702181961304, w0=71.62724214260008, w1=15.800544484701303\n",
      "[ 0.01951696  2.18561855]\n",
      "Gradient Descent(725/9): loss=19.46792944735518, w0=71.613580270245, w1=14.270611498475963\n",
      "[-2.56787923  1.84467333]\n",
      "Gradient Descent(726/9): loss=17.110422701057104, w0=73.41109572834239, w1=12.979340169229888\n",
      "[-1.33644818 -1.31645808]\n",
      "Gradient Descent(727/9): loss=15.517938912060034, w0=74.34660945767305, w1=13.900860823034105\n",
      "[ 2.4600871   1.84957789]\n",
      "Gradient Descent(728/9): loss=16.028646290760847, w0=72.62454848570664, w1=12.606156302127184\n",
      "[-1.54210473  0.28623984]\n",
      "Gradient Descent(729/9): loss=15.991468479687565, w0=73.70402179798849, w1=12.405788417213621\n",
      "[ 1.76555389 -1.12149299]\n",
      "Gradient Descent(730/9): loss=16.046635188098623, w0=72.46813407351753, w1=13.190833509351716\n",
      "[-1.13664713 -0.64566482]\n",
      "Gradient Descent(731/9): loss=15.768576237168638, w0=73.26378706695844, w1=13.642798879980328\n",
      "[-0.35637193 -1.85014898]\n",
      "Gradient Descent(732/9): loss=15.399640520257497, w0=73.51324741854015, w1=14.937903166857808\n",
      "[-0.83571213  6.36023446]\n",
      "Gradient Descent(733/9): loss=16.47309979323056, w0=74.0982459094827, w1=10.485739044050451\n",
      "[ 0.37529147 -4.3883305 ]\n",
      "Gradient Descent(734/9): loss=20.1912946756431, w0=73.83554188179158, w1=13.55757039588863\n",
      "[ 3.63224384 -1.65366989]\n",
      "Gradient Descent(735/9): loss=15.535594846902875, w0=71.2929711908466, w1=14.715139315531525\n",
      "[-0.90681123  0.67045323]\n",
      "Gradient Descent(736/9): loss=18.150929731951063, w0=71.9277390542037, w1=14.245822053672233\n",
      "[-2.61151549  1.96253317]\n",
      "Gradient Descent(737/9): loss=16.612577766317255, w0=73.75579989449051, w1=12.87204883766851\n",
      "[ 2.35448544  0.17609999]\n",
      "Gradient Descent(738/9): loss=15.67718098632082, w0=72.10766008849694, w1=12.748778846854478\n",
      "[-3.5893453 -0.8504027]\n",
      "Gradient Descent(739/9): loss=16.35662848779979, w0=74.62020179663546, w1=13.344060735313146\n",
      "[ 2.66425155 -2.37443725]\n",
      "Gradient Descent(740/9): loss=16.2745976073315, w0=72.75522571325011, w1=15.0061668094104\n",
      "[-1.66348901  1.52338525]\n",
      "Gradient Descent(741/9): loss=16.696016193237558, w0=73.91966802295524, w1=13.93979713669428\n",
      "[ 1.80473588  2.52242073]\n",
      "Gradient Descent(742/9): loss=15.68750587650583, w0=72.65635290521001, w1=12.174102623438934\n",
      "[-0.10579213 -1.12013796]\n",
      "Gradient Descent(743/9): loss=16.441443535495228, w0=72.73040739552741, w1=12.958199192996865\n",
      "[-0.50974872 -3.01849965]\n",
      "Gradient Descent(744/9): loss=15.680650255529256, w0=73.08723150231961, w1=15.071148950022101\n",
      "[-1.65410172  5.07235224]\n",
      "Gradient Descent(745/9): loss=16.673583440870484, w0=74.24510270355219, w1=11.520502380776753\n",
      "[ 3.25105955 -2.57497317]\n",
      "Gradient Descent(746/9): loss=17.75751225049528, w0=71.9693610195764, w1=13.32298360030293\n",
      "[-1.01763503  1.82043837]\n",
      "Gradient Descent(747/9): loss=16.275400730859257, w0=72.68170554249252, w1=12.048676741524304\n",
      "[-0.12522419 -2.16304213]\n",
      "Gradient Descent(748/9): loss=16.597223943524796, w0=72.76936247710842, w1=13.562806233213538\n",
      "[-1.64795535  1.72042539]\n",
      "Gradient Descent(749/9): loss=15.526921506113506, w0=73.92293122254634, w1=12.358508461222172\n",
      "[ 3.49588776  0.33433859]\n",
      "Gradient Descent(750/9): loss=16.212263343924707, w0=71.47580979002157, w1=12.124471450106304\n",
      "[-3.88463384 -1.44705831]\n",
      "Gradient Descent(751/9): loss=17.956992940246163, w0=74.19505347670906, w1=13.137412270136831\n",
      "[ 3.97957002  1.39953867]\n",
      "Gradient Descent(752/9): loss=15.8504915375192, w0=71.40935446261636, w1=12.15773520004562\n",
      "[-2.06408651 -3.74300115]\n",
      "Gradient Descent(753/9): loss=18.03549717913122, w0=72.85421501808722, w1=14.777836008042216\n",
      "[ 4.58982992  3.22219882]\n",
      "Gradient Descent(754/9): loss=16.325121390184652, w0=69.64133407711793, w1=12.522296831354204\n",
      "[-3.00025631 -0.35246755]\n",
      "Gradient Descent(755/9): loss=22.51490946275249, w0=71.74151349561892, w1=12.769024112954174\n",
      "[-2.11055347  1.00869689]\n",
      "Gradient Descent(756/9): loss=16.843412899873236, w0=73.21890092696091, w1=12.062936293265704\n",
      "[-0.31341868 -2.6987449 ]\n",
      "Gradient Descent(757/9): loss=16.392329267565547, w0=73.4382940045978, w1=13.952057726664606\n",
      "[ 2.9169689   0.31835987]\n",
      "Gradient Descent(758/9): loss=15.507864543665297, w0=71.39641577262614, w1=13.72920581761497\n",
      "[-2.04611947  0.24595325]\n",
      "Gradient Descent(759/9): loss=17.21727628827236, w0=72.82869940332567, w1=13.557038539198317\n",
      "[ 0.24376086 -0.3211775 ]\n",
      "Gradient Descent(760/9): loss=15.497093565233076, w0=72.65806680268174, w1=13.781862792637627\n",
      "[-0.23891057  0.02930462]\n",
      "Gradient Descent(761/9): loss=15.633691205459897, w0=72.82530420121431, w1=13.761349559619045\n",
      "[ 1.01450402  0.30420191]\n",
      "Gradient Descent(762/9): loss=15.53534892547023, w0=72.11515138681393, w1=13.548408223836871\n",
      "[-0.64266367 -2.01907241]\n",
      "Gradient Descent(763/9): loss=16.082997506269173, w0=72.56501595776167, w1=14.96175890994133\n",
      "[-0.4658125   4.58930301]\n",
      "Gradient Descent(764/9): loss=16.749770756528903, w0=72.8910847043609, w1=11.749246805677705\n",
      "[ 0.75100834 -3.39208363]\n",
      "Gradient Descent(765/9): loss=16.96428246017031, w0=72.36537886820818, w1=14.123705350043975\n",
      "[-0.76646362  0.64678002]\n",
      "Gradient Descent(766/9): loss=16.024347481903508, w0=72.90190340412967, w1=13.670959333111513\n",
      "[ 1.38770788  0.18360265]\n",
      "Gradient Descent(767/9): loss=15.481014847429481, w0=71.93050788750507, w1=13.542437477642645\n",
      "[-3.28271674 -1.85098525]\n",
      "Gradient Descent(768/9): loss=16.317304108262757, w0=74.22840960504669, w1=14.83812715457252\n",
      "[ 1.0972213   1.25141978]\n",
      "Gradient Descent(769/9): loss=16.7451666840456, w0=73.46035469418291, w1=13.962133311609598\n",
      "[ 3.50395789  0.68072683]\n",
      "Gradient Descent(770/9): loss=15.516102740425188, w0=71.00758416917914, w1=13.48562452835499\n",
      "[-4.51382915 -3.57989662]\n",
      "Gradient Descent(771/9): loss=17.999575688387875, w0=74.16726457565842, w1=15.991552165104501\n",
      "[-2.42711481  1.73053166]\n",
      "Gradient Descent(772/9): loss=18.921920909112924, w0=75.86624494212943, w1=14.780180006272296\n",
      "[ 2.63162484  2.44340817]\n",
      "Gradient Descent(773/9): loss=19.539918474696538, w0=74.02410755451801, w1=13.069794290640063\n",
      "[ 1.71763727 -1.63284376]\n",
      "Gradient Descent(774/9): loss=15.736489781838868, w0=72.82176146728605, w1=14.212784923202289\n",
      "[ 2.85637478 -1.2195343 ]\n",
      "Gradient Descent(775/9): loss=15.76605329063732, w0=70.82229912306649, w1=15.06645893133206\n",
      "[-0.73406462 -0.06124196]\n",
      "Gradient Descent(776/9): loss=19.6992299187516, w0=71.33614435993793, w1=15.109328304673431\n",
      "[-3.73399769  1.80783986]\n",
      "Gradient Descent(777/9): loss=18.630158458277986, w0=73.94994274224767, w1=13.843840401771656\n",
      "[ 0.25465712  0.22839895]\n",
      "Gradient Descent(778/9): loss=15.667364062674565, w0=73.77168275546529, w1=13.683961140002499\n",
      "[ 0.22299567  0.50629597]\n",
      "Gradient Descent(779/9): loss=15.520874304304842, w0=73.6155857845022, w1=13.329553963178054\n",
      "[-1.3661669  -1.58510121]\n",
      "Gradient Descent(780/9): loss=15.448895446610683, w0=74.57190261788342, w1=14.439124809626746\n",
      "[ 0.2309705   0.15918225]\n",
      "Gradient Descent(781/9): loss=16.66274114828583, w0=74.41022326961009, w1=14.327697233773161\n",
      "[ 0.75146386 -0.35254125]\n",
      "Gradient Descent(782/9): loss=16.368491238230387, w0=73.88419857075135, w1=14.574476106027124\n",
      "[ 1.59287573  1.52425053]\n",
      "Gradient Descent(783/9): loss=16.159354830288123, w0=72.76918556259824, w1=13.507500735223907\n",
      "[ 5.49238032  2.78348761]\n",
      "Gradient Descent(784/9): loss=15.523948129117585, w0=68.9245193373957, w1=11.559059408005627\n",
      "[-4.87145262  2.4391816 ]\n",
      "Gradient Descent(785/9): loss=26.7761817170449, w0=72.33453617321926, w1=9.85163228511907\n",
      "[-4.38773462 -3.75675589]\n",
      "Gradient Descent(786/9): loss=22.427581240103247, w0=75.40595040807045, w1=12.481361410300588\n",
      "[-0.02040314  1.49333791]\n",
      "Gradient Descent(787/9): loss=18.11457224687974, w0=75.42023260823083, w1=11.436024874336299\n",
      "[ 1.78464409 -5.29864171]\n",
      "Gradient Descent(788/9): loss=19.734815688474, w0=74.17098174512236, w1=15.145074069448471\n",
      "[ 2.33703146  3.69802515]\n",
      "Gradient Descent(789/9): loss=17.157219452004753, w0=72.53505972447, w1=12.556456461458302\n",
      "[-0.16864962 -1.39874916]\n",
      "Gradient Descent(790/9): loss=16.100024643368332, w0=72.65311445955108, w1=13.53558087430908\n",
      "[-0.60923461 -0.46720103]\n",
      "Gradient Descent(791/9): loss=15.59276566338255, w0=73.07957868646899, w1=13.862621594288235\n",
      "[-1.70288592 -0.61301831]\n",
      "Gradient Descent(792/9): loss=15.482169109445964, w0=74.2715988304726, w1=14.291734412561311\n",
      "[-0.63107722  0.46246727]\n",
      "Gradient Descent(793/9): loss=16.19350370522286, w0=74.71335288342921, w1=13.968007322658742\n",
      "[ 1.29937141  0.75356273]\n",
      "Gradient Descent(794/9): loss=16.512495830919722, w0=73.80379289728184, w1=13.440513414648386\n",
      "[ 0.21433209 -0.30600442]\n",
      "Gradient Descent(795/9): loss=15.516640315301352, w0=73.65376043365623, w1=13.654716507063188\n",
      "[ 2.35753647 -0.5866924 ]\n",
      "Gradient Descent(796/9): loss=15.465942929861223, w0=72.0034849019707, w1=14.065401189078344\n",
      "[-1.73468962  1.30237354]\n",
      "Gradient Descent(797/9): loss=16.3900174818645, w0=73.21776763359684, w1=13.153739709329077\n",
      "[-1.28420556 -2.39208033]\n",
      "Gradient Descent(798/9): loss=15.44191672168795, w0=74.11671152812451, w1=14.828195942605092\n",
      "[ 2.83244213  5.34684905]\n",
      "Gradient Descent(799/9): loss=16.633583056049183, w0=72.13400203930357, w1=11.085401604592448\n",
      "[ 0.24191948 -2.92358194]\n",
      "Gradient Descent(800/9): loss=18.924957205159487, w0=71.96465840058862, w1=13.131908965230425\n",
      "[-0.07435674 -1.26466485]\n",
      "Gradient Descent(801/9): loss=16.32984235677587, w0=72.01670811870402, w1=14.017174363613552\n",
      "[-2.17146757  0.31128949]\n",
      "Gradient Descent(802/9): loss=16.34595818316614, w0=73.53673542017849, w1=13.799271719539867\n",
      "[-0.1245816   0.13792961]\n",
      "Gradient Descent(803/9): loss=15.466426114998933, w0=73.6239425425833, w1=13.702720993452408\n",
      "[ 0.95395239 -0.43362614]\n",
      "Gradient Descent(804/9): loss=15.46521105597208, w0=72.95617586669786, w1=14.006259289626136\n",
      "[-0.16619768  0.63502354]\n",
      "Gradient Descent(805/9): loss=15.5815498898848, w0=73.07251424017738, w1=13.561742812048006\n",
      "[ 1.68085891 -1.17023809]\n",
      "Gradient Descent(806/9): loss=15.413763058730558, w0=71.89591300278025, w1=14.380909477534447\n",
      "[-1.73962436  1.04635567]\n",
      "Gradient Descent(807/9): loss=16.769180504672452, w0=73.11365005310813, w1=13.648460511380124\n",
      "[ 0.18169231  2.35918322]\n",
      "Gradient Descent(808/9): loss=15.416374813269842, w0=72.9864654382743, w1=11.997032260641443\n",
      "[-1.45503161 -0.116892  ]\n",
      "Gradient Descent(809/9): loss=16.532322887852473, w0=74.00498756876094, w1=12.078856663158442\n",
      "[ 2.63131675 -4.09511207]\n",
      "Gradient Descent(810/9): loss=16.61989343560669, w0=72.16306584675189, w1=14.945435114607701\n",
      "[-0.34132337  0.10989008]\n",
      "Gradient Descent(811/9): loss=17.09947717765387, w0=72.40199220729107, w1=14.868512061444672\n",
      "[ 0.00960299  0.45017666]\n",
      "Gradient Descent(812/9): loss=16.748039449489617, w0=72.39527011228817, w1=14.553388397311583\n",
      "[-0.00522369  1.42834231]\n",
      "Gradient Descent(813/9): loss=16.36606551439986, w0=72.39892669291912, w1=13.55354877862972\n",
      "[-0.71208986 -1.15558732]\n",
      "Gradient Descent(814/9): loss=15.789122073383048, w0=72.89738959286038, w1=14.362459902265668\n",
      "[ 1.11190161 -0.41569544]\n",
      "Gradient Descent(815/9): loss=15.854128390111791, w0=72.11905846829812, w1=14.653446708148772\n",
      "[-1.40575213  1.16331995]\n",
      "Gradient Descent(816/9): loss=16.764866102359107, w0=73.10308495661491, w1=13.839122742675416\n",
      "[-0.94904779 -0.50024241]\n",
      "Gradient Descent(817/9): loss=15.468685142430733, w0=73.76741841021398, w1=14.189292431597364\n",
      "[ 1.0045781   0.15649126]\n",
      "Gradient Descent(818/9): loss=15.749739178868689, w0=73.064213740019, w1=14.079748553052664\n",
      "[ 2.12086593  3.9334943 ]\n",
      "Gradient Descent(819/9): loss=15.592292483155155, w0=71.57960759003986, w1=11.326302543862607\n",
      "[-3.90304879 -7.72837312]\n",
      "Gradient Descent(820/9): loss=19.173911900137448, w0=74.31174173991855, w1=16.736163729784202\n",
      "[ 0.04520408  1.74484678]\n",
      "Gradient Descent(821/9): loss=21.20610389585714, w0=74.28009888620466, w1=15.514770982539018\n",
      "[-0.4712311   4.72164267]\n",
      "Gradient Descent(822/9): loss=17.94289193817356, w0=74.6099606582854, w1=12.209621114203\n",
      "[ 1.91030721 -2.04214868]\n",
      "Gradient Descent(823/9): loss=17.05843272267774, w0=73.27274560993312, w1=13.639125189378\n",
      "[-1.98112876  0.09352077]\n",
      "Gradient Descent(824/9): loss=15.398818301753048, w0=74.65953573893353, w1=13.573660653489723\n",
      "[ 3.49149728  0.55629592]\n",
      "Gradient Descent(825/9): loss=16.322751441816255, w0=72.2154876455513, w1=13.184253509877912\n",
      "[ 2.84263222  1.12381221]\n",
      "Gradient Descent(826/9): loss=16.011046187741208, w0=70.22564509369367, w1=12.397584961449354\n",
      "[-3.7496748   1.26341853]\n",
      "Gradient Descent(827/9): loss=20.67854939666977, w0=72.8504174507481, w1=11.513191988206744\n",
      "[-3.45369836 -0.88451665]\n",
      "Gradient Descent(828/9): loss=17.41783734617306, w0=75.26800630375793, w1=12.132353640517444\n",
      "[ 1.03139627 -0.98182358]\n",
      "Gradient Descent(829/9): loss=18.242080144365282, w0=74.54602891498158, w1=12.81963014781283\n",
      "[ 2.38564286 -1.56415192]\n",
      "Gradient Descent(830/9): loss=16.38762804238771, w0=72.87607891490619, w1=13.914536491479568\n",
      "[-1.54723249  2.34422529]\n",
      "Gradient Descent(831/9): loss=15.567720271640836, w0=73.95914166003625, w1=12.273578786761405\n",
      "[ 1.48631146 -2.0071089 ]\n",
      "Gradient Descent(832/9): loss=16.334525654171824, w0=72.91872363885102, w1=13.678555013498634\n",
      "[-0.06524356  0.12990357]\n",
      "Gradient Descent(833/9): loss=15.476043960237876, w0=72.96439413343388, w1=13.587622512887535\n",
      "[-0.15378096 -3.31415495]\n",
      "Gradient Descent(834/9): loss=15.446004469400956, w0=73.07204080600577, w1=15.907530977174318\n",
      "[-2.6204454 -0.8836674]\n",
      "Gradient Descent(835/9): loss=18.35765493830996, w0=74.90635258444458, w1=16.526098156371688\n",
      "[ 0.89539574  5.08743932]\n",
      "Gradient Descent(836/9): loss=21.326087041983076, w0=74.27957556946299, w1=12.964890633852297\n",
      "[-1.49617807 -2.51104016]\n",
      "Gradient Descent(837/9): loss=16.004165089714824, w0=75.32690021497606, w1=14.722618744350896\n",
      "[ 0.79956003 -0.28052044]\n",
      "Gradient Descent(838/9): loss=18.22479612275897, w0=74.76720819095452, w1=14.918983053146896\n",
      "[ 2.81527832 -0.05108611]\n",
      "Gradient Descent(839/9): loss=17.506923922102725, w0=72.79651336625238, w1=14.95474332851417\n",
      "[-0.89271071 -1.57583419]\n",
      "Gradient Descent(840/9): loss=16.597453612766632, w0=73.42141086124529, w1=16.057827258778783\n",
      "[-1.35318754  0.10494152]\n",
      "Gradient Descent(841/9): loss=18.71735259575401, w0=74.3686421366932, w1=15.98436819574169\n",
      "[ 2.57601614  0.41058119]\n",
      "Gradient Descent(842/9): loss=19.100049792609525, w0=72.56543083937026, w1=15.696961366095422\n",
      "[-0.51231857 -0.46202025]\n",
      "Gradient Descent(843/9): loss=18.10933396716702, w0=72.9240538357959, w1=16.020375540814236\n",
      "[-0.93767988  1.17530238]\n",
      "Gradient Descent(844/9): loss=18.681773607704542, w0=73.58042974983513, w1=15.197663871782975\n",
      "[ 1.5405543  4.837949 ]\n",
      "Gradient Descent(845/9): loss=16.902609783175198, w0=72.50204173836204, w1=11.811099568575429\n",
      "[ 1.42438632 -2.21210867]\n",
      "Gradient Descent(846/9): loss=17.091559493862896, w0=71.50497131674551, w1=13.3595756401574\n",
      "[ 0.61987257 -1.29639624]\n",
      "Gradient Descent(847/9): loss=16.99327657089004, w0=71.07106051561112, w1=14.267053011165867\n",
      "[-0.76992469 -0.34939907]\n",
      "Gradient Descent(848/9): loss=18.16639705434593, w0=71.61000779712332, w1=14.511632357368097\n",
      "[-1.69052843  1.13377269]\n",
      "Gradient Descent(849/9): loss=17.33610075680066, w0=72.79337770140076, w1=13.717991472411013\n",
      "[ 0.57931867  0.30880345]\n",
      "Gradient Descent(850/9): loss=15.539548617150615, w0=72.38785463139638, w1=13.501829060368197\n",
      "[-1.56781766 -1.76507479]\n",
      "Gradient Descent(851/9): loss=15.796611481520069, w0=73.48532699247033, w1=14.73738141539381\n",
      "[-2.6246609   1.81446515]\n",
      "Gradient Descent(852/9): loss=16.195071436133915, w0=75.32258962489182, w1=13.4672558100583\n",
      "[ 5.87253045 -2.85812529]\n",
      "Gradient Descent(853/9): loss=17.443711614453107, w0=71.21181831131489, w1=15.467943512190903\n",
      "[-1.37331343  1.52943669]\n",
      "Gradient Descent(854/9): loss=19.52999716660633, w0=72.1731377139997, w1=14.39733782839204\n",
      "[ 1.9017046  -1.47037352]\n",
      "Gradient Descent(855/9): loss=16.43498476037047, w0=70.84194449563333, w1=15.426599291822884\n",
      "[-2.55172287  1.32222437]\n",
      "Gradient Descent(856/9): loss=20.287168931607553, w0=72.62815050319361, w1=14.501042231024309\n",
      "[ 1.81492491  2.51219496]\n",
      "Gradient Descent(857/9): loss=16.1290709893456, w0=71.35770306551544, w1=12.742505756578208\n",
      "[-1.79351051 -1.41055552]\n",
      "Gradient Descent(858/9): loss=17.532096597380548, w0=72.61316042031162, w1=13.72989461714315\n",
      "[ 0.40669775  4.68233453]\n",
      "Gradient Descent(859/9): loss=15.648901596586139, w0=72.32847199722853, w1=10.452260446761864\n",
      "[-2.75384486 -5.45007425]\n",
      "Gradient Descent(860/9): loss=20.43466749529793, w0=74.25616339591363, w1=14.267312421544556\n",
      "[ 2.9423586  -1.54083992]\n",
      "Gradient Descent(861/9): loss=16.15899898821972, w0=72.1965123750025, w1=15.345900365806603\n",
      "[-2.37640261  1.31223794]\n",
      "Gradient Descent(862/9): loss=17.72937051022279, w0=73.85999420395304, w1=14.427333806052937\n",
      "[ 1.08943863 -0.33049667]\n",
      "Gradient Descent(863/9): loss=15.995099869130337, w0=73.09738716330408, w1=14.658681475254975\n",
      "[-2.85671146  5.56256164]\n",
      "Gradient Descent(864/9): loss=16.10018483921347, w0=75.0970851886675, w1=10.764888329934886\n",
      "[ 1.27072059 -1.49355184]\n",
      "Gradient Descent(865/9): loss=20.696721568207757, w0=74.20758077547819, w1=11.810374617433412\n",
      "[-1.78627416 -2.61743858]\n",
      "Gradient Descent(866/9): loss=17.196618420470838, w0=75.45797268975929, w1=13.642581622358675\n",
      "[ 4.76756081  1.68561215]\n",
      "Gradient Descent(867/9): loss=17.740708744294707, w0=72.12068012362538, w1=12.462653113955161\n",
      "[-0.63972209 -0.51655847]\n",
      "Gradient Descent(868/9): loss=16.591340952789775, w0=72.56848558473251, w1=12.824244040301918\n",
      "[ 0.37974924 -0.2475332 ]\n",
      "Gradient Descent(869/9): loss=15.863836274871513, w0=72.30266111668959, w1=12.997517276990969\n",
      "[ 0.70580211 -1.56691083]\n",
      "Gradient Descent(870/9): loss=15.993443025505252, w0=71.8085996369214, w1=14.094354855954435\n",
      "[ 1.27371506  2.29709634]\n",
      "Gradient Descent(871/9): loss=16.677871785912078, w0=70.91699909652911, w1=12.486387419212257\n",
      "[-1.9917764   0.02927492]\n",
      "Gradient Descent(872/9): loss=18.70411641183949, w0=72.31124257900635, w1=12.465894977414035\n",
      "[-4.46356147  0.79846693]\n",
      "Gradient Descent(873/9): loss=16.38263021176227, w0=75.43573561106008, w1=11.906968128460125\n",
      "[-1.38917954 -0.84996503]\n",
      "Gradient Descent(874/9): loss=18.916332963441164, w0=76.40816128817323, w1=12.501943651898276\n",
      "[ 6.83835322 -3.87502948]\n",
      "Gradient Descent(875/9): loss=20.713146930867605, w0=71.62131403675299, w1=15.214464288605233\n",
      "[ 1.99700552  0.46077149]\n",
      "Gradient Descent(876/9): loss=18.289378568521606, w0=70.22341017152482, w1=14.891924244245585\n",
      "[-3.312147    0.25625181]\n",
      "Gradient Descent(877/9): loss=21.097080416798224, w0=72.54191307331287, w1=14.712547974768286\n",
      "[-0.99377977  0.80043402]\n",
      "Gradient Descent(878/9): loss=16.42858831739247, w0=73.23755891477347, w1=14.152244162915679\n",
      "[-0.09951287  3.81164549]\n",
      "Gradient Descent(879/9): loss=15.61362573017017, w0=73.30721792290073, w1=11.484092316743663\n",
      "[ 0.31352405 -3.18123427]\n",
      "Gradient Descent(880/9): loss=17.377226087757162, w0=73.08775108772663, w1=13.71095630843305\n",
      "[-1.22174632 -0.75122248]\n",
      "Gradient Descent(881/9): loss=15.433877956299938, w0=73.94297351165355, w1=14.236812044208204\n",
      "[ 2.02010364  1.51078604]\n",
      "Gradient Descent(882/9): loss=15.8831217089928, w0=72.52890096713308, w1=13.179261813137208\n",
      "[-1.2170202   1.53186638]\n",
      "Gradient Descent(883/9): loss=15.723651748889877, w0=73.38081510632973, w1=12.106955344351386\n",
      "[-1.07804308  0.63859187]\n",
      "Gradient Descent(884/9): loss=16.33189408955828, w0=74.13544526454542, w1=11.659941036816814\n",
      "[ 4.85901212 -1.69419108]\n",
      "Gradient Descent(885/9): loss=17.395752540246292, w0=70.73413677830888, w1=12.845874795366436\n",
      "[-0.58380099 -2.72420516]\n",
      "Gradient Descent(886/9): loss=18.86301314151354, w0=71.14279747067101, w1=14.752818406601364\n",
      "[-1.76883146  1.85543277]\n",
      "Gradient Descent(887/9): loss=18.509955651175826, w0=72.38097949413815, w1=13.45401546631091\n",
      "[-0.79121045  0.08013249]\n",
      "Gradient Descent(888/9): loss=15.8029500473556, w0=72.93482680708078, w1=13.397922725026133\n",
      "[-0.09656115  1.09160519]\n",
      "Gradient Descent(889/9): loss=15.453707326702116, w0=73.00241961328835, w1=12.633799088722059\n",
      "[ 2.04197202 -0.51242077]\n",
      "Gradient Descent(890/9): loss=15.78615938486867, w0=71.5730391988897, w1=12.99249362924841\n",
      "[-2.98004676 -0.92004927]\n",
      "Gradient Descent(891/9): loss=16.98529776236446, w0=73.65907192774193, w1=13.636528116779958\n",
      "[ 2.05195004 -0.60037223]\n",
      "Gradient Descent(892/9): loss=15.464850681953434, w0=72.22270689924505, w1=14.056788677538036\n",
      "[ 1.09917531  1.22422489]\n",
      "Gradient Descent(893/9): loss=16.126147261984453, w0=71.4532841829329, w1=13.199831256389047\n",
      "[-3.29435172 -1.65639373]\n",
      "Gradient Descent(894/9): loss=17.119028396580315, w0=73.75933038935128, w1=14.359306866458832\n",
      "[-1.62245061  0.08514514]\n",
      "Gradient Descent(895/9): loss=15.881033534225228, w0=74.8950458139146, w1=14.299705268133858\n",
      "[ 1.84003796 -1.668768  ]\n",
      "Gradient Descent(896/9): loss=17.003880722405384, w0=73.6070192441091, w1=15.467842870855916\n",
      "[ 3.43668447 -0.21280719]\n",
      "Gradient Descent(897/9): loss=17.411234125314717, w0=71.20134011491129, w1=15.61680790366741\n",
      "[-2.35238219  1.07755603]\n",
      "Gradient Descent(898/9): loss=19.858925867258193, w0=72.84800764929692, w1=14.862518680574011\n",
      "[-0.30616431  2.2055501 ]\n",
      "Gradient Descent(899/9): loss=16.441384230264, w0=73.06232266809776, w1=13.318633613673246\n",
      "[ 0.67355685  0.7521937 ]\n",
      "Gradient Descent(900/9): loss=15.425680187923986, w0=72.59083287179014, w1=12.79209802334623\n",
      "[-1.02323269 -0.63573303]\n",
      "Gradient Descent(901/9): loss=15.869461820962437, w0=73.30709575322071, w1=13.237111142895507\n",
      "[ 1.96526653 -0.32563112]\n",
      "Gradient Descent(902/9): loss=15.415402336151356, w0=71.93140918361499, w1=13.465052929575329\n",
      "[-1.61879407 -0.86048873]\n",
      "Gradient Descent(903/9): loss=16.314215909653942, w0=73.0645650354586, w1=14.067395038895192\n",
      "[-0.13117405  1.64643173]\n",
      "Gradient Descent(904/9): loss=15.584875599371015, w0=73.15638687312791, w1=12.914892826571984\n",
      "[-1.10860981  0.22136915]\n",
      "Gradient Descent(905/9): loss=15.554856419707003, w0=73.93241374215485, w1=12.759934424221916\n",
      "[ 1.30346192 -1.72346404]\n",
      "Gradient Descent(906/9): loss=15.848763912277168, w0=73.01999040087497, w1=13.966359249493916\n",
      "[ 2.25319257  0.75958369]\n",
      "Gradient Descent(907/9): loss=15.541819690939539, w0=71.44275560233434, w1=13.434650666406899\n",
      "[-3.04165366 -2.49013868]\n",
      "Gradient Descent(908/9): loss=17.10031167014355, w0=73.57191316381179, w1=15.177747742197122\n",
      "[ 1.52041234  1.18883858]\n",
      "Gradient Descent(909/9): loss=16.866189364085496, w0=72.50762452428272, w1=14.345560733049085\n",
      "[ 0.78782975  3.13577327]\n",
      "Gradient Descent(910/9): loss=16.06986636827112, w0=71.95614369871096, w1=12.150519444694712\n",
      "[-3.31762003 -5.37175556]\n",
      "Gradient Descent(911/9): loss=17.164090266069373, w0=74.27847771772561, w1=15.91074833926876\n",
      "[ 1.48097013  0.42973735]\n",
      "Gradient Descent(912/9): loss=18.82553063135835, w0=73.24179862836044, w1=15.609932197135302\n",
      "[-0.57447047  0.82659739]\n",
      "Gradient Descent(913/9): loss=17.656164409393888, w0=73.64392795797603, w1=15.031314024077941\n",
      "[ 3.45150184  0.68122445]\n",
      "Gradient Descent(914/9): loss=16.650873699033518, w0=71.22787666871073, w1=14.55445690786939\n",
      "[-0.08501294 -1.52807884]\n",
      "Gradient Descent(915/9): loss=18.097697369643328, w0=71.2873857288962, w1=15.624112097399358\n",
      "[-0.03965631  6.03751902]\n",
      "Gradient Descent(916/9): loss=19.698206732753846, w0=71.31514514483598, w1=11.397848785535746\n",
      "[-3.48275523 -2.56997091]\n",
      "Gradient Descent(917/9): loss=19.510744921719006, w0=73.75307380613948, w1=13.196828424794147\n",
      "[-1.61618884 -1.18008327]\n",
      "Gradient Descent(918/9): loss=15.531309740015347, w0=74.88440599187317, w1=14.02288671152283\n",
      "[ 1.2784533   1.65704445]\n",
      "Gradient Descent(919/9): loss=16.79822667702753, w0=73.98948868268099, w1=12.862955598597889\n",
      "[ 3.02800681  3.41372938]\n",
      "Gradient Descent(920/9): loss=15.81798887001063, w0=71.86988391256513, w1=10.473345030248307\n",
      "[ 0.20856187 -2.55892665]\n",
      "Gradient Descent(921/9): loss=20.91895259520364, w0=71.72389060268154, w1=12.26459368266761\n",
      "[-0.61001755 -0.62865824]\n",
      "Gradient Descent(922/9): loss=17.356643957539102, w0=72.15090288928026, w1=12.704654452364089\n",
      "[ 0.31618552 -0.35031356]\n",
      "Gradient Descent(923/9): loss=16.33949165318628, w0=71.92957302503461, w1=12.94987394700839\n",
      "[-1.29635999 -2.28526798]\n",
      "Gradient Descent(924/9): loss=16.456976346118974, w0=72.83702501772788, w1=14.549561532257888\n",
      "[-1.10952137  1.63391998]\n",
      "Gradient Descent(925/9): loss=16.062553841459412, w0=73.61368997959843, w1=13.405817548054202\n",
      "[-1.23509905 -1.07872775]\n",
      "Gradient Descent(926/9): loss=15.439743875702016, w0=74.47825931563523, w1=14.16092697040675\n",
      "[ 0.06026842  3.25125456]\n",
      "Gradient Descent(927/9): loss=16.319241926571348, w0=74.4360714191711, w1=11.885048781183409\n",
      "[-0.42029061 -0.58898977]\n",
      "Gradient Descent(928/9): loss=17.309616598665777, w0=74.7302748482035, w1=12.297341620727256\n",
      "[-1.94305074 -1.38917411]\n",
      "Gradient Descent(929/9): loss=17.11644298928582, w0=76.0904103635787, w1=13.269763499169128\n",
      "[ 2.68208446 -0.69997021]\n",
      "Gradient Descent(930/9): loss=19.318100724583754, w0=74.21295124179538, w1=13.759742643318255\n",
      "[-1.02450444  2.85658327]\n",
      "Gradient Descent(931/9): loss=15.847403699320614, w0=74.93010435022646, w1=11.76013435673854\n",
      "[ 0.23471044 -1.37737936]\n",
      "Gradient Descent(932/9): loss=18.202908590581075, w0=74.76580704123938, w1=12.72429990848448\n",
      "[ 1.02443352  0.67087361]\n",
      "Gradient Descent(933/9): loss=16.754434695642942, w0=74.04870358031188, w1=12.254688381230528\n",
      "[-1.64931473 -2.28361901]\n",
      "Gradient Descent(934/9): loss=16.421077450372973, w0=75.20322389092345, w1=13.853221685501712\n",
      "[ 3.23773472  1.87895358]\n",
      "Gradient Descent(935/9): loss=17.278359300261158, w0=72.9368095855667, w1=12.537954177444044\n",
      "[-1.00423822  0.55900967]\n",
      "Gradient Descent(936/9): loss=15.893106815679479, w0=73.63977633909084, w1=12.146647410955424\n",
      "[-0.34592334 -3.64735267]\n",
      "Gradient Descent(937/9): loss=16.33422665918618, w0=73.88192267984671, w1=14.699794276538919\n",
      "[ 2.52484396  1.29220613]\n",
      "Gradient Descent(938/9): loss=16.303060117381502, w0=72.11453191024013, w1=13.795249988820096\n",
      "[ 1.33154156  1.13622834]\n",
      "Gradient Descent(939/9): loss=16.131150337162975, w0=71.18245281680124, w1=12.9998901473491\n",
      "[ 1.6605035   0.16871921]\n",
      "Gradient Descent(940/9): loss=17.730153642931466, w0=70.02010036957707, w1=12.881786697945936\n",
      "[-3.27853585  3.71493484]\n",
      "Gradient Descent(941/9): loss=20.923599503143226, w0=72.3150754624504, w1=10.281332310153726\n",
      "[-2.93890156 -2.59589703]\n",
      "Gradient Descent(942/9): loss=20.979775854397285, w0=74.37230655636303, w1=12.098460233805055\n",
      "[ 3.16933643 -4.23492172]\n",
      "Gradient Descent(943/9): loss=16.92127331389816, w0=72.15377105455198, w1=15.062905437923895\n",
      "[-3.32470789  2.34008079]\n",
      "Gradient Descent(944/9): loss=17.289110002703573, w0=74.48106657981683, w1=13.424848887723966\n",
      "[ 0.21191096 -1.08518507]\n",
      "Gradient Descent(945/9): loss=16.092048997433725, w0=74.33272890860852, w1=14.184478438204994\n",
      "[ 2.20669085  1.15632193]\n",
      "Gradient Descent(946/9): loss=16.17379532297339, w0=72.78804531575906, w1=13.375053084312835\n",
      "[-3.90250112 -2.65841779]\n",
      "Gradient Descent(947/9): loss=15.51932026956565, w0=75.51979609652737, w1=15.235945536254073\n",
      "[ 2.35193819  2.23547671]\n",
      "Gradient Descent(948/9): loss=19.405322963928764, w0=73.87343936093487, w1=13.67111184015643\n",
      "[ 1.9209675  -1.77135493]\n",
      "Gradient Descent(949/9): loss=15.572124919571076, w0=72.5287621123155, w1=14.911060288478373\n",
      "[-4.36933507  4.42440141]\n",
      "Gradient Descent(950/9): loss=16.70300103614513, w0=75.58729665843809, w1=11.813979301959929\n",
      "[-0.02889134 -2.70123277]\n",
      "Gradient Descent(951/9): loss=19.403004961219935, w0=75.60752059744208, w1=13.704842241190482\n",
      "[ 1.61475646 -0.28998448]\n",
      "Gradient Descent(952/9): loss=18.087598813821952, w0=74.47719107640648, w1=13.907831380513269\n",
      "[ 4.04863786  0.57493379]\n",
      "Gradient Descent(953/9): loss=16.17759363568671, w0=71.6431445766782, w1=13.505377726558287\n",
      "[-5.8602007  -0.45516077]\n",
      "Gradient Descent(954/9): loss=16.748750276574754, w0=75.74528506709794, w1=13.823990264980877\n",
      "[ 5.89800606  1.30179608]\n",
      "Gradient Descent(955/9): loss=18.449741919146664, w0=71.616680827232, w1=12.912733008531134\n",
      "[ 0.16645978 -2.67489345]\n",
      "Gradient Descent(956/9): loss=16.953189683187663, w0=71.50015897976158, w1=14.785158425293414\n",
      "[-1.65230374  0.34945526]\n",
      "Gradient Descent(957/9): loss=17.846775375793914, w0=72.65677160067438, w1=14.540539743321194\n",
      "[-1.13209118  2.5248193 ]\n",
      "Gradient Descent(958/9): loss=16.151545474902736, w0=73.44923542414705, w1=12.77316623501598\n",
      "[-0.1346036  -1.48063431]\n",
      "Gradient Descent(959/9): loss=15.647552764710769, w0=73.54345794099814, w1=13.80961025181391\n",
      "[-0.27953329  0.71563863]\n",
      "Gradient Descent(960/9): loss=15.471438246001897, w0=73.73913124275484, w1=13.308663207758382\n",
      "[ 0.44054275 -1.68868508]\n",
      "Gradient Descent(961/9): loss=15.499622421877424, w0=73.430751315562, w1=14.490742760896957\n",
      "[ 0.72643148  0.63018761]\n",
      "Gradient Descent(962/9): loss=15.90634015929266, w0=72.9222492783598, w1=14.04961143238205\n",
      "[-2.81183888  5.11152447]\n",
      "Gradient Descent(963/9): loss=15.617350609232334, w0=74.89053649597373, w1=10.471544300674621\n",
      "[ 4.53112553 -2.24855293]\n",
      "Gradient Descent(964/9): loss=21.185014551997337, w0=71.71874862802828, w1=12.045531348815727\n",
      "[-4.28716887 -2.36881453]\n",
      "Gradient Descent(965/9): loss=17.65491114199846, w0=74.71976683565308, w1=13.703701521634002\n",
      "[ 0.83427495 -0.91221446]\n",
      "Gradient Descent(966/9): loss=16.42749016897502, w0=74.13577437289887, w1=14.342251646094871\n",
      "[ 2.62670285  3.7915737 ]\n",
      "Gradient Descent(967/9): loss=16.112232521282394, w0=72.29708237698352, w1=11.68815005637574\n",
      "[-1.69497283 -1.19036033]\n",
      "Gradient Descent(968/9): loss=17.487580366167037, w0=73.48356335775394, w1=12.52140228713758\n",
      "[ 1.84657213 -4.23785214]\n",
      "Gradient Descent(969/9): loss=15.863048960453098, w0=72.19096286494069, w1=15.487898784783228\n",
      "[ 0.83322649  1.99923317]\n",
      "Gradient Descent(970/9): loss=18.01055350570657, w0=71.6077043226971, w1=14.088435563917642\n",
      "[ 0.95312307 -0.77197772]\n",
      "Gradient Descent(971/9): loss=16.99282482384991, w0=70.94051817109906, w1=14.628819969088147\n",
      "[-2.69721078  1.35011873]\n",
      "Gradient Descent(972/9): loss=18.81536672718823, w0=72.82856571636565, w1=13.683736859057529\n",
      "[-0.42968854 -0.57015439]\n",
      "Gradient Descent(973/9): loss=15.514979087976291, w0=73.12934769259286, w1=14.082844931238467\n",
      "[ 0.63032886 -0.95629896]\n",
      "Gradient Descent(974/9): loss=15.58131462452116, w0=72.68811749159214, w1=14.752254202054852\n",
      "[-0.707075    3.27061028]\n",
      "Gradient Descent(975/9): loss=16.37906869577186, w0=73.18306999226293, w1=12.462827006809043\n",
      "[-3.3477414  -0.05529316]\n",
      "Gradient Descent(976/9): loss=15.909059939894851, w0=75.52648897048739, w1=12.501532219689569\n",
      "[ 5.8497047  -3.71519705]\n",
      "Gradient Descent(977/9): loss=18.35648376978672, w0=71.43169567847202, w1=15.102170153329396\n",
      "[-1.55410279  1.35495317]\n",
      "Gradient Descent(978/9): loss=18.436015832946637, w0=72.51956763186654, w1=14.153702936185573\n",
      "[ 0.16097817  1.55082004]\n",
      "Gradient Descent(979/9): loss=15.91283181203482, w0=72.40688291631898, w1=13.068128908795158\n",
      "[-3.79599613  0.90270187]\n",
      "Gradient Descent(980/9): loss=15.864007538202717, w0=75.06408020582445, w1=12.436237602478021\n",
      "[ 2.8124936  -1.19983994]\n",
      "Gradient Descent(981/9): loss=17.497037764968663, w0=73.09533468335499, w1=13.276125560673297\n",
      "[ 2.60761121  2.0580912 ]\n",
      "Gradient Descent(982/9): loss=15.426330138110425, w0=71.27000683664595, w1=11.83546172218183\n",
      "[-3.01519391  0.21115506]\n",
      "Gradient Descent(983/9): loss=18.785784370600876, w0=73.38064257509429, w1=11.687653183265121\n",
      "[ 1.35938078 -2.37546042]\n",
      "Gradient Descent(984/9): loss=16.99538627856384, w0=72.42907602680006, w1=13.35047547529674\n",
      "[ 1.14336778  1.81871044]\n",
      "Gradient Descent(985/9): loss=15.768218245205393, w0=71.6287185805742, w1=12.077378165800198\n",
      "[-2.77923575 -3.02145397]\n",
      "Gradient Descent(986/9): loss=17.75560978763937, w0=73.57418360854372, w1=14.192395943729304\n",
      "[ 0.2646965  -0.36660078]\n",
      "Gradient Descent(987/9): loss=15.679120044666313, w0=73.38889606179752, w1=14.44901649277194\n",
      "[-1.68516501 -0.81224391]\n",
      "Gradient Descent(988/9): loss=15.860173083053812, w0=74.56851156837062, w1=15.017587228769766\n",
      "[ 1.97675479  3.56721   ]\n",
      "Gradient Descent(989/9): loss=17.380706590718795, w0=73.18478321727625, w1=12.520540229226116\n",
      "[ 1.41462787 -2.35554282]\n",
      "Gradient Descent(990/9): loss=15.851849166160434, w0=72.19454371110665, w1=14.169420201300024\n",
      "[-0.39372629  5.23385574]\n",
      "Gradient Descent(991/9): loss=16.228052583643677, w0=72.47015211276911, w1=10.505721185313346\n",
      "[-1.68262229 -1.69551468]\n",
      "Gradient Descent(992/9): loss=20.147498260691606, w0=73.64798771405063, w1=11.692581457906341\n",
      "[ 0.39502409  0.08921514]\n",
      "Gradient Descent(993/9): loss=17.045487697641413, w0=73.37147084912726, w1=11.630130862365949\n",
      "[ 2.40934485 -1.99056353]\n",
      "Gradient Descent(994/9): loss=17.099370777560097, w0=71.68492945313822, w1=13.023525335743047\n",
      "[ 0.6662204   0.13306283]\n",
      "Gradient Descent(995/9): loss=16.784369714904244, w0=71.21857517445827, w1=12.930381351452901\n",
      "[-2.91552079  1.77849938]\n",
      "Gradient Descent(996/9): loss=17.690302416010972, w0=73.25943973043289, w1=11.685431786387552\n",
      "[-1.0302398  -1.05304275]\n",
      "Gradient Descent(997/9): loss=16.99620390533215, w0=73.9806075887675, w1=12.422561708825686\n",
      "[ 1.17911328 -2.89709348]\n",
      "Gradient Descent(998/9): loss=16.180440245208143, w0=73.15522828928815, w1=14.450527144602686\n",
      "[-1.17799982  1.37443519]\n",
      "Gradient Descent(999/9): loss=15.866746442017995, w0=73.97982816472754, w1=13.488422510975973\n",
      "[ 4.10814925  1.16193159]\n",
      "Gradient Descent(1000/9): loss=15.621159433502907, w0=71.10412369062966, w1=12.675070396703829\n",
      "[-2.89587647 -5.14463213]\n",
      "Gradient Descent(1001/9): loss=18.107220596187826, w0=73.1312372212202, w1=16.27631288911585\n",
      "[ 3.84119079  3.42968474]\n",
      "Gradient Descent(1002/9): loss=19.309608087806318, w0=70.4424036679483, w1=13.87553356770744\n",
      "[-5.40052275  0.07213536]\n",
      "Gradient Descent(1003/9): loss=19.529803458399083, w0=74.22276959280671, w1=13.825038813855278\n",
      "[ 3.02516932 -0.56172071]\n",
      "Gradient Descent(1004/9): loss=15.876891946175846, w0=72.10515106996301, w1=14.218243313953238\n",
      "[ 0.55802053 -1.01687786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1005/9): loss=16.3651899629743, w0=71.71453669584409, w1=14.930057814563606\n",
      "[-1.32659361  2.1651535 ]\n",
      "Gradient Descent(1006/9): loss=17.68486770167277, w0=72.64315222545255, w1=13.41445036141259\n",
      "[ 0.33291006  0.21630347]\n",
      "Gradient Descent(1007/9): loss=15.59976808905541, w0=72.41011518572763, w1=13.26303793448912\n",
      "[-1.08087848 -0.99359153]\n",
      "Gradient Descent(1008/9): loss=15.79991903275056, w0=73.1667301241285, w1=13.95855200241707\n",
      "[-1.2790732   0.32903602]\n",
      "Gradient Descent(1009/9): loss=15.508620421408343, w0=74.06208136171787, w1=13.728226790425785\n",
      "[ 1.38723032 -0.80519902]\n",
      "Gradient Descent(1010/9): loss=15.711801962138754, w0=73.0910201354905, w1=14.291866103668198\n",
      "[-0.20603744 -0.04401259]\n",
      "Gradient Descent(1011/9): loss=15.736269243341766, w0=73.23524634589363, w1=14.322674919253615\n",
      "[ 1.40874188  3.94171734]\n",
      "Gradient Descent(1012/9): loss=15.742902160084073, w0=72.2491270317005, w1=11.563472782226803\n",
      "[-0.57697999 -2.65579508]\n",
      "Gradient Descent(1013/9): loss=17.767673337330052, w0=72.6530130277384, w1=13.42252933702668\n",
      "[-1.47183704 -0.43229528]\n",
      "Gradient Descent(1014/9): loss=15.592904978887635, w0=73.68329895849327, w1=13.72513603225535\n",
      "[ 0.34196178 -1.21766055]\n",
      "Gradient Descent(1015/9): loss=15.491811446959986, w0=73.4439257137323, w1=14.577498418915907\n",
      "[-1.23149013 -0.59819484]\n",
      "Gradient Descent(1016/9): loss=15.999705458833484, w0=74.30596880354192, w1=14.99623480429506\n",
      "[ 2.93966234  0.4363051 ]\n",
      "Gradient Descent(1017/9): loss=17.04792728128131, w0=72.24820516247692, w1=14.69082123596886\n",
      "[-4.29177576  2.6707454 ]\n",
      "Gradient Descent(1018/9): loss=16.666041987075847, w0=75.25244819203118, w1=12.821299455580759\n",
      "[ 0.6936034  -2.02104069]\n",
      "Gradient Descent(1019/9): loss=17.520554112869064, w0=74.76692581179155, w1=14.236027941672798\n",
      "[ 3.57417267  0.01152431]\n",
      "Gradient Descent(1020/9): loss=16.756764553329813, w0=72.2650049419042, w1=14.227960921880875\n",
      "[-0.8687706  -0.57744375]\n",
      "Gradient Descent(1021/9): loss=16.195160926283684, w0=72.87314435853803, w1=14.632171547761045\n",
      "[-1.93534978 -0.6958177 ]\n",
      "Gradient Descent(1022/9): loss=16.138495784797975, w0=74.22788920422596, w1=15.119243937344955\n",
      "[ 0.88580243 -1.4640063 ]\n",
      "Gradient Descent(1023/9): loss=17.166067009756755, w0=73.60782750237405, w1=16.144048350732273\n",
      "[ 4.01953288  1.91079828]\n",
      "Gradient Descent(1024/9): loss=18.984499136338567, w0=70.79415448388023, w1=14.80648955204584\n",
      "[-3.77437457  0.92578883]\n",
      "Gradient Descent(1025/9): loss=19.390475450588447, w0=73.4362166841924, w1=14.158437368223687\n",
      "[ 0.84706794  1.62414651]\n",
      "Gradient Descent(1026/9): loss=15.626345524601732, w0=72.84326912835544, w1=13.02153480907759\n",
      "[ 0.53713168  0.3741095 ]\n",
      "Gradient Descent(1027/9): loss=15.59239524358178, w0=72.46727695011539, w1=12.759658159094064\n",
      "[-0.84446862  0.60894018]\n",
      "Gradient Descent(1028/9): loss=15.986797969936331, w0=73.05840498236101, w1=12.333400032409301\n",
      "[ 2.65168463 -2.27621211]\n",
      "Gradient Descent(1029/9): loss=16.070638064278064, w0=71.20222573822929, w1=13.926748508432143\n",
      "[-0.31371841  0.3509625 ]\n",
      "Gradient Descent(1030/9): loss=17.673405124465315, w0=71.42182862554668, w1=13.681074758054955\n",
      "[-1.29531856  0.66276952]\n",
      "Gradient Descent(1031/9): loss=17.15852806668177, w0=72.3285516205521, w1=13.2171360962252\n",
      "[-3.63643147 -0.6758858 ]\n",
      "Gradient Descent(1032/9): loss=15.886331022458686, w0=74.87405365196393, w1=13.690256154419512\n",
      "[ 3.72383519 -0.07843638]\n",
      "Gradient Descent(1033/9): loss=16.656460213167865, w0=72.26736901785158, w1=13.745161622005964\n",
      "[-0.4355861  -0.50001301]\n",
      "Gradient Descent(1034/9): loss=15.94802501901337, w0=72.57227928725497, w1=14.095170729287403\n",
      "[-3.44734515 -1.31431454]\n",
      "Gradient Descent(1035/9): loss=15.835666428787919, w0=74.98542089357875, w1=15.015190904890696\n",
      "[ 1.51028213  1.65217819]\n",
      "Gradient Descent(1036/9): loss=17.995319184523293, w0=73.92822340165374, w1=13.858666171328661\n",
      "[ 0.63449065 -0.48314782]\n",
      "Gradient Descent(1037/9): loss=15.658859968706901, w0=73.4840799480688, w1=14.196869644312754\n",
      "[ 2.73127832 -1.6707769 ]\n",
      "Gradient Descent(1038/9): loss=15.66112512247843, w0=71.57218512584087, w1=15.366413475378799\n",
      "[-2.68416083  2.1923253 ]\n",
      "Gradient Descent(1039/9): loss=18.647897212277485, w0=73.4510977037396, w1=13.831785762876388\n",
      "[ 0.91906175 -0.86136693]\n",
      "Gradient Descent(1040/9): loss=15.46021778352637, w0=72.80775448146845, w1=14.43474261675931\n",
      "[-0.77789525  0.78708701]\n",
      "Gradient Descent(1041/9): loss=15.960108621936508, w0=73.35228115906071, w1=13.883781707735412\n",
      "[-0.70382051  1.44950623]\n",
      "Gradient Descent(1042/9): loss=15.46922675301857, w0=73.8449555149457, w1=12.869127345317711\n",
      "[-0.03385636  1.29635693]\n",
      "Gradient Descent(1043/9): loss=15.724113910830553, w0=73.86865496601163, w1=11.961677493400252\n",
      "[ 3.03412545 -3.06905281]\n",
      "Gradient Descent(1044/9): loss=16.703261900671993, w0=71.7447671516699, w1=14.110014462635487\n",
      "[-0.83531658 -0.48973957]\n",
      "Gradient Descent(1045/9): loss=16.78446856717059, w0=72.3294887606056, w1=14.452832164424995\n",
      "[-3.89979737  3.65633783]\n",
      "Gradient Descent(1046/9): loss=16.32443461139285, w0=75.05934692085866, w1=11.893395684792653\n",
      "[-2.97883822 -1.74583534]\n",
      "Gradient Descent(1047/9): loss=18.202450856684077, w0=77.14453367425307, w1=13.115480420332148\n",
      "[ 4.31673028 -0.58019652]\n",
      "Gradient Descent(1048/9): loss=22.865825473920655, w0=74.12282247936956, w1=13.521617983415101\n",
      "[ 0.61651307  0.24969067]\n",
      "Gradient Descent(1049/9): loss=15.730303906928393, w0=73.69126332810009, w1=13.346834511518093\n",
      "[ 1.76995557 -0.06690704]\n",
      "Gradient Descent(1050/9): loss=15.473656204774073, w0=72.45229442944562, w1=13.393669442278473\n",
      "[-4.36578506 -1.38599023]\n",
      "Gradient Descent(1051/9): loss=15.74375805265712, w0=75.50834397359539, w1=14.363862604133425\n",
      "[ 2.14771382 -0.67456892]\n",
      "Gradient Descent(1052/9): loss=18.22858096353768, w0=74.00494429912078, w1=14.836060848078583\n",
      "[-0.95028769  0.86656261]\n",
      "Gradient Descent(1053/9): loss=16.55850473110131, w0=74.67014567908227, w1=14.22946702240359\n",
      "[ 2.76462382 -0.95861345]\n",
      "Gradient Descent(1054/9): loss=16.61394964404013, w0=72.73490900263221, w1=14.900496438687481\n",
      "[ 2.52939921  0.66391929]\n",
      "Gradient Descent(1055/9): loss=16.551449228201964, w0=70.96432955869136, w1=14.435752935170992\n",
      "[-0.87505167  0.09601491]\n",
      "Gradient Descent(1056/9): loss=18.556395064028877, w0=71.57686572597846, w1=14.368542498094243\n",
      "[-3.8721247  -0.19775311]\n",
      "Gradient Descent(1057/9): loss=17.255038437062282, w0=74.28735301556469, w1=14.5069696761428\n",
      "[ 2.8525426  -0.26167713]\n",
      "Gradient Descent(1058/9): loss=16.406969177832394, w0=72.29057319405987, w1=14.690143666130055\n",
      "[ 1.21320622 -0.95219483]\n",
      "Gradient Descent(1059/9): loss=16.621814166793147, w0=71.44132883764672, w1=15.356680046682984\n",
      "[-2.76608469 -2.4602679 ]\n",
      "Gradient Descent(1060/9): loss=18.863442293002542, w0=73.37758812340422, w1=17.078867576024493\n",
      "[-3.97337598  4.00340146]\n",
      "Gradient Descent(1061/9): loss=21.866346743376948, w0=76.15895130815586, w1=14.276486553723823\n",
      "[ 4.81552142  2.13759103]\n",
      "Gradient Descent(1062/9): loss=19.807508829236788, w0=72.78808631718785, w1=12.780172835143363\n",
      "[ 1.4261055   0.89719695]\n",
      "Gradient Descent(1063/9): loss=15.758500564773428, w0=71.78981247060918, w1=12.152134971909863\n",
      "[-2.41863973 -3.21723924]\n",
      "Gradient Descent(1064/9): loss=17.39829157043586, w0=73.48286028441083, w1=14.404202443086604\n",
      "[-0.90726049  1.52073475]\n",
      "Gradient Descent(1065/9): loss=15.831077593625814, w0=74.11794262748263, w1=13.339688120682759\n",
      "[-2.38574603  5.62990644]\n",
      "Gradient Descent(1066/9): loss=15.73519626865159, w0=75.7879648451735, w1=9.398753611428493\n",
      "[ 4.14274839 -0.88330759]\n",
      "Gradient Descent(1067/9): loss=26.823125180157938, w0=72.8880409755686, w1=10.017068922545707\n",
      "[-0.42018435 -5.47549309]\n",
      "Gradient Descent(1068/9): loss=21.463207619813574, w0=73.18217002364275, w1=13.849914088928207\n",
      "[ 4.05303858  5.37764976]\n",
      "Gradient Descent(1069/9): loss=15.46065675346418, w0=70.34504301808597, w1=10.085559258349157\n",
      "[-2.66077594 -4.24743862]\n",
      "Gradient Descent(1070/9): loss=25.493969393271822, w0=72.20758617754433, w1=13.058766295567255\n",
      "[-3.06344483  2.04875181]\n",
      "Gradient Descent(1071/9): loss=16.06454845683862, w0=74.35199755977123, w1=11.62464003152063\n",
      "[ 1.57588429 -1.41718265]\n",
      "Gradient Descent(1072/9): loss=17.666296622749606, w0=73.24887855339853, w1=12.616667886214664\n",
      "[ 1.98748412 -1.97394894]\n",
      "Gradient Descent(1073/9): loss=15.759325271549686, w0=71.85763967257608, w1=13.998432141581294\n",
      "[-1.11896292  0.64040356]\n",
      "Gradient Descent(1074/9): loss=16.55187640089175, w0=72.64091371631939, w1=13.55014964807499\n",
      "[-0.58505096 -0.01781342]\n",
      "Gradient Descent(1075/9): loss=15.601578479975512, w0=73.05044939098485, w1=13.562619045062489\n",
      "[-2.90615097 -2.39305516]\n",
      "Gradient Descent(1076/9): loss=15.418964078009212, w0=75.08475507301456, w1=15.23775765998869\n",
      "[ 3.72492254  1.53123815]\n",
      "Gradient Descent(1077/9): loss=18.534790919332764, w0=72.47730929297887, w1=14.165890957853927\n",
      "[-1.72383756  1.13473597]\n",
      "Gradient Descent(1078/9): loss=15.954736509803226, w0=73.68399558275894, w1=13.371575781153409\n",
      "[ 0.53322528  0.70673285]\n",
      "Gradient Descent(1079/9): loss=15.467813335942804, w0=73.31073788350305, w1=12.8768627874685\n",
      "[ 2.10199032 -1.05700682]\n",
      "Gradient Descent(1080/9): loss=15.567743104520819, w0=71.83934466257807, w1=13.616767558142538\n",
      "[ 1.73683363 -1.06667012]\n",
      "Gradient Descent(1081/9): loss=16.453177540553604, w0=70.62356112477684, w1=14.363436640546162\n",
      "[-3.30264744  3.50103618]\n",
      "Gradient Descent(1082/9): loss=19.34178571215609, w0=72.93541433009239, w1=11.912711311352284\n",
      "[-1.9541533  -1.47906735]\n",
      "Gradient Descent(1083/9): loss=16.67789800501486, w0=74.303321643018, w1=12.948058453282028\n",
      "[ 3.21789286 -1.63890559]\n",
      "Gradient Descent(1084/9): loss=16.036659664499314, w0=72.05079664132433, w1=14.095292363361162\n",
      "[ 2.85696379  0.32855643]\n",
      "Gradient Descent(1085/9): loss=16.348037524244987, w0=70.05092198624385, w1=13.865302861919819\n",
      "[-4.373228    2.03766077]\n",
      "Gradient Descent(1086/9): loss=20.71875240893805, w0=73.11218158532921, w1=12.43894032281122\n",
      "[ 1.97478713 -0.9837227 ]\n",
      "Gradient Descent(1087/9): loss=15.944005953117902, w0=71.72983059509966, w1=13.127546211376401\n",
      "[-2.4307272  -1.02953135]\n",
      "Gradient Descent(1088/9): loss=16.671089358090473, w0=73.43133963510785, w1=13.84821815950729\n",
      "[-1.45795182  0.74469613]\n",
      "Gradient Descent(1089/9): loss=15.463227906260785, w0=74.45190591153799, w1=13.326930865784039\n",
      "[-1.76936421 -1.26802683]\n",
      "Gradient Descent(1090/9): loss=16.06802234002641, w0=75.69046085536887, w1=14.214549648069486\n",
      "[-0.06824558  1.14959139]\n",
      "Gradient Descent(1091/9): loss=18.527579971294504, w0=75.7382327640529, w1=13.409835673185377\n",
      "[ 1.41907218 -0.04101695]\n",
      "Gradient Descent(1092/9): loss=18.375656800236168, w0=74.74488223988287, w1=13.438547538583691\n",
      "[ 0.72203232  1.48833298]\n",
      "Gradient Descent(1093/9): loss=16.439377948983363, w0=74.23945961258008, w1=12.396714455994761\n",
      "[ 1.7716201  -1.76954783]\n",
      "Gradient Descent(1094/9): loss=16.419350866493538, w0=72.99932554149201, w1=13.635397936479624\n",
      "[-1.37763763 -0.29586919]\n",
      "Gradient Descent(1095/9): loss=15.441400393819492, w0=73.96367188589946, w1=13.842506371849996\n",
      "[-1.12063557  1.32332168]\n",
      "Gradient Descent(1096/9): loss=15.67598004256215, w0=74.74811678704076, w1=12.91618119909991\n",
      "[ 1.87892965 -1.39294934]\n",
      "Gradient Descent(1097/9): loss=16.602012832007674, w0=73.43286603530277, w1=13.891245735091193\n",
      "[-2.66646679  2.87134754]\n",
      "Gradient Descent(1098/9): loss=15.480220419556485, w0=75.29939278738141, w1=11.881302455474806\n",
      "[ 0.95348319 -1.47216666]\n",
      "Gradient Descent(1099/9): loss=18.674301635432972, w0=74.63195455575045, w1=12.911819117437725\n",
      "[ 2.35036243 -3.33419016]\n",
      "Gradient Descent(1100/9): loss=16.44230483619636, w0=72.98670085507146, w1=15.245752228594062\n",
      "[-1.20250817 -0.75964257]\n",
      "Gradient Descent(1101/9): loss=16.99252856171998, w0=73.82845657509357, w1=15.777502029248657\n",
      "[-1.88927263  0.73500779]\n",
      "Gradient Descent(1102/9): loss=18.168669983433205, w0=75.15094741707281, w1=15.262996576107788\n",
      "[-0.23685335  0.12197156]\n",
      "Gradient Descent(1103/9): loss=18.700210728730035, w0=75.31674475988717, w1=15.177616485533148\n",
      "[-0.08310136  1.02676045]\n",
      "Gradient Descent(1104/9): loss=18.873232905956783, w0=75.37491571470903, w1=14.458884170033128\n",
      "[ 1.47667123  3.33920072]\n",
      "Gradient Descent(1105/9): loss=18.03054392813238, w0=74.34124585580396, w1=12.121443665964517\n",
      "[ 2.05667205 -2.43429941]\n",
      "Gradient Descent(1106/9): loss=16.856778520546335, w0=72.90157541792647, w1=13.825453251813814\n",
      "[-0.48594502  2.03381025]\n",
      "Gradient Descent(1107/9): loss=15.522624146097087, w0=73.24173693532472, w1=12.40178607651169\n",
      "[-2.33465891 -2.10764697]\n",
      "Gradient Descent(1108/9): loss=15.96821212657697, w0=74.8759981729382, w1=13.877138958520224\n",
      "[ 0.23264131  1.30890669]\n",
      "Gradient Descent(1109/9): loss=16.716344294791252, w0=74.71314925387006, w1=12.960904273016025\n",
      "[ 2.24838954 -0.51991424]\n",
      "Gradient Descent(1110/9): loss=16.527571819370344, w0=73.13927657461544, w1=13.324844242860177\n",
      "[-1.09969862 -1.15581211]\n",
      "Gradient Descent(1111/9): loss=15.409837551417779, w0=73.90906560610583, w1=14.133912722008093\n",
      "[ 0.04979002 -1.08824207]\n",
      "Gradient Descent(1112/9): loss=15.789077703368749, w0=73.87421259364223, w1=14.895682167531\n",
      "[-1.5011973   6.17592023]\n",
      "Gradient Descent(1113/9): loss=16.556741595880066, w0=74.92505070561441, w1=10.572538006969904\n",
      "[-1.00011935 -1.24487073]\n",
      "Gradient Descent(1114/9): loss=20.942009869999463, w0=75.62513425224387, w1=11.443947520676128\n",
      "[ 1.1677606  -1.80292426]\n",
      "Gradient Descent(1115/9): loss=20.175332539601474, w0=74.80770182930969, w1=12.70599449989541\n",
      "[ 2.09068154 -1.80591204]\n",
      "Gradient Descent(1116/9): loss=16.83097227299783, w0=73.3442247482797, w1=13.970132924914237\n",
      "[ 3.00867534 -3.37340536]\n",
      "Gradient Descent(1117/9): loss=15.50740918043498, w0=71.2381520121377, w1=16.33151667921826\n",
      "[ 0.85379755  3.03304569]\n",
      "Gradient Descent(1118/9): loss=21.565376718356738, w0=70.64049372830532, w1=14.208384692950638\n",
      "[-2.174724    3.08085705]\n",
      "Gradient Descent(1119/9): loss=19.17171030069111, w0=72.16280052633331, w1=12.051784756692653\n",
      "[-4.44954701 -1.72319823]\n",
      "Gradient Descent(1120/9): loss=17.04509449252804, w0=75.27748343530692, w1=13.25802352075044\n",
      "[ 1.84378764 -0.71279885]\n",
      "Gradient Descent(1121/9): loss=17.3777188358202, w0=73.98683209060115, w1=13.756982718752884\n",
      "[ 0.6150401   2.51129626]\n",
      "Gradient Descent(1122/9): loss=15.664389469328382, w0=73.55630402209181, w1=11.999075339622568\n",
      "[-0.30981255 -1.63590008]\n",
      "Gradient Descent(1123/9): loss=16.516453135123175, w0=73.7731728047713, w1=13.144205392928452\n",
      "[ 1.94433821 -1.02142444]\n",
      "Gradient Descent(1124/9): loss=15.557011022393583, w0=72.41213605761806, w1=13.859202502216815\n",
      "[ 0.28202129 -0.7942529 ]\n",
      "Gradient Descent(1125/9): loss=15.846667450339199, w0=72.21472115312416, w1=14.415179529938463\n",
      "[-2.1166014   4.03339008]\n",
      "Gradient Descent(1126/9): loss=16.405774447916638, w0=73.69634213328006, w1=11.591806476572618\n",
      "[ 1.28986922 -2.46067971]\n",
      "Gradient Descent(1127/9): loss=17.248953303728932, w0=72.7934336809792, w1=13.314282276248923\n",
      "[-2.87799688 -2.35955528]\n",
      "Gradient Descent(1128/9): loss=15.524815717331547, w0=74.80803149433025, w1=14.965970970590135\n",
      "[-0.08780763  1.39801817]\n",
      "Gradient Descent(1129/9): loss=17.63663386337596, w0=74.86949683798176, w1=13.987358249532273\n",
      "[ 5.81095555  2.20285082]\n",
      "Gradient Descent(1130/9): loss=16.75595803706476, w0=70.80182795486958, w1=12.445362674358849\n",
      "[-2.48704235 -3.6044987 ]\n",
      "Gradient Descent(1131/9): loss=19.026093952620855, w0=72.54275759651038, w1=14.968511763140004\n",
      "[-1.00407557 -1.71878677]\n",
      "Gradient Descent(1132/9): loss=16.776273570697068, w0=73.24561049734672, w1=16.171662500143388\n",
      "[ 0.89094737  1.0109277 ]\n",
      "Gradient Descent(1133/9): loss=19.01035244621764, w0=72.62194733632612, w1=15.464013110590974\n",
      "[-0.80066062  0.34302786]\n",
      "Gradient Descent(1134/9): loss=17.58038743015098, w0=73.18240977225065, w1=15.223893610217607\n",
      "[ 0.59905733  2.12958147]\n",
      "Gradient Descent(1135/9): loss=16.913189343543806, w0=72.7630696446599, w1=13.733186583247479\n",
      "[-2.20847987  0.33668007]\n",
      "Gradient Descent(1136/9): loss=15.55891455344968, w0=74.30900555298906, w1=13.497510537197781\n",
      "[ 2.80629165 -0.35788448]\n",
      "Gradient Descent(1137/9): loss=15.901243562688027, w0=72.34460139512143, w1=13.74802967182869\n",
      "[ 0.85226094  1.23904374]\n",
      "Gradient Descent(1138/9): loss=15.872489746044035, w0=71.74801873492153, w1=12.880699054363069\n",
      "[-2.68309483 -0.97133997]\n",
      "Gradient Descent(1139/9): loss=16.76020483965844, w0=73.6261851143275, w1=13.560637034455207\n",
      "[-1.47202043  2.2700103 ]\n",
      "Gradient Descent(1140/9): loss=15.444361652100604, w0=74.65659941355297, w1=11.971629826178685\n",
      "[ 0.37194879 -0.03567722]\n",
      "Gradient Descent(1141/9): loss=17.45148931016255, w0=74.3962352593793, w1=11.99660387846324\n",
      "[ 0.9778014   0.01528751]\n",
      "Gradient Descent(1142/9): loss=17.093240622630557, w0=73.71177427979094, w1=11.985902618781878\n",
      "[ 0.09396468 -1.38756306]\n",
      "Gradient Descent(1143/9): loss=16.588922015311436, w0=73.64599900080252, w1=12.95719676340537\n",
      "[ 1.00134836  2.54335149]\n",
      "Gradient Descent(1144/9): loss=15.584378288860531, w0=72.94505514930331, w1=11.176850717202823\n",
      "[ 2.20245762 -0.2130737 ]\n",
      "Gradient Descent(1145/9): loss=18.098327954944004, w0=71.40333481788363, w1=11.326002310307555\n",
      "[-0.86232056 -0.97758812]\n",
      "Gradient Descent(1146/9): loss=19.492281469978582, w0=72.00695921052011, w1=12.010313991840857\n",
      "[-0.31076256 -1.05025766]\n",
      "Gradient Descent(1147/9): loss=17.293590374654798, w0=72.2244930051349, w1=12.745494352618074\n",
      "[-1.75418677 -0.81467005]\n",
      "Gradient Descent(1148/9): loss=16.22726515485009, w0=73.45242374097947, w1=13.315763385659753\n",
      "[-1.65221409  1.08295934]\n",
      "Gradient Descent(1149/9): loss=15.411888914830476, w0=74.6089736028045, w1=12.557691847254496\n",
      "[ 2.96238446  1.10998303]\n",
      "Gradient Descent(1150/9): loss=16.675629207183494, w0=72.53530448062786, w1=11.780703729587374\n",
      "[-0.64048241 -1.85042639]\n",
      "Gradient Descent(1151/9): loss=17.116953431290938, w0=72.98364216642246, w1=13.076002203128818\n",
      "[-1.27591397  0.39657722]\n",
      "Gradient Descent(1152/9): loss=15.515515632699373, w0=73.87678194423242, w1=12.798398152564786\n",
      "[ 2.81815847 -2.78516128]\n",
      "Gradient Descent(1153/9): loss=15.787845300615325, w0=71.90407101856977, w1=14.74801105087204\n",
      "[-0.57393889 -2.0863414 ]\n",
      "Gradient Descent(1154/9): loss=17.15602143657195, w0=72.30582824270252, w1=16.20845003257477\n",
      "[-1.05622144  5.78778722]\n",
      "Gradient Descent(1155/9): loss=19.597056945753604, w0=73.04518325055734, w1=12.156998982037155\n",
      "[-2.84708469 -3.307699  ]\n",
      "Gradient Descent(1156/9): loss=16.29160879140015, w0=75.03814253015842, w1=14.472388284801434\n",
      "[ 2.02098843  3.34875245]\n",
      "Gradient Descent(1157/9): loss=17.399743165470923, w0=73.62345062646838, w1=12.12826156839916\n",
      "[-1.42496706 -3.98208163]\n",
      "Gradient Descent(1158/9): loss=16.353392148370027, w0=74.62092756550578, w1=14.915718707356874\n",
      "[ 1.71671853  2.12767596]\n",
      "Gradient Descent(1159/9): loss=17.297416758617327, w0=73.41922459559916, w1=13.426345532363387\n",
      "[ 0.87230496  0.52817783]\n",
      "Gradient Descent(1160/9): loss=15.395162251945486, w0=72.80861112686631, w1=13.056621048075986\n",
      "[ 0.58634108 -2.55555711]\n",
      "Gradient Descent(1161/9): loss=15.593154352481971, w0=72.39817237060112, w1=14.845511023313072\n",
      "[-4.43601895  0.45214426]\n",
      "Gradient Descent(1162/9): loss=16.71977446193319, w0=75.5033856381158, w1=14.52901004396637\n",
      "[ 2.54080719  1.83337493]\n",
      "Gradient Descent(1163/9): loss=18.37726538435878, w0=73.72482060846873, w1=13.245647594135102\n",
      "[-1.43713084 -0.10188629]\n",
      "Gradient Descent(1164/9): loss=15.506117848174416, w0=74.7308121929924, w1=13.316967999079788\n",
      "[ 0.08127441 -0.45550818]\n",
      "Gradient Descent(1165/9): loss=16.43145745487306, w0=74.67392010793249, w1=13.63582372349383\n",
      "[-0.11456118  1.7677244 ]\n",
      "Gradient Descent(1166/9): loss=16.350270622072177, w0=74.75411293571673, w1=12.39841664295281\n",
      "[ 0.60902757 -2.06422311]\n",
      "Gradient Descent(1167/9): loss=17.03656694506772, w0=74.32779363466659, w1=13.84337281721915\n",
      "[-2.50998148  2.29560951]\n",
      "Gradient Descent(1168/9): loss=15.986457581938858, w0=76.08478067395536, w1=12.236446163280167\n",
      "[ 0.80223275 -1.08206745]\n",
      "Gradient Descent(1169/9): loss=20.053189443134517, w0=75.52321774934717, w1=12.993893377985113\n",
      "[-0.11379134  0.25059794]\n",
      "Gradient Descent(1170/9): loss=17.98877771123908, w0=75.60287168885097, w1=12.818474823030577\n",
      "[ 4.85098299 -2.90700719]\n",
      "Gradient Descent(1171/9): loss=18.270129786525388, w0=72.2071835964952, w1=14.853379857835506\n",
      "[-0.38212037  1.62638458]\n",
      "Gradient Descent(1172/9): loss=16.91986914423809, w0=72.4746678585281, w1=13.714910651894753\n",
      "[-2.70384977 -0.24669504]\n",
      "Gradient Descent(1173/9): loss=15.749135645331332, w0=74.3673626981331, w1=13.887597178569399\n",
      "[ 4.61970943 -0.64731042]\n",
      "Gradient Descent(1174/9): loss=16.045210314796655, w0=71.13356609624712, w1=14.340714475529115\n",
      "[ 1.17224286 -0.29868803]\n",
      "Gradient Descent(1175/9): loss=18.090118945724395, w0=70.31299609152909, w1=14.549796098431477\n",
      "[-1.13471611  1.25455165]\n",
      "Gradient Descent(1176/9): loss=20.401387034384573, w0=71.10729736984493, w1=13.671609943645503\n",
      "[ -4.19211422e+00   2.98950018e-03]\n",
      "Gradient Descent(1177/9): loss=17.794963836947336, w0=74.04177732420717, w1=13.669517293520421\n",
      "[ 0.9840566  -0.39352353]\n",
      "Gradient Descent(1178/9): loss=15.683544602388588, w0=73.35293770694238, w1=13.944983766518138\n",
      "[ 1.9979373  -1.67499308]\n",
      "Gradient Descent(1179/9): loss=15.495868001509539, w0=71.95438159994376, w1=15.117478920204405\n",
      "[-4.51323943  2.017155  ]\n",
      "Gradient Descent(1180/9): loss=17.62421164338814, w0=75.11364919756053, w1=13.705470420393894\n",
      "[ 1.58327668 -1.01850368]\n",
      "Gradient Descent(1181/9): loss=17.067074735756314, w0=74.00535552239674, w1=14.418422995651\n",
      "[ 2.20878451  0.6970368 ]\n",
      "Gradient Descent(1182/9): loss=16.079545454075753, w0=72.4592063621996, w1=13.930497238606064\n",
      "[ 0.72411322  0.46059458]\n",
      "Gradient Descent(1183/9): loss=15.835866438166917, w0=71.95232710922996, w1=13.608081029771983\n",
      "[-0.67173864  1.02381303]\n",
      "Gradient Descent(1184/9): loss=16.294065545187127, w0=72.42254416064411, w1=12.891411907960533\n",
      "[-1.59184853  0.66475576]\n",
      "Gradient Descent(1185/9): loss=15.938586295175101, w0=73.53683812926813, w1=12.426082873806022\n",
      "[-1.2053527  0.9657407]\n",
      "Gradient Descent(1186/9): loss=15.970459617346691, w0=74.38058501865265, w1=11.750064384899204\n",
      "[ 0.55415276 -3.46227666]\n",
      "Gradient Descent(1187/9): loss=17.472147313185218, w0=73.99267808749046, w1=14.173658044873605\n",
      "[-0.29524455 -0.12631135]\n",
      "Gradient Descent(1188/9): loss=15.870798156999898, w0=74.1993492746489, w1=14.262075986712851\n",
      "[ 1.55787913 -0.56364376]\n",
      "Gradient Descent(1189/9): loss=16.10183350529531, w0=73.10883388457997, w1=14.656626621913594\n",
      "[ 1.59961345  0.39554813]\n",
      "Gradient Descent(1190/9): loss=16.09558017614605, w0=71.9891044671881, w1=14.379742931194098\n",
      "[ 0.95449168  0.9097337 ]\n",
      "Gradient Descent(1191/9): loss=16.642189715592515, w0=71.3209602899562, w1=13.742929338707318\n",
      "[-1.86320136  0.55751936]\n",
      "Gradient Descent(1192/9): loss=17.366818396833843, w0=72.62520124295324, w1=13.352665783794222\n",
      "[ 1.51339909 -0.63951543]\n",
      "Gradient Descent(1193/9): loss=15.61755202147969, w0=71.56582188211745, w1=13.800326583445502\n",
      "[-3.74465576  0.82879329]\n",
      "Gradient Descent(1194/9): loss=16.930449597275455, w0=74.18708091283936, w1=13.220171283591721\n",
      "[ 2.7041329  -0.56384879]\n",
      "Gradient Descent(1195/9): loss=15.818435093375653, w0=72.2941878856237, w1=13.614865433919375\n",
      "[ 1.37951548  0.17841313]\n",
      "Gradient Descent(1196/9): loss=15.894755187217841, w0=71.32852704710625, w1=13.489976239484651\n",
      "[-4.575083  3.472377]\n",
      "Gradient Descent(1197/9): loss=17.31732920623841, w0=74.5310851454579, w1=11.059312339753147\n",
      "[ 1.51136429 -0.90061725]\n",
      "Gradient Descent(1198/9): loss=19.080342500973558, w0=73.47313014504736, w1=11.68974441748746\n",
      "[ 0.61894068 -2.31602905]\n",
      "Gradient Descent(1199/9): loss=17.003938399917075, w0=73.03987166912245, w1=13.310964752960695\n",
      "[-1.61616523 -2.9811249 ]\n",
      "Gradient Descent(1200/9): loss=15.43239654476869, w0=74.1711873318968, w1=15.397752182712594\n",
      "[ 2.4320152   1.65561339]\n",
      "Gradient Descent(1201/9): loss=17.61012333518029, w0=72.46877668885078, w1=14.238822808735538\n",
      "[-2.91991835 -2.07152242]\n",
      "Gradient Descent(1202/9): loss=16.014444542587025, w0=74.51271953427165, w1=15.688888500870302\n",
      "[ 1.13716384  3.23222658]\n",
      "Gradient Descent(1203/9): loss=18.568851026068213, w0=73.71670484829403, w1=13.426329897722741\n",
      "[-1.89548632 -2.06256381]\n",
      "Gradient Descent(1204/9): loss=15.47668538398766, w0=75.0435452736218, w1=14.870124562305188\n",
      "[ 1.52189411  0.78510564]\n",
      "Gradient Descent(1205/9): loss=17.88310160683954, w0=73.97821939507288, w1=14.320550613056923\n",
      "[-2.14293691  1.54628619]\n",
      "Gradient Descent(1206/9): loss=15.973523750688843, w0=75.47827523224404, w1=13.238150280156784\n",
      "[ 6.66831513  1.96124242]\n",
      "Gradient Descent(1207/9): loss=17.80076352316203, w0=70.81045464225234, w1=11.865280588648062\n",
      "[-6.34048844 -1.16502814]\n",
      "Gradient Descent(1208/9): loss=19.772888025796625, w0=75.24879655238908, w1=12.680800289554591\n",
      "[-0.76992414 -0.19975731]\n",
      "Gradient Descent(1209/9): loss=17.615785430564568, w0=75.78774344856055, w1=12.820630409678241\n",
      "[-1.75397143  1.41840698]\n",
      "Gradient Descent(1210/9): loss=18.71265513027365, w0=77.015523450092, w1=11.82774552139412\n",
      "[ 4.96175649 -0.49823982]\n",
      "Gradient Descent(1211/9): loss=23.675543879464364, w0=73.54229390443426, w1=12.176513394492146\n",
      "[ 0.43095976  0.00575406]\n",
      "Gradient Descent(1212/9): loss=16.265896039338706, w0=73.24062207213123, w1=12.17248555027062\n",
      "[ 3.29003137 -3.15507902]\n",
      "Gradient Descent(1213/9): loss=16.241729374162336, w0=70.93760011289392, w1=14.38104086325611\n",
      "[-3.55869995 -1.87733144]\n",
      "Gradient Descent(1214/9): loss=18.568210759418676, w0=73.4286900807412, w1=15.695172868115941\n",
      "[ 1.99984789  0.92421801]\n",
      "Gradient Descent(1215/9): loss=17.84910155171442, w0=72.02879655986905, w1=15.048220263297614\n",
      "[-5.09325908  3.87676119]\n",
      "Gradient Descent(1216/9): loss=17.41626746485862, w0=75.59407791301996, w1=12.334487427648249\n",
      "[ 3.83797008 -1.60939521]\n",
      "Gradient Descent(1217/9): loss=18.68701663480685, w0=72.90749885889443, w1=13.461064071163683\n",
      "[ 2.74931347  1.68017806]\n",
      "Gradient Descent(1218/9): loss=15.460723172370525, w0=70.98297942660639, w1=12.284939429281039\n",
      "[-2.82736333  0.39402703]\n",
      "Gradient Descent(1219/9): loss=18.769856930040184, w0=72.96213375631812, w1=12.009120506735323\n",
      "[ 1.88232288  1.97373003]\n",
      "Gradient Descent(1220/9): loss=16.522249898573133, w0=71.64450774130826, w1=10.627509483524035\n",
      "[ 0.01585793 -3.13722481]\n",
      "Gradient Descent(1221/9): loss=20.813702408862405, w0=71.63340719165339, w1=12.82356685042921\n",
      "[-0.67574946 -1.04710763]\n",
      "Gradient Descent(1222/9): loss=16.979806100762975, w0=72.10643181306324, w1=13.556542191013158\n",
      "[-2.6738559   0.67337521]\n",
      "Gradient Descent(1223/9): loss=16.093905749070206, w0=73.97813094022807, w1=13.085179545751807\n",
      "[ 0.94727849  2.18171029]\n",
      "Gradient Descent(1224/9): loss=15.697786904677962, w0=73.31503599423195, w1=11.557982342034927\n",
      "[-1.31439403 -1.79244912]\n",
      "Gradient Descent(1225/9): loss=17.232634044243895, w0=74.23511181415203, w1=12.812696728329811\n",
      "[ 4.12357499 -2.02490506]\n",
      "Gradient Descent(1226/9): loss=16.05126197644485, w0=71.34860932451957, w1=14.230130268739867\n",
      "[-1.98021714 -0.91524845]\n",
      "Gradient Descent(1227/9): loss=17.5595720382227, w0=72.73476132346957, w1=14.870804182910174\n",
      "[-0.12254893  0.87694224]\n",
      "Gradient Descent(1228/9): loss=16.50978632666265, w0=72.82054557472152, w1=14.256944611428082\n",
      "[-2.9724517   0.41706496]\n",
      "Gradient Descent(1229/9): loss=15.799975417876741, w0=74.90126176340036, w1=13.964999141816168\n",
      "[ 2.59648712 -0.0530234 ]\n",
      "Gradient Descent(1230/9): loss=16.795410016861155, w0=73.08372077953227, w1=14.00211552281097\n",
      "[ 0.25832122  2.88562084]\n",
      "Gradient Descent(1231/9): loss=15.544432638897913, w0=72.90289592613038, w1=11.98218093755152\n",
      "[-2.36982187 -0.58360692]\n",
      "Gradient Descent(1232/9): loss=16.58363885778427, w0=74.5617712383099, w1=12.390705784405665\n",
      "[ 0.63565802  0.19398965]\n",
      "Gradient Descent(1233/9): loss=16.782576454209256, w0=74.11681062481415, w1=12.254913028313617\n",
      "[ 3.07462574 -2.17016796]\n",
      "Gradient Descent(1234/9): loss=16.474527504817573, w0=71.96457260690835, w1=13.774030603811518\n",
      "[-1.70773079 -0.62774658]\n",
      "Gradient Descent(1235/9): loss=16.31278436833401, w0=73.15998416125086, w1=14.213453208646412\n",
      "[-1.00964662  2.20494738]\n",
      "Gradient Descent(1236/9): loss=15.664045302899416, w0=73.86673679816326, w1=12.669990043177918\n",
      "[ 1.95638617 -2.47639501]\n",
      "Gradient Descent(1237/9): loss=15.877771440021094, w0=72.49726647679326, w1=14.403466548706678\n",
      "[ 1.16009986  0.50523686]\n",
      "Gradient Descent(1238/9): loss=16.129878713139536, w0=71.68519657733464, w1=14.049800743614984\n",
      "[-2.38089398  3.10262471]\n",
      "Gradient Descent(1239/9): loss=16.842386954796986, w0=73.35182236251222, w1=11.877963447972842\n",
      "[ 3.14295892 -2.82010928]\n",
      "Gradient Descent(1240/9): loss=16.670364003400756, w0=71.15175111788056, w1=13.852039944083812\n",
      "[-2.39092275  0.96949507]\n",
      "Gradient Descent(1241/9): loss=17.749649804453615, w0=72.82539704573493, w1=13.173393395811175\n",
      "[-1.53285169 -1.43373919]\n",
      "Gradient Descent(1242/9): loss=15.542561363081704, w0=73.89839322995556, w1=14.177010826209969\n",
      "[ 1.46100398  1.160196  ]\n",
      "Gradient Descent(1243/9): loss=15.81169312467851, w0=72.87569044607375, w1=13.364873625639795\n",
      "[-0.07781841  0.14190187]\n",
      "Gradient Descent(1244/9): loss=15.479940662126019, w0=72.93016333200546, w1=13.265542319092424\n",
      "[ 0.28390742 -1.8360313 ]\n",
      "Gradient Descent(1245/9): loss=15.474982473137349, w0=72.73142813872572, w1=14.550764229791763\n",
      "[ 1.71696636  2.26568617]\n",
      "Gradient Descent(1246/9): loss=16.117663515574243, w0=71.52955168560798, w1=12.964783914150592\n",
      "[-2.35174712  3.18282227]\n",
      "Gradient Descent(1247/9): loss=17.074964866484063, w0=73.1757746712775, w1=10.736808325337467\n",
      "[ 4.42472433 -5.20077677]\n",
      "Gradient Descent(1248/9): loss=19.154628742092026, w0=70.0784676437403, w1=14.377352064709683\n",
      "[-3.3270791   5.16094392]\n",
      "Gradient Descent(1249/9): loss=20.958339686615798, w0=72.40742301481956, w1=10.764691322669087\n",
      "[-0.3144101  -5.92370155]\n",
      "Gradient Descent(1250/9): loss=19.464497916230187, w0=72.62751008379698, w1=14.91128240520688\n",
      "[-1.02587433  0.86646647]\n",
      "Gradient Descent(1251/9): loss=16.632636581075758, w0=73.34562211755755, w1=14.304755873028233\n",
      "[ 1.37241859  2.55691834]\n",
      "Gradient Descent(1252/9): loss=15.727572657124052, w0=72.3849291051494, w1=12.514913034205101\n",
      "[-3.50825886  2.61079351]\n",
      "Gradient Descent(1253/9): loss=16.264440854063974, w0=74.84071030402622, w1=10.68735758034855\n",
      "[ 2.54185121 -4.87377332]\n",
      "Gradient Descent(1254/9): loss=20.480787711426455, w0=73.0614144597515, w1=14.098998907041993\n",
      "[-1.82060107  0.53169734]\n",
      "Gradient Descent(1255/9): loss=15.604675614688967, w0=74.3358352068619, w1=13.726810768190713\n",
      "[-0.4225899  -1.92970163]\n",
      "Gradient Descent(1256/9): loss=15.95920822508812, w0=74.63164813925427, w1=15.077601910794115\n",
      "[ 2.31264262 -0.0872838 ]\n",
      "Gradient Descent(1257/9): loss=17.55726886627959, w0=73.01279830566277, w1=15.13870056956812\n",
      "[ 1.83502036 -0.20794215]\n",
      "Gradient Descent(1258/9): loss=16.8015239505172, w0=71.72828405343687, w1=15.284260072915583\n",
      "[-1.30715151  0.485643  ]\n",
      "Gradient Descent(1259/9): loss=18.2396950507577, w0=72.64329011272525, w1=14.944309976107983\n",
      "[-0.34211419  2.31388374]\n",
      "Gradient Descent(1260/9): loss=16.670071775294268, w0=72.88277004739975, w1=13.324591358351146\n",
      "[ 0.40517219  1.32921177]\n",
      "Gradient Descent(1261/9): loss=15.482442107967104, w0=72.59914951628659, w1=12.39414312088051\n",
      "[-2.08189184  1.14004965]\n",
      "Gradient Descent(1262/9): loss=16.216472640221717, w0=74.05647380538747, w1=11.596108366250057\n",
      "[ 1.96401322 -1.25885307]\n",
      "Gradient Descent(1263/9): loss=17.45061263905897, w0=72.6816645496621, w1=12.477305513742365\n",
      "[-0.95201346  0.45340993]\n",
      "Gradient Descent(1264/9): loss=16.075727280747074, w0=73.34807397487549, w1=12.159918564926258\n",
      "[-1.49006148 -1.01279731]\n",
      "Gradient Descent(1265/9): loss=16.258282016634517, w0=74.3911170131151, w1=12.86887667991857\n",
      "[ 1.71849317 -0.4856443 ]\n",
      "Gradient Descent(1266/9): loss=16.174366474758187, w0=73.18817179628448, w1=13.208827688932557\n",
      "[-5.63380883  2.15603292]\n",
      "Gradient Descent(1267/9): loss=15.428168694668008, w0=77.1318379773767, w1=11.699604644682678\n",
      "[ 1.21219253 -1.68008429]\n",
      "Gradient Descent(1268/9): loss=24.335079258006274, w0=76.28330320839609, w1=12.87566364596974\n",
      "[ 2.25579213 -1.55620389]\n",
      "Gradient Descent(1269/9): loss=20.036525336849877, w0=74.70424871569202, w1=13.965006366869993\n",
      "[ 3.12529477 -1.34592593]\n",
      "Gradient Descent(1270/9): loss=16.498153688517945, w0=72.51654237704544, w1=14.907154521043434\n",
      "[ 0.68568859 -2.75618034]\n",
      "Gradient Descent(1271/9): loss=16.70684286407807, w0=72.03656036622264, w1=16.83648075752073\n",
      "[ 0.08310462  1.54429951]\n",
      "Gradient Descent(1272/9): loss=21.810313796100097, w0=71.97838713462828, w1=15.7554710985837\n",
      "[-2.80721677  0.91271947]\n",
      "Gradient Descent(1273/9): loss=18.840742610066208, w0=73.94343887017918, w1=15.116567468520692\n",
      "[-2.09140199 -0.66508438]\n",
      "Gradient Descent(1274/9): loss=16.936471150184662, w0=75.40742026256983, w1=15.58212653460018\n",
      "[ 1.05293392  4.87334687]\n",
      "Gradient Descent(1275/9): loss=19.829397840444763, w0=74.67036651558448, w1=12.17078372886634\n",
      "[ 2.2825375  -0.87620735]\n",
      "Gradient Descent(1276/9): loss=17.18983479702905, w0=73.07259026452967, w1=12.784128876084285\n",
      "[ 1.57202209  1.42505537]\n",
      "Gradient Descent(1277/9): loss=15.652299981567808, w0=71.97217480293376, w1=11.786590114653627\n",
      "[ 2.88496586  0.64815617]\n",
      "Gradient Descent(1278/9): loss=17.692727293897153, w0=69.95269869908486, w1=11.332880794625584\n",
      "[-5.79461436 -2.92985576]\n",
      "Gradient Descent(1279/9): loss=23.272217495185267, w0=74.00892875185801, w1=13.383779830037057\n",
      "[ 0.26839514 -2.20711985]\n",
      "Gradient Descent(1280/9): loss=15.646106727271881, w0=73.82105215686985, w1=14.928763725422067\n",
      "[-0.03346314  3.97280029]\n",
      "Gradient Descent(1281/9): loss=16.574695790013305, w0=73.8444763555875, w1=12.147803523824992\n",
      "[-0.5130688   0.04382862]\n",
      "Gradient Descent(1282/9): loss=16.424433590717676, w0=74.20362451689509, w1=12.117123489421383\n",
      "[ 2.43876779  0.19619165]\n",
      "Gradient Descent(1283/9): loss=16.727991518828535, w0=72.49648706560637, w1=11.979789337807247\n",
      "[-2.26759605 -4.07989388]\n",
      "Gradient Descent(1284/9): loss=16.82872375653356, w0=74.08380430140615, w1=14.835715056462435\n",
      "[ 1.26678689  2.81422799]\n",
      "Gradient Descent(1285/9): loss=16.61721644692523, w0=73.19705347874711, w1=12.865755464668549\n",
      "[ 2.03169495  0.70995685]\n",
      "Gradient Descent(1286/9): loss=15.579051204940752, w0=71.77486701061987, w1=12.36878567069076\n",
      "[-3.52916512 -1.5319702 ]\n",
      "Gradient Descent(1287/9): loss=17.15673104022476, w0=74.24528259583754, w1=13.44116480996526\n",
      "[ 3.19495402 -1.81715895]\n",
      "Gradient Descent(1288/9): loss=15.839174318180268, w0=72.00881478173011, w1=14.713176075599044\n",
      "[-2.14895712 -1.67501037]\n",
      "Gradient Descent(1289/9): loss=16.97235442911292, w0=73.51308476345326, w1=15.885683335128197\n",
      "[-2.00164619  1.83291967]\n",
      "Gradient Descent(1290/9): loss=18.30425201296845, w0=74.91423709324694, w1=14.60263956908367\n",
      "[ 2.10870685  3.59514515]\n",
      "Gradient Descent(1291/9): loss=17.329081040363228, w0=73.43814229761159, w1=12.086037962540065\n",
      "[-0.48139493 -0.67841798]\n",
      "Gradient Descent(1292/9): loss=16.367451883225353, w0=73.77511874670552, w1=12.560930550991772\n",
      "[ 1.47894002 -0.25854999]\n",
      "Gradient Descent(1293/9): loss=15.923743097517168, w0=72.73986073310807, w1=12.74191554536616\n",
      "[-1.5917458  -0.72564417]\n",
      "Gradient Descent(1294/9): loss=15.81155193889935, w0=73.85408279399203, w1=13.24986646381448\n",
      "[-0.32951275 -0.75365796]\n",
      "Gradient Descent(1295/9): loss=15.56919251044564, w0=74.0847417199446, w1=13.77742703335682\n",
      "[ 0.28231482  2.36169146]\n",
      "Gradient Descent(1296/9): loss=15.742902772931847, w0=73.88712134852402, w1=12.124243008712853\n",
      "[ 2.31237894 -1.58931918]\n",
      "Gradient Descent(1297/9): loss=16.480479283910025, w0=72.26845609141877, w1=13.236766436109733\n",
      "[-3.08597556  0.48365584]\n",
      "Gradient Descent(1298/9): loss=15.941189415005097, w0=74.42863898585821, w1=12.898207350179337\n",
      "[ 2.67878561 -3.63327103]\n",
      "Gradient Descent(1299/9): loss=16.198753267267957, w0=72.55348906006424, w1=15.441497067993444\n",
      "[ 0.84292276  0.83065819]\n",
      "Gradient Descent(1300/9): loss=17.584307812805207, w0=71.96344312979404, w1=14.860036333187907\n",
      "[ 1.22572137 -0.61114798]\n",
      "Gradient Descent(1301/9): loss=17.223621915632016, w0=71.10543816768062, w1=15.287839921312086\n",
      "[-0.38560166 -0.00483698]\n",
      "Gradient Descent(1302/9): loss=19.415281118996678, w0=71.37535932839131, w1=15.291225804312484\n",
      "[-2.02941907  8.84989701]\n",
      "Gradient Descent(1303/9): loss=18.86711957893225, w0=72.79595268077381, w1=9.096297899570327\n",
      "[-1.98597002 -4.13130793]\n",
      "Gradient Descent(1304/9): loss=25.11703608598308, w0=74.18613169209956, w1=11.988213450698865\n",
      "[ 1.09757782 -1.77955937]\n",
      "Gradient Descent(1305/9): loss=16.896191544358658, w0=73.4178272171771, w1=13.23390501209514\n",
      "[-1.20812493 -2.39847001]\n",
      "Gradient Descent(1306/9): loss=15.423774764565282, w0=74.2635146704366, w1=14.912834020214227\n",
      "[-0.96651497  0.99046869]\n",
      "Gradient Descent(1307/9): loss=16.88286157908958, w0=74.94007514936501, w1=14.219505939812624\n",
      "[-0.02895482  1.19803587]\n",
      "Gradient Descent(1308/9): loss=17.01444517583569, w0=74.96034352049813, w1=13.380880827536437\n",
      "[ 3.87830629 -1.43887565]\n",
      "Gradient Descent(1309/9): loss=16.77925205062675, w0=72.24552912098902, w1=14.388093780015843\n",
      "[-0.87314893  1.34408089]\n",
      "Gradient Descent(1310/9): loss=16.34803001941328, w0=72.85673337401528, w1=13.447237157065512\n",
      "[ 0.53224838 -0.26946959]\n",
      "Gradient Descent(1311/9): loss=15.481982138933075, w0=72.48415950743315, w1=13.63586587253414\n",
      "[-3.63843837  1.06199227]\n",
      "Gradient Descent(1312/9): loss=15.725937465746718, w0=75.03106636672447, w1=12.892471284800346\n",
      "[ 1.8076493  -2.93079235]\n",
      "Gradient Descent(1313/9): loss=17.067149224831148, w0=73.76571185653918, w1=14.944025930039833\n",
      "[ 0.60961099  1.19716587]\n",
      "Gradient Descent(1314/9): loss=16.56928770809675, w0=73.33898416028995, w1=14.106009821249515\n",
      "[ 1.76652005 -1.27416356]\n",
      "Gradient Descent(1315/9): loss=15.58302737589788, w0=72.10242012729745, w1=14.997924316223655\n",
      "[ 0.57793676 -0.64789554]\n",
      "Gradient Descent(1316/9): loss=17.248209885825542, w0=71.69786439794856, w1=15.451451196199866\n",
      "[-3.37697665  1.90480869]\n",
      "Gradient Descent(1317/9): loss=18.60346467795308, w0=74.06174805095932, w1=14.11808511135673\n",
      "[ 2.79396496 -1.10722833]\n",
      "Gradient Descent(1318/9): loss=15.884426126445295, w0=72.10597257819389, w1=14.893144942442476\n",
      "[-2.46278946  2.71381782]\n",
      "Gradient Descent(1319/9): loss=17.090395512278047, w0=73.82992520051397, w1=12.993472466987154\n",
      "[-0.78848481 -1.27032359]\n",
      "Gradient Descent(1320/9): loss=15.647752236422862, w0=74.38186456817964, w1=13.882698980559462\n",
      "[ 0.94386448 -0.13540837]\n",
      "Gradient Descent(1321/9): loss=16.058896460323112, w0=73.72115943180758, w1=13.977484837310435\n",
      "[ 1.23805788  3.43260017]\n",
      "Gradient Descent(1322/9): loss=15.601042461755153, w0=72.85451891783147, w1=11.574664715571998\n",
      "[-1.73396237 -0.59414545]\n",
      "Gradient Descent(1323/9): loss=17.29702881069208, w0=74.06829257855568, w1=11.990566528205674\n",
      "[ 3.09510602 -0.29117234]\n",
      "Gradient Descent(1324/9): loss=16.794490529510377, w0=71.90171836733228, w1=12.194387164712008\n",
      "[ 0.85470662 -5.3281843 ]\n",
      "Gradient Descent(1325/9): loss=17.181033874373224, w0=71.3034237350483, w1=15.924116173697382\n",
      "[-3.16937276  0.47380691]\n",
      "Gradient Descent(1326/9): loss=20.354484363313286, w0=73.5219846641968, w1=15.592451336089647\n",
      "[ 2.20740031  0.87705315]\n",
      "Gradient Descent(1327/9): loss=17.643726989861438, w0=71.97680444425055, w1=14.97851413446476\n",
      "[-0.1250788   2.31438799]\n",
      "Gradient Descent(1328/9): loss=17.376490466609532, w0=72.06435960111052, w1=13.358442543543623\n",
      "[ 1.56209483 -0.93984051]\n",
      "Gradient Descent(1329/9): loss=16.149152911084876, w0=70.97089321978632, w1=14.01633089913987\n",
      "[ 0.57014815  2.62851577]\n",
      "Gradient Descent(1330/9): loss=18.228098918604143, w0=70.57178951432896, w1=12.176369858581342\n",
      "[-2.00565326 -3.89650242]\n",
      "Gradient Descent(1331/9): loss=19.94024144507135, w0=71.97574679353175, w1=14.903921551836806\n",
      "[-4.02041835 -0.83541272]\n",
      "Gradient Descent(1332/9): loss=17.268866613334346, w0=74.79003963572652, w1=15.488710459300645\n",
      "[ 2.38885557  1.37879933]\n",
      "Gradient Descent(1333/9): loss=18.52310838648968, w0=73.1178407381261, w1=14.523550928485369\n",
      "[-2.00030322 -0.36453613]\n",
      "Gradient Descent(1334/9): loss=15.946189574843972, w0=74.51805299554549, w1=14.778726217506435\n",
      "[ 2.38387879 -0.52326936]\n",
      "Gradient Descent(1335/9): loss=16.97885461696503, w0=72.84933784219925, w1=15.145014772070159\n",
      "[-1.99322228  1.84333938]\n",
      "Gradient Descent(1336/9): loss=16.871331343392942, w0=74.2445934404362, w1=13.854677204953363\n",
      "[ 1.60599139 -0.32294878]\n",
      "Gradient Descent(1337/9): loss=15.908075250015775, w0=73.12039946971281, w1=14.08074135376551\n",
      "[-2.69443786  1.60766002]\n",
      "Gradient Descent(1338/9): loss=15.581560784056133, w0=75.0065059732321, w1=12.955379340306914\n",
      "[ 3.36385549 -1.01433596]\n",
      "Gradient Descent(1339/9): loss=16.989822394999273, w0=72.65180712935239, w1=13.66541451241084\n",
      "[ 1.45285098 -0.42758996]\n",
      "Gradient Descent(1340/9): loss=15.609286254513954, w0=71.63481144304875, w1=13.964727486715828\n",
      "[ 1.24231615  0.01835891]\n",
      "Gradient Descent(1341/9): loss=16.879831592616455, w0=70.76519013594176, w1=13.951876250902043\n",
      "[-2.70558981  0.72482316]\n",
      "Gradient Descent(1342/9): loss=18.694599628833338, w0=72.6591030000977, w1=13.444500036087293\n",
      "[-0.77368789  0.53386102]\n",
      "Gradient Descent(1343/9): loss=15.588005408002497, w0=73.2006845262275, w1=13.070797322588072\n",
      "[-0.15100572 -0.26036921]\n",
      "Gradient Descent(1344/9): loss=15.473840266858373, w0=73.30638853334008, w1=13.253055771139111\n",
      "[-0.42981094  1.28373837]\n",
      "Gradient Descent(1345/9): loss=15.411652197663708, w0=73.60725618960106, w1=12.35443891392939\n",
      "[ 2.62106607 -1.44498632]\n",
      "Gradient Descent(1346/9): loss=16.06809727395525, w0=71.77250994219055, w1=13.36592933984995\n",
      "[-2.25907387 -2.18536362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1347/9): loss=16.549708493225964, w0=73.35386165427933, w1=14.895683873264282\n",
      "[-0.19258678  3.40672106]\n",
      "Gradient Descent(1348/9): loss=16.390171806786395, w0=73.4886723998293, w1=12.510979131978887\n",
      "[ 0.78443851 -0.54253962]\n",
      "Gradient Descent(1349/9): loss=15.87407383371674, w0=72.93956544288008, w1=12.890756866301043\n",
      "[ 3.29835526  1.27599781]\n",
      "Gradient Descent(1350/9): loss=15.622106485306631, w0=70.63071675863996, w1=11.997558395934115\n",
      "[-5.98401156 -1.37159191]\n",
      "Gradient Descent(1351/9): loss=20.030609250983158, w0=74.81952485039695, w1=12.957672732472819\n",
      "[ 0.9607355   0.61476544]\n",
      "Gradient Descent(1352/9): loss=16.68588261968898, w0=74.14700999712038, w1=12.527336927588337\n",
      "[ 3.45124631 -1.66518948]\n",
      "Gradient Descent(1353/9): loss=16.2032769859973, w0=71.73113758159933, w1=13.692969565617583\n",
      "[ 0.59409174  0.86391969]\n",
      "Gradient Descent(1354/9): loss=16.629774743199278, w0=71.31527336222008, w1=13.088225779799679\n",
      "[-3.73594052  1.27250132]\n",
      "Gradient Descent(1355/9): loss=17.42004398948467, w0=73.93043172480075, w1=12.19747485903613\n",
      "[ 1.59113528 -2.99246399]\n",
      "Gradient Descent(1356/9): loss=16.410526782965203, w0=72.81663703070414, w1=14.292199655348544\n",
      "[ 1.04192376  0.01778045]\n",
      "Gradient Descent(1357/9): loss=15.829856082415796, w0=72.08729039977757, w1=14.279753339073912\n",
      "[-0.05258455  1.36153152]\n",
      "Gradient Descent(1358/9): loss=16.43390050480172, w0=72.12409958774819, w1=13.326681274626248\n",
      "[-0.45653088  1.93349201]\n",
      "Gradient Descent(1359/9): loss=16.08183937741641, w0=72.44367120138651, w1=11.973236868133753\n",
      "[-0.0989091  -1.95035714]\n",
      "Gradient Descent(1360/9): loss=16.88208539765677, w0=72.51290757223325, w1=13.338486865782974\n",
      "[ 0.57114009 -0.625061  ]\n",
      "Gradient Descent(1361/9): loss=15.700851969362288, w0=72.11310950593715, w1=13.776029563017207\n",
      "[-2.33835488 -0.79012433]\n",
      "Gradient Descent(1362/9): loss=16.12694886456413, w0=73.7499579188435, w1=14.329116596654742\n",
      "[-0.92428547 -0.56415729]\n",
      "Gradient Descent(1363/9): loss=15.850615962434578, w0=74.39695774959222, w1=14.724026699980765\n",
      "[ 3.31425363  1.3743023 ]\n",
      "Gradient Descent(1364/9): loss=16.768390793977467, w0=72.0769802100271, w1=13.76201509140899\n",
      "[-2.22909125 -0.66851879]\n",
      "Gradient Descent(1365/9): loss=16.16620892639339, w0=73.63734408213946, w1=14.229978246827725\n",
      "[-1.40708268 -0.23077879]\n",
      "Gradient Descent(1366/9): loss=15.726306625563907, w0=74.62230195699976, w1=14.39152340063518\n",
      "[ 0.7083862  -0.13780983]\n",
      "Gradient Descent(1367/9): loss=16.683884139648516, w0=74.12643161443646, w1=14.487990284872541\n",
      "[ 0.3602202  -1.41323241]\n",
      "Gradient Descent(1368/9): loss=16.240736107424215, w0=73.87427747757336, w1=15.477252970011506\n",
      "[ 0.38106991  3.0372855 ]\n",
      "Gradient Descent(1369/9): loss=17.549378202311242, w0=73.60752854111081, w1=13.351153117622006\n",
      "[ 0.13003825  1.55353556]\n",
      "Gradient Descent(1370/9): loss=15.443326148523878, w0=73.51650176291807, w1=12.263678228164167\n",
      "[-3.08125798 -1.06424519]\n",
      "Gradient Descent(1371/9): loss=16.15002833987527, w0=75.67338234583848, w1=13.008649862293968\n",
      "[ 2.69197313 -1.23114219]\n",
      "Gradient Descent(1372/9): loss=18.32775360622612, w0=73.78900115764728, w1=13.87044939417631\n",
      "[ 1.52445197  1.78232723]\n",
      "Gradient Descent(1373/9): loss=15.584777239592988, w0=72.72188478086989, w1=12.622820331889018\n",
      "[-2.98020277  0.71043568]\n",
      "Gradient Descent(1374/9): loss=15.916633198246299, w0=74.80802671693056, w1=12.12551535681049\n",
      "[ 2.55938189 -2.10802047]\n",
      "Gradient Descent(1375/9): loss=17.449069275831278, w0=73.01645939583422, w1=13.601129687554108\n",
      "[ 0.15238112  1.85336234]\n",
      "Gradient Descent(1376/9): loss=15.431751692378963, w0=72.90979260863715, w1=12.303776052334548\n",
      "[-2.89433075 -0.88639262]\n",
      "Gradient Descent(1377/9): loss=16.15107875231774, w0=74.93582413216164, w1=12.92425088303588\n",
      "[ 2.54053633 -0.82579957]\n",
      "Gradient Descent(1378/9): loss=16.88807793902047, w0=73.15744869944949, w1=13.502310578634741\n",
      "[ 0.62438974 -0.41464659]\n",
      "Gradient Descent(1379/9): loss=15.395455688046393, w0=72.72037588488861, w1=13.792563193577271\n",
      "[-1.24944909 -2.61275092]\n",
      "Gradient Descent(1380/9): loss=15.599303241691121, w0=73.59499024559759, w1=15.621488839683355\n",
      "[-1.46334962 -0.27095838]\n",
      "Gradient Descent(1381/9): loss=17.724811996301835, w0=74.61933497753557, w1=15.811159702787858\n",
      "[ 0.91923881  1.22251811]\n",
      "Gradient Descent(1382/9): loss=18.982070827812276, w0=73.97586780742361, w1=14.955397024995253\n",
      "[ 1.08657962 -2.10739722]\n",
      "Gradient Descent(1383/9): loss=16.707235414115985, w0=73.21526207588505, w1=16.430575080165525\n",
      "[-0.4164729   2.86743154]\n",
      "Gradient Descent(1384/9): loss=19.742776736174832, w0=73.50679310489953, w1=14.423373002874813\n",
      "[-0.78208324 -1.03872829]\n",
      "Gradient Descent(1385/9): loss=15.853792555723084, w0=74.05425137394043, w1=15.150482806886648\n",
      "[ 2.23798187  4.60873225]\n",
      "Gradient Descent(1386/9): loss=17.07067506347251, w0=72.48766406562575, w1=11.924370229069108\n",
      "[ 0.60093907 -2.15868678]\n",
      "Gradient Descent(1387/9): loss=16.92045848765539, w0=72.06700671620887, w1=13.435450976940967\n",
      "[ 0.53473999  0.57157044]\n",
      "Gradient Descent(1388/9): loss=16.1395279665467, w0=71.69268872301933, w1=13.035351670802463\n",
      "[-1.64238738 -1.11845408]\n",
      "Gradient Descent(1389/9): loss=16.766590120229672, w0=72.84235988718928, w1=13.818269524442046\n",
      "[ 0.53391781 -0.20792078]\n",
      "Gradient Descent(1390/9): loss=15.545152492052509, w0=72.46861742069552, w1=13.963814072712433\n",
      "[-2.74138695 -0.82704009]\n",
      "Gradient Descent(1391/9): loss=15.843628892700526, w0=74.38758828748279, w1=14.542742133514128\n",
      "[ 1.12365874 -1.63523337]\n",
      "Gradient Descent(1392/9): loss=16.54895691068838, w0=73.60102717296701, w1=15.6874054922798\n",
      "[ 2.56658287  0.89603153]\n",
      "Gradient Descent(1393/9): loss=17.869998979419325, w0=71.80441916106483, w1=15.060183423706006\n",
      "[-2.16883878  3.75041118]\n",
      "Gradient Descent(1394/9): loss=17.744141498651036, w0=73.32260630784067, w1=12.434895598898557\n",
      "[ 0.20272868 -0.59928347]\n",
      "Gradient Descent(1395/9): loss=15.932120374016236, w0=73.18069623466603, w1=12.854394025921795\n",
      "[ 0.31068361 -0.88200927]\n",
      "Gradient Descent(1396/9): loss=15.587809462394697, w0=72.96321770444659, w1=13.47180051603512\n",
      "[-0.1198688  -1.09764246]\n",
      "Gradient Descent(1397/9): loss=15.440601834305097, w0=73.04712586414503, w1=14.240150236365869\n",
      "[-0.09044872  0.11927798]\n",
      "Gradient Descent(1398/9): loss=15.70547486056683, w0=73.11043996916068, w1=14.15665564748495\n",
      "[-3.05727065  0.43444644]\n",
      "Gradient Descent(1399/9): loss=15.63184675350826, w0=75.25052942675012, w1=13.852543142529685\n",
      "[ 2.02318563  0.06298108]\n",
      "Gradient Descent(1400/9): loss=17.36954554415986, w0=73.83429948769354, w1=13.808456383032372\n",
      "[ 0.35538629  0.60632233]\n",
      "Gradient Descent(1401/9): loss=15.585928073982354, w0=73.58552908213836, w1=13.384030750746966\n",
      "[ 2.16862312 -2.37654836]\n",
      "Gradient Descent(1402/9): loss=15.43298270574184, w0=72.06749290093921, w1=15.047614601697418\n",
      "[-0.70410911 -0.3891638 ]\n",
      "Gradient Descent(1403/9): loss=17.367110641107192, w0=72.56036927466405, w1=15.320029264182388\n",
      "[ 0.13976569  0.31112799]\n",
      "Gradient Descent(1404/9): loss=17.34832068670368, w0=72.46253329236328, w1=15.102239672358799\n",
      "[ -1.48844914e-03   1.96887552e+00]\n",
      "Gradient Descent(1405/9): loss=17.04778878017592, w0=72.46357520676113, w1=13.724026808750645\n",
      "[-5.12471574 -0.74606163]\n",
      "Gradient Descent(1406/9): loss=15.760470525711732, w0=76.05087622462307, w1=14.246269952631426\n",
      "[ 5.20899563 -3.85333696]\n",
      "Gradient Descent(1407/9): loss=19.480091375286, w0=72.40457928682441, w1=16.943605821602844\n",
      "[ 2.82411032  0.73783323]\n",
      "Gradient Descent(1408/9): loss=21.780631798354293, w0=70.42770206620088, w1=16.42712255839428\n",
      "[-1.38207487  1.28039426]\n",
      "Gradient Descent(1409/9): loss=23.837109447092885, w0=71.3951544766364, w1=15.530846579483432\n",
      "[-1.06047112  0.31802334]\n",
      "Gradient Descent(1410/9): loss=19.29212256607218, w0=72.13748425736581, w1=15.308230242192776\n",
      "[-0.65845211  2.11242162]\n",
      "Gradient Descent(1411/9): loss=17.726300683188917, w0=72.59840073248537, w1=13.829535106932575\n",
      "[-2.37519931  1.65155322]\n",
      "Gradient Descent(1412/9): loss=15.688950737979033, w0=74.26104025157085, w1=12.673447854428723\n",
      "[ 1.75323541  1.2562549 ]\n",
      "Gradient Descent(1413/9): loss=16.178578009987216, w0=73.03377546706855, w1=11.794069424414586\n",
      "[ 1.66560501 -1.93877211]\n",
      "Gradient Descent(1414/9): loss=16.840422158224456, w0=71.86785195704958, w1=13.151209900351944\n",
      "[-0.96670042 -0.16097232]\n",
      "Gradient Descent(1415/9): loss=16.456682713163353, w0=72.54454224818426, w1=13.263890525283111\n",
      "[-0.88142148 -1.15286503]\n",
      "Gradient Descent(1416/9): loss=15.689962424977258, w0=73.1615372840685, w1=14.070896047259668\n",
      "[-1.91679671  0.26485873]\n",
      "Gradient Descent(1417/9): loss=15.569399757322893, w0=74.50329498167073, w1=13.885494934235448\n",
      "[ 0.25996739  0.14271843]\n",
      "Gradient Descent(1418/9): loss=16.19950908902835, w0=74.32131780613203, w1=13.785592036561008\n",
      "[ 1.20626228  0.73799174]\n",
      "Gradient Descent(1419/9): loss=15.960440103224286, w0=73.47693421127806, w1=13.268997817845394\n",
      "[-1.99959003  0.19149723]\n",
      "Gradient Descent(1420/9): loss=15.424834928121566, w0=74.87664722989756, w1=13.134949753993416\n",
      "[ 0.98855906  2.05918485]\n",
      "Gradient Descent(1421/9): loss=16.69782809527825, w0=74.1846558899565, w1=11.693520357425253\n",
      "[ 0.24596878 -0.53110076]\n",
      "Gradient Descent(1422/9): loss=17.37783236728859, w0=74.01247774139098, w1=12.065290891088141\n",
      "[-1.4923799  -0.68689886]\n",
      "Gradient Descent(1423/9): loss=16.64434319598519, w0=75.05714367420521, w1=12.54612009221934\n",
      "[ 2.20357492  0.86080553]\n",
      "Gradient Descent(1424/9): loss=17.376160532550113, w0=73.51464123222891, w1=11.943556219417356\n",
      "[-1.21897608 -2.556897  ]\n",
      "Gradient Descent(1425/9): loss=16.590134317422375, w0=74.36792449155845, w1=13.733384122158915\n",
      "[ 0.14983795  2.6234576 ]\n",
      "Gradient Descent(1426/9): loss=15.994803204941109, w0=74.26303792702774, w1=11.896963804410646\n",
      "[ 2.34187627 -1.50756502]\n",
      "Gradient Descent(1427/9): loss=17.108027320597554, w0=72.62372454009284, w1=12.952259317267837\n",
      "[ 1.60731425 -0.71056494]\n",
      "Gradient Descent(1428/9): loss=15.74957358357021, w0=71.49860456846028, w1=13.449654773826783\n",
      "[ 0.99754248 -1.21422777]\n",
      "Gradient Descent(1429/9): loss=16.997921944101343, w0=70.8003248307365, w1=14.299614216293099\n",
      "[-1.18369577  0.73597025]\n",
      "Gradient Descent(1430/9): loss=18.831020760851143, w0=71.6289118670136, w1=13.784435041894419\n",
      "[-0.59797072  0.1545801 ]\n",
      "Gradient Descent(1431/9): loss=16.818445177387858, w0=72.04749137374404, w1=13.676228975186014\n",
      "[-1.62660655  2.31570865]\n",
      "Gradient Descent(1432/9): loss=16.18199189977328, w0=73.18611595958728, w1=12.05523292260143\n",
      "[ 1.9613145  -1.50901306]\n",
      "Gradient Descent(1433/9): loss=16.406269880837122, w0=71.81319580744491, w1=13.11154206489183\n",
      "[-1.18050918 -0.75774347]\n",
      "Gradient Descent(1434/9): loss=16.54993761131471, w0=72.63955223280665, w1=13.641962495764199\n",
      "[-3.04636215 -1.60429758]\n",
      "Gradient Descent(1435/9): loss=15.613150307426084, w0=74.77200573440187, w1=14.764970800717421\n",
      "[ 1.94713989  1.20890397]\n",
      "Gradient Descent(1436/9): loss=17.304198162006827, w0=73.40900781408503, w1=13.918738022751695\n",
      "[-2.45685483 -0.45473932]\n",
      "Gradient Descent(1437/9): loss=15.488881974244098, w0=75.12880619373041, w1=14.237055545333309\n",
      "[ 1.49904227 -0.54163823]\n",
      "Gradient Descent(1438/9): loss=17.35607216056043, w0=74.0794766037706, w1=14.616202303303666\n",
      "[ 0.76777155  2.06499272]\n",
      "Gradient Descent(1439/9): loss=16.340240495319136, w0=73.54203651945201, w1=13.170707401949937\n",
      "[-0.87831396 -0.0136593 ]\n",
      "Gradient Descent(1440/9): loss=15.464410330910276, w0=74.1568562945503, w1=13.180268911033506\n",
      "[-2.81058566  0.06158813]\n",
      "Gradient Descent(1441/9): loss=15.803048877387724, w0=76.12426625946983, w1=13.137157219764566\n",
      "[ 2.17239914 -1.80055096]\n",
      "Gradient Descent(1442/9): loss=19.44998421416666, w0=74.60358686106198, w1=14.397542892246278\n",
      "[ 3.11774515  1.36024733]\n",
      "Gradient Descent(1443/9): loss=16.664705264357067, w0=72.42116525498352, w1=13.44536976210778\n",
      "[ 2.21074046  0.03373214]\n",
      "Gradient Descent(1444/9): loss=15.767329748242913, w0=70.8736469317649, w1=13.421757265700922\n",
      "[-3.22144732  1.98166777]\n",
      "Gradient Descent(1445/9): loss=18.316432977708367, w0=73.128660057327, w1=12.034589825873422\n",
      "[-0.47452425 -2.92608141]\n",
      "Gradient Descent(1446/9): loss=16.443733301713912, w0=73.4608270353771, w1=14.082846811699744\n",
      "[-2.83951462  1.96735897]\n",
      "Gradient Descent(1447/9): loss=15.581702052080248, w0=75.4484872663038, w1=12.705695531079332\n",
      "[ 1.84327552 -1.72071796]\n",
      "Gradient Descent(1448/9): loss=18.00651469144401, w0=74.15819440312352, w1=13.910198103834531\n",
      "[-0.2882397  -0.91299674]\n",
      "Gradient Descent(1449/9): loss=15.852030215951064, w0=74.35996219502259, w1=14.549295823057298\n",
      "[ 0.38344949 -0.22877743]\n",
      "Gradient Descent(1450/9): loss=16.52611302730286, w0=74.0915475538226, w1=14.709440024512649\n",
      "[ 3.81818553  0.47030743]\n",
      "Gradient Descent(1451/9): loss=16.46010610142341, w0=71.41881768273441, w1=14.38022482563533\n",
      "[-3.7119317   1.91942704]\n",
      "Gradient Descent(1452/9): loss=17.549357255944617, w0=74.01716987312798, w1=13.036625894631385\n",
      "[ 3.27635621 -0.23038291]\n",
      "Gradient Descent(1453/9): loss=15.745594451421965, w0=71.72372052771058, w1=13.197893932567094\n",
      "[-3.03537023  0.90367733]\n",
      "Gradient Descent(1454/9): loss=16.658365038078575, w0=73.84847968719913, w1=12.565319802245794\n",
      "[ 1.77592826  0.49810539]\n",
      "Gradient Descent(1455/9): loss=15.957711925285345, w0=72.60532990685594, w1=12.216646027651016\n",
      "[-2.51259937 -1.17151809]\n",
      "Gradient Descent(1456/9): loss=16.420635780322172, w0=74.36414946857403, w1=13.036708688148131\n",
      "[ 2.57051551  1.52093946]\n",
      "Gradient Descent(1457/9): loss=16.0567074436791, w0=72.56478861420166, w1=11.972051063752513\n",
      "[-3.52060761 -0.40862598]\n",
      "Gradient Descent(1458/9): loss=16.788227022666657, w0=75.02921393860852, w1=12.258089253062785\n",
      "[-0.04695728  0.55067193]\n",
      "Gradient Descent(1459/9): loss=17.637688520585968, w0=75.06208403153724, w1=11.872618901608323\n",
      "[ 1.3286799  -2.62632968]\n",
      "Gradient Descent(1460/9): loss=18.240461162509106, w0=74.13200810258502, w1=13.711049680572012\n",
      "[ 1.49198148 -0.98028624]\n",
      "Gradient Descent(1461/9): loss=15.763840485335098, w0=73.08762106752681, w1=14.39725004547983\n",
      "[ 0.35461277  1.09470037]\n",
      "Gradient Descent(1462/9): loss=15.82810553996592, w0=72.83939212913296, w1=13.63095978818379\n",
      "[ 0.27293846 -0.35461068]\n",
      "Gradient Descent(1463/9): loss=15.500624452465683, w0=72.64833520966005, w1=13.879187264163663\n",
      "[-1.35262759  0.77580206]\n",
      "Gradient Descent(1464/9): loss=15.674069091691246, w0=73.59517452511649, w1=13.33612581910092\n",
      "[ 0.9405824   0.71161519]\n",
      "Gradient Descent(1465/9): loss=15.441572968270838, w0=72.93676684708053, w1=12.837995188005577\n",
      "[ 0.26317738 -0.86918953]\n",
      "Gradient Descent(1466/9): loss=15.655568283747765, w0=72.7525426802216, w1=13.446427859883958\n",
      "[-0.58115625 -1.72481407]\n",
      "Gradient Descent(1467/9): loss=15.532987585380932, w0=73.15935205562306, w1=14.653797709999237\n",
      "[ 1.08016319  0.22347945]\n",
      "Gradient Descent(1468/9): loss=16.084180520575377, w0=72.40323782174359, w1=14.497362095004899\n",
      "[ 1.48961412 -0.05941625]\n",
      "Gradient Descent(1469/9): loss=16.3003524386678, w0=71.36050793541563, w1=14.538953473311624\n",
      "[-5.04439426  1.13366161]\n",
      "Gradient Descent(1470/9): loss=17.815928634099283, w0=74.89158392090208, w1=13.745390344705196\n",
      "[-0.52633916 -3.13059808]\n",
      "Gradient Descent(1471/9): loss=16.69744204807185, w0=75.26002133283116, w1=15.936809002500604\n",
      "[ 2.51529495  2.64629685]\n",
      "Gradient Descent(1472/9): loss=20.337322929008483, w0=73.49931486745648, w1=14.084401206435945\n",
      "[ 0.40208197  1.0928274 ]\n",
      "Gradient Descent(1473/9): loss=15.589805238554984, w0=73.2178574888462, w1=13.319422025548008\n",
      "[ 0.26171217  0.27188547]\n",
      "Gradient Descent(1474/9): loss=15.401627281597452, w0=73.0346589664462, w1=13.129102199516982\n",
      "[ 0.24028047 -2.35836887]\n",
      "Gradient Descent(1475/9): loss=15.480960298267846, w0=72.86646263825135, w1=14.779960405572421\n",
      "[ 0.71966669  2.26722228]\n",
      "Gradient Descent(1476/9): loss=16.322571015205657, w0=72.3626959559841, w1=13.192904811175461\n",
      "[ 0.67157791 -1.02063869]\n",
      "Gradient Descent(1477/9): loss=15.860608149855363, w0=71.89259141863728, w1=13.907351893330079\n",
      "[-3.92929001 -1.37966624]\n",
      "Gradient Descent(1478/9): loss=16.459189324075762, w0=74.64309442731184, w1=14.873118264306635\n",
      "[-0.40537879  0.93033202]\n",
      "Gradient Descent(1479/9): loss=17.266810887886507, w0=74.9268595798459, w1=14.221885853427436\n",
      "[ 2.31731206  0.16252201]\n",
      "Gradient Descent(1480/9): loss=16.9945411267465, w0=73.30474113596117, w1=14.108120444693258\n",
      "[-1.4684582   0.19138317]\n",
      "Gradient Descent(1481/9): loss=15.583394708988301, w0=74.33266187923428, w1=13.974152227053066\n",
      "[ 2.56146555 -2.03711376]\n",
      "Gradient Descent(1482/9): loss=16.047613488986634, w0=72.53963599396647, w1=15.400131859220386\n",
      "[-3.13569638 -2.34033322]\n",
      "Gradient Descent(1483/9): loss=17.514366942348833, w0=74.73462346174975, w1=17.038365115440758\n",
      "[-2.99803921  1.17620092]\n",
      "Gradient Descent(1484/9): loss=22.755702666783442, w0=76.83325091130021, w1=16.215024469627902\n",
      "[ 5.71792311  6.2894116 ]\n",
      "Gradient Descent(1485/9): loss=25.390278395981277, w0=72.83070473694335, w1=11.812436352810305\n",
      "[ 0.19230271 -2.10433987]\n",
      "Gradient Descent(1486/9): loss=16.883077753304054, w0=72.69609284003246, w1=13.285474261846739\n",
      "[ 0.57006241 -0.52349459]\n",
      "Gradient Descent(1487/9): loss=15.583451956294518, w0=72.29704915198597, w1=13.65192047827295\n",
      "[-2.41842694 -0.33447735]\n",
      "Gradient Descent(1488/9): loss=15.897593413567641, w0=73.98994800694109, w1=13.886054620635635\n",
      "[ 1.1694211   0.58741422]\n",
      "Gradient Descent(1489/9): loss=15.710670954451334, w0=73.17135323740693, w1=13.4748646649205\n",
      "[-1.88738201 -1.18327062]\n",
      "Gradient Descent(1490/9): loss=15.393411170306548, w0=74.49252064560899, w1=14.303154102079674\n",
      "[-1.38995603 -1.24499818]\n",
      "Gradient Descent(1491/9): loss=16.443235312484468, w0=75.46548986914476, w1=15.174652830511565\n",
      "[ 3.65239469  0.54963718]\n",
      "Gradient Descent(1492/9): loss=19.180152841595806, w0=72.9088135840091, w1=14.789906807125822\n",
      "[-0.45056014  0.98070714]\n",
      "Gradient Descent(1493/9): loss=16.318346762063076, w0=73.22420567866767, w1=14.103411809438287\n",
      "[ 0.70845361 -2.26992509]\n",
      "Gradient Descent(1494/9): loss=15.582818506550405, w0=72.72828815251465, w1=15.692359371963741\n",
      "[-2.87441141  3.45859899]\n",
      "Gradient Descent(1495/9): loss=17.993761928582455, w0=74.74037614202173, w1=13.271340078744446\n",
      "[ 2.82321587 -0.43830903]\n",
      "Gradient Descent(1496/9): loss=16.453712177693717, w0=72.76412503236126, w1=13.578156399508604\n",
      "[-0.62130651 -1.29137505]\n",
      "Gradient Descent(1497/9): loss=15.531075890479487, w0=73.19903958625308, w1=14.482118937469517\n",
      "[-0.47717622 -0.29263387]\n",
      "Gradient Descent(1498/9): loss=15.892798603355928, w0=73.5330629386653, w1=14.686962649180595\n",
      "[ 1.770403  -1.8520579]\n",
      "Gradient Descent(1499/9): loss=16.143208602431592, w0=72.29378084118973, w1=15.983403176019193\n",
      "[-0.62756319  0.99300146]\n",
      "Gradient Descent(1500/9): loss=19.020262703068102, w0=72.73307507519971, w1=15.28830215322623\n",
      "[-2.9482965   3.42260048]\n",
      "Gradient Descent(1501/9): loss=17.178660890995687, w0=74.79688262580045, w1=12.892481814213106\n",
      "[-1.35899446  0.76557633]\n",
      "Gradient Descent(1502/9): loss=16.687753088007074, w0=75.74817875056118, w1=12.356578384980327\n",
      "[ 5.18621049  0.95099106]\n",
      "Gradient Descent(1503/9): loss=19.028291009644786, w0=72.117831407588, w1=11.690884644573652\n",
      "[-1.49136937 -2.16980276]\n",
      "Gradient Descent(1504/9): loss=17.67743484396651, w0=73.16178996332793, w1=13.209746578020422\n",
      "[-0.51301596 -0.86000714]\n",
      "Gradient Descent(1505/9): loss=15.431058088629522, w0=73.52090113209628, w1=13.81175157942531\n",
      "[ 0.47495553 -2.29981117]\n",
      "Gradient Descent(1506/9): loss=15.466772628274137, w0=73.18843226302205, w1=15.421619398871973\n",
      "[-1.40761488  2.69750459]\n",
      "Gradient Descent(1507/9): loss=17.27695323954381, w0=74.17376268098711, w1=13.533366185285331\n",
      "[ 2.37696533 -0.50048288]\n",
      "Gradient Descent(1508/9): loss=15.774387041397526, w0=72.50988695211221, w1=13.88370419990722\n",
      "[ 2.31202702  0.17452141]\n",
      "Gradient Descent(1509/9): loss=15.774848021698993, w0=70.89146803926815, w1=13.761539216331931\n",
      "[-0.62666588  2.20559325]\n",
      "Gradient Descent(1510/9): loss=18.311493557946147, w0=71.33013415630525, w1=12.217623942549997\n",
      "[-3.41253467  2.07492499]\n",
      "Gradient Descent(1511/9): loss=18.11055290185872, w0=73.71890842591041, w1=10.765176448449006\n",
      "[-2.66853084 -2.35808681]\n",
      "Gradient Descent(1512/9): loss=19.16054741014924, w0=75.58688001566091, w1=12.415837213047286\n",
      "[ 3.62575568 -3.41757651]\n",
      "Gradient Descent(1513/9): loss=18.58063133872492, w0=73.04885103775102, w1=14.808140768164424\n",
      "[-1.91947263  2.39139767]\n",
      "Gradient Descent(1514/9): loss=16.298278675805697, w0=74.39248187708336, w1=13.134162400187076\n",
      "[-1.83026078  0.03583759]\n",
      "Gradient Descent(1515/9): loss=16.049007181561244, w0=75.67366441966615, w1=13.109076086045308\n",
      "[-0.71271836  2.19236342]\n",
      "Gradient Descent(1516/9): loss=18.28616050737802, w0=76.17256727481582, w1=11.574421692487952\n",
      "[ 3.6556135  -1.52922819]\n",
      "Gradient Descent(1517/9): loss=21.344253578609223, w0=73.61363782720974, w1=12.644881424083884\n",
      "[-2.35614985  0.12626168]\n",
      "Gradient Descent(1518/9): loss=15.785468381625012, w0=75.26294272297486, w1=12.556498249809648\n",
      "[ 3.31509847 -3.05169461]\n",
      "Gradient Descent(1519/9): loss=17.750571384294688, w0=72.94237379108624, w1=14.692684475233552\n",
      "[ 2.46692351  0.40447588]\n",
      "Gradient Descent(1520/9): loss=16.183331526372168, w0=71.21552733420647, w1=14.409551356196957\n",
      "[ 0.66905623  0.10891043]\n",
      "Gradient Descent(1521/9): loss=17.97805027630086, w0=70.74718797629126, w1=14.333314057744744\n",
      "[-2.87287363  0.92236714]\n",
      "Gradient Descent(1522/9): loss=18.993132833134197, w0=72.75819951959029, w1=13.687657058018841\n",
      "[-3.10041122 -0.26657622]\n",
      "Gradient Descent(1523/9): loss=15.551007641088864, w0=74.92848737394017, w1=13.874260409791102\n",
      "[ 1.25615261 -1.92176937]\n",
      "Gradient Descent(1524/9): loss=16.79962389844062, w0=74.04918054668207, w1=15.219498966654411\n",
      "[ 1.90894248  2.28525136]\n",
      "Gradient Descent(1525/9): loss=17.184524191289686, w0=72.71292081326315, w1=13.619823013304572\n",
      "[-0.79858403  3.23538287]\n",
      "Gradient Descent(1526/9): loss=15.564484546625286, w0=73.27192963488096, w1=11.355055003079139\n",
      "[ 0.10155582 -1.8750878 ]\n",
      "Gradient Descent(1527/9): loss=17.64321430242247, w0=73.20084055808182, w1=12.667616465537357\n",
      "[ 2.42197425 -0.76387892]\n",
      "Gradient Descent(1528/9): loss=15.71996987823998, w0=71.50545858443891, w1=13.202331709174096\n",
      "[ 0.03299898 -2.01108982]\n",
      "Gradient Descent(1529/9): loss=17.023658600521482, w0=71.48235930005819, w1=14.610094584359784\n",
      "[-4.16598482  3.1679081 ]\n",
      "Gradient Descent(1530/9): loss=17.66564948236131, w0=74.39854867494263, w1=12.392558913995272\n",
      "[-1.72417975 -0.65344878]\n",
      "Gradient Descent(1531/9): loss=16.586939301105943, w0=75.60547449982886, w1=12.849973058249065\n",
      "[-0.17242586 -0.01218236]\n",
      "Gradient Descent(1532/9): loss=18.255811185004102, w0=75.72617260143004, w1=12.858500708104406\n",
      "[ 1.49514198 -1.30026929]\n",
      "Gradient Descent(1533/9): loss=18.536761362596938, w0=74.67957321787799, w1=13.768689209851672\n",
      "[ 2.33234657 -0.9891479 ]\n",
      "Gradient Descent(1534/9): loss=16.38765630292072, w0=73.04693061782064, w1=14.461092739493877\n",
      "[-2.70961355  4.84164748]\n",
      "Gradient Descent(1535/9): loss=15.897943891819793, w0=74.94366010124531, w1=11.07193950426605\n",
      "[ 2.18428359 -1.3589469 ]\n",
      "Gradient Descent(1536/9): loss=19.645391009667854, w0=73.4146615853992, w1=12.023202332722692\n",
      "[-1.14747324 -0.89085429]\n",
      "Gradient Descent(1537/9): loss=16.453887731318385, w0=74.21789285307399, w1=12.646800332731901\n",
      "[ 1.79538548 -1.04371899]\n",
      "Gradient Descent(1538/9): loss=16.15962022059262, w0=72.96112301844178, w1=13.37740362276818\n",
      "[ 2.51330233  0.64666269]\n",
      "Gradient Descent(1539/9): loss=15.446498997122122, w0=71.20181138906412, w1=12.924739740012887\n",
      "[-4.11053749  0.34708411]\n",
      "Gradient Descent(1540/9): loss=17.72834862351349, w0=74.07918763546233, w1=12.68178086642328\n",
      "[-2.14810372 -0.49211036]\n",
      "Gradient Descent(1541/9): loss=16.01255632035221, w0=75.58286023737686, w1=13.026258117457223\n",
      "[-0.24988362 -0.03072129]\n",
      "Gradient Descent(1542/9): loss=18.10831740031783, w0=75.75777877317657, w1=13.047763023940922\n",
      "[ 3.88734137 -0.64505317]\n",
      "Gradient Descent(1543/9): loss=18.514473109858955, w0=73.0366398131396, w1=13.499300244141997\n",
      "[-0.55226295 -0.76777338]\n",
      "Gradient Descent(1544/9): loss=15.419176772342567, w0=73.42322387820727, w1=14.03674160875279\n",
      "[-0.59051242  0.4144446 ]\n",
      "Gradient Descent(1545/9): loss=15.549388106623118, w0=73.83658257001305, w1=13.746630389618312\n",
      "[ 1.73865373 -1.38152147]\n",
      "Gradient Descent(1546/9): loss=15.568750712062174, w0=72.61952496155814, w1=14.713695417292213\n",
      "[ 1.06221798  0.64351664]\n",
      "Gradient Descent(1547/9): loss=16.374650553285615, w0=71.87597237400077, w1=14.263233768699262\n",
      "[ 0.70789747 -0.03374653]\n",
      "Gradient Descent(1548/9): loss=16.698131282939652, w0=71.38044414242249, w1=14.28685634001639\n",
      "[-5.30761443  1.82574115]\n",
      "Gradient Descent(1549/9): loss=17.542327270288737, w0=75.0957742409722, w1=13.008837534992468\n",
      "[ 1.07671169 -1.26474365]\n",
      "Gradient Descent(1550/9): loss=17.120085199907834, w0=74.34207606034995, w1=13.894158088210622\n",
      "[ 3.35027985  0.25893491]\n",
      "Gradient Descent(1551/9): loss=16.021083933474014, w0=71.99688016274236, w1=13.712903649332636\n",
      "[-2.51754923  2.05683612]\n",
      "Gradient Descent(1552/9): loss=16.254235706581774, w0=73.75916462508918, w1=12.273118362235982\n",
      "[ 3.23759901 -0.35422265]\n",
      "Gradient Descent(1553/9): loss=16.222047846151334, w0=71.49284531949738, w1=12.521074214793767\n",
      "[ 2.25456089  0.11658143]\n",
      "Gradient Descent(1554/9): loss=17.467320095755767, w0=69.91465269791792, w1=12.439467216330488\n",
      "[-5.76120356 -5.63399263]\n",
      "Gradient Descent(1555/9): loss=21.636673441411567, w0=73.94749519146951, w1=16.383262057877623\n",
      "[ 3.8486232   4.04160675]\n",
      "Gradient Descent(1556/9): loss=19.814767032045513, w0=71.25345895289735, w1=13.554137335585693\n",
      "[-0.59788932 -0.09581448]\n",
      "Gradient Descent(1557/9): loss=17.470402129335092, w0=71.67198147540613, w1=13.621207474696185\n",
      "[-1.95665765  0.08659186]\n",
      "Gradient Descent(1558/9): loss=16.71124382803468, w0=73.04164183354828, w1=13.560593175007854\n",
      "[-0.11566693 -0.6446643 ]\n",
      "Gradient Descent(1559/9): loss=15.420981357605948, w0=73.12260868209763, w1=14.011858183245213\n",
      "[-3.2073823  2.1931615]\n",
      "Gradient Descent(1560/9): loss=15.542151544328963, w0=75.36777629110118, w1=12.476645130924155\n",
      "[ 1.52857361 -2.69222195]\n",
      "Gradient Descent(1561/9): loss=18.03939568306494, w0=74.29777476740486, w1=14.361200497059553\n",
      "[-2.810709   -0.56432892]\n",
      "Gradient Descent(1562/9): loss=16.278258657815705, w0=76.26527106693227, w1=14.75623073923813\n",
      "[ 3.23697012  1.11293149]\n",
      "Gradient Descent(1563/9): loss=20.61509499189526, w0=73.99939198553739, w1=13.977178694931935\n",
      "[ 0.25247938  1.02098997]\n",
      "Gradient Descent(1564/9): loss=15.758468157482096, w0=73.82265641871042, w1=13.262485717205289\n",
      "[-0.1818822  -1.00853002]\n",
      "Gradient Descent(1565/9): loss=15.54926163394039, w0=73.94997395837198, w1=13.96845673046957\n",
      "[ 0.95379311  0.30201359]\n",
      "Gradient Descent(1566/9): loss=15.72052544667252, w0=73.282318780704, w1=13.75704721966083\n",
      "[-2.79150827  2.90578758]\n",
      "Gradient Descent(1567/9): loss=15.424412477597315, w0=75.23637457241583, w1=11.722995911127784\n",
      "[ 1.23188251 -0.35685441]\n",
      "Gradient Descent(1568/9): loss=18.815475335386207, w0=74.37405681647911, w1=11.972793997119473\n",
      "[-0.21295571 -1.51948719]\n",
      "Gradient Descent(1569/9): loss=17.104635066636337, w0=74.52312581666767, w1=13.03643503207035\n",
      "[ 3.49738802 -2.50103087]\n",
      "Gradient Descent(1570/9): loss=16.239606305666054, w0=72.07495420249846, w1=14.7871566382426\n",
      "[-1.60047167 -0.65821463]\n",
      "Gradient Descent(1571/9): loss=16.983534289379094, w0=73.19528437369718, w1=15.247906878359409\n",
      "[-0.34499327  2.68389831]\n",
      "Gradient Descent(1572/9): loss=16.954008354481285, w0=73.43677966385101, w1=13.369178061458808\n",
      "[-0.90246827  0.37347167]\n",
      "Gradient Descent(1573/9): loss=15.402200948455004, w0=74.06850745609825, w1=13.107747890133735\n",
      "[ 1.69949182 -1.33616983]\n",
      "Gradient Descent(1574/9): loss=15.755057992912924, w0=72.87886318056421, w1=14.043066771436232\n",
      "[ 1.15293162 -0.7968308 ]\n",
      "Gradient Descent(1575/9): loss=15.630708835695819, w0=72.07181104485248, w1=14.60084833368501\n",
      "[-1.79432122 -0.88198023]\n",
      "Gradient Descent(1576/9): loss=16.761138316420364, w0=73.32783589638807, w1=15.218234495901179\n",
      "[ 1.42779598  1.05081124]\n",
      "Gradient Descent(1577/9): loss=16.897692423081196, w0=72.32837870950206, w1=14.482666629136055\n",
      "[ 1.40307234  0.19790497]\n",
      "Gradient Descent(1578/9): loss=16.354983351553383, w0=71.34622806995425, w1=14.34413314823755\n",
      "[-1.20217224  2.57739732]\n",
      "Gradient Descent(1579/9): loss=17.656255280244718, w0=72.1877486362124, w1=12.539955024119596\n",
      "[ 1.98605795 -0.05222727]\n",
      "Gradient Descent(1580/9): loss=16.439269622176713, w0=70.79750807053387, w1=12.576514114004208\n",
      "[-5.6899123   2.29500577]\n",
      "Gradient Descent(1581/9): loss=18.90981273121601, w0=74.78044668091391, w1=10.970010074032757\n",
      "[ 0.05472779  0.1884528 ]\n",
      "Gradient Descent(1582/9): loss=19.640068649477882, w0=74.74213722677793, w1=10.838093112096878\n",
      "[-0.87327004 -1.9429333 ]\n",
      "Gradient Descent(1583/9): loss=19.923627860855003, w0=75.35342625298811, w1=12.198146424450266\n",
      "[ 1.96960771 -4.16777928]\n",
      "Gradient Descent(1584/9): loss=18.327872468215965, w0=73.97470085564129, w1=15.115591922449328\n",
      "[ 2.30407796  1.95085275]\n",
      "Gradient Descent(1585/9): loss=16.955668641287012, w0=72.36184628520927, w1=13.749994998709589\n",
      "[-1.04050995  0.69181876]\n",
      "Gradient Descent(1586/9): loss=15.856796771968598, w0=73.09020325286161, w1=13.265721863896168\n",
      "[ 0.80084878 -2.21634049]\n",
      "Gradient Descent(1587/9): loss=15.429534515484411, w0=72.52960910597554, w1=14.817160210105824\n",
      "[ 0.60292399 -0.77926514]\n",
      "Gradient Descent(1588/9): loss=16.572358246006853, w0=72.10756230998092, w1=15.362645808232768\n",
      "[ 2.67539104  2.64445955]\n",
      "Gradient Descent(1589/9): loss=17.862331572415485, w0=70.23478858515432, w1=13.511524120234556\n",
      "[-1.56880567 -0.01873712]\n",
      "Gradient Descent(1590/9): loss=20.065542491841228, w0=71.33295255128532, w1=13.524640104895871\n",
      "[-4.61041619  4.14443863]\n",
      "Gradient Descent(1591/9): loss=17.309597710115426, w0=74.5602438876287, w1=10.623533066559576\n",
      "[ 0.27798449 -1.33446106]\n",
      "Gradient Descent(1592/9): loss=20.26655372002841, w0=74.36565474771308, w1=11.557655811746422\n",
      "[ 0.04567152 -0.0497568 ]\n",
      "Gradient Descent(1593/9): loss=17.807344239308943, w0=74.3336846856662, w1=11.592485571109245\n",
      "[ 3.74404851 -0.36519959]\n",
      "Gradient Descent(1594/9): loss=17.707253705767197, w0=71.7128507252431, w1=11.848125285606644\n",
      "[-1.39021008 -1.01549142]\n",
      "Gradient Descent(1595/9): loss=17.966819373103455, w0=72.68599778211953, w1=12.558969281836383\n",
      "[ 1.24262266 -1.65369372]\n",
      "Gradient Descent(1596/9): loss=15.994557774490744, w0=71.81616191846264, w1=13.716554887566764\n",
      "[-1.13962318  3.94481508]\n",
      "Gradient Descent(1597/9): loss=16.50582247490453, w0=72.61389814679099, w1=10.955184331209992\n",
      "[ 0.01233454 -4.20266993]\n",
      "Gradient Descent(1598/9): loss=18.803725164112734, w0=72.60526397076598, w1=13.897053279381279\n",
      "[ 0.84137515 -0.7108796 ]\n",
      "Gradient Descent(1599/9): loss=15.710099501092412, w0=72.01630136457456, w1=14.394668998507195\n",
      "[-0.89150881  1.22560472]\n",
      "Gradient Descent(1600/9): loss=16.620617872113957, w0=72.64035753195404, w1=13.53674569214726\n",
      "[-1.33212031 -0.29079215]\n",
      "Gradient Descent(1601/9): loss=15.601087523362413, w0=73.57284174870907, w1=13.740300198545308\n",
      "[-0.25817995  2.37076731]\n",
      "Gradient Descent(1602/9): loss=15.458738972609813, w0=73.75356771152414, w1=12.080763079724747\n",
      "[-0.36417957  1.76303088]\n",
      "Gradient Descent(1603/9): loss=16.470054607220227, w0=74.00849341137454, w1=10.84664146347584\n",
      "[ 0.33801051 -4.08453534]\n",
      "Gradient Descent(1604/9): loss=19.10772538881475, w0=73.77188605173745, w1=13.705816201671373\n",
      "[-0.31460869  2.55461176]\n",
      "Gradient Descent(1605/9): loss=15.525674141853804, w0=73.99211213185143, w1=11.917587969866863\n",
      "[-0.08098828 -2.51597714]\n",
      "Gradient Descent(1606/9): loss=16.849739019733573, w0=74.04880392985204, w1=13.678771964422708\n",
      "[ 6.37691205  1.55736569]\n",
      "Gradient Descent(1607/9): loss=15.690623579377977, w0=69.58496549657993, w1=12.58861598264592\n",
      "[-8.2738792   4.67854042]\n",
      "Gradient Descent(1608/9): loss=22.661093492457738, w0=75.37668093445443, w1=9.31363768564816\n",
      "[ 2.93493452 -6.93679643]\n",
      "Gradient Descent(1609/9): loss=26.232919662517553, w0=73.32222677244674, w1=14.169395184160948\n",
      "[ 3.79071846 -0.72485971]\n",
      "Gradient Descent(1610/9): loss=15.624119596094097, w0=70.66872385296729, w1=14.676796980028234\n",
      "[-4.63938616  1.84235414]\n",
      "Gradient Descent(1611/9): loss=19.548226233933768, w0=73.91629416265611, w1=13.387149080394874\n",
      "[ 1.39448142 -0.65534882]\n",
      "Gradient Descent(1612/9): loss=15.583845409250676, w0=72.94015717129192, w1=13.845893255575893\n",
      "[ 2.1408461   1.84171255]\n",
      "Gradient Descent(1613/9): loss=15.515506843272398, w0=71.44156490312243, w1=12.556694473908662\n",
      "[-1.27941871 -0.48342352]\n",
      "Gradient Descent(1614/9): loss=17.527482358143814, w0=72.33715799672252, w1=12.89509093740044\n",
      "[ 0.56549524  0.60500569]\n",
      "Gradient Descent(1615/9): loss=16.014477697548713, w0=71.94131132787699, w1=12.471586955181092\n",
      "[-0.62394533 -0.06145581]\n",
      "Gradient Descent(1616/9): loss=16.808824178366443, w0=72.3780730562977, w1=12.514606019858542\n",
      "[-0.14897872 -0.96215938]\n",
      "Gradient Descent(1617/9): loss=16.270992710860778, w0=72.48235815711513, w1=13.188117588951673\n",
      "[-0.05705511  0.9559073 ]\n",
      "Gradient Descent(1618/9): loss=15.757719583194707, w0=72.52229673484912, w1=12.518982477969647\n",
      "[-3.33549091 -1.67063786]\n",
      "Gradient Descent(1619/9): loss=16.14509167052065, w0=74.85714037435197, w1=13.688428983032178\n",
      "[ 1.41007377 -0.72594596]\n",
      "Gradient Descent(1620/9): loss=16.629495007207858, w0=73.87008873268177, w1=14.196591153510997\n",
      "[-0.47880957  0.50143017]\n",
      "Gradient Descent(1621/9): loss=15.808829468075885, w0=74.20525542973195, w1=13.845590036633466\n",
      "[ 0.49357784 -1.62688483]\n",
      "Gradient Descent(1622/9): loss=15.868085386676906, w0=73.85975094513628, w1=14.98440941771127\n",
      "[ 2.56085712 -0.75587936]\n",
      "Gradient Descent(1623/9): loss=16.67802557012192, w0=72.06715096381038, w1=15.513524973056693\n",
      "[-2.59149538  2.62733928]\n",
      "Gradient Descent(1624/9): loss=18.206568179029446, w0=73.88119772827457, w1=13.67438747452931\n",
      "[-0.84901825  0.63679677]\n",
      "Gradient Descent(1625/9): loss=15.57728344361329, w0=74.47551050163271, w1=13.22862973568221\n",
      "[-1.31197376 -0.95157026]\n",
      "Gradient Descent(1626/9): loss=16.11548482088286, w0=75.39389213551465, w1=13.894728919106946\n",
      "[-1.3883475  -0.31914618]\n",
      "Gradient Descent(1627/9): loss=17.67694449048006, w0=76.36573538240663, w1=14.118131245844978\n",
      "[ 6.59729114 -2.35363608]\n",
      "Gradient Descent(1628/9): loss=20.307695879556217, w0=71.74763158655483, w1=15.765676500919906\n",
      "[ 0.22489662  3.48320253]\n",
      "Gradient Descent(1629/9): loss=19.19421074880442, w0=71.5902039508012, w1=13.327434730107756\n",
      "[-1.50103279 -1.10022293]\n",
      "Gradient Descent(1630/9): loss=16.848809717700895, w0=72.64092690443483, w1=14.097590784399372\n",
      "[-3.8785324   0.76556443]\n",
      "Gradient Descent(1631/9): loss=15.789975994955173, w0=75.3558995825188, w1=13.561695681925958\n",
      "[ 0.83667234 -0.73934843]\n",
      "Gradient Descent(1632/9): loss=17.515124266282747, w0=74.77022894767012, w1=14.079239582296758\n",
      "[ 0.95845661 -1.01560932]\n",
      "Gradient Descent(1633/9): loss=16.655345367770487, w0=74.09930932167562, w1=14.790166108899973\n",
      "[ 1.5244619   4.36468691]\n",
      "Gradient Descent(1634/9): loss=16.56885665182514, w0=73.03218599259921, w1=11.734885269040493\n",
      "[-3.29622506 -2.84701309]\n",
      "Gradient Descent(1635/9): loss=16.94235165768149, w0=75.33954353512117, w1=13.72779443311942\n",
      "[ 2.8754522   0.96693975]\n",
      "Gradient Descent(1636/9): loss=17.508943935896895, w0=73.32672699797644, w1=13.05093660797965\n",
      "[-0.35024386 -1.21591069]\n",
      "Gradient Descent(1637/9): loss=15.478350307620254, w0=73.57189769658821, w1=13.902074091737129\n",
      "[  1.17671161e-03   2.11248528e+00]\n",
      "Gradient Descent(1638/9): loss=15.51371779673655, w0=73.57107399845904, w1=12.423334395194058\n",
      "[-0.04015574 -0.1789285 ]\n",
      "Gradient Descent(1639/9): loss=15.982261764851415, w0=73.59918301296456, w1=12.548584343555033\n",
      "[ 2.77813784 -0.14756398]\n",
      "Gradient Descent(1640/9): loss=15.865979772533619, w0=71.65448652490768, w1=12.651879126452032\n",
      "[-2.43533447  1.089576  ]\n",
      "Gradient Descent(1641/9): loss=17.07241620413798, w0=73.35922065732932, w1=11.889175925787956\n",
      "[-2.53095392 -2.24033648]\n",
      "Gradient Descent(1642/9): loss=16.65292301956724, w0=75.13088840435097, w1=13.457411458334333\n",
      "[ 1.38503614 -0.10238466]\n",
      "Gradient Descent(1643/9): loss=17.073359317099182, w0=74.16136310555603, w1=13.529080721222101\n",
      "[ 0.46000997 -2.36984033]\n",
      "Gradient Descent(1644/9): loss=15.763333516650205, w0=73.83935612372343, w1=15.187968952650209\n",
      "[-2.6787025  2.9292178]\n",
      "Gradient Descent(1645/9): loss=16.99370722440805, w0=75.7144478725865, w1=13.137516494512772\n",
      "[ 1.58125128  0.21313147]\n",
      "Gradient Descent(1646/9): loss=18.373909644503286, w0=74.60757197522585, w1=12.988324463166657\n",
      "[ 0.39455121 -1.42528483]\n",
      "Gradient Descent(1647/9): loss=16.36945706419522, w0=74.33138613103618, w1=13.986023843191527\n",
      "[ 4.94828491  0.33594613]\n",
      "Gradient Descent(1648/9): loss=16.05222939927666, w0=70.8675866905662, w1=13.750861554147859\n",
      "[-2.19040415 -1.57937524]\n",
      "Gradient Descent(1649/9): loss=18.366200313250204, w0=72.40086959801096, w1=14.856424223534077\n",
      "[-1.25514317  0.8789915 ]\n",
      "Gradient Descent(1650/9): loss=16.732326841418068, w0=73.27946982015919, w1=14.241130172008534\n",
      "[ 2.91960727  0.44737297]\n",
      "Gradient Descent(1651/9): loss=15.675870786734837, w0=71.2357447307085, w1=13.927969095608892\n",
      "[-1.30626509 -0.35860479]\n",
      "Gradient Descent(1652/9): loss=17.604401725971382, w0=72.15013029139513, w1=14.178992451142319\n",
      "[ 1.0100539   3.18011787]\n",
      "Gradient Descent(1653/9): loss=16.284513878069582, w0=71.44309256390166, w1=11.952909941461924\n",
      "[-4.08198867 -1.49422132]\n",
      "Gradient Descent(1654/9): loss=18.264235600610117, w0=74.30048463191726, w1=12.998864867338247\n",
      "[ 2.62464939 -1.50120429]\n",
      "Gradient Descent(1655/9): loss=16.00807922435434, w0=72.46323005817841, w1=14.049707869367317\n",
      "[-5.15994145  2.12848992]\n",
      "Gradient Descent(1656/9): loss=15.893359819287863, w0=76.07518907479863, w1=12.559764926348874\n",
      "[ 1.34032964 -1.70848362]\n",
      "Gradient Descent(1657/9): loss=19.676762842980402, w0=75.13695832449102, w1=13.755703457496326\n",
      "[ 2.92922214  1.18054534]\n",
      "Gradient Descent(1658/9): loss=17.122364833898448, w0=73.08650282410606, w1=12.929321721605058\n",
      "[-2.55164804 -1.57759503]\n",
      "Gradient Descent(1659/9): loss=15.558864195219986, w0=74.87265645292295, w1=14.033638244344637\n",
      "[ 2.29453258  0.96048449]\n",
      "Gradient Descent(1660/9): loss=16.78550600306395, w0=73.26648364933754, w1=13.361299101270955\n",
      "[ 1.6519875  -0.19440644]\n",
      "Gradient Descent(1661/9): loss=15.393275159231818, w0=72.11009240015683, w1=13.497383611829871\n",
      "[-1.09988747 -1.43455293]\n",
      "Gradient Descent(1662/9): loss=16.08677026729947, w0=72.88001363079366, w1=14.5015706624368\n",
      "[ 1.79432762  2.89357676]\n",
      "Gradient Descent(1663/9): loss=15.993645057251612, w0=71.62398429689769, w1=12.476066931279602\n",
      "[-1.99527114 -0.99773828]\n",
      "Gradient Descent(1664/9): loss=17.283885987024338, w0=73.02067409766421, w1=13.17448372659495\n",
      "[ 1.48419034 -1.52426161]\n",
      "Gradient Descent(1665/9): loss=15.46980235968406, w0=71.98174085843061, w1=14.241466856875595\n",
      "[-0.0822719   1.24366153]\n",
      "Gradient Descent(1666/9): loss=16.536932445368816, w0=72.0393311918256, w1=13.370903787665346\n",
      "[-1.24571382  0.13411591]\n",
      "Gradient Descent(1667/9): loss=16.17880658031461, w0=72.91133086640113, w1=13.277022653194349\n",
      "[-0.01224437 -0.49578983]\n",
      "Gradient Descent(1668/9): loss=15.479617431211052, w0=72.91990192210807, w1=13.624075533391633\n",
      "[-0.6562666  1.1431023]\n",
      "Gradient Descent(1669/9): loss=15.466253731040121, w0=73.37928854260119, w1=12.823903926159089\n",
      "[ 3.37121186 -3.82186708]\n",
      "Gradient Descent(1670/9): loss=15.604573992074414, w0=71.01944024082827, w1=15.499210882853788\n",
      "[-2.85164736  4.74026906]\n",
      "Gradient Descent(1671/9): loss=20.011708500484122, w0=73.01559339202355, w1=12.181022537941587\n",
      "[ 1.11139656 -0.20746956]\n",
      "Gradient Descent(1672/9): loss=16.26791900077096, w0=72.23761579960063, w1=12.326251230688674\n",
      "[ 0.27330642 -4.21330336]\n",
      "Gradient Descent(1673/9): loss=16.60901564046723, w0=72.04630130288746, w1=15.27556357934801\n",
      "[-3.40250388  2.89863838]\n",
      "Gradient Descent(1674/9): loss=17.77670723973537, w0=74.42805402105567, w1=13.246516712686876\n",
      "[ 1.0822799   0.24277655]\n",
      "Gradient Descent(1675/9): loss=16.056205709483766, w0=73.6704580911452, w1=13.076573126365663\n",
      "[-0.5473783  -0.53124742]\n",
      "Gradient Descent(1676/9): loss=15.538038233082846, w0=74.05362290054016, w1=13.448446318017957\n",
      "[ 2.94992438  0.01480302]\n",
      "Gradient Descent(1677/9): loss=15.674949381406076, w0=71.98867583746306, w1=13.43808420662572\n",
      "[ 0.41030121  1.28503729]\n",
      "Gradient Descent(1678/9): loss=16.238588098684225, w0=71.70146499175652, w1=12.538558103910589\n",
      "[ 0.04571934 -4.22831319]\n",
      "Gradient Descent(1679/9): loss=17.096733271187574, w0=71.66946145279444, w1=15.498377337138558\n",
      "[-2.97218085  2.22270314]\n",
      "Gradient Descent(1680/9): loss=18.742827900548047, w0=73.7499880499628, w1=13.942485136713827\n",
      "[-1.01071456  1.03935421]\n",
      "Gradient Descent(1681/9): loss=15.596965275564452, w0=74.45748824223226, w1=13.214937189832613\n",
      "[ 1.04220866 -0.13524667]\n",
      "Gradient Descent(1682/9): loss=16.097884031634948, w0=73.72794217800444, w1=13.309609859313841\n",
      "[ 2.85436847  1.81424726]\n",
      "Gradient Descent(1683/9): loss=15.494542068498879, w0=71.72988424877364, w1=12.039636779012653\n",
      "[ 1.38611044 -0.29181654]\n",
      "Gradient Descent(1684/9): loss=17.645903863220514, w0=70.75960694211193, w1=12.243908359466188\n",
      "[-3.0961732  -1.54517343]\n",
      "Gradient Descent(1685/9): loss=19.36087013702317, w0=72.92692818128211, w1=13.325529759212221\n",
      "[ 0.8752011  -1.91361092]\n",
      "Gradient Descent(1686/9): loss=15.465116249845416, w0=72.31428741243442, w1=14.665057402344088\n",
      "[-1.33295878  5.1116342 ]\n",
      "Gradient Descent(1687/9): loss=16.568251179286065, w0=73.24735855941118, w1=11.086913459245322\n",
      "[-0.80373753 -3.74645264]\n",
      "Gradient Descent(1688/9): loss=18.249715415087273, w0=73.8099748297, w1=13.709430309186274\n",
      "[-1.19445119  1.39648549]\n",
      "Gradient Descent(1689/9): loss=15.545428280126542, w0=74.6460906641175, w1=12.731890468331473\n",
      "[ 2.50572092 -0.1575927 ]\n",
      "Gradient Descent(1690/9): loss=16.579686761001277, w0=72.89208601987161, w1=12.842205361271015\n",
      "[ 0.78160002  1.43208747]\n",
      "Gradient Descent(1691/9): loss=15.669831581658476, w0=72.34496600522532, w1=11.839744133065551\n",
      "[ 1.39818802 -1.71904434]\n",
      "Gradient Descent(1692/9): loss=17.180894626493455, w0=71.36623439374684, w1=13.043075170564457\n",
      "[-1.3764189   0.54459971]\n",
      "Gradient Descent(1693/9): loss=17.33920367688067, w0=72.32972762609062, w1=12.661855375456026\n",
      "[-0.77934158 -0.49341098]\n",
      "Gradient Descent(1694/9): loss=16.185168351112466, w0=72.87526673159688, w1=13.007243059586898\n",
      "[-2.04157807 -1.75123939]\n",
      "Gradient Descent(1695/9): loss=15.585137641938042, w0=74.30437137881358, w1=14.233110631924118\n",
      "[ 1.25525103 -1.59376198]\n",
      "Gradient Descent(1696/9): loss=16.1801962618471, w0=73.42569565626299, w1=15.348744018072088\n",
      "[-0.25279335  1.5771541 ]\n",
      "Gradient Descent(1697/9): loss=17.141209546075395, w0=73.60265100208218, w1=14.24473614833891\n",
      "[ 0.07117731  2.23536114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1698/9): loss=15.726175307536602, w0=73.55282688834373, w1=12.679983347878999\n",
      "[ 3.34126453 -0.17534886]\n",
      "Gradient Descent(1699/9): loss=15.739187045273432, w0=71.21394171598371, w1=12.802727551501945\n",
      "[-4.7553341  -2.42617028]\n",
      "Gradient Descent(1700/9): loss=17.778201130391423, w0=74.54267558868061, w1=14.501046747866319\n",
      "[ 2.59155626  4.57499623]\n",
      "Gradient Descent(1701/9): loss=16.68714251815213, w0=72.72858620822495, w1=11.298549384546536\n",
      "[-2.30307921 -2.57233713]\n",
      "Gradient Descent(1702/9): loss=17.92442627505834, w0=74.34074165340421, w1=13.099185377689103\n",
      "[ 0.60692414  0.14773938]\n",
      "Gradient Descent(1703/9): loss=16.00620398067098, w0=73.9158947556034, w1=12.995767812472186\n",
      "[ 0.42382686  2.84754777]\n",
      "Gradient Descent(1704/9): loss=15.696414120707969, w0=73.61921595492764, w1=11.002484371300236\n",
      "[-0.04955289 -2.27928538]\n",
      "Gradient Descent(1705/9): loss=18.507125386464537, w0=73.65390297863037, w1=12.597984136758166\n",
      "[ 3.2893611  -0.23613195]\n",
      "Gradient Descent(1706/9): loss=15.839403416509976, w0=71.351350205839, w1=12.763276501926862\n",
      "[-5.661107    2.07400658]\n",
      "Gradient Descent(1707/9): loss=17.529320683745173, w0=75.31412510875599, w1=11.311471894060057\n",
      "[ 0.98210575 -0.44985037]\n",
      "Gradient Descent(1708/9): loss=19.777131686554196, w0=74.6266510853301, w1=11.626367149800679\n",
      "[-1.66002947 -2.82135704]\n",
      "Gradient Descent(1709/9): loss=17.991415646531134, w0=75.7886717135545, w1=13.601317080263346\n",
      "[-0.69456168  0.45042203]\n",
      "Gradient Descent(1710/9): loss=18.50516977509377, w0=76.27486489264592, w1=13.286021658692267\n",
      "[ 3.46320301 -2.62890748]\n",
      "Gradient Descent(1711/9): loss=19.8476561855733, w0=73.85062278614839, w1=15.126256897628792\n",
      "[-0.58777971  0.77937483]\n",
      "Gradient Descent(1712/9): loss=16.89640008403136, w0=74.2620685797215, w1=14.58069451716414\n",
      "[ 3.0563724 -0.3078577]\n",
      "Gradient Descent(1713/9): loss=16.460622539339788, w0=72.12260790258473, w1=14.7961949082427\n",
      "[-2.60931702 -1.45667388]\n",
      "Gradient Descent(1714/9): loss=16.93843927988914, w0=73.94912981402169, w1=15.815866620806824\n",
      "[ 1.98229417 -0.16116843]\n",
      "Gradient Descent(1715/9): loss=18.32934469718456, w0=72.561523894168, w1=15.928684518735041\n",
      "[-0.98348978  1.63261264]\n",
      "Gradient Descent(1716/9): loss=18.652823496567983, w0=73.24996674149565, w1=14.78585567094027\n",
      "[ 1.35673671  1.12927055]\n",
      "Gradient Descent(1717/9): loss=16.23985897770759, w0=72.30025104615233, w1=13.995366283380696\n",
      "[-0.18500553 -0.57648887]\n",
      "Gradient Descent(1718/9): loss=16.012528298862097, w0=72.42975491715771, w1=14.398908494841173\n",
      "[-1.62174272  2.20993333]\n",
      "Gradient Descent(1719/9): loss=16.18174094240655, w0=73.56497482187703, w1=12.851955160956308\n",
      "[ 1.257343    1.46387636]\n",
      "Gradient Descent(1720/9): loss=15.619662281933042, w0=72.6848347250047, w1=11.82724171002123\n",
      "[-3.2726198  -1.70707294]\n",
      "Gradient Descent(1721/9): loss=16.936711272830074, w0=74.97566858538231, w1=13.022192770315888\n",
      "[ 1.64064217 -1.2691261 ]\n",
      "Gradient Descent(1722/9): loss=16.904685775792856, w0=73.82721906734159, w1=13.91058104298694\n",
      "[-0.58467695 -0.78315931]\n",
      "Gradient Descent(1723/9): loss=15.6209146274033, w0=74.23649293157574, w1=14.458792563125868\n",
      "[ 3.57870047 -0.58687433]\n",
      "Gradient Descent(1724/9): loss=16.30940679602709, w0=71.73140260341746, w1=14.869604591785913\n",
      "[-3.49435303  0.56779774]\n",
      "Gradient Descent(1725/9): loss=17.57252140822985, w0=74.17744972268818, w1=14.472146173101923\n",
      "[ 0.89217404  1.66014342]\n",
      "Gradient Descent(1726/9): loss=16.26866084762103, w0=73.55292789174494, w1=13.310045775925277\n",
      "[ 0.80734654  1.33961709]\n",
      "Gradient Descent(1727/9): loss=15.433823281862372, w0=72.9877853133919, w1=12.372313811833516\n",
      "[ 0.66556813 -0.47377792]\n",
      "Gradient Descent(1728/9): loss=16.045913560200955, w0=72.52188762332028, w1=12.703958353531734\n",
      "[-3.5018369  -0.88961265]\n",
      "Gradient Descent(1729/9): loss=15.984803607291141, w0=74.97317345150313, w1=13.326687209835898\n",
      "[ 2.87143788  2.38126798]\n",
      "Gradient Descent(1730/9): loss=16.807538943748526, w0=72.96316693239442, w1=11.659799621801259\n",
      "[ 1.1104298  -2.28171007]\n",
      "Gradient Descent(1731/9): loss=17.096628650701636, w0=72.18586607370825, w1=13.256996672216399\n",
      "[ 0.89118544 -2.35632195]\n",
      "Gradient Descent(1732/9): loss=16.024582994550897, w0=71.56203626448561, w1=14.906422038466921\n",
      "[-1.24548239  1.98184419]\n",
      "Gradient Descent(1733/9): loss=17.903352119242452, w0=72.43387393617495, w1=13.519131108107677\n",
      "[-1.53631532  1.20396451]\n",
      "Gradient Descent(1734/9): loss=15.756506122579792, w0=73.50929465910204, w1=12.676355949106838\n",
      "[ 3.74384602 -1.80249216]\n",
      "Gradient Descent(1735/9): loss=15.73177138122485, w0=70.88860244423734, w1=13.938100461816727\n",
      "[-0.31530854  2.89855036]\n",
      "Gradient Descent(1736/9): loss=18.38372874812969, w0=71.10931842176403, w1=11.909115207887014\n",
      "[-5.21783613 -1.46303452]\n",
      "Gradient Descent(1737/9): loss=19.005522095339405, w0=74.76180371281262, w1=12.933239368995443\n",
      "[ 4.11776618  0.00507008]\n",
      "Gradient Descent(1738/9): loss=16.612542633072316, w0=71.87936738634335, w1=12.929690313798407\n",
      "[-0.46742484  1.76922412]\n",
      "Gradient Descent(1739/9): loss=16.53763241621549, w0=72.20656477178942, w1=11.691233432956482\n",
      "[-0.08651739  0.88909466]\n",
      "Gradient Descent(1740/9): loss=17.576389312345096, w0=72.26712694308907, w1=11.06886717288878\n",
      "[ 3.43483237 -5.86159045]\n",
      "Gradient Descent(1741/9): loss=18.819129354335015, w0=69.86274428343403, w1=15.17198048573161\n",
      "[-1.83707976 -0.41992297]\n",
      "Gradient Descent(1742/9): loss=22.704263715164146, w0=71.14870011837917, w1=15.465926563458078\n",
      "[-3.63496384  1.72242651]\n",
      "Gradient Descent(1743/9): loss=19.65939961610291, w0=73.69317480729899, w1=14.260228003890132\n",
      "[-2.41930289  4.51020823]\n",
      "Gradient Descent(1744/9): loss=15.77019154670545, w0=75.38668682992869, w1=11.103082242447375\n",
      "[-1.04878621 -0.52717072]\n",
      "Gradient Descent(1745/9): loss=20.39990571716731, w0=76.12083717830053, w1=11.472101745510512\n",
      "[ 2.18843774  0.86407679]\n",
      "Gradient Descent(1746/9): loss=21.396862915785423, w0=74.58893076072378, w1=10.867247994152281\n",
      "[ 3.21644867 -0.90562503]\n",
      "Gradient Descent(1747/9): loss=19.63689693859712, w0=72.33741669111299, w1=11.501185515161495\n",
      "[ 0.72522961  0.65203704]\n",
      "Gradient Descent(1748/9): loss=17.800623460048694, w0=71.82975596598277, w1=11.044759587588612\n",
      "[-2.36608325 -2.50547012]\n",
      "Gradient Descent(1749/9): loss=19.42227664402836, w0=73.48601424290074, w1=12.798588668871359\n",
      "[-0.33053364  1.2811188 ]\n",
      "Gradient Descent(1750/9): loss=15.6363023757015, w0=73.71738778754812, w1=11.90180551034281\n",
      "[-1.18816436 -4.17668661]\n",
      "Gradient Descent(1751/9): loss=16.720444635973077, w0=74.5491028409754, w1=14.825486140563262\n",
      "[-1.60368901 -1.07544222]\n",
      "Gradient Descent(1752/9): loss=17.07918077127033, w0=75.67168515074263, w1=15.578295696491972\n",
      "[ 2.41481026  0.27901637]\n",
      "Gradient Descent(1753/9): loss=20.414792517068637, w0=73.98131796896374, w1=15.382984236480883\n",
      "[-0.78012326  1.99181629]\n",
      "Gradient Descent(1754/9): loss=17.433366251633082, w0=74.52740424993756, w1=13.988712833648936\n",
      "[ 4.32333656 -0.34105835]\n",
      "Gradient Descent(1755/9): loss=16.276167799606164, w0=71.50106865715863, w1=14.227453677785658\n",
      "[-3.42616575  1.89589617]\n",
      "Gradient Descent(1756/9): loss=17.27260791016195, w0=73.89938468490722, w1=12.900326356553263\n",
      "[ 3.26768282  2.57023934]\n",
      "Gradient Descent(1757/9): loss=15.737024512904917, w0=71.61200670957817, w1=11.101158818926182\n",
      "[-4.91015714 -0.72772811]\n",
      "Gradient Descent(1758/9): loss=19.62906604669039, w0=75.04911671009494, w1=11.610568492469262\n",
      "[ 3.79680077  0.24259335]\n",
      "Gradient Descent(1759/9): loss=18.673091639236215, w0=72.39135616903009, w1=11.440753145627054\n",
      "[ 2.90742079 -3.96017448]\n",
      "Gradient Descent(1760/9): loss=17.871877902184455, w0=70.35616161941752, w1=14.212875278790086\n",
      "[-5.21747383  0.1753746 ]\n",
      "Gradient Descent(1761/9): loss=19.969869779639218, w0=74.00839330247517, w1=14.09011305760733\n",
      "[-0.62066445 -0.30218548]\n",
      "Gradient Descent(1762/9): loss=15.827416948401977, w0=74.44285841419669, w1=14.301642897074304\n",
      "[-1.39590929  0.5878495 ]\n",
      "Gradient Descent(1763/9): loss=16.383700150596088, w0=75.4199949186191, w1=13.890148245849213\n",
      "[ 4.02453865 -1.51736369]\n",
      "Gradient Descent(1764/9): loss=17.73020966941459, w0=72.60281786461812, w1=14.952302826663193\n",
      "[-0.93184347 -3.22905826]\n",
      "Gradient Descent(1765/9): loss=16.708961564080784, w0=73.25510829474115, w1=17.21264360891358\n",
      "[ 2.80955691  5.66822679]\n",
      "Gradient Descent(1766/9): loss=22.354028695397865, w0=71.2884184597343, w1=13.244884852722986\n",
      "[-2.23790654 -0.44825474]\n",
      "Gradient Descent(1767/9): loss=17.42448209475695, w0=72.8549530354484, w1=13.558663173738362\n",
      "[ 1.30577347  0.39345485]\n",
      "Gradient Descent(1768/9): loss=15.485351355247793, w0=71.94091160734291, w1=13.28324478145506\n",
      "[-2.74444388  4.37606865]\n",
      "Gradient Descent(1769/9): loss=16.32050620243936, w0=73.86202232251732, w1=10.219996728833188\n",
      "[ 3.59591811 -3.10394537]\n",
      "Gradient Descent(1770/9): loss=20.860130098335187, w0=71.34487964477314, w1=12.3927584881453\n",
      "[-2.75587341  0.05545777]\n",
      "Gradient Descent(1771/9): loss=17.87600536544623, w0=73.27399102988193, w1=12.35393805177698\n",
      "[ 2.70287115 -2.50059128]\n",
      "Gradient Descent(1772/9): loss=16.019770471604538, w0=71.38198122589094, w1=14.104351948839437\n",
      "[-0.63144917  2.04622112]\n",
      "Gradient Descent(1773/9): loss=17.408733895836402, w0=71.82399564485131, w1=12.671997166531344\n",
      "[-2.06501048 -2.5363177 ]\n",
      "Gradient Descent(1774/9): loss=16.79243159415408, w0=73.26950297748049, w1=14.447419554430605\n",
      "[-2.75860161  4.84963242]\n",
      "Gradient Descent(1775/9): loss=15.85441454772015, w0=75.20052410771439, w1=11.052676858911703\n",
      "[ 2.23968513 -4.31501637]\n",
      "Gradient Descent(1776/9): loss=20.14870450715867, w0=73.63274451702502, w1=14.073188319571216\n",
      "[-1.48019557 -0.84095   ]\n",
      "Gradient Descent(1777/9): loss=15.619395029927992, w0=74.66888141466535, w1=14.661853322934085\n",
      "[  1.84630694e-03   2.27352059e+00]\n",
      "Gradient Descent(1778/9): loss=17.029873101399026, w0=74.66758899980736, w1=13.070388907875632\n",
      "[ 0.21313649 -0.90961766]\n",
      "Gradient Descent(1779/9): loss=16.413141254041737, w0=74.51839345387722, w1=13.707121266528352\n",
      "[ 6.33074956  0.63116013]\n",
      "Gradient Descent(1780/9): loss=16.16141042526279, w0=70.0868687630849, w1=13.26530917900164\n",
      "[-2.44381925 -2.73451564]\n",
      "Gradient Descent(1781/9): loss=20.551467485873683, w0=71.79754224032308, w1=15.179470125521345\n",
      "[-2.7840078 -1.2860068]\n",
      "Gradient Descent(1782/9): loss=17.95005216782678, w0=73.74634770063027, w1=16.07967488637417\n",
      "[-1.33850918  0.76558862]\n",
      "Gradient Descent(1783/9): loss=18.86813474947862, w0=74.68330412397984, w1=15.543762855096464\n",
      "[ 1.26245651  0.14634021]\n",
      "Gradient Descent(1784/9): loss=18.481231277494643, w0=73.79958456895106, w1=15.441324711551562\n",
      "[-2.67713838 -0.97105732]\n",
      "Gradient Descent(1785/9): loss=17.437696546364364, w0=75.67358143500935, w1=16.121064834230157\n",
      "[ 2.28145603  0.08260945]\n",
      "Gradient Descent(1786/9): loss=21.705648625622658, w0=74.07656221620799, w1=16.063238219188293\n",
      "[ 0.34966244  3.57007407]\n",
      "Gradient Descent(1787/9): loss=19.029453460005993, w0=73.83179850689508, w1=13.564186372434396\n",
      "[ 2.92175138 -0.59375468]\n",
      "Gradient Descent(1788/9): loss=15.534111359085655, w0=71.78657253801019, w1=13.979814645247096\n",
      "[-4.00832428  2.70831877]\n",
      "Gradient Descent(1789/9): loss=16.646990182635637, w0=74.59239953062989, w1=12.083991502757966\n",
      "[ 3.10281498  1.19854964]\n",
      "Gradient Descent(1790/9): loss=17.20292827520521, w0=72.42042904346806, w1=11.245006755941953\n",
      "[-2.99338019 -2.30322126]\n",
      "Gradient Descent(1791/9): loss=18.26433757920639, w0=74.51579517658278, w1=12.857261639040141\n",
      "[-1.28689626 -0.48191035]\n",
      "Gradient Descent(1792/9): loss=16.32609739277209, w0=75.41662256149563, w1=13.194598886199383\n",
      "[ 2.17676198 -0.90106822]\n",
      "Gradient Descent(1793/9): loss=17.67946156909936, w0=73.8928891786989, w1=13.825346641457955\n",
      "[-0.46275917  0.87961458]\n",
      "Gradient Descent(1794/9): loss=15.625000210488412, w0=74.21682059833152, w1=13.209616433321527\n",
      "[ 1.9065683   0.31197488]\n",
      "Gradient Descent(1795/9): loss=15.848234703346057, w0=72.88222279096848, w1=12.99123401441223\n",
      "[-1.20446325  0.79378211]\n",
      "Gradient Descent(1796/9): loss=15.589941572739303, w0=73.72534706327198, w1=12.435586539952896\n",
      "[-0.12171307 -0.58894415]\n",
      "Gradient Descent(1797/9): loss=16.02405110287331, w0=73.8105462130276, w1=12.847847443927547\n",
      "[-0.02810448  0.61267928]\n",
      "Gradient Descent(1798/9): loss=15.718964839949582, w0=73.83021934676485, w1=12.418971946595684\n",
      "[-1.72621302  3.01987018]\n",
      "Gradient Descent(1799/9): loss=16.092280481632397, w0=75.03856845726956, w1=10.305062820953479\n",
      "[ 2.28492657 -1.91778376]\n",
      "Gradient Descent(1800/9): loss=21.94698358153631, w0=73.43911985795029, w1=11.64751145162638\n",
      "[-3.00138218 -4.33432337]\n",
      "Gradient Descent(1801/9): loss=17.074909299217975, w0=75.54008738431041, w1=14.681537813482292\n",
      "[ 4.12683952  3.09631076]\n",
      "Gradient Descent(1802/9): loss=18.630709451133175, w0=72.6512997224559, w1=12.514120284818137\n",
      "[-1.25348597  0.1731581 ]\n",
      "Gradient Descent(1803/9): loss=16.058553666216064, w0=73.52873990069229, w1=12.39290961767861\n",
      "[-0.11736887 -0.56871752]\n",
      "Gradient Descent(1804/9): loss=16.004027773434782, w0=73.61089811235078, w1=12.791011883026508\n",
      "[ 0.46785304 -2.00762994]\n",
      "Gradient Descent(1805/9): loss=15.673279021199368, w0=73.28340098156022, w1=14.196352839712985\n",
      "[ 1.79593966 -0.79627921]\n",
      "Gradient Descent(1806/9): loss=15.642729949607496, w0=72.02624321787708, w1=14.7537482871275\n",
      "[-3.14197838  3.04786539]\n",
      "Gradient Descent(1807/9): loss=17.000976295087504, w0=74.22562808634956, w1=12.620242511683369\n",
      "[-0.46748238 -0.46047315]\n",
      "Gradient Descent(1808/9): loss=16.189270257071925, w0=74.55286575560663, w1=12.942573719579523\n",
      "[ 2.25572792 -1.23514055]\n",
      "Gradient Descent(1809/9): loss=16.322616555865448, w0=72.97385621135308, w1=13.807172105794262\n",
      "[ 3.85924267  2.90628791]\n",
      "Gradient Descent(1810/9): loss=15.490723842036214, w0=70.27238634386345, w1=11.772770568587854\n",
      "[-2.44614671 -2.14640843]\n",
      "Gradient Descent(1811/9): loss=21.40755200347918, w0=71.98468904291481, w1=13.27525647199164\n",
      "[-3.02650527  0.59674043]\n",
      "Gradient Descent(1812/9): loss=16.2638344599472, w0=74.10324272909682, w1=12.857538172299627\n",
      "[ 1.52780481 -1.86383819]\n",
      "Gradient Descent(1813/9): loss=15.906938294975093, w0=73.03377936054086, w1=14.162224906833892\n",
      "[ 1.01060822  0.86375925]\n",
      "Gradient Descent(1814/9): loss=15.652636602921312, w0=72.32635360332434, w1=13.557593433627293\n",
      "[-1.00827055  2.16515435]\n",
      "Gradient Descent(1815/9): loss=15.85701489696352, w0=73.03214298843194, w1=12.041985388337404\n",
      "[-0.73980589 -0.53517201]\n",
      "Gradient Descent(1816/9): loss=16.4536815251661, w0=73.55000710954694, w1=12.416605798152126\n",
      "[-4.16721742  1.29906876]\n",
      "Gradient Descent(1817/9): loss=15.983775520599481, w0=76.46705930252156, w1=11.507257666032736\n",
      "[ 4.95156528 -2.01058365]\n",
      "Gradient Descent(1818/9): loss=22.365576940265512, w0=73.00096360543613, w1=12.914666222179843\n",
      "[-2.35286406 -0.21619925]\n",
      "Gradient Descent(1819/9): loss=15.588438791223865, w0=74.64796844575982, w1=13.066005697077195\n",
      "[ 2.26433475 -0.15736751]\n",
      "Gradient Descent(1820/9): loss=16.388185387113104, w0=73.0629341232508, w1=13.176162955879315\n",
      "[-2.67498643  1.06948263]\n",
      "Gradient Descent(1821/9): loss=15.45863671205212, w0=74.93542462081949, w1=12.427525111719438\n",
      "[ 0.16010134 -1.42024417]\n",
      "Gradient Descent(1822/9): loss=17.286702374076988, w0=74.82335367955902, w1=13.421696031965313\n",
      "[ 1.18406696  0.1752479 ]\n",
      "Gradient Descent(1823/9): loss=16.557151448338924, w0=73.99450680458217, w1=13.299022503090502\n",
      "[-0.26712974  1.04515   ]\n",
      "Gradient Descent(1824/9): loss=15.64762182730501, w0=74.18149762440372, w1=12.567417505564542\n",
      "[ 4.40630881 -1.4083183 ]\n",
      "Gradient Descent(1825/9): loss=16.195924130605544, w0=71.09708145648042, w1=13.55324031668974\n",
      "[-4.87431771 -1.04380898]\n",
      "Gradient Descent(1826/9): loss=17.801645234973552, w0=74.50910385144859, w1=14.283906603092358\n",
      "[ 0.03117678  2.78534182]\n",
      "Gradient Descent(1827/9): loss=16.447585462321907, w0=74.48728010772265, w1=12.33416732959895\n",
      "[ 3.1879907   2.66109594]\n",
      "Gradient Descent(1828/9): loss=16.754076447192457, w0=72.25568661552971, w1=10.471400171118113\n",
      "[-4.6317521  -1.78019731]\n",
      "Gradient Descent(1829/9): loss=20.4498255662763, w0=75.49791308883057, w1=11.7175382904185\n",
      "[ 2.38559799  0.45371501]\n",
      "Gradient Descent(1830/9): loss=19.36730508190834, w0=73.82799449370587, w1=11.399937786016375\n",
      "[ 0.22556871 -2.61346969]\n",
      "Gradient Descent(1831/9): loss=17.691235877226386, w0=73.67009639456604, w1=13.22936656667316\n",
      "[-1.05490208  0.29248523]\n",
      "Gradient Descent(1832/9): loss=15.487977982492465, w0=74.40852784969717, w1=13.024626905064865\n",
      "[ 0.59048889 -1.12211371]\n",
      "Gradient Descent(1833/9): loss=16.1106123863457, w0=73.99518562913129, w1=13.810106499725574\n",
      "[ 1.57397395 -1.88243602]\n",
      "Gradient Descent(1834/9): loss=15.686353325130858, w0=72.89340386739639, w1=15.12781171106036\n",
      "[-1.33099498  1.6186224 ]\n",
      "Gradient Descent(1835/9): loss=16.8242108688381, w0=73.82510035291754, w1=13.994776029541004\n",
      "[ 2.11017873  2.05850107]\n",
      "Gradient Descent(1836/9): loss=15.659608342231659, w0=72.34797524209486, w1=12.553825280058032\n",
      "[ 0.8673018  -1.94245732]\n",
      "Gradient Descent(1837/9): loss=16.261929017049546, w0=71.74086398165498, w1=13.91354540541209\n",
      "[-4.09754324 -0.36383107]\n",
      "Gradient Descent(1838/9): loss=16.685987999384796, w0=74.60914425092254, w1=14.16822715704407\n",
      "[ 2.57365778 -1.42818045]\n",
      "Gradient Descent(1839/9): loss=16.487818911964737, w0=72.8075838031777, w1=15.167953470785699\n",
      "[ 0.24618312 -1.11260864]\n",
      "Gradient Descent(1840/9): loss=16.929229188171288, w0=72.6352556166061, w1=15.946779516862724\n",
      "[ 0.65846229  1.77945252]\n",
      "Gradient Descent(1841/9): loss=18.646018565754964, w0=72.17433201209815, w1=14.701162755212149\n",
      "[ 0.80305124 -0.17892723]\n",
      "Gradient Descent(1842/9): loss=16.758599184077944, w0=71.6121961467602, w1=14.826411813192122\n",
      "[-4.13934665 -0.74320687]\n",
      "Gradient Descent(1843/9): loss=17.706788402723593, w0=74.50973880095721, w1=15.34665662250564\n",
      "[-0.41485171  5.79610291]\n",
      "Gradient Descent(1844/9): loss=17.867733412665775, w0=74.80013499772849, w1=11.2893845821017\n",
      "[ 3.01882063 -1.26708298]\n",
      "Gradient Descent(1845/9): loss=18.91899471448872, w0=72.68696055899233, w1=12.176342666236565\n",
      "[-1.37343974  0.37163727]\n",
      "Gradient Descent(1846/9): loss=16.419475342591177, w0=73.64836837862036, w1=11.916196579532642\n",
      "[-1.43793015  0.20232108]\n",
      "Gradient Descent(1847/9): loss=16.67099490087356, w0=74.65491948570687, w1=11.77457182057385\n",
      "[ 0.5637901  -2.93259515]\n",
      "Gradient Descent(1848/9): loss=17.765797201478577, w0=74.26026641521428, w1=13.827388423433078\n",
      "[-1.11040206  3.20202772]\n",
      "Gradient Descent(1849/9): loss=15.913237927673238, w0=75.03754785439045, w1=11.585969019719261\n",
      "[ 4.03512757 -4.20309199]\n",
      "Gradient Descent(1850/9): loss=18.699135486647, w0=72.21295855215092, w1=14.52813341254262\n",
      "[-2.57937882 -1.40276991]\n",
      "Gradient Descent(1851/9): loss=16.51972213198511, w0=74.01852372513727, w1=15.510072351428864\n",
      "[-1.87088066  3.62496508]\n",
      "Gradient Descent(1852/9): loss=17.70959239248268, w0=75.32814018546614, w1=12.972596792088812\n",
      "[ 2.26547919  2.40932637]\n",
      "Gradient Descent(1853/9): loss=17.583492815224627, w0=73.7423047541776, w1=11.286068330066932\n",
      "[ 0.98193011 -3.45490748]\n",
      "Gradient Descent(1854/9): loss=17.892448644537193, w0=73.05495367394744, w1=13.704503568849818\n",
      "[-3.55318438  2.78652305]\n",
      "Gradient Descent(1855/9): loss=15.439706326691859, w0=75.54218273815121, w1=11.753937432362378\n",
      "[ 3.3803152  -0.66628794]\n",
      "Gradient Descent(1856/9): loss=19.402375717298046, w0=73.17596209562943, w1=12.220338991942278\n",
      "[ 3.32820145 -3.97445175]\n",
      "Gradient Descent(1857/9): loss=16.185855873123025, w0=70.8462210808358, w1=15.002455214448315\n",
      "[-1.51174914 -0.53005399]\n",
      "Gradient Descent(1858/9): loss=19.54088055501857, w0=71.90444547766572, w1=15.373493006787545\n",
      "[ 1.83696238  4.4039357 ]\n",
      "Gradient Descent(1859/9): loss=18.14441280187432, w0=70.6185718085484, w1=12.290738016179382\n",
      "[-1.81729623  0.71016148]\n",
      "Gradient Descent(1860/9): loss=19.671467282203473, w0=71.89067916886549, w1=11.793624980905424\n",
      "[-0.01446309 -2.93071992]\n",
      "Gradient Descent(1861/9): loss=17.791878544757786, w0=71.90080333092747, w1=13.845128922947776\n",
      "[ 1.8467199  -1.15882943]\n",
      "Gradient Descent(1862/9): loss=16.423042289657428, w0=70.60809940300969, w1=14.656309522586168\n",
      "[-4.37520887  1.24024072]\n",
      "Gradient Descent(1863/9): loss=19.68489973900647, w0=73.67074561117286, w1=13.788141021917124\n",
      "[ 0.09493067  1.02144855]\n",
      "Gradient Descent(1864/9): loss=15.504449981622017, w0=73.60429414000278, w1=13.073127039751217\n",
      "[ 0.1429181   0.37384859]\n",
      "Gradient Descent(1865/9): loss=15.516709142631315, w0=73.50425147013685, w1=12.811433028070564\n",
      "[-3.21056059 -1.87691851]\n",
      "Gradient Descent(1866/9): loss=15.6313057942463, w0=75.751643883462, w1=14.125275986052136\n",
      "[ 0.94792259  1.28028571]\n",
      "Gradient Descent(1867/9): loss=18.614462441110007, w0=75.0880980711178, w1=13.229075988999318\n",
      "[ 4.43755902 -1.76677613]\n",
      "Gradient Descent(1868/9): loss=17.026831066167354, w0=71.981806754101, w1=14.465819279587407\n",
      "[-1.18906574  1.08842157]\n",
      "Gradient Descent(1869/9): loss=16.732914435333814, w0=72.8141527719289, w1=13.703924178842666\n",
      "[ 2.49829115  1.64123597]\n",
      "Gradient Descent(1870/9): loss=15.526112578982312, w0=71.06534896511477, w1=12.555059000723938\n",
      "[ 1.06145002  0.03311769]\n",
      "Gradient Descent(1871/9): loss=18.29664874617889, w0=70.32233395418824, w1=12.531876617619428\n",
      "[-3.59990655 -1.79929469]\n",
      "Gradient Descent(1872/9): loss=20.250252000435207, w0=72.8422685415499, w1=13.79138289877289\n",
      "[-2.1436572   0.72469109]\n",
      "Gradient Descent(1873/9): loss=15.536452532042802, w0=74.34282857982545, w1=13.284099135969937\n",
      "[ 1.05493716 -3.39594908]\n",
      "Gradient Descent(1874/9): loss=15.955122654598387, w0=73.60437256946851, w1=15.661263493200794\n",
      "[-0.78745109  1.59138098]\n",
      "Gradient Descent(1875/9): loss=17.813660156009895, w0=74.15558833513091, w1=14.547296809906848\n",
      "[ 1.33558112  1.92752645]\n",
      "Gradient Descent(1876/9): loss=16.32699050234871, w0=73.22068155068897, w1=13.198028292436089\n",
      "[-0.74464498 -1.124964  ]\n",
      "Gradient Descent(1877/9): loss=15.428242928774125, w0=73.7419330400371, w1=13.985503092188932\n",
      "[ 2.03010727 -0.58889698]\n",
      "Gradient Descent(1878/9): loss=15.614156908339158, w0=72.32085795302945, w1=14.397730975141197\n",
      "[-3.88688002 -0.15641217]\n",
      "Gradient Descent(1879/9): loss=16.28069371066278, w0=75.04167396992794, w1=14.507219493307469\n",
      "[ 1.16928063  0.52677164]\n",
      "Gradient Descent(1880/9): loss=17.441091716790634, w0=74.22317752687535, w1=14.138479341898755\n",
      "[ 3.1616116  -0.19436488]\n",
      "Gradient Descent(1881/9): loss=16.03463270280702, w0=72.01004940972993, w1=14.274534756519566\n",
      "[-0.96223397 -1.43761354]\n",
      "Gradient Descent(1882/9): loss=16.525923546957166, w0=72.6836131874889, w1=15.280864232589819\n",
      "[-0.08936369  2.18635088]\n",
      "Gradient Descent(1883/9): loss=17.194200192428813, w0=72.74616777373794, w1=13.750418614060598\n",
      "[-0.70840304  0.13092863]\n",
      "Gradient Descent(1884/9): loss=15.572546133870262, w0=73.24204990462846, w1=13.658768571962524\n",
      "[-0.72641332 -0.74459081]\n",
      "Gradient Descent(1885/9): loss=15.40326377617165, w0=73.75053922623734, w1=14.179982135705064\n",
      "[ 1.64026509  6.28439126]\n",
      "Gradient Descent(1886/9): loss=15.735326340386921, w0=72.60235366546311, w1=9.78090825347579\n",
      "[ 0.16132023 -2.01971042]\n",
      "Gradient Descent(1887/9): loss=22.46559743754233, w0=72.48942950395572, w1=11.194705547102918\n",
      "[ 1.00742387 -2.65598584]\n",
      "Gradient Descent(1888/9): loss=18.320120197462312, w0=71.78423279754146, w1=13.053895637909731\n",
      "[ 0.90103891 -1.24204786]\n",
      "Gradient Descent(1889/9): loss=16.61612858835497, w0=71.15350556266269, w1=13.923329142930871\n",
      "[-1.57523544 -1.05166127]\n",
      "Gradient Descent(1890/9): loss=17.77497702772983, w0=72.25617037042018, w1=14.659492035107105\n",
      "[-0.95322224 -0.84612585]\n",
      "Gradient Descent(1891/9): loss=16.620292045789213, w0=72.92342593659578, w1=15.251780128331434\n",
      "[-2.37534909  3.41424257]\n",
      "Gradient Descent(1892/9): loss=17.024633491002284, w0=74.58617029634932, w1=12.861810328532204\n",
      "[ 1.16104005  0.13720381]\n",
      "Gradient Descent(1893/9): loss=16.41174220239974, w0=73.77344226468072, w1=12.765767661994623\n",
      "[ 2.80479596 -0.77753985]\n",
      "Gradient Descent(1894/9): loss=15.755716279382687, w0=71.8100850939535, w1=13.310045560095572\n",
      "[ 0.33402331 -1.33684596]\n",
      "Gradient Descent(1895/9): loss=16.501167278044036, w0=71.57626878029536, w1=14.245837734272525\n",
      "[-1.49111932  1.68253132]\n",
      "Gradient Descent(1896/9): loss=17.154528151127334, w0=72.62005230649524, w1=13.068065809106725\n",
      "[ 1.68906771 -0.21344484]\n",
      "Gradient Descent(1897/9): loss=15.697664524460276, w0=71.4377049105605, w1=13.21747719758535\n",
      "[-1.81890969 -2.45798261]\n",
      "Gradient Descent(1898/9): loss=17.1430424741688, w0=72.71094169522453, w1=14.938065025087154\n",
      "[-0.05631771  3.89205395]\n",
      "Gradient Descent(1899/9): loss=16.619217026457655, w0=72.75036409392423, w1=12.213627260619262\n",
      "[-0.52880615 -2.0734056 ]\n",
      "Gradient Descent(1900/9): loss=16.33510130298191, w0=73.1205283985405, w1=13.665011183426724\n",
      "[ 0.28195941 -1.22708724]\n",
      "Gradient Descent(1901/9): loss=15.418088352794259, w0=72.92315681041464, w1=14.52397224827713\n",
      "[ 1.12720103  1.11812216]\n",
      "Gradient Descent(1902/9): loss=15.999860561338295, w0=72.13411609157498, w1=13.741286733932812\n",
      "[-2.61782956  0.48874978]\n",
      "Gradient Descent(1903/9): loss=16.092673300813765, w0=73.96659678009419, w1=13.399161887678309\n",
      "[ 0.38008017  3.53697086]\n",
      "Gradient Descent(1904/9): loss=15.615377742636706, w0=73.70054066365546, w1=10.923282288288192\n",
      "[-2.37751159 -4.42864332]\n",
      "Gradient Descent(1905/9): loss=18.73622478427035, w0=75.36479877758131, w1=14.023332613312768\n",
      "[ 8.20069856  3.53021007]\n",
      "Gradient Descent(1906/9): loss=17.67791462757295, w0=69.6243097830624, w1=11.552185567138153\n",
      "[-6.21961706  1.16807523]\n",
      "Gradient Descent(1907/9): loss=23.976594701047006, w0=73.97804172589449, w1=10.734532902894815\n",
      "[-0.14166032 -1.70135965]\n",
      "Gradient Descent(1908/9): loss=19.387903098782733, w0=74.07720395169316, w1=11.925484659828017\n",
      "[-0.12066899 -1.11444711]\n",
      "Gradient Descent(1909/9): loss=16.900465163645578, w0=74.16167224726989, w1=12.705597634093651\n",
      "[-0.2258704  -3.15611332]\n",
      "Gradient Descent(1910/9): loss=16.062009975303763, w0=74.31978152463657, w1=14.91487696034776\n",
      "[ 1.9267787  -2.70785196]\n",
      "Gradient Descent(1911/9): loss=16.941930356237602, w0=72.97103643174364, w1=16.810373333442204\n",
      "[-3.87043311  9.43193923]\n",
      "Gradient Descent(1912/9): loss=20.984666424845642, w0=75.68033960798164, w1=10.208015875213594\n",
      "[ 5.76577793 -3.25370519]\n",
      "Gradient Descent(1913/9): loss=23.58538155327122, w0=71.64429505411142, w1=12.485609509669708\n",
      "[-0.47786018  0.53559976]\n",
      "Gradient Descent(1914/9): loss=17.240642715667246, w0=71.97879718028953, w1=12.11068967672984\n",
      "[-1.45983271  0.33258174]\n",
      "Gradient Descent(1915/9): loss=17.187776173623053, w0=73.0006800781098, w1=11.877882461943663\n",
      "[-1.04888163 -0.64871317]\n",
      "Gradient Descent(1916/9): loss=16.71181291309695, w0=73.73489722243265, w1=12.331981682474854\n",
      "[ 0.83762996 -0.8279636 ]\n",
      "Gradient Descent(1917/9): loss=16.141760381434228, w0=73.14855624803992, w1=12.911556205620439\n",
      "[-0.46414791  0.25491387]\n",
      "Gradient Descent(1918/9): loss=15.557854220542058, w0=73.4734597819668, w1=12.733116495359852\n",
      "[ 1.01009718 -2.64025347]\n",
      "Gradient Descent(1919/9): loss=15.68070752456362, w0=72.7663917577595, w1=14.581293921606525\n",
      "[-1.64152317  3.29046363]\n",
      "Gradient Descent(1920/9): loss=16.131772834008302, w0=73.91545797695068, w1=12.277969380758966\n",
      "[ 2.73861353 -3.82435125]\n",
      "Gradient Descent(1921/9): loss=16.30113453703809, w0=71.99842850896864, w1=14.955015252864317\n",
      "[-3.95649145  1.88297833]\n",
      "Gradient Descent(1922/9): loss=17.31329876642432, w0=74.76797252065256, w1=13.636930422196238\n",
      "[ 2.97182061  1.16049275]\n",
      "Gradient Descent(1923/9): loss=16.48465908219503, w0=72.68769809546251, w1=12.824585500117994\n",
      "[-1.81736391  2.88558584]\n",
      "Gradient Descent(1924/9): loss=15.784237231718729, w0=73.95985283214186, w1=10.804675415269099\n",
      "[ 0.33581144 -2.51190076]\n",
      "Gradient Descent(1925/9): loss=19.185531332462165, w0=73.7247848226331, w1=12.563005947626609\n",
      "[ 1.93166556 -1.31565534]\n",
      "Gradient Descent(1926/9): loss=15.898884645872226, w0=72.37261892722466, w1=13.483964685417739\n",
      "[-1.84492819 -0.21867211]\n",
      "Gradient Descent(1927/9): loss=15.81029658753841, w0=73.6640686613107, w1=13.637035165872001\n",
      "[ 2.39824785  0.82623391]\n",
      "Gradient Descent(1928/9): loss=15.466767364316134, w0=71.9852951677753, w1=13.058671428658819\n",
      "[-3.08577302  1.54296387]\n",
      "Gradient Descent(1929/9): loss=16.330777729099314, w0=74.14533627913943, w1=11.97859671781158\n",
      "[ 0.84335594 -1.66484153]\n",
      "Gradient Descent(1930/9): loss=16.875015202576883, w0=73.55498711877053, w1=13.143985790600452\n",
      "[ 0.45365484 -2.15513589]\n",
      "Gradient Descent(1931/9): loss=15.476321556275355, w0=73.237428732201, w1=14.652580914277607\n",
      "[-1.10460944  1.66434795]\n",
      "Gradient Descent(1932/9): loss=16.075293848455964, w0=74.01065533748749, w1=13.487537352762528\n",
      "[ 1.49201518 -2.46464918]\n",
      "Gradient Descent(1933/9): loss=15.642771820522603, w0=72.96624470958197, w1=15.212791780101178\n",
      "[-1.50386426  4.16677958]\n",
      "Gradient Descent(1934/9): loss=16.94135608107422, w0=74.01894968847546, w1=12.29604607183996\n",
      "[ 2.10256063 -2.60509278]\n",
      "Gradient Descent(1935/9): loss=16.349253471456407, w0=72.54715724623892, w1=14.119611015239466\n",
      "[ 1.08161674  5.32248212]\n",
      "Gradient Descent(1936/9): loss=15.869451765634652, w0=71.79002552880473, w1=10.393873534091096\n",
      "[ 1.00484399 -3.20400542]\n",
      "Gradient Descent(1937/9): loss=21.277941031179715, w0=71.08663473612134, w1=12.636677329640879\n",
      "[-1.85849374 -1.65906757]\n",
      "Gradient Descent(1938/9): loss=18.177300500541325, w0=72.38758035165066, w1=13.798024629027761\n",
      "[-2.10886393 -0.10299167]\n",
      "Gradient Descent(1939/9): loss=15.847276788940595, w0=73.8637851012494, w1=13.870118799032246\n",
      "[ 0.59106123  0.68038204]\n",
      "Gradient Descent(1940/9): loss=15.62446840925524, w0=73.45004223704414, w1=13.393851372308369\n",
      "[ 0.13857737 -1.08229869]\n",
      "Gradient Descent(1941/9): loss=15.401760693750425, w0=73.35303807896068, w1=14.151460452425571\n",
      "[ 3.01259946 -0.00522894]\n",
      "Gradient Descent(1942/9): loss=15.61325792356574, w0=71.24421845796638, w1=14.155120710281242\n",
      "[-5.24058318  5.22484341]\n",
      "Gradient Descent(1943/9): loss=17.71461834742358, w0=74.91262668355122, w1=10.497730324209146\n",
      "[ 1.86699022 -3.72870886]\n",
      "Gradient Descent(1944/9): loss=21.142098946202744, w0=73.60573353053145, w1=13.107826524288093\n",
      "[ 0.68087198  0.22109625]\n",
      "Gradient Descent(1945/9): loss=15.503650648748101, w0=73.1291231477455, w1=12.953059151216625\n",
      "[-2.65970695 -1.27814569]\n",
      "Gradient Descent(1946/9): loss=15.538149040682672, w0=74.99091801120247, w1=13.847761137173846\n",
      "[-1.45155055  1.71346058]\n",
      "Gradient Descent(1947/9): loss=16.893515519865403, w0=76.00700339730214, w1=12.648338731442875\n",
      "[-1.85043391 -0.82661149]\n",
      "Gradient Descent(1948/9): loss=19.411884314785365, w0=77.30230713164845, w1=13.2269667775209\n",
      "[ 4.88728265 -0.75282439]\n",
      "Gradient Descent(1949/9): loss=23.451403725885665, w0=73.88120927328364, w1=13.753943850171737\n",
      "[-0.42665287  1.22692016]\n",
      "Gradient Descent(1950/9): loss=15.595942472810068, w0=74.17986628469625, w1=12.89509973736547\n",
      "[-0.55510511 -4.48866459]\n",
      "Gradient Descent(1951/9): loss=15.949222507868653, w0=74.56843986233541, w1=16.03716495186448\n",
      "[ 0.74887706  3.11469883]\n",
      "Gradient Descent(1952/9): loss=19.468367444888546, w0=74.04422592161553, w1=13.856875768949013\n",
      "[ 0.14723404 -0.50342256]\n",
      "Gradient Descent(1953/9): loss=15.738491944887594, w0=73.94116209698497, w1=14.209271560438555\n",
      "[ 1.33151012 -0.47837817]\n",
      "Gradient Descent(1954/9): loss=15.861475997802714, w0=73.00910501052138, w1=14.544136279557277\n",
      "[ 0.10264831  6.9651104 ]\n",
      "Gradient Descent(1955/9): loss=15.99294728861953, w0=72.93725119051939, w1=9.668558997372028\n",
      "[ 1.85455034 -3.68356155]\n",
      "Gradient Descent(1956/9): loss=22.711940165278055, w0=71.63906595287514, w1=12.247052085787772\n",
      "[-3.13538674 -1.48639494]\n",
      "Gradient Descent(1957/9): loss=17.514887908912545, w0=73.83383666771603, w1=13.287528545817782\n",
      "[ 0.38971787 -0.67878327]\n",
      "Gradient Descent(1958/9): loss=15.550109115528729, w0=73.56103415598352, w1=13.762676837913268\n",
      "[ 1.43501296  1.4455028 ]\n",
      "Gradient Descent(1959/9): loss=15.46159674686529, w0=72.55652508222919, w1=12.750824878774358\n",
      "[-2.64925263 -1.76841749]\n",
      "Gradient Descent(1960/9): loss=15.923403512353019, w0=74.41100192493973, w1=13.988717123613679\n",
      "[ 0.69841195  2.04926238]\n",
      "Gradient Descent(1961/9): loss=16.139364532350335, w0=73.92211356330978, w1=12.55423345737666\n",
      "[ 1.57698143 -2.44470216]\n",
      "Gradient Descent(1962/9): loss=16.01145585661497, w0=72.81822655922124, w1=14.265524968534955\n",
      "[ 2.22287425  2.05078936]\n",
      "Gradient Descent(1963/9): loss=15.807781614958595, w0=71.2622145853873, w1=12.829972417242747\n",
      "[-3.53703506 -0.00553195]\n",
      "Gradient Descent(1964/9): loss=17.660886427733125, w0=73.73813913068457, w1=12.83384478209107\n",
      "[ 1.23456388 -1.01816269]\n",
      "Gradient Descent(1965/9): loss=15.693124810021027, w0=72.87394441803018, w1=13.546558663854947\n",
      "[ 0.05934547  0.16070049]\n",
      "Gradient Descent(1966/9): loss=15.476312663548937, w0=72.83240258925105, w1=13.434068322300712\n",
      "[-0.87475888  0.12747731]\n",
      "Gradient Descent(1967/9): loss=15.493429645561566, w0=73.44473380394479, w1=13.344834205563185\n",
      "[ 3.32275931 -1.38889872]\n",
      "Gradient Descent(1968/9): loss=15.406356037002979, w0=71.11880228981934, w1=14.317063310675934\n",
      "[-2.71234981 -0.58920549]\n",
      "Gradient Descent(1969/9): loss=18.10203899472343, w0=73.01744715562985, w1=14.729507156785159\n",
      "[ 0.34346139  2.38427922]\n",
      "Gradient Descent(1970/9): loss=16.20510046251089, w0=72.77702418072202, w1=13.060511701748363\n",
      "[-0.32991707 -1.3208412 ]\n",
      "Gradient Descent(1971/9): loss=15.607344175079493, w0=73.00796612753703, w1=13.985100541393715\n",
      "[ 0.3011473  -0.58902678]\n",
      "Gradient Descent(1972/9): loss=15.554481818977068, w0=72.79716301765404, w1=14.397419284998783\n",
      "[-1.29520269  2.40955243]\n",
      "Gradient Descent(1973/9): loss=15.930365544423264, w0=73.70380490155306, w1=12.710732581738796\n",
      "[ 1.0103873 -1.5507503]\n",
      "Gradient Descent(1974/9): loss=15.765554871811686, w0=72.9965337941937, w1=13.796257790509914\n",
      "[-0.73918881  0.00360565]\n",
      "Gradient Descent(1975/9): loss=15.480208222982718, w0=73.51396596013142, w1=13.793733835923938\n",
      "[ 3.74726754 -1.30329228]\n",
      "Gradient Descent(1976/9): loss=15.459402260683879, w0=70.89087868366387, w1=14.706038432614731\n",
      "[-3.06997206 -1.0863212 ]\n",
      "Gradient Descent(1977/9): loss=19.025134190208448, w0=73.0398591258083, w1=15.4664632712987\n",
      "[-0.55461015  0.60542164]\n",
      "Gradient Descent(1978/9): loss=17.391751284174074, w0=73.42808622794236, w1=15.04266812310467\n",
      "[ 0.03340407 -1.02811221]\n",
      "Gradient Descent(1979/9): loss=16.616303130083132, w0=73.40470337656069, w1=15.762346672770779\n",
      "[ 3.33400052  3.36050231]\n",
      "Gradient Descent(1980/9): loss=17.997233657039217, w0=71.0709030102931, w1=13.409995053847563\n",
      "[-2.21164897  1.30624887]\n",
      "Gradient Descent(1981/9): loss=17.85922484442465, w0=72.61905728871841, w1=12.49562084554387\n",
      "[-4.03970997 -5.71029546]\n",
      "Gradient Descent(1982/9): loss=16.097827187725077, w0=75.44685426926571, w1=16.492827666568235\n",
      "[-0.02051227 -0.43295384]\n",
      "Gradient Descent(1983/9): loss=22.24287824170712, w0=75.461212857334, w1=16.795895354289954\n",
      "[-0.30969362  6.17714704]\n",
      "Gradient Descent(1984/9): loss=23.232997271540153, w0=75.67799839036235, w1=12.471892423470017\n",
      "[-1.94651351 -3.30881273]\n",
      "Gradient Descent(1985/9): loss=18.735648569161164, w0=77.04055784405774, w1=14.788061336437892\n",
      "[ 3.71161747  5.21017549]\n",
      "Gradient Descent(1986/9): loss=23.260416358892357, w0=74.44242561640895, w1=11.140938494883962\n",
      "[ 3.58231402 -2.84034499]\n",
      "Gradient Descent(1987/9): loss=18.780349916321143, w0=71.93480580256104, w1=13.129179988369703\n",
      "[-2.06050772  0.81713737]\n",
      "Gradient Descent(1988/9): loss=16.37092278882753, w0=73.37716120945663, w1=12.557183827843227\n",
      "[-1.24120435  0.45601316]\n",
      "Gradient Descent(1989/9): loss=15.814881767150853, w0=74.2460042576874, w1=12.237974615299178\n",
      "[ 0.32284409  1.47360972]\n",
      "Gradient Descent(1990/9): loss=16.610074585950734, w0=74.02001339350853, w1=11.206447813718096\n",
      "[ 2.67143942 -1.35596751]\n",
      "Gradient Descent(1991/9): loss=18.233358242325508, w0=72.15000579899893, w1=12.155625072066906\n",
      "[ 0.09865539  1.33353711]\n",
      "Gradient Descent(1992/9): loss=16.916763681018974, w0=72.08094702477337, w1=11.222149091703374\n",
      "[-5.2802911  -1.76462629]\n",
      "Gradient Descent(1993/9): loss=18.669838141119566, w0=75.77715079276217, w1=12.457387493656617\n",
      "[ 1.67486794  0.81201614]\n",
      "Gradient Descent(1994/9): loss=18.99167462503845, w0=74.6047432376236, w1=11.888976196681089\n",
      "[-0.66882881 -3.553906  ]\n",
      "Gradient Descent(1995/9): loss=17.51023491450548, w0=75.07292340123934, w1=14.376710394544766\n",
      "[ 2.98384583 -0.41992653]\n",
      "Gradient Descent(1996/9): loss=17.370613527613585, w0=72.9842313210875, w1=14.670658964187874\n",
      "[-1.84348511  2.76013512]\n",
      "Gradient Descent(1997/9): loss=16.14301884548936, w0=74.2746708983972, w1=12.738564380461789\n",
      "[ 1.08245861 -1.05880201]\n",
      "Gradient Descent(1998/9): loss=16.141472286983173, w0=73.51694986973628, w1=13.4797257895652\n",
      "[ 0.08493322 -1.63119111]\n",
      "Gradient Descent(1999/9): loss=15.410758583788608, w0=73.4574966164224, w1=14.621559566825644\n",
      "[-0.62499257  5.63100253]\n",
      "Gradient Descent(2000/9): loss=16.051173632295644, w0=73.8949914175764, w1=10.6798577948087\n",
      "[ 1.60530619 -3.65629873]\n",
      "Gradient Descent(2001/9): loss=19.486123093006572, w0=72.77127708662807, w1=13.239266908696917\n",
      "[ 0.02296897  0.15682926]\n",
      "Gradient Descent(2002/9): loss=15.55137374822339, w0=72.75519881002147, w1=13.12948642371144\n",
      "[-1.05994244 -0.79561262]\n",
      "Gradient Descent(2003/9): loss=15.592328337161549, w0=73.49715851751787, w1=13.686415258095378\n",
      "[-3.62828812  1.61331292]\n",
      "Gradient Descent(2004/9): loss=15.42790343796801, w0=76.03696020127846, w1=12.557096214716998\n",
      "[ 2.20420356 -1.14992524]\n",
      "Gradient Descent(2005/9): loss=19.57362749484582, w0=74.49401770969496, w1=13.362043880334268\n",
      "[-0.18005484  1.81193108]\n",
      "Gradient Descent(2006/9): loss=16.11292566689437, w0=74.62005609618555, w1=12.09369212163013\n",
      "[-2.12615373 -3.24341598]\n",
      "Gradient Descent(2007/9): loss=17.225729841092345, w0=76.1083637082184, w1=14.364083310663567\n",
      "[ 2.81994616  1.17339335]\n",
      "Gradient Descent(2008/9): loss=19.73748485025479, w0=74.13440139343604, w1=13.542707967012998\n",
      "[ 1.73717724 -3.53936568]\n",
      "Gradient Descent(2009/9): loss=15.741074890982825, w0=72.91837732311241, w1=16.020263941831296\n",
      "[-0.58001157  6.03816312]\n",
      "Gradient Descent(2010/9): loss=18.683605751248503, w0=73.32438542233999, w1=11.793549758421705\n",
      "[ 2.27616809 -2.51091907]\n",
      "Gradient Descent(2011/9): loss=16.807924164739976, w0=71.73106775941942, w1=13.551193104274594\n",
      "[-3.08185891 -0.1937663 ]\n",
      "Gradient Descent(2012/9): loss=16.609699303810615, w0=73.88836899881107, w1=13.686829513711869\n",
      "[-0.64515867  2.06731592]\n",
      "Gradient Descent(2013/9): loss=15.584020226925059, w0=74.33998006511783, w1=12.239708366511676\n",
      "[ 0.24530945 -1.12562849]\n",
      "Gradient Descent(2014/9): loss=16.701811649346496, w0=74.168263447692, w1=13.02764831116411\n",
      "[ 1.42299207 -1.38422751]\n",
      "Gradient Descent(2015/9): loss=15.87030533658962, w0=73.17216900147952, w1=13.996607568801036\n",
      "[-0.75569904 -0.32677854]\n",
      "Gradient Descent(2016/9): loss=15.52689005508933, w0=73.70115833032622, w1=14.225352547914879\n",
      "[ 0.96092597  0.49146406]\n",
      "Gradient Descent(2017/9): loss=15.746798171342897, w0=73.02851014794659, w1=13.881327708290573\n",
      "[ 1.08549868 -2.421225  ]\n",
      "Gradient Descent(2018/9): loss=15.501757008867882, w0=72.26866107146017, w1=15.57618520780139\n",
      "[-1.60345387  4.1245998 ]\n",
      "Gradient Descent(2019/9): loss=18.10906690035468, w0=73.39107877958837, w1=12.68896534530409\n",
      "[ 2.08772531 -2.41131602]\n",
      "Gradient Descent(2020/9): loss=15.703248068457475, w0=71.92967106403074, w1=14.376886558417228\n",
      "[-0.62564422 -0.22278609]\n",
      "Gradient Descent(2021/9): loss=16.718938883722462, w0=72.36762201525785, w1=14.53283682484688\n",
      "[ 1.09235355  0.1526367 ]\n",
      "Gradient Descent(2022/9): loss=16.369439191902806, w0=71.60297453186719, w1=14.425991134326246\n",
      "[-3.48239246  2.22408662]\n",
      "Gradient Descent(2023/9): loss=17.263261230791198, w0=74.04064925656684, w1=12.869130502153284\n",
      "[-0.32433311 -1.18129753]\n",
      "Gradient Descent(2024/9): loss=15.851093813460043, w0=74.26768243573144, w1=13.696038775705421\n",
      "[ 0.48206261  0.17131117]\n",
      "Gradient Descent(2025/9): loss=15.883391102721262, w0=73.93023861070935, w1=13.576120958320324\n",
      "[ 3.88847655  1.25343026]\n",
      "Gradient Descent(2026/9): loss=15.592984583707608, w0=71.20830502403244, w1=12.698719779632967\n",
      "[-3.44123864 -1.56747293]\n",
      "Gradient Descent(2027/9): loss=17.86576172230212, w0=73.6171720723546, w1=13.795950832094176\n",
      "[-0.10683906  1.22294008]\n",
      "Gradient Descent(2028/9): loss=15.488136534689337, w0=73.69195941264975, w1=12.939892776156759\n",
      "[ 1.00746558  0.58939   ]\n",
      "Gradient Descent(2029/9): loss=15.610807390956813, w0=72.9867335045795, w1=12.527319779603618\n",
      "[ 0.0274905  -3.95702673]\n",
      "Gradient Descent(2030/9): loss=15.886596140351502, w0=72.96749015660563, w1=15.297238491851527\n",
      "[-2.35410214  1.28257254]\n",
      "Gradient Descent(2031/9): loss=17.090867227394554, w0=74.61536165782263, w1=14.399437712462987\n",
      "[ 0.20363696  1.82924759]\n",
      "Gradient Descent(2032/9): loss=16.68193654369297, w0=74.47281578547766, w1=13.118964400549276\n",
      "[ 4.23375913  0.91937745]\n",
      "Gradient Descent(2033/9): loss=16.14585271724261, w0=71.50918439207277, w1=12.475400188152502\n",
      "[-1.67847251  0.55786049]\n",
      "Gradient Descent(2034/9): loss=17.482853581734457, w0=72.6841151502177, w1=12.084897843798174\n",
      "[ 0.52550615 -1.67288795]\n",
      "Gradient Descent(2035/9): loss=16.544573939033345, w0=72.31626084328371, w1=13.255919409700983\n",
      "[-3.04146425 -1.57776661]\n",
      "Gradient Descent(2036/9): loss=15.88884019864727, w0=74.44528581982904, w1=14.360356033211115\n",
      "[ 2.14640275  2.21657566]\n",
      "Gradient Descent(2037/9): loss=16.436473762755973, w0=72.94280389645714, w1=12.808753073994566\n",
      "[ 1.48467644  1.90748075]\n",
      "Gradient Descent(2038/9): loss=15.672623062939401, w0=71.90353039134281, w1=11.473516549222632\n",
      "[-2.60402279 -2.23126032]\n",
      "Gradient Descent(2039/9): loss=18.364893250501645, w0=73.72634634294823, w1=13.035398775837868\n",
      "[-1.6985816  -1.40385735]\n",
      "Gradient Descent(2040/9): loss=15.578090587960327, w0=74.91535345980357, w1=14.018098922015788\n",
      "[ 3.91934687 -0.73363111]\n",
      "Gradient Descent(2041/9): loss=16.84533785954284, w0=72.17181065248786, w1=14.531640702124868\n",
      "[ 1.21128446 -0.384507  ]\n",
      "Gradient Descent(2042/9): loss=16.568731348899092, w0=71.3239115323068, w1=14.800795603548766\n",
      "[-1.86682564  2.10107562]\n",
      "Gradient Descent(2043/9): loss=18.198988863512938, w0=72.63068947857845, w1=13.330042666658965\n",
      "[ 0.09645106  0.50882704]\n",
      "Gradient Descent(2044/9): loss=15.617027078737216, w0=72.56317373428912, w1=12.973863735493858\n",
      "[-5.63917718  0.46963657]\n",
      "Gradient Descent(2045/9): loss=15.780825837677979, w0=76.51059775870584, w1=12.64511813348132\n",
      "[ 1.05170026 -0.70321822]\n",
      "Gradient Descent(2046/9): loss=20.90766315443518, w0=75.77440757804828, w1=13.1373708884748\n",
      "[ 5.0272013  -0.68934857]\n",
      "Gradient Descent(2047/9): loss=18.520891082295154, w0=72.25536666650042, w1=13.619914887385647\n",
      "[-0.940926   -0.56186975]\n",
      "Gradient Descent(2048/9): loss=15.935014825214976, w0=72.91401486932587, w1=14.013223712843807\n",
      "[ 0.67231735  0.31910433]\n",
      "Gradient Descent(2049/9): loss=15.60036972539681, w0=72.44339272415137, w1=13.789850683356066\n",
      "[-7.21205394  2.93500303]\n",
      "Gradient Descent(2050/9): loss=15.795680761707807, w0=77.49183047997671, w1=11.735348560961022\n",
      "[ 0.97084621 -3.43051929]\n",
      "Gradient Descent(2051/9): loss=25.718508325629248, w0=76.81223812979093, w1=14.136712061188502\n",
      "[ 5.43237048  1.44782701]\n",
      "Gradient Descent(2052/9): loss=21.790986310409288, w0=73.00957879644832, w1=13.123233153108645\n",
      "[-2.14505727 -0.10246858]\n",
      "Gradient Descent(2053/9): loss=15.489852137335996, w0=74.51111888254816, w1=13.19496116065855\n",
      "[-0.72917992 -1.61578113]\n",
      "Gradient Descent(2054/9): loss=16.16721363582587, w0=75.02154482822132, w1=14.32600795003388\n",
      "[ 3.19166976 -2.17264955]\n",
      "Gradient Descent(2055/9): loss=17.236336232880646, w0=72.78737599933889, w1=15.846862637283516\n",
      "[-2.30468461 -0.77964065]\n",
      "Gradient Descent(2056/9): loss=18.31588233540003, w0=74.40065522970745, w1=16.392611091285833\n",
      "[ 2.73342213 -0.97949971]\n",
      "Gradient Descent(2057/9): loss=20.240806378296664, w0=72.48725973740115, w1=17.07826088970912\n",
      "[ 1.67900368  4.81651395]\n",
      "Gradient Descent(2058/9): loss=22.18601536396224, w0=71.31195715800328, w1=13.706701128044251\n",
      "[-1.17632721 -0.52524786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2059/9): loss=17.375742123844805, w0=72.13538620208267, w1=14.074374631599095\n",
      "[-1.73271678 -0.63150416]\n",
      "Gradient Descent(2060/9): loss=16.233802032834852, w0=73.34828794600132, w1=14.516427543793094\n",
      "[ 2.6029488   1.06802903]\n",
      "Gradient Descent(2061/9): loss=15.92475480516855, w0=71.5262237827427, w1=13.76880722564587\n",
      "[-1.81416644  2.8950462 ]\n",
      "Gradient Descent(2062/9): loss=16.99005426519051, w0=72.79614029328347, w1=11.742274882483866\n",
      "[ 0.31170443 -2.60507465]\n",
      "Gradient Descent(2063/9): loss=17.019125808075735, w0=72.5779471941022, w1=13.565827138507089\n",
      "[-2.85829485  1.53949814]\n",
      "Gradient Descent(2064/9): loss=15.645905702757865, w0=74.5787535867678, w1=12.488178439957652\n",
      "[ 0.51555513 -1.38678219]\n",
      "Gradient Descent(2065/9): loss=16.702853800954184, w0=74.21786499245908, w1=13.458925969607018\n",
      "[ 3.05629807 -2.75744633]\n",
      "Gradient Descent(2066/9): loss=15.812939232112988, w0=72.0784563452984, w1=15.389138402357485\n",
      "[-0.78648005  3.71232527]\n",
      "Gradient Descent(2067/9): loss=17.94752001269822, w0=72.6289923801974, w1=12.790510709970963\n",
      "[-0.96979751 -0.04404451]\n",
      "Gradient Descent(2068/9): loss=15.844453078758571, w0=73.3078506338915, w1=12.821341863525499\n",
      "[-1.73625803 -2.40818997]\n",
      "Gradient Descent(2069/9): loss=15.602710776905742, w0=74.52323125316282, w1=14.507074841257921\n",
      "[ 2.54616863 -0.44643821]\n",
      "Gradient Descent(2070/9): loss=16.669225243104616, w0=72.74091320966255, w1=14.81958158987625\n",
      "[-2.59299988  2.39687105]\n",
      "Gradient Descent(2071/9): loss=16.436421907197904, w0=74.55601312916045, w1=13.141771856398877\n",
      "[ 2.37922295 -0.66866721]\n",
      "Gradient Descent(2072/9): loss=16.239426792654136, w0=72.89055706256676, w1=13.609838901761847\n",
      "[ 1.10330988  1.63580113]\n",
      "Gradient Descent(2073/9): loss=15.47570595473121, w0=72.11824014551564, w1=12.464778107914153\n",
      "[-0.9428437   1.18606444]\n",
      "Gradient Descent(2074/9): loss=16.592047626923808, w0=72.77823073345317, w1=11.63453300128487\n",
      "[ 1.42891422 -4.48013065]\n",
      "Gradient Descent(2075/9): loss=17.221200182393797, w0=71.77799077652956, w1=14.77062445789104\n",
      "[-0.9137823   0.83954233]\n",
      "Gradient Descent(2076/9): loss=17.368138534603467, w0=72.4176383883511, w1=14.182944827305278\n",
      "[ 3.22191422  1.91143179]\n",
      "Gradient Descent(2077/9): loss=16.017092253497772, w0=70.16229843633516, w1=12.844942575117015\n",
      "[-2.60462604  1.36707333]\n",
      "Gradient Descent(2078/9): loss=20.490887335173465, w0=71.98553666159295, w1=11.887991245454693\n",
      "[-1.50404545 -0.38924446]\n",
      "Gradient Descent(2079/9): loss=17.508612141069403, w0=73.03836847358313, w1=12.160462366057502\n",
      "[-0.87144218 -1.12857959]\n",
      "Gradient Descent(2080/9): loss=16.288752043987383, w0=73.64837800246194, w1=12.950468081607905\n",
      "[ 0.86449077 -1.23728103]\n",
      "Gradient Descent(2081/9): loss=15.588757189716766, w0=73.04323446636351, w1=13.816564800819165\n",
      "[ 1.09953257 -1.28331264]\n",
      "Gradient Descent(2082/9): loss=15.47404474730019, w0=72.27356166596347, w1=14.714883646460558\n",
      "[-2.64419153  5.90166439]\n",
      "Gradient Descent(2083/9): loss=16.669279437439023, w0=74.12449573516106, w1=10.58371857424094\n",
      "[-0.18050588 -2.70184194]\n",
      "Gradient Descent(2084/9): loss=19.924204452595955, w0=74.2508498499742, w1=12.475007931736698\n",
      "[ 0.72914862 -2.08658151]\n",
      "Gradient Descent(2085/9): loss=16.348458891270788, w0=73.74044581584795, w1=13.935614991779897\n",
      "[ 1.71871711  2.52484513]\n",
      "Gradient Descent(2086/9): loss=15.589503197593306, w0=72.53734383888259, w1=12.168223398375885\n",
      "[ 0.37772513 -2.31114831]\n",
      "Gradient Descent(2087/9): loss=16.532094873940302, w0=72.27293624699784, w1=13.786027214302294\n",
      "[-0.67520794 -0.28475185]\n",
      "Gradient Descent(2088/9): loss=15.95400819690832, w0=72.74558180715857, w1=13.985353511075438\n",
      "[-1.89563614  4.03963529]\n",
      "Gradient Descent(2089/9): loss=15.664062802439348, w0=74.0725271034583, w1=11.157608805403818\n",
      "[-2.15621421 -5.11750746]\n",
      "Gradient Descent(2090/9): loss=18.3850834540224, w0=75.5818770520692, w1=14.739864026390926\n",
      "[ 2.30520762  0.59262725]\n",
      "Gradient Descent(2091/9): loss=18.797248040813653, w0=73.96823172110074, w1=14.32502494805818\n",
      "[ 1.44627931  1.85769804]\n",
      "Gradient Descent(2092/9): loss=15.97051128977095, w0=72.95583620121182, w1=13.024636322681449\n",
      "[-1.24736937 -1.33057812]\n",
      "Gradient Descent(2093/9): loss=15.546586007208756, w0=73.82899475972063, w1=13.95604101000416\n",
      "[ 2.07714074 -0.248468  ]\n",
      "Gradient Descent(2094/9): loss=15.642483752488458, w0=72.37499624266772, w1=14.129968608376371\n",
      "[-2.65201123 -0.91220219]\n",
      "Gradient Descent(2095/9): loss=16.019516690022424, w0=74.23140410385821, w1=14.768510142926122\n",
      "[ 0.56883999  3.69553676]\n",
      "Gradient Descent(2096/9): loss=16.655823980374954, w0=73.83321611139094, w1=12.18163441060925\n",
      "[ 0.29150705 -1.97095305]\n",
      "Gradient Descent(2097/9): loss=16.373810215673437, w0=73.6291611732437, w1=13.561301543787195\n",
      "[ 3.97466875 -3.35944804]\n",
      "Gradient Descent(2098/9): loss=15.445408911099456, w0=70.84689304976956, w1=15.912915174874685\n",
      "[-3.41079291  1.97788355]\n",
      "Gradient Descent(2099/9): loss=21.34010100230729, w0=73.23444808625892, w1=14.528396690722357\n",
      "[-0.87291908 -0.23864162]\n",
      "Gradient Descent(2100/9): loss=15.937525776273903, w0=73.84549144006375, w1=14.695445824619439\n",
      "[-0.07465822  1.32175966]\n",
      "Gradient Descent(2101/9): loss=16.27700612860546, w0=73.89775219603108, w1=13.770214063943698\n",
      "[ 1.02491682  1.7156599 ]\n",
      "Gradient Descent(2102/9): loss=15.610388918590342, w0=73.1803104206392, w1=12.569252135504003\n",
      "[-0.56938391  0.06702577]\n",
      "Gradient Descent(2103/9): loss=15.8068106430202, w0=73.57887915517517, w1=12.522334097064691\n",
      "[ 2.95318149 -1.79542363]\n",
      "Gradient Descent(2104/9): loss=15.884774799335679, w0=71.5116521103467, w1=13.779130637149814\n",
      "[-2.34585105  1.46175874]\n",
      "Gradient Descent(2105/9): loss=17.018956482256396, w0=73.15374784803032, w1=12.755899519495168\n",
      "[ 1.46284894  0.39194703]\n",
      "Gradient Descent(2106/9): loss=15.657664833882576, w0=72.12975358759333, w1=12.481536600696781\n",
      "[ 0.26015517  0.08892985]\n",
      "Gradient Descent(2107/9): loss=16.56170941558546, w0=71.9476449656973, w1=12.419285709073087\n",
      "[-1.37901918 -2.99274241]\n",
      "Gradient Descent(2108/9): loss=16.854371218727437, w0=72.91295839272624, w1=14.514205394550618\n",
      "[ 0.1251995   1.09137433]\n",
      "Gradient Descent(2109/9): loss=15.993542346356149, w0=72.82531874352894, w1=13.750243364394494\n",
      "[ 0.04145245 -0.22212231]\n",
      "Gradient Descent(2110/9): loss=15.53227586768603, w0=72.79630203176959, w1=13.905728978506817\n",
      "[ 2.06942573 -1.51446498]\n",
      "Gradient Descent(2111/9): loss=15.600445733943218, w0=71.34770402129847, w1=14.965854464178884\n",
      "[-1.94869868  0.90067727]\n",
      "Gradient Descent(2112/9): loss=18.38407914869934, w0=72.71179310000791, w1=14.335380378468841\n",
      "[-0.39074924  1.7969692 ]\n",
      "Gradient Descent(2113/9): loss=15.921408712907363, w0=72.98531756692526, w1=13.07750193677014\n",
      "[-2.83823578 -0.52060652]\n",
      "Gradient Descent(2114/9): loss=15.514392859974512, w0=74.972082612497, w1=13.44192649847815\n",
      "[-0.17187936 -1.24153705]\n",
      "Gradient Descent(2115/9): loss=16.794713274463717, w0=75.09239816283124, w1=14.311002431761036\n",
      "[ 1.25579238  0.05195641]\n",
      "Gradient Descent(2116/9): loss=17.34866764854595, w0=74.21334349737417, w1=14.274632947306147\n",
      "[ 1.97747139 -2.66282391]\n",
      "Gradient Descent(2117/9): loss=16.12450512226196, w0=72.82911352571593, w1=16.13860968379336\n",
      "[ 1.90268768  2.15729345]\n",
      "Gradient Descent(2118/9): loss=19.02877861854062, w0=71.49723214856962, w1=14.62850427101316\n",
      "[-3.47210016  0.23466558]\n",
      "Gradient Descent(2119/9): loss=17.659796424986055, w0=73.92770225921294, w1=14.46423836804425\n",
      "[ 1.12654561  1.2199756 ]\n",
      "Gradient Descent(2120/9): loss=16.071372232408287, w0=73.13912033224726, w1=13.610255444680725\n",
      "[-4.64952509 -1.01542007]\n",
      "Gradient Descent(2121/9): loss=15.406390386014484, w0=76.39378789683383, w1=14.321049496571767\n",
      "[ 4.32759659 -1.35786178]\n",
      "Gradient Descent(2122/9): loss=20.544396177076614, w0=73.36447028603112, w1=15.271552742613036\n",
      "[-3.45741652  4.80209889]\n",
      "Gradient Descent(2123/9): loss=16.99372224302486, w0=75.78466185291738, w1=11.910083518328808\n",
      "[ 1.88830922 -1.29534141]\n",
      "Gradient Descent(2124/9): loss=19.71964783904938, w0=74.46284540117196, w1=12.816822506825021\n",
      "[ 0.31568277 -1.66441124]\n",
      "Gradient Descent(2125/9): loss=16.288790353702964, w0=74.24186746068884, w1=13.981910372320964\n",
      "[ 1.58930406 -0.41815236]\n",
      "Gradient Descent(2126/9): loss=15.961289549184297, w0=73.12935462142752, w1=14.274617026368245\n",
      "[ 1.06060761  0.17435249]\n",
      "Gradient Descent(2127/9): loss=15.715365734918818, w0=72.38692929132584, w1=14.152570280052124\n",
      "[-0.38009014  2.51933535]\n",
      "Gradient Descent(2128/9): loss=16.0235745973643, w0=72.6529923907031, w1=12.38903553361505\n",
      "[-1.76191137  1.35156723]\n",
      "Gradient Descent(2129/9): loss=16.18607130381081, w0=73.88633034805542, w1=11.442938474535596\n",
      "[ 3.21159657 -1.68013782]\n",
      "Gradient Descent(2130/9): loss=17.635585775995768, w0=71.63821274610706, w1=12.619034948951688\n",
      "[-3.14370517 -0.55830343]\n",
      "Gradient Descent(2131/9): loss=17.12695730651414, w0=73.83880636414584, w1=13.00984735104085\n",
      "[ 0.1106599  -0.96491112]\n",
      "Gradient Descent(2132/9): loss=15.644723951384448, w0=73.76134443346125, w1=13.68528513793495\n",
      "[ 0.14532106 -2.11357586]\n",
      "Gradient Descent(2133/9): loss=15.516259801595046, w0=73.65961969233967, w1=15.164788239977833\n",
      "[ 0.8275673   1.81410673]\n",
      "Gradient Descent(2134/9): loss=16.87249550343012, w0=73.0803225802148, w1=13.894913531596647\n",
      "[ 0.1248622 -1.0335879]\n",
      "Gradient Descent(2135/9): loss=15.494896200657433, w0=72.99291903805694, w1=14.618425062824702\n",
      "[ 1.46557103  1.16941486]\n",
      "Gradient Descent(2136/9): loss=16.079522485408507, w0=71.96701931843333, w1=13.799834661134474\n",
      "[-1.7182631   2.09516073]\n",
      "Gradient Descent(2137/9): loss=16.317462354633342, w0=73.1698034851069, w1=12.33322214921119\n",
      "[ 0.26777782 -2.23939772]\n",
      "Gradient Descent(2138/9): loss=16.050810559651826, w0=72.98235901096683, w1=13.900800553188684\n",
      "[ 0.16649255  2.06897657]\n",
      "Gradient Descent(2139/9): loss=15.523081219197396, w0=72.86581422645233, w1=12.452516950854433\n",
      "[-0.05222361  0.29967519]\n",
      "Gradient Descent(2140/9): loss=16.005091283929893, w0=72.90237075213437, w1=12.242744316459161\n",
      "[-2.7930743  -2.33076201]\n",
      "Gradient Descent(2141/9): loss=16.22758912263594, w0=74.85752275977352, w1=13.874277723070557\n",
      "[ 2.56389129  0.45410128]\n",
      "Gradient Descent(2142/9): loss=16.686152416799306, w0=73.06279885695541, w1=13.556406829376636\n",
      "[-0.40635792  0.98459645]\n",
      "Gradient Descent(2143/9): loss=15.415537838006601, w0=73.34724940277131, w1=12.867189312124479\n",
      "[-0.42836822 -0.07774367]\n",
      "Gradient Descent(2144/9): loss=15.574902062682185, w0=73.6471071578506, w1=12.9216098806315\n",
      "[ 2.29403081 -2.38180944]\n",
      "Gradient Descent(2145/9): loss=15.603996976539065, w0=72.04128559151187, w1=14.588876490669282\n",
      "[ 0.28035611 -0.73906074]\n",
      "Gradient Descent(2146/9): loss=16.78555930860798, w0=71.84503631378551, w1=15.106219011703432\n",
      "[ 3.9065749   3.81986367]\n",
      "Gradient Descent(2147/9): loss=17.758284559785768, w0=69.1104338867974, w1=12.43231444265797\n",
      "[-5.43349127 -3.99813338]\n",
      "Gradient Descent(2148/9): loss=24.68519555145976, w0=72.91387777641178, w1=15.231007807065641\n",
      "[ 1.86904126  1.97584226]\n",
      "Gradient Descent(2149/9): loss=16.991622415699297, w0=71.60554889677577, w1=13.847918227947604\n",
      "[-1.42764847  0.72005659]\n",
      "Gradient Descent(2150/9): loss=16.878977493213366, w0=72.60490282323157, w1=13.343878617285984\n",
      "[-1.86879455  2.48085414]\n",
      "Gradient Descent(2151/9): loss=15.632486996273135, w0=73.91305900964075, w1=11.60728072132833\n",
      "[-0.12539826 -2.51781132]\n",
      "Gradient Descent(2152/9): loss=17.33055344704065, w0=74.00083779395796, w1=13.369748642247957\n",
      "[-0.72402248 -1.52630115]\n",
      "Gradient Descent(2153/9): loss=15.641798855071812, w0=74.50765353146647, w1=14.438159449536188\n",
      "[ 3.8311856   2.75344577]\n",
      "Gradient Descent(2154/9): loss=16.581770321359404, w0=71.82582361213602, w1=12.510747411399652\n",
      "[ 1.04540906 -4.47164132]\n",
      "Gradient Descent(2155/9): loss=16.932990918614234, w0=71.09403727354194, w1=15.640896331991296\n",
      "[-0.28461746  1.45786876]\n",
      "Gradient Descent(2156/9): loss=20.14099219664321, w0=71.2932694961852, w1=14.620388200227518\n",
      "[-2.24401299  0.87817968]\n",
      "Gradient Descent(2157/9): loss=18.037763694252547, w0=72.86407858739304, w1=14.005662421554304\n",
      "[-2.46256036 -0.58976954]\n",
      "Gradient Descent(2158/9): loss=15.616582243599096, w0=74.58787084159506, w1=14.418501099639306\n",
      "[ 1.92823432  0.95435739]\n",
      "Gradient Descent(2159/9): loss=16.6637017468759, w0=73.23810682066933, w1=13.750450929667053\n",
      "[-0.56627563 -0.43414119]\n",
      "Gradient Descent(2160/9): loss=15.424095202319016, w0=73.63449976015997, w1=14.054349762050219\n",
      "[-0.21995166  1.79275167]\n",
      "Gradient Descent(2161/9): loss=15.608988502296215, w0=73.78846592417385, w1=12.799423591634907\n",
      "[ 0.85661311 -0.00565715]\n",
      "Gradient Descent(2162/9): loss=15.739571169452983, w0=73.18883674764055, w1=12.803383594112415\n",
      "[ 3.77067647  2.87339893]\n",
      "Gradient Descent(2163/9): loss=15.620119674683117, w0=70.54936321745137, w1=10.792004343877625\n",
      "[-3.60752129 -3.26206406]\n",
      "Gradient Descent(2164/9): loss=22.764076721552517, w0=73.07462812066001, w1=13.075449184301203\n",
      "[ 0.18913746 -1.39170246]\n",
      "Gradient Descent(2165/9): loss=15.491647159977397, w0=72.94223189693227, w1=14.049640908228222\n",
      "[ 0.26094652  0.0183781 ]\n",
      "Gradient Descent(2166/9): loss=15.610140066172036, w0=72.7595693335569, w1=14.03677624130095\n",
      "[-0.0580467  -0.05652997]\n",
      "Gradient Descent(2167/9): loss=15.683814298173093, w0=72.80020202052101, w1=14.076347221801825\n",
      "[ 2.13118029 -0.50405488]\n",
      "Gradient Descent(2168/9): loss=15.685754113354703, w0=71.30837581924784, w1=14.429185635368496\n",
      "[ 0.92450786 -0.88601036]\n",
      "Gradient Descent(2169/9): loss=17.80783437007849, w0=70.66122031488732, w1=15.049392890789855\n",
      "[-3.53952598  1.62639335]\n",
      "Gradient Descent(2170/9): loss=20.08339532243084, w0=73.13888850091634, w1=13.910917546444768\n",
      "[-2.74223934 -1.87591903]\n",
      "Gradient Descent(2171/9): loss=15.490874486147607, w0=75.05845603561717, w1=15.224060867343821\n",
      "[ 3.9214942  5.758124 ]\n",
      "Gradient Descent(2172/9): loss=18.464053773269704, w0=72.31341009510774, w1=11.19337406501715\n",
      "[-3.07780043 -2.91257796]\n",
      "Gradient Descent(2173/9): loss=18.480261239714164, w0=74.46787039681224, w1=13.232178638358754\n",
      "[ 1.35715817 -0.36909358]\n",
      "Gradient Descent(2174/9): loss=16.10560177578414, w0=73.5178596780045, w1=13.49054414160624\n",
      "[-0.84695335 -0.47796964]\n",
      "Gradient Descent(2175/9): loss=15.411020573107113, w0=74.11072702202684, w1=13.825122886603442\n",
      "[-1.75230752  3.28104737]\n",
      "Gradient Descent(2176/9): loss=15.779127279156228, w0=75.3373422858679, w1=11.528389724761144\n",
      "[ 2.47510583 -2.17842515]\n",
      "Gradient Descent(2177/9): loss=19.37750125660144, w0=73.60476820716565, w1=13.05328732746429\n",
      "[ 2.78720213  1.474521  ]\n",
      "Gradient Descent(2178/9): loss=15.525119736593398, w0=71.6537267192505, w1=12.021122624978657\n",
      "[-4.87479431 -3.84945207]\n",
      "Gradient Descent(2179/9): loss=17.79475026871186, w0=75.06608273817967, w1=14.715739071577966\n",
      "[-3.05957064  3.3346648 ]\n",
      "Gradient Descent(2180/9): loss=17.72004562925008, w0=77.20778218381413, w1=12.381473710566418\n",
      "[ 0.68185975  0.35890504]\n",
      "Gradient Descent(2181/9): loss=23.64810277772351, w0=76.73048035772553, w1=12.13024018210014\n",
      "[ 3.94966991 -0.42195638]\n",
      "Gradient Descent(2182/9): loss=22.201392215279913, w0=73.9657114231973, w1=12.42560964994459\n",
      "[ 1.64357663  2.41426544]\n",
      "Gradient Descent(2183/9): loss=16.16710472269428, w0=72.8152077831801, w1=10.735623839994455\n",
      "[-1.20892411 -2.9596703 ]\n",
      "Gradient Descent(2184/9): loss=19.265482629119635, w0=73.6614546579184, w1=12.807393046545196\n",
      "[ 2.13385243 -1.68773204]\n",
      "Gradient Descent(2185/9): loss=15.679434675412715, w0=72.16775796035571, w1=13.988805472318816\n",
      "[-1.21408954  0.09204328]\n",
      "Gradient Descent(2186/9): loss=16.149598453622982, w0=73.01762063728174, w1=13.924375176072836\n",
      "[-2.15686848 -0.15106885]\n",
      "Gradient Descent(2187/9): loss=15.522921567585124, w0=74.52742856999849, w1=14.03012337037511\n",
      "[ 0.50421848  0.67309371]\n",
      "Gradient Descent(2188/9): loss=16.298133194243633, w0=74.17447563249627, w1=13.558957770505652\n",
      "[ 2.63702817  1.2470056 ]\n",
      "Gradient Descent(2189/9): loss=15.776715128427425, w0=72.32855591605025, w1=12.686053852743152\n",
      "[-0.68473382  1.18270013]\n",
      "Gradient Descent(2190/9): loss=16.1668006814682, w0=72.80786958980514, w1=11.858163762062453\n",
      "[ 0.35433293 -1.25126442]\n",
      "Gradient Descent(2191/9): loss=16.81872139191575, w0=72.55983654124432, w1=12.734048853617923\n",
      "[ 1.15443476 -2.03770194]\n",
      "Gradient Descent(2192/9): loss=15.93333568904466, w0=71.75173220581128, w1=14.160440212121843\n",
      "[-0.12972693 -2.4310427 ]\n",
      "Gradient Descent(2193/9): loss=16.806757706006007, w0=71.84254105987608, w1=15.862170104434789\n",
      "[-4.01548184  1.29389692]\n",
      "Gradient Descent(2194/9): loss=19.27719346191274, w0=74.6533783482753, w1=14.956442257384971\n",
      "[-1.96639433  1.81367263]\n",
      "Gradient Descent(2195/9): loss=17.400314131577233, w0=76.02985437658968, w1=13.686871417704504\n",
      "[-0.61371353  1.38235887]\n",
      "Gradient Descent(2196/9): loss=19.150008269765426, w0=76.45945385006019, w1=12.719220210503504\n",
      "[ 1.47538146  1.4446553 ]\n",
      "Gradient Descent(2197/9): loss=20.685358020789575, w0=75.42668683050802, w1=11.707961501531097\n",
      "[ 3.4060378  -4.79090592]\n",
      "Gradient Descent(2198/9): loss=19.22978146057004, w0=73.04246036977543, w1=15.061595647387259\n",
      "[ 0.33295548  3.79658643]\n",
      "Gradient Descent(2199/9): loss=16.668681593930017, w0=72.80939153642335, w1=12.403985149740954\n",
      "[ 2.36090595 -1.14235431]\n",
      "Gradient Descent(2200/9): loss=16.081867351029945, w0=71.15675737199925, w1=13.203633163942456\n",
      "[-2.35022242 -0.89062559]\n",
      "Gradient Descent(2201/9): loss=17.707734078868143, w0=72.80191306645432, w1=13.827071076760069\n",
      "[-0.29915262 -0.3868316 ]\n",
      "Gradient Descent(2202/9): loss=15.567253278216056, w0=73.01131990096671, w1=14.097853193461523\n",
      "[ 0.91247327  1.38999437]\n",
      "Gradient Descent(2203/9): loss=15.616868841255803, w0=72.37258860917036, w1=13.124857131520413\n",
      "[-1.94080472  0.34741854]\n",
      "Gradient Descent(2204/9): loss=15.873276622497709, w0=73.73115190990337, w1=12.881664151441747\n",
      "[ 1.17769932 -0.67923921]\n",
      "Gradient Descent(2205/9): loss=15.66030373969294, w0=72.90676238937658, w1=13.357131598383141\n",
      "[ 1.46893312 -1.71613323]\n",
      "Gradient Descent(2206/9): loss=15.468347182444983, w0=71.87850920767515, w1=14.558424860861326\n",
      "[ 1.474418    0.60810919]\n",
      "Gradient Descent(2207/9): loss=16.969394807013153, w0=70.84641660593024, w1=14.132748430668162\n",
      "[-3.94236656 -2.51719469]\n",
      "Gradient Descent(2208/9): loss=18.594257206808443, w0=73.60607320118199, w1=15.89478471326287\n",
      "[ 1.49168626 -1.48947718]\n",
      "Gradient Descent(2209/9): loss=18.350894109015293, w0=72.56189282149015, w1=16.937418737027453\n",
      "[-5.020364    6.76177638]\n",
      "Gradient Descent(2210/9): loss=21.63168766504342, w0=76.07614762198253, w1=12.204175274085369\n",
      "[ 3.25399274  0.8018829 ]\n",
      "Gradient Descent(2211/9): loss=20.069775093203432, w0=73.79835270522157, w1=11.642857240835793\n",
      "[-0.31293834  1.59668065]\n",
      "Gradient Descent(2212/9): loss=17.200131538096546, w0=74.0174095459166, w1=10.525180783344856\n",
      "[-2.43744142 -2.45199638]\n",
      "Gradient Descent(2213/9): loss=20.012233622138226, w0=75.72361854271652, w1=12.241578250533408\n",
      "[ 2.77199568 -1.3327102 ]\n",
      "Gradient Descent(2214/9): loss=19.10408863791756, w0=73.78322156919812, w1=13.174475393057222\n",
      "[-1.18499664 -2.26128846]\n",
      "Gradient Descent(2215/9): loss=15.55217972789171, w0=74.61271921581896, w1=14.757377313683596\n",
      "[ 1.91744891  3.97350387]\n",
      "Gradient Descent(2216/9): loss=17.07171468540377, w0=73.27050498214147, w1=11.97592460488909\n",
      "[-1.36609717 -3.24776816]\n",
      "Gradient Descent(2217/9): loss=16.51685096621976, w0=74.2267729986776, w1=14.249362320291226\n",
      "[ 1.8800772   0.50064314]\n",
      "Gradient Descent(2218/9): loss=16.1171738327053, w0=72.91071895927209, w1=13.898912120283134\n",
      "[ 1.89552565 -1.05409026]\n",
      "Gradient Descent(2219/9): loss=15.547174342923004, w0=71.58385100730624, w1=14.636775304993527\n",
      "[ 0.48182984  3.42482878]\n",
      "Gradient Descent(2220/9): loss=17.517456515027245, w0=71.24657012223308, w1=12.239395159915073\n",
      "[-3.45169986  0.44384813]\n",
      "Gradient Descent(2221/9): loss=18.25090620026079, w0=73.66276002191192, w1=11.928701470991339\n",
      "[-1.50190565  0.84681066]\n",
      "Gradient Descent(2222/9): loss=16.65672611647743, w0=74.71409397928451, w1=11.335934008142992\n",
      "[ 0.93900173 -3.14404329]\n",
      "Gradient Descent(2223/9): loss=18.692225062917394, w0=74.0567927649111, w1=13.536764314642243\n",
      "[ 0.02478517  1.80142676]\n",
      "Gradient Descent(2224/9): loss=15.678501227687413, w0=74.039443142868, w1=12.27576558058067\n",
      "[-3.10591514 -1.04084787]\n",
      "Gradient Descent(2225/9): loss=16.38853276861145, w0=76.21358374360187, w1=13.004359092198753\n",
      "[ 2.88386201 -1.93468176]\n",
      "Gradient Descent(2226/9): loss=19.76108061146008, w0=74.19488033443103, w1=14.358636322989778\n",
      "[ 2.38590075  2.95835593]\n",
      "Gradient Descent(2227/9): loss=16.17800442757224, w0=72.52474981171711, w1=12.287787168701453\n",
      "[ 0.69097956 -0.70597844]\n",
      "Gradient Descent(2228/9): loss=16.392043718269974, w0=72.04106411895674, w1=12.781972076383564\n",
      "[-0.55411525  1.15660363]\n",
      "Gradient Descent(2229/9): loss=16.414135110526463, w0=72.42894479470625, w1=11.97234953220336\n",
      "[-1.47414992 -1.39704143]\n",
      "Gradient Descent(2230/9): loss=16.896052113836475, w0=73.46084973610897, w1=12.950278529822109\n",
      "[ 3.12924074 -2.72803701]\n",
      "Gradient Descent(2231/9): loss=15.539970432989374, w0=71.27038121583901, w1=14.859904437953082\n",
      "[-0.94393432  4.45288138]\n",
      "Gradient Descent(2232/9): loss=18.385711508193715, w0=71.93113523636272, w1=11.742887473830091\n",
      "[ 0.0565838  -3.54209841]\n",
      "Gradient Descent(2233/9): loss=17.822762226123213, w0=71.89152657353634, w1=14.22235636101751\n",
      "[-1.53164373  0.08961062]\n",
      "Gradient Descent(2234/9): loss=16.64500433829829, w0=72.96367718727036, w1=14.159628927310353\n",
      "[-0.80648634 -1.50511379]\n",
      "Gradient Descent(2235/9): loss=15.671561905957299, w0=73.5282176226987, w1=15.213208579038389\n",
      "[ 0.25655974 -1.79891702]\n",
      "Gradient Descent(2236/9): loss=16.915839528461017, w0=73.34862580481789, w1=16.47245048962677\n",
      "[-0.90090348  1.6347275 ]\n",
      "Gradient Descent(2237/9): loss=19.8656246536834, w0=73.97925823901869, w1=15.328141236575876\n",
      "[ 3.47295326 -0.12238993]\n",
      "Gradient Descent(2238/9): loss=17.329075264910635, w0=71.54819095720757, w1=15.413814188032472\n",
      "[-1.6670718   1.44415307]\n",
      "Gradient Descent(2239/9): loss=18.7800511049519, w0=72.71514121836053, w1=14.402907040485617\n",
      "[ 0.1818201   3.43647871]\n",
      "Gradient Descent(2240/9): loss=15.979525606454425, w0=72.5878671501285, w1=11.997371945480532\n",
      "[ 3.15335034 -1.62719508]\n",
      "Gradient Descent(2241/9): loss=16.73381125924748, w0=70.3805219112635, w1=13.13640849802008\n",
      "[-4.05466986 -1.69234755]\n",
      "Gradient Descent(2242/9): loss=19.68876671005678, w0=73.21879081450669, w1=14.321051785834296\n",
      "[-0.96936592 -1.78080256]\n",
      "Gradient Descent(2243/9): loss=15.742636168144731, w0=73.89734695742852, w1=15.567613580670253\n",
      "[-2.76499861  0.47953095]\n",
      "Gradient Descent(2244/9): loss=17.747614304251325, w0=75.83284598385517, w1=15.23194191603423\n",
      "[ 5.17318119 -2.61451152]\n",
      "Gradient Descent(2245/9): loss=20.144109438503904, w0=72.21161914910715, w1=17.062099980263238\n",
      "[-1.39107646  4.88181414]\n",
      "Gradient Descent(2246/9): loss=22.38832786390107, w0=73.1853726689535, w1=13.644830079064096\n",
      "[-1.63439014  1.75427641]\n",
      "Gradient Descent(2247/9): loss=15.405411265885684, w0=74.3294457692985, w1=12.416836589292503\n",
      "[-1.62760973 -1.6203304 ]\n",
      "Gradient Descent(2248/9): loss=16.486895136723092, w0=75.46877257880914, w1=13.551067870831249\n",
      "[ 2.07381103  2.53485447]\n",
      "Gradient Descent(2249/9): loss=17.753421183436263, w0=74.0171048573579, w1=11.776669743441413\n",
      "[ 0.5565591  -4.27052201]\n",
      "Gradient Descent(2250/9): loss=17.09756179451204, w0=73.62751349045358, w1=14.766035149408824\n",
      "[ 1.81257705  4.14967238]\n",
      "Gradient Descent(2251/9): loss=16.268842572194774, w0=72.35870955279182, w1=11.861264482847387\n",
      "[ 0.38891117 -1.57506334]\n",
      "Gradient Descent(2252/9): loss=17.13288591840052, w0=72.08647173220699, w1=12.963808818446656\n",
      "[ 0.58506525 -1.70255479]\n",
      "Gradient Descent(2253/9): loss=16.24793421674878, w0=71.67692605772548, w1=14.155597169142041\n",
      "[-2.60045479 -2.01199893]\n",
      "Gradient Descent(2254/9): loss=16.921635897830146, w0=73.49724441088337, w1=15.563996418893414\n",
      "[-1.42610495  0.26146133]\n",
      "Gradient Descent(2255/9): loss=17.578677732565207, w0=74.49551787916016, w1=15.380973486799437\n",
      "[-0.93375856  0.53506653]\n",
      "Gradient Descent(2256/9): loss=17.915200988272723, w0=75.14914887053963, w1=15.006426919089126\n",
      "[ 1.25590865  0.43605077]\n",
      "Gradient Descent(2257/9): loss=18.272249793490417, w0=74.2700128168459, w1=14.701191377727065\n",
      "[ 0.67623287  0.48159218]\n",
      "Gradient Descent(2258/9): loss=16.60826991191619, w0=73.79664980971252, w1=14.364076849086366\n",
      "[ 2.08035732 -0.47222583]\n",
      "Gradient Descent(2259/9): loss=15.90330570156108, w0=72.34039968452582, w1=14.694634931843213\n",
      "[-0.84312647  1.03925806]\n",
      "Gradient Descent(2260/9): loss=16.578508610571543, w0=72.93058821353, w1=13.967154293273376\n",
      "[-1.80431364  3.10202396]\n",
      "Gradient Descent(2261/9): loss=15.57069337239344, w0=74.19360775992564, w1=11.795737519938305\n",
      "[-2.15512282 -0.40457942]\n",
      "Gradient Descent(2262/9): loss=17.208490857501964, w0=75.70219373183421, w1=12.07894311520601\n",
      "[ 3.02635411 -1.04884182]\n",
      "Gradient Descent(2263/9): loss=19.266851574558135, w0=73.58374585456639, w1=12.81313239256844\n",
      "[-1.31534078 -3.1229376 ]\n",
      "Gradient Descent(2264/9): loss=15.650051278033855, w0=74.50448440265177, w1=14.999188709537876\n",
      "[ 1.28901972  1.52419577]\n",
      "Gradient Descent(2265/9): loss=17.273022606096337, w0=73.6021705953594, w1=13.932251670165927\n",
      "[ 1.19566822  1.56013192]\n",
      "Gradient Descent(2266/9): loss=15.535792346138233, w0=72.76520283798929, w1=12.840159328511835\n",
      "[-0.81582074  0.52801214]\n",
      "Gradient Descent(2267/9): loss=15.730173934083435, w0=73.33627735629727, w1=12.470550832258786\n",
      "[-0.97969096  0.26327576]\n",
      "Gradient Descent(2268/9): loss=15.895988427056325, w0=74.02206102834026, w1=12.286257799894601\n",
      "[ 1.93445743 -2.66759285]\n",
      "Gradient Descent(2269/9): loss=16.36314807260689, w0=72.66794082774639, w1=14.153572792374947\n",
      "[ 0.24630016  0.90690312]\n",
      "Gradient Descent(2270/9): loss=15.808857974783336, w0=72.49553071416933, w1=13.518740606472495\n",
      "[-0.45378678  0.57074546]\n",
      "Gradient Descent(2271/9): loss=15.705363792240009, w0=72.81318146128736, w1=13.11921878422433\n",
      "[ 0.94894225 -0.7077061 ]\n",
      "Gradient Descent(2272/9): loss=15.566421438743149, w0=72.14892188885501, w1=13.6146130517952\n",
      "[-2.48436814  3.51019208]\n",
      "Gradient Descent(2273/9): loss=16.0504995867082, w0=73.88797958550796, w1=11.15747859645506\n",
      "[ 3.27468604 -4.80747648]\n",
      "Gradient Descent(2274/9): loss=18.258725075444723, w0=71.59569935526392, w1=14.522712135526081\n",
      "[ 0.42106752 -0.01848615]\n",
      "Gradient Descent(2275/9): loss=17.37179213561186, w0=71.30095208991709, w1=14.535652441383732\n",
      "[-3.49961772  2.04095317]\n",
      "Gradient Descent(2276/9): loss=17.929357052825328, w0=73.75068449429673, w1=13.106985221314527\n",
      "[-0.05679037 -1.79956005]\n",
      "Gradient Descent(2277/9): loss=15.559666643872703, w0=73.79043775317753, w1=14.366677253033327\n",
      "[-3.01026052  0.62845843]\n",
      "Gradient Descent(2278/9): loss=15.902505108585022, w0=75.89762011811213, w1=13.926756354868152\n",
      "[ 1.11997756  1.51177239]\n",
      "Gradient Descent(2279/9): loss=18.875433941628877, w0=75.11363582448693, w1=12.868515684372445\n",
      "[ 4.02642609 -2.29540378]\n",
      "Gradient Descent(2280/9): loss=17.228347800495136, w0=72.29513756038311, w1=14.47529833381632\n",
      "[-3.97173882 -0.99033133]\n",
      "Gradient Descent(2281/9): loss=16.380268690314296, w0=75.0753547352085, w1=15.168530266175164\n",
      "[ 0.4537274 -1.3834721]\n",
      "Gradient Descent(2282/9): loss=18.398691993581462, w0=74.75774555530047, w1=16.136960739632414\n",
      "[ 4.7489161   1.29588085]\n",
      "Gradient Descent(2283/9): loss=19.98776184253905, w0=71.43350428530606, w1=15.229844141710933\n",
      "[-2.57087672  1.11728244]\n",
      "Gradient Descent(2284/9): loss=18.64794540475608, w0=73.23311799090759, w1=14.44774643657226\n",
      "[-0.88944861  2.2915639 ]\n",
      "Gradient Descent(2285/9): loss=15.85628134682886, w0=73.85573202098273, w1=12.84365170918168\n",
      "[ 2.6815145  -1.34489673]\n",
      "Gradient Descent(2286/9): loss=15.74598974094229, w0=71.97867187135208, w1=13.785079420757588\n",
      "[-2.96437658 -2.71483931]\n",
      "Gradient Descent(2287/9): loss=16.29745382005112, w0=74.05373547928274, w1=15.68546693827235\n",
      "[ 0.83814988  1.81961168]\n",
      "Gradient Descent(2288/9): loss=18.107222593257, w0=73.46703056075756, w1=14.411738764386534\n",
      "[ 1.06065474 -0.54211725]\n",
      "Gradient Descent(2289/9): loss=15.83520769471383, w0=72.72457224074951, w1=14.791220841702916\n",
      "[-3.50924797  5.29796018]\n",
      "Gradient Descent(2290/9): loss=16.40799459464786, w0=75.18104582316883, w1=11.082648712426245\n",
      "[ 1.76247025 -2.00149814]\n",
      "Gradient Descent(2291/9): loss=20.03946327185565, w0=73.94731664708064, w1=12.483697411799346\n",
      "[-2.70209303  0.38069704]\n",
      "Gradient Descent(2292/9): loss=16.09537311308049, w0=75.83878176563425, w1=12.217209486987212\n",
      "[ 2.77280064 -1.46841391]\n",
      "Gradient Descent(2293/9): loss=19.421000323700667, w0=73.89782132034314, w1=13.245099226928513\n",
      "[ 1.64771643 -0.17279419]\n",
      "Gradient Descent(2294/9): loss=15.595756740811762, w0=72.74441981964115, w1=13.366055159299256\n",
      "[-1.79345116 -0.504334  ]\n",
      "Gradient Descent(2295/9): loss=15.543323181254381, w0=73.9998356341731, w1=13.719088956286141\n",
      "[-1.54173203  5.46632817]\n",
      "Gradient Descent(2296/9): loss=15.6636954562732, w0=75.07904805711549, w1=9.892659238009529\n",
      "[-1.93064777 -6.61728284]\n",
      "Gradient Descent(2297/9): loss=23.412700703948214, w0=76.43050149518871, w1=14.524757227441428\n",
      "[ 1.14058915  0.63550528]\n",
      "Gradient Descent(2298/9): loss=20.851012636161347, w0=75.63208908984637, w1=14.079903534703153\n",
      "[ 4.46175436  3.92454302]\n",
      "Gradient Descent(2299/9): loss=18.299515212015457, w0=72.50886103466968, w1=11.332723422203383\n",
      "[-0.07697842 -2.38221296]\n",
      "Gradient Descent(2300/9): loss=17.998829140635976, w0=72.56274592893799, w1=13.000272496386398\n",
      "[-0.35161134 -1.41386339]\n",
      "Gradient Descent(2301/9): loss=15.768128421179162, w0=72.80887386912443, w1=13.989976867714443\n",
      "[-0.75434379 -2.29104845]\n",
      "Gradient Descent(2302/9): loss=15.633708610135747, w0=73.33691452171409, w1=15.59371078469203\n",
      "[-0.81597045 -1.26699453]\n",
      "Gradient Descent(2303/9): loss=17.62130655847403, w0=73.90809383911585, w1=16.480606957954603\n",
      "[-2.30819474  1.68261952]\n",
      "Gradient Descent(2304/9): loss=20.077175360500252, w0=75.52383015485863, w1=15.302773290829593\n",
      "[ 0.91788488 -1.09315207]\n",
      "Gradient Descent(2305/9): loss=19.533908495736554, w0=74.88131073845103, w1=16.067979742701553\n",
      "[-2.71988341  2.54067494]\n",
      "Gradient Descent(2306/9): loss=19.99535319705489, w0=76.78522912242657, w1=14.289507282638944\n",
      "[ 3.00337237 -0.58158491]\n",
      "Gradient Descent(2307/9): loss=21.808384420672947, w0=74.68286846471463, w1=14.69661671948732\n",
      "[ 1.98541464  2.40806556]\n",
      "Gradient Descent(2308/9): loss=17.090902025642162, w0=73.29307821676123, w1=13.010970827699905\n",
      "[-0.36824082 -0.39865443]\n",
      "Gradient Descent(2309/9): loss=15.495747572018258, w0=73.55084679287928, w1=13.290028927611276\n",
      "[ 0.20530067  0.63678636]\n",
      "Gradient Descent(2310/9): loss=15.436882959372126, w0=73.40713632450431, w1=12.844278472656153\n",
      "[ 1.41638555 -0.83110967]\n",
      "Gradient Descent(2311/9): loss=15.59418477047059, w0=72.41566643697725, w1=13.426055239691642\n",
      "[ 0.89279348  0.3326976 ]\n",
      "Gradient Descent(2312/9): loss=15.772993834972093, w0=71.79071100198924, w1=13.193166920098864\n",
      "[-2.25012328 -2.24427852]\n",
      "Gradient Descent(2313/9): loss=16.556763690316043, w0=73.36579729485234, w1=14.764161882852498\n",
      "[ 0.22353159  1.24954764]\n",
      "Gradient Descent(2314/9): loss=16.213376089741505, w0=73.20932518475766, w1=13.88947853611814\n",
      "[-0.02388043  1.4058485 ]\n",
      "Gradient Descent(2315/9): loss=15.473420308399334, w0=73.22604148601333, w1=12.90538458702101\n",
      "[ 0.56606817 -1.22114092]\n",
      "Gradient Descent(2316/9): loss=15.55311798953765, w0=72.82979376459565, w1=13.760183229862717\n",
      "[-0.50861847  1.42053817]\n",
      "Gradient Descent(2317/9): loss=15.532927312644793, w0=73.18582669352823, w1=12.765806513105062\n",
      "[-2.03282912 -0.42402006]\n",
      "Gradient Descent(2318/9): loss=15.646560999348084, w0=74.60880707586944, w1=13.062620555399903\n",
      "[ 3.88736867  0.01259005]\n",
      "Gradient Descent(2319/9): loss=16.337332065443004, w0=71.88764900386138, w1=13.053807518467803\n",
      "[ 0.74340326 -1.27525566]\n",
      "Gradient Descent(2320/9): loss=16.465387240582697, w0=71.36726672133669, w1=13.946486481068307\n",
      "[-3.76955721  3.02170221]\n",
      "Gradient Descent(2321/9): loss=17.350827159332574, w0=74.00595676971365, w1=11.831294931127669\n",
      "[ 1.91718077 -1.53071795]\n",
      "Gradient Descent(2322/9): loss=16.99802475748931, w0=72.66393023397735, w1=12.902797495849839\n",
      "[ 0.44639792  1.51493767]\n",
      "Gradient Descent(2323/9): loss=15.750748106284817, w0=72.35145169302045, w1=11.842341130172821\n",
      "[ 1.15291574 -2.37853334]\n",
      "Gradient Descent(2324/9): loss=17.170505405500332, w0=71.54441067708869, w1=13.507314469900987\n",
      "[-2.59824896  1.35009345]\n",
      "Gradient Descent(2325/9): loss=16.91666374317553, w0=73.36318494618753, w1=12.562249054496512\n",
      "[ 0.40086598 -1.08891874]\n",
      "Gradient Descent(2326/9): loss=15.809156073813273, w0=73.08257876037513, w1=13.324492174934546\n",
      "[ 3.32380902 -0.1464096 ]\n",
      "Gradient Descent(2327/9): loss=15.420267516307579, w0=70.75591244563729, w1=13.42697889287026\n",
      "[-1.54191042 -1.58285419]\n",
      "Gradient Descent(2328/9): loss=18.608024536422782, w0=71.83524973758301, w1=14.534976824937798\n",
      "[-1.852882    0.10604451]\n",
      "Gradient Descent(2329/9): loss=17.00654172281948, w0=73.13226713600505, w1=14.460745664874203\n",
      "[-0.03320833  0.69420077]\n",
      "Gradient Descent(2330/9): loss=15.880167115765776, w0=73.15551296585319, w1=13.974805124959689\n",
      "[-3.92794321  5.71913119]\n",
      "Gradient Descent(2331/9): loss=15.518024785318685, w0=75.90507321522458, w1=9.971413291722309\n",
      "[ 4.66104709 -2.2138571 ]\n",
      "Gradient Descent(2332/9): loss=24.949024637039905, w0=72.64234025255139, w1=11.521113264121043\n",
      "[ 2.85012342 -2.4514463 ]\n",
      "Gradient Descent(2333/9): loss=17.51622261306761, w0=70.64725385667063, w1=13.237125675503744\n",
      "[-4.84274315 -1.33501596]\n",
      "Gradient Descent(2334/9): loss=18.917738172797193, w0=74.03717405974413, w1=14.17163684976044\n",
      "[ 0.89142594  1.96975973]\n",
      "Gradient Descent(2335/9): loss=15.901479377300024, w0=73.41317590300815, w1=12.792805040922163\n",
      "[ 2.67385068 -0.53861659]\n",
      "Gradient Descent(2336/9): loss=15.628919499281567, w0=71.54148042507555, w1=13.169836656150352\n",
      "[-1.27054466 -1.21287968]\n",
      "Gradient Descent(2337/9): loss=16.96942510843591, w0=72.43086168574875, w1=14.018852428862177\n",
      "[-1.2364      2.38241665]\n",
      "Gradient Descent(2338/9): loss=15.903660390160795, w0=73.29634168368278, w1=12.351160775415526\n",
      "[-0.14264738 -1.36336549]\n",
      "Gradient Descent(2339/9): loss=16.022705220421944, w0=73.39619484961287, w1=13.305516615944605\n",
      "[ 0.31607149 -0.06353175]\n",
      "Gradient Descent(2340/9): loss=15.406289828184345, w0=73.17494480541565, w1=13.349988843231845\n",
      "[ 1.23532335  1.0342348 ]\n",
      "Gradient Descent(2341/9): loss=15.401379760624648, w0=72.31021845762385, w1=12.626024481733484\n",
      "[-1.48672735  0.58773689]\n",
      "Gradient Descent(2342/9): loss=16.234115761308814, w0=73.35092760345994, w1=12.214608661752795\n",
      "[ 0.99143222  0.64015419]\n",
      "Gradient Descent(2343/9): loss=16.187756466650615, w0=72.6569250510729, w1=11.766500728978079\n",
      "[ 0.32722656  1.39644825]\n",
      "Gradient Descent(2344/9): loss=17.05631760144812, w0=72.42786645668797, w1=10.788986950641517\n",
      "[-0.52938092 -3.75641933]\n",
      "Gradient Descent(2345/9): loss=19.380915788761993, w0=72.79843309759109, w1=13.418480479197104\n",
      "[-3.07572909 -1.30994151]\n",
      "Gradient Descent(2346/9): loss=15.510517172282748, w0=74.95144345974306, w1=14.335439537972873\n",
      "[ 3.37504237  2.35561774]\n",
      "Gradient Descent(2347/9): loss=17.12571099748493, w0=72.58891380378928, w1=12.68650712126148\n",
      "[ 2.62352876  2.3173735 ]\n",
      "Gradient Descent(2348/9): loss=15.948993483538548, w0=70.75244366986178, w1=11.064345672378414\n",
      "[-1.22412643 -2.10547203]\n",
      "Gradient Descent(2349/9): loss=21.53244222442286, w0=71.60933217334116, w1=12.538176091588795\n",
      "[-2.97033673 -0.78077384]\n",
      "Gradient Descent(2350/9): loss=17.24805465738877, w0=73.68856788145544, w1=13.084717782209719\n",
      "[-1.50427149  2.50297992]\n",
      "Gradient Descent(2351/9): loss=15.541770941735601, w0=74.74155792343639, w1=11.332631835176963\n",
      "[ 2.2809187  -2.70575015]\n",
      "Gradient Descent(2352/9): loss=18.738690300238332, w0=73.14491483065049, w1=13.226656936868947\n",
      "[-1.69406352 -0.68890469]\n",
      "Gradient Descent(2353/9): loss=15.429007979966272, w0=74.33075929323017, w1=13.70889021833194\n",
      "[ 3.74899227 -0.97554412]\n",
      "Gradient Descent(2354/9): loss=15.949664881152081, w0=71.706464705807, w1=14.391771104148232\n",
      "[-1.22804307  1.74307934]\n",
      "Gradient Descent(2355/9): loss=17.061823710608795, w0=72.5660948536041, w1=13.171615564945867\n",
      "[ 0.27613041  1.26993917]\n",
      "Gradient Descent(2356/9): loss=15.698215888542217, w0=72.37280356899922, w1=12.28265814421713\n",
      "[-0.74868283 -2.49358015]\n",
      "Gradient Descent(2357/9): loss=16.52658694026093, w0=72.89688154902844, w1=14.028164252163858\n",
      "[ 2.50711197  0.2440192 ]\n",
      "Gradient Descent(2358/9): loss=15.615108127400273, w0=71.14190317313324, w1=13.857350810653992\n",
      "[-0.81525504 -1.01739637]\n",
      "Gradient Descent(2359/9): loss=17.772785760341723, w0=71.71258170438784, w1=14.56952827083455\n",
      "[-1.58457876  3.65046589]\n",
      "Gradient Descent(2360/9): loss=17.23005571545161, w0=72.82178683602332, w1=12.014202149635562\n",
      "[ 2.11154868 -1.78732713]\n",
      "Gradient Descent(2361/9): loss=16.571203874593404, w0=71.34370275658809, w1=13.265331139581102\n",
      "[-4.35488619  2.35933415]\n",
      "Gradient Descent(2362/9): loss=17.310545091532443, w0=74.39212309217136, w1=11.613797234058634\n",
      "[-0.65104556 -2.85600509]\n",
      "Gradient Descent(2363/9): loss=17.72973045447226, w0=74.84785498691136, w1=13.61300079955463\n",
      "[ 0.95417987 -2.09942008]\n",
      "Gradient Descent(2364/9): loss=16.60212462352799, w0=74.17992907831514, w1=15.082594856201057\n",
      "[-0.55373896 -1.2278656 ]\n",
      "Gradient Descent(2365/9): loss=17.063008166491688, w0=74.56754635273026, w1=15.942100774023451\n",
      "[-1.60778761  1.42478214]\n",
      "Gradient Descent(2366/9): loss=19.228625528188264, w0=75.69299767945832, w1=14.944753277905068\n",
      "[ 1.74999661  1.39833273]\n",
      "Gradient Descent(2367/9): loss=19.336842257369117, w0=74.46800005136936, w1=13.965920363654392\n",
      "[ 0.1639148  0.6832432]\n",
      "Gradient Descent(2368/9): loss=16.1933165766599, w0=74.35325968799347, w1=13.4876501235409\n",
      "[ 2.98704789  0.54990878]\n",
      "Gradient Descent(2369/9): loss=15.94701753865074, w0=72.26232616410427, w1=13.10271397576937\n",
      "[-2.1674054  -4.32567994]\n",
      "Gradient Descent(2370/9): loss=15.989046774446821, w0=73.77950994102541, w1=16.130689934610054\n",
      "[ 2.5158402   0.72305911]\n",
      "Gradient Descent(2371/9): loss=19.01762654379021, w0=72.01842180199301, w1=15.624548555850252\n",
      "[-0.13604238  0.13747231]\n",
      "Gradient Descent(2372/9): loss=18.49949924174797, w0=72.11365146826743, w1=15.528317941252247\n",
      "[ 0.13251889  2.03310907]\n",
      "Gradient Descent(2373/9): loss=18.180799395498184, w0=72.02088824549936, w1=14.10514159071394\n",
      "[-3.39959739  3.09261551]\n",
      "Gradient Descent(2374/9): loss=16.391776155973744, w0=74.40060641591776, w1=11.940310736525893\n",
      "[-1.62852278 -4.18899945]\n",
      "Gradient Descent(2375/9): loss=17.183141859332864, w0=75.54057235865395, w1=14.872610352512313\n",
      "[ 2.99323958  0.54477967]\n",
      "Gradient Descent(2376/9): loss=18.87968908544005, w0=73.44530465063595, w1=14.491264580618612\n",
      "[ 0.8376876   0.02894833]\n",
      "Gradient Descent(2377/9): loss=15.908965093631378, w0=72.85892332824162, w1=14.471000752247047\n",
      "[-0.93710762  2.94864471]\n",
      "Gradient Descent(2378/9): loss=15.97182605592703, w0=73.5148986635317, w1=12.406949451860719\n",
      "[-4.44448096 -2.84638118]\n",
      "Gradient Descent(2379/9): loss=15.985713420262199, w0=76.62603533594952, w1=14.399416279734897\n",
      "[ 4.13130411  2.49102889]\n",
      "Gradient Descent(2380/9): loss=21.36030508464115, w0=73.73412246173363, w1=12.655696058210687\n",
      "[-1.56991976 -1.6319162 ]\n",
      "Gradient Descent(2381/9): loss=15.822277585757416, w0=74.83306629408948, w1=13.798037400460153\n",
      "[-3.111936   -0.29486538]\n",
      "Gradient Descent(2382/9): loss=16.621035836424397, w0=77.01142149365086, w1=14.004443168578948\n",
      "[ 2.56035608  2.06736736]\n",
      "Gradient Descent(2383/9): loss=22.433460275037447, w0=75.21917223894916, w1=12.557286016245275\n",
      "[ 2.84243932 -0.57942536]\n",
      "Gradient Descent(2384/9): loss=17.66461735506151, w0=73.22946471640448, w1=12.962883770098045\n",
      "[-3.9418785  -3.66524146]\n",
      "Gradient Descent(2385/9): loss=15.52152117409586, w0=75.98877966336451, w1=15.528552794950249\n",
      "[ 4.58380198  1.6021996 ]\n",
      "Gradient Descent(2386/9): loss=21.115890186356395, w0=72.78011827599552, w1=14.407013076997623\n",
      "[ 0.95738367  2.33126984]\n",
      "Gradient Descent(2387/9): loss=15.947828243646248, w0=72.10994970702315, w1=12.77512418569844\n",
      "[ 1.55089537  1.79302239]\n",
      "Gradient Descent(2388/9): loss=16.335005367109524, w0=71.02432294925049, w1=11.520008514428495\n",
      "[-5.85061329 -3.26989795]\n",
      "Gradient Descent(2389/9): loss=19.881647527319075, w0=75.11975225020348, w1=13.808937080761972\n",
      "[ 3.34239487  1.08093902]\n",
      "Gradient Descent(2390/9): loss=17.10691034995688, w0=72.78007583898697, w1=13.05227976821607\n",
      "[ 0.29544079  0.91153238]\n",
      "Gradient Descent(2391/9): loss=15.609256150817385, w0=72.57326728851517, w1=12.414207105306208\n",
      "[-2.14219981 -0.31906712]\n",
      "Gradient Descent(2392/9): loss=16.21321028073043, w0=74.0728071551866, w1=12.637554090469246\n",
      "[-0.05400191 -0.99242268]\n",
      "Gradient Descent(2393/9): loss=16.043834248296886, w0=74.11060849134493, w1=13.33224996979507\n",
      "[-1.48450761  1.34439143]\n",
      "Gradient Descent(2394/9): loss=15.730248869003312, w0=75.14976382013302, w1=12.391175967644994\n",
      "[ 2.79314665  1.01633159]\n",
      "Gradient Descent(2395/9): loss=17.70041811596876, w0=73.19456116352929, w1=11.679743857268507\n",
      "[-0.98495619 -6.39928957]\n",
      "Gradient Descent(2396/9): loss=17.010767597341307, w0=73.88403049339972, w1=16.159246559700165\n",
      "[ 1.66169359  0.36100582]\n",
      "Gradient Descent(2397/9): loss=19.149953447324037, w0=72.7208449805019, w1=15.906542484117638\n",
      "[ 2.43266348  1.61929305]\n",
      "Gradient Descent(2398/9): loss=18.49484854885099, w0=71.01798054500351, w1=14.77303735064505\n",
      "[-3.58546804  2.69502529]\n",
      "Gradient Descent(2399/9): loss=18.81218729563476, w0=73.52780817057494, w1=12.886519649244093\n",
      "[ 1.7113232  -1.31998249]\n",
      "Gradient Descent(2400/9): loss=15.589178079260058, w0=72.32988193084874, w1=13.810507393070043\n",
      "[-0.3184676   0.35630221]\n",
      "Gradient Descent(2401/9): loss=15.90528715046937, w0=72.55280924875773, w1=13.561095845221578\n",
      "[-3.41640549 -1.91209634]\n",
      "Gradient Descent(2402/9): loss=15.663823555147072, w0=74.9442930923995, w1=14.899563285823744\n",
      "[-2.19895184 -0.04635555]\n",
      "Gradient Descent(2403/9): loss=17.755738455977024, w0=76.48355938331815, w1=14.932012171020498\n",
      "[ 5.3993052   0.05928898]\n",
      "Gradient Descent(2404/9): loss=21.527368442283446, w0=72.70404574199173, w1=14.890509887958892\n",
      "[ 0.24531361  4.34162879]\n",
      "Gradient Descent(2405/9): loss=16.55503959660522, w0=72.53232621751826, w1=11.851369732790026\n",
      "[ 1.20954637 -3.49695486]\n",
      "Gradient Descent(2406/9): loss=17.001651916282093, w0=71.68564375778503, w1=14.29923813591866\n",
      "[-0.07253312 -0.5123454 ]\n",
      "Gradient Descent(2407/9): loss=17.014978511648255, w0=71.73641694030017, w1=14.657879918492458\n",
      "[-0.89215035  1.04674619]\n",
      "Gradient Descent(2408/9): loss=17.2928381871959, w0=72.36092218819277, w1=13.925157585209195\n",
      "[-0.1574957  -0.00853905]\n",
      "Gradient Descent(2409/9): loss=15.920342886137028, w0=72.47116917724348, w1=13.931134921368407\n",
      "[-0.17603326  0.8437801 ]\n",
      "Gradient Descent(2410/9): loss=15.826240104842821, w0=72.59439245802083, w1=13.340488848505201\n",
      "[ 0.57941123 -0.17852847]\n",
      "Gradient Descent(2411/9): loss=15.640250263869545, w0=72.18880459639588, w1=13.465458778123502\n",
      "[-0.77864804 -2.19775805]\n",
      "Gradient Descent(2412/9): loss=15.996631692397253, w0=72.73385822325955, w1=15.003889411701326\n",
      "[-0.24649471  2.66611916]\n",
      "Gradient Descent(2413/9): loss=16.704281315186723, w0=72.90640452267479, w1=13.13760599655063\n",
      "[-0.28260778 -1.84687884]\n",
      "Gradient Descent(2414/9): loss=15.519491174871956, w0=73.10422996724432, w1=14.4304211854418\n",
      "[-0.51956106  1.89712129]\n",
      "Gradient Descent(2415/9): loss=15.855802966967946, w0=73.46792270942996, w1=13.102436285406144\n",
      "[ 3.6944232  -0.32446419]\n",
      "Gradient Descent(2416/9): loss=15.472194638426211, w0=70.88182646635606, w1=13.329561216223201\n",
      "[-0.96598309 -0.91631948]\n",
      "Gradient Descent(2417/9): loss=18.306262999868274, w0=71.55801462833159, w1=13.970984854294388\n",
      "[-4.1029458  -0.46580877]\n",
      "Gradient Descent(2418/9): loss=17.013249368975234, w0=74.43007668760153, w1=14.297050993206286\n",
      "[-3.96371305  0.54044633]\n",
      "Gradient Descent(2419/9): loss=16.365332762891356, w0=77.20467582385344, w1=13.91873856530653\n",
      "[ 1.65969673  0.34321514]\n",
      "Gradient Descent(2420/9): loss=23.129257567539423, w0=76.04288811311365, w1=13.678487967468204\n",
      "[ 2.42022037 -1.53336063]\n",
      "Gradient Descent(2421/9): loss=19.18405106472207, w0=74.3487338538342, w1=14.75184041137907\n",
      "[ 2.57814218  1.28116404]\n",
      "Gradient Descent(2422/9): loss=16.751356684260482, w0=72.54403433022145, w1=13.855025581923494\n",
      "[ 1.37252093  3.45057521]\n",
      "Gradient Descent(2423/9): loss=15.737483608181925, w0=71.58326968151137, w1=11.439622935314402\n",
      "[ 0.46270208 -3.2245253 ]\n",
      "Gradient Descent(2424/9): loss=18.930036133147286, w0=71.25937822687841, w1=13.696790642238643\n",
      "[-4.84811325  0.6998955 ]\n",
      "Gradient Descent(2425/9): loss=17.479133529517767, w0=74.65305749949506, w1=13.206863792555305\n",
      "[ 0.07710938  1.15044676]\n",
      "Gradient Descent(2426/9): loss=16.346735709800978, w0=74.59908093114126, w1=12.401551058469412\n",
      "[ 1.98047003 -0.18011677]\n",
      "Gradient Descent(2427/9): loss=16.818823760760047, w0=73.21275191239047, w1=12.527632798970352\n",
      "[-0.40352419  1.84735291]\n",
      "Gradient Descent(2428/9): loss=15.842409977222296, w0=73.49521884764823, w1=11.234485761581468\n",
      "[ 1.4983996  -1.24922494]\n",
      "Gradient Descent(2429/9): loss=17.926669486332628, w0=72.44633912498594, w1=12.108943220278679\n",
      "[ 0.54160024 -3.00567076]\n",
      "Gradient Descent(2430/9): loss=16.684590355621115, w0=72.06721895421356, w1=14.212912753654408\n",
      "[-1.01957143  0.7968925 ]\n",
      "Gradient Descent(2431/9): loss=16.4070794063282, w0=72.78091895587711, w1=13.655088004860454\n",
      "[ 2.78780095 -0.23354972]\n",
      "Gradient Descent(2432/9): loss=15.532852226802905, w0=70.82945829371921, w1=13.81857280993582\n",
      "[ 0.93520793  1.50205349]\n",
      "Gradient Descent(2433/9): loss=18.480091730659716, w0=70.17481274240488, w1=12.767135365860256\n",
      "[-3.08397334 -1.69769931]\n",
      "Gradient Descent(2434/9): loss=20.50419219552759, w0=72.33359408221554, w1=13.955524884236484\n",
      "[ 0.50833848 -0.55919759]\n",
      "Gradient Descent(2435/9): loss=15.960201469118614, w0=71.97775714503022, w1=14.346963193755276\n",
      "[-1.54778546 -1.32200228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2436/9): loss=16.628094773619285, w0=73.06120696371974, w1=15.272364792414182\n",
      "[-1.96913535  1.84701894]\n",
      "Gradient Descent(2437/9): loss=17.019767250665712, w0=74.4396017098136, w1=13.979451537799063\n",
      "[ 0.13621674  1.40378007]\n",
      "Gradient Descent(2438/9): loss=16.167048450595495, w0=74.34424999159357, w1=12.99680548564468\n",
      "[ 1.31560473 -2.86417634]\n",
      "Gradient Descent(2439/9): loss=16.054081872443295, w0=73.42332667987219, w1=15.001728923431184\n",
      "[-1.75900261  2.27832442]\n",
      "Gradient Descent(2440/9): loss=16.552527749688252, w0=74.65462850367939, w1=13.406901832393281\n",
      "[ 1.78240817 -0.73189998]\n",
      "Gradient Descent(2441/9): loss=16.314299652467728, w0=73.40694278349483, w1=13.919231820264592\n",
      "[ 0.4108213   2.23933219]\n",
      "Gradient Descent(2442/9): loss=15.488863362358856, w0=73.11936787519502, w1=12.351699285536569\n",
      "[ 2.09428127 -1.3766115 ]\n",
      "Gradient Descent(2443/9): loss=16.037329273108934, w0=71.65337098309588, w1=13.315327338238475\n",
      "[-0.80658772 -2.41300505]\n",
      "Gradient Descent(2444/9): loss=16.745102921832483, w0=72.21798238883926, w1=15.004430869860709\n",
      "[-3.0453377  -0.08028528]\n",
      "Gradient Descent(2445/9): loss=17.12709404734557, w0=74.34971877843998, w1=15.06063056667277\n",
      "[ 1.39003142  1.81553003]\n",
      "Gradient Descent(2446/9): loss=17.19289235483204, w0=73.3766967875451, w1=13.789759544742616\n",
      "[-0.09799774  2.58545616]\n",
      "Gradient Descent(2447/9): loss=15.437378306514983, w0=73.44529520852785, w1=11.979940235205529\n",
      "[ 1.48452718 -3.5304592 ]\n",
      "Gradient Descent(2448/9): loss=16.52200311826249, w0=72.40612618467074, w1=14.45126167563894\n",
      "[-0.35224377  1.76453644]\n",
      "Gradient Descent(2449/9): loss=16.251932539060142, w0=72.65269682591904, w1=13.216086166130545\n",
      "[ 1.84733575  1.61377514]\n",
      "Gradient Descent(2450/9): loss=15.626222136933011, w0=71.35956180102926, w1=12.08644356523646\n",
      "[-1.777213    1.12225535]\n",
      "Gradient Descent(2451/9): loss=18.227361634293484, w0=72.60361090308461, w1=11.300864823463563\n",
      "[-1.10798438 -2.82520686]\n",
      "Gradient Descent(2452/9): loss=17.997841032670053, w0=73.3791999718101, w1=13.278509625851472\n",
      "[-0.8607417   0.56134854]\n",
      "Gradient Descent(2453/9): loss=15.40976532009032, w0=73.98171916136604, w1=12.885565645744167\n",
      "[ 2.17781818 -1.17022136]\n",
      "Gradient Descent(2454/9): loss=15.798925538558054, w0=72.45724643772259, w1=13.704720597356173\n",
      "[-1.68373508  1.37969357]\n",
      "Gradient Descent(2455/9): loss=15.761215205412785, w0=73.63586099479069, w1=12.738935096149627\n",
      "[-0.96163971 -0.72468396]\n",
      "Gradient Descent(2456/9): loss=15.718724539057794, w0=74.30900879018387, w1=13.246213867812406\n",
      "[ 0.36551402 -0.79139597]\n",
      "Gradient Descent(2457/9): loss=15.928349252932113, w0=74.0531489769792, w1=13.800191047467496\n",
      "[ 1.81618901 -1.59027454]\n",
      "Gradient Descent(2458/9): loss=15.725453939045629, w0=72.78181667235796, w1=14.913383224048568\n",
      "[-1.87845323  2.97140491]\n",
      "Gradient Descent(2459/9): loss=16.54471976890843, w0=74.09673393229497, w1=12.833399789371216\n",
      "[ 3.82608614 -0.54099614]\n",
      "Gradient Descent(2460/9): loss=15.91700138439968, w0=71.41847363761869, w1=13.212097089711826\n",
      "[-1.18835665 -3.18222668]\n",
      "Gradient Descent(2461/9): loss=17.180350139270864, w0=72.25032329276196, w1=15.439655768662591\n",
      "[ 0.87373703  1.02233479]\n",
      "Gradient Descent(2462/9): loss=17.85112593750651, w0=71.63870737295838, w1=14.724021417708375\n",
      "[-3.21272998 -1.06318437]\n",
      "Gradient Descent(2463/9): loss=17.529908025338216, w0=73.88761835549782, w1=15.468250475219678\n",
      "[ 3.18999191  6.9837488 ]\n",
      "Gradient Descent(2464/9): loss=17.539267317567386, w0=71.65462401941038, w1=10.579626314794794\n",
      "[ 0.21048554 -2.16831585]\n",
      "Gradient Descent(2465/9): loss=20.934786559134714, w0=71.50728414454865, w1=12.09744740691647\n",
      "[ 1.11342926 -4.37003235]\n",
      "Gradient Descent(2466/9): loss=17.93725358977275, w0=70.72788365956048, w1=15.156470050345627\n",
      "[-4.75158843  2.43950155]\n",
      "Gradient Descent(2467/9): loss=20.083922306862355, w0=74.05399555732585, w1=13.448818967821463\n",
      "[-0.88661586  1.70523095]\n",
      "Gradient Descent(2468/9): loss=15.675220976659103, w0=74.67462665722425, w1=12.25515730323261\n",
      "[-1.45585458  0.15997103]\n",
      "Gradient Descent(2469/9): loss=17.088828176518643, w0=75.69372486491531, w1=12.143177584371927\n",
      "[ 0.54655614 -1.37631992]\n",
      "Gradient Descent(2470/9): loss=19.158577462462294, w0=75.31113556741249, w1=13.106601525295988\n",
      "[ 0.45556968 -1.27472563]\n",
      "Gradient Descent(2471/9): loss=17.49006902832529, w0=74.99223679184466, w1=13.998909465340379\n",
      "[ 1.34463982  2.59639976]\n",
      "Gradient Descent(2472/9): loss=16.962807209516132, w0=74.05098892130772, w1=12.181429631558355\n",
      "[-1.06630414 -0.66518743]\n",
      "Gradient Descent(2473/9): loss=16.515232147746737, w0=74.79740181697107, w1=12.647060834890587\n",
      "[ 2.04344129 -0.53012448]\n",
      "Gradient Descent(2474/9): loss=16.862767989257247, w0=73.36699291318372, w1=13.01814796826002\n",
      "[ 2.48775434  1.2429368 ]\n",
      "Gradient Descent(2475/9): loss=15.495078426325751, w0=71.6255648759177, w1=12.148092209212635\n",
      "[-1.11691515  0.54041835]\n",
      "Gradient Descent(2476/9): loss=17.664201831928093, w0=72.40740547846609, w1=11.769799362066248\n",
      "[-0.33612577 -0.99667799]\n",
      "Gradient Descent(2477/9): loss=17.240745000648126, w0=72.64269351530986, w1=12.467473954135077\n",
      "[-2.63601383 -3.13202327]\n",
      "Gradient Descent(2478/9): loss=16.110250510897046, w0=74.4879031944013, w1=14.659890241682794\n",
      "[ 0.31494545  0.70279163]\n",
      "Gradient Descent(2479/9): loss=16.795093240314046, w0=74.26744138109036, w1=14.167936101998233\n",
      "[-1.95066645  0.90246774]\n",
      "Gradient Descent(2480/9): loss=16.096583767375023, w0=75.63290789583884, w1=13.536208683850262\n",
      "[ 1.19688187  0.32589635]\n",
      "Gradient Descent(2481/9): loss=18.122911287439596, w0=74.79509058462948, w1=13.308081238068487\n",
      "[ 1.70381751  0.46578164]\n",
      "Gradient Descent(2482/9): loss=16.527370059286582, w0=73.6024183276123, w1=12.982034088433707\n",
      "[ 1.03078973  2.82796492]\n",
      "Gradient Descent(2483/9): loss=15.557314728570125, w0=72.88086551484705, w1=11.002458644557091\n",
      "[ 0.11810966 -4.75813635]\n",
      "Gradient Descent(2484/9): loss=18.539588870767165, w0=72.7981887519751, w1=14.333154087804274\n",
      "[-2.2360983   0.79474902]\n",
      "Gradient Descent(2485/9): loss=15.872944923851662, w0=74.36345756323166, w1=13.776829773231018\n",
      "[ 0.99260134 -1.21644941]\n",
      "Gradient Descent(2486/9): loss=16.001980383428457, w0=73.66863662843484, w1=14.628344358444219\n",
      "[-1.63052263 -0.36828981]\n",
      "Gradient Descent(2487/9): loss=16.11577104221225, w0=74.81000247233527, w1=14.886147225108484\n",
      "[ 1.00380642 -0.40739544]\n",
      "Gradient Descent(2488/9): loss=17.52416727436508, w0=74.1073379800797, w1=15.171324033008858\n",
      "[ 0.37502677 -4.28483484]\n",
      "Gradient Descent(2489/9): loss=17.147485544719085, w0=73.84481923854663, w1=18.170708424291558\n",
      "[-5.5337385   2.29056329]\n",
      "Gradient Descent(2490/9): loss=26.540353437214925, w0=77.71843618966926, w1=16.5673141234465\n",
      "[ 4.53396154  3.61902936]\n",
      "Gradient Descent(2491/9): loss=29.940692860089925, w0=74.54466310924379, w1=14.033993570483657\n",
      "[ 0.15914121  0.83133526]\n",
      "Gradient Descent(2492/9): loss=16.321678315955136, w0=74.43326426411738, w1=13.452058885949189\n",
      "[ 2.29027433  1.80244455]\n",
      "Gradient Descent(2493/9): loss=16.035320623220183, w0=72.83007223496587, w1=12.19034770116935\n",
      "[-1.43346201  0.08736992]\n",
      "Gradient Descent(2494/9): loss=16.324696880475972, w0=73.83349564336326, w1=12.129188758230162\n",
      "[-0.03557581 -2.86868565]\n",
      "Gradient Descent(2495/9): loss=16.44341482674282, w0=73.8583987083316, w1=14.137268712498443\n",
      "[-2.04198383 -0.82936897]\n",
      "Gradient Descent(2496/9): loss=15.76139497381152, w0=75.28778739276662, w1=14.717826994434837\n",
      "[ 0.24668346  2.4035265 ]\n",
      "Gradient Descent(2497/9): loss=18.14010129802394, w0=75.11510897001102, w1=13.035358443580142\n",
      "[ 4.41586385  0.85372808]\n",
      "Gradient Descent(2498/9): loss=17.142974089704936, w0=72.02400427695392, w1=12.437748790792261\n",
      "[-4.01016244 -0.25323968]\n",
      "Gradient Descent(2499/9): loss=16.735077501070013, w0=74.83111798250997, w1=12.615016563686801\n",
      "[ 1.31186741 -0.59648021]\n",
      "Gradient Descent(2500/9): loss=16.94122308483928, w0=73.91281079718506, w1=13.032552711756331\n",
      "[ 3.01107077  2.34720043]\n",
      "Gradient Descent(2501/9): loss=15.677375448207888, w0=71.80506125606384, w1=11.389512408799886\n",
      "[-0.57172277 -1.40734342]\n",
      "Gradient Descent(2502/9): loss=18.67870910412139, w0=72.20526719450046, w1=12.374652800571619\n",
      "[-0.68482894  0.12709183]\n",
      "Gradient Descent(2503/9): loss=16.58905091169923, w0=72.6846474546097, w1=12.285688522744097\n",
      "[-4.87606968 -1.62163152]\n",
      "Gradient Descent(2504/9): loss=16.284342157448684, w0=76.09789623240879, w1=13.420830589326538\n",
      "[-1.77603204 -2.00224185]\n",
      "Gradient Descent(2505/9): loss=19.318757146807034, w0=77.34111866218906, w1=14.822399883050663\n",
      "[ 1.21267529  2.16539151]\n",
      "Gradient Descent(2506/9): loss=24.47719306311752, w0=76.4922459623824, w1=13.3066258228208\n",
      "[-1.4623604  0.1616436]\n",
      "Gradient Descent(2507/9): loss=20.515505433926986, w0=77.51589824543146, w1=13.193475302819019\n",
      "[ 2.04146999 -0.58935606]\n",
      "Gradient Descent(2508/9): loss=24.339395416351557, w0=76.08686924979112, w1=13.606024542393847\n",
      "[ 2.26213573 -1.9819339 ]\n",
      "Gradient Descent(2509/9): loss=19.294142407246134, w0=74.50337423827885, w1=14.993378272522047\n",
      "[ 2.33501424  0.80653302]\n",
      "Gradient Descent(2510/9): loss=17.262867358479372, w0=72.86886426886987, w1=14.42880515616943\n",
      "[ 3.95352674  1.52276395]\n",
      "Gradient Descent(2511/9): loss=15.926613403819763, w0=70.10139555039727, w1=13.36287039439952\n",
      "[-3.95850249  0.8611091 ]\n",
      "Gradient Descent(2512/9): loss=20.48882647248134, w0=72.87234729217374, w1=12.760094025255883\n",
      "[ 1.58690229 -1.68162242]\n",
      "Gradient Descent(2513/9): loss=15.733675814669736, w0=71.7615156871673, w1=13.937229720942021\n",
      "[-0.63217495 -0.37696048]\n",
      "Gradient Descent(2514/9): loss=16.66468345933304, w0=72.20403815248179, w1=14.20110205647497\n",
      "[-2.85287987  1.4979679 ]\n",
      "Gradient Descent(2515/9): loss=16.240012764658164, w0=74.20105406496857, w1=13.15252452984995\n",
      "[ 0.61674841 -0.69730622]\n",
      "Gradient Descent(2516/9): loss=15.850858121201446, w0=73.76933017721844, w1=13.640638885760488\n",
      "[ 0.72574978  1.86807566]\n",
      "Gradient Descent(2517/9): loss=15.5118429965906, w0=73.26130532773851, w1=12.332985924995887\n",
      "[-0.11726698 -1.674149  ]\n",
      "Gradient Descent(2518/9): loss=16.04391063691332, w0=73.343392211616, w1=13.504890225373801\n",
      "[ 1.19013374 -1.4091092 ]\n",
      "Gradient Descent(2519/9): loss=15.387428480208252, w0=72.51029859711001, w1=14.491266661931121\n",
      "[-3.52637813 -1.20757997]\n",
      "Gradient Descent(2520/9): loss=16.20454166627971, w0=74.97876328731361, w1=15.336572641971078\n",
      "[ 3.05536505  1.25618732]\n",
      "Gradient Descent(2521/9): loss=18.52919786113745, w0=72.840007755804, w1=14.457241519618972\n",
      "[ 2.39981977 -1.03468862]\n",
      "Gradient Descent(2522/9): loss=15.966688495975697, w0=71.16013391418254, w1=15.181523556149434\n",
      "[-1.28391782  6.51354324]\n",
      "Gradient Descent(2523/9): loss=19.11049421696229, w0=72.05887639023273, w1=10.622043290276952\n",
      "[-2.694416   -1.66967079]\n",
      "Gradient Descent(2524/9): loss=20.231693170851894, w0=73.94496759166685, w1=11.790812846305515\n",
      "[ 0.86586438 -3.56642455]\n",
      "Gradient Descent(2525/9): loss=17.024008959000952, w0=73.3388625272818, w1=14.287310031601791\n",
      "[ 3.09645895  0.63793331]\n",
      "Gradient Descent(2526/9): loss=15.713004633258313, w0=71.17134125966408, w1=13.840756717956454\n",
      "[-2.73042301  0.56209299]\n",
      "Gradient Descent(2527/9): loss=17.703738860052063, w0=73.0826373685834, w1=13.447291624330747\n",
      "[ 1.83351366  2.93727817]\n",
      "Gradient Descent(2528/9): loss=15.408734021492489, w0=71.79917780955937, w1=11.39119690336451\n",
      "[-1.27694877 -2.94656335]\n",
      "Gradient Descent(2529/9): loss=18.683966532322593, w0=72.69304195076288, w1=13.453791251448548\n",
      "[-2.12564762  1.43435481]\n",
      "Gradient Descent(2530/9): loss=15.56675224075804, w0=74.18099528419103, w1=12.449742887265538\n",
      "[ 2.3723766   0.58421135]\n",
      "Gradient Descent(2531/9): loss=16.30975600734356, w0=72.52033166319761, w1=12.04079494099499\n",
      "[-3.22076837 -0.72742409]\n",
      "Gradient Descent(2532/9): loss=16.720350652316043, w0=74.77486952163468, w1=12.549991802231892\n",
      "[-1.67704189 -1.26419768]\n",
      "Gradient Descent(2533/9): loss=16.91468087411687, w0=75.94879884304791, w1=13.434930179160023\n",
      "[ 2.27568529  0.39929939]\n",
      "Gradient Descent(2534/9): loss=18.911076114334968, w0=74.355819137504, w1=13.15542060908213\n",
      "[ 2.12732847 -2.37758592]\n",
      "Gradient Descent(2535/9): loss=16.002283226088522, w0=72.86668920606216, w1=14.819730751312127\n",
      "[-0.56174986  2.66065562]\n",
      "Gradient Descent(2536/9): loss=16.374976343877442, w0=73.25991411008943, w1=12.957271814659931\n",
      "[-1.66412761  1.96736971]\n",
      "Gradient Descent(2537/9): loss=15.522938238074017, w0=74.4248034401968, w1=11.580113019582065\n",
      "[ 1.76578389 -3.46880142]\n",
      "Gradient Descent(2538/9): loss=17.829573251846753, w0=73.18875471568624, w1=14.00827401084198\n",
      "[-0.02256023  2.39323689]\n",
      "Gradient Descent(2539/9): loss=15.531106617629831, w0=73.20454687868641, w1=12.333008187239642\n",
      "[ 1.44853919 -2.27999123]\n",
      "Gradient Descent(2540/9): loss=16.04734714107573, w0=72.19056944443115, w1=13.929002044898501\n",
      "[-3.28020936 -0.02568442]\n",
      "Gradient Descent(2541/9): loss=16.09551187887862, w0=74.48671599839246, w1=13.946981138545242\n",
      "[-2.04184353  0.09593189]\n",
      "Gradient Descent(2542/9): loss=16.206436648280416, w0=75.91600646839453, w1=13.879828818577819\n",
      "[ 2.50714888  0.41712891]\n",
      "Gradient Descent(2543/9): loss=18.903597903215395, w0=74.16100225430277, w1=13.587838584601585\n",
      "[ 0.72072885 -0.12601132]\n",
      "Gradient Descent(2544/9): loss=15.76764758281993, w0=73.65649206084439, w1=13.676046510698106\n",
      "[ 1.17102237 -1.35536912]\n",
      "Gradient Descent(2545/9): loss=15.470889927218735, w0=72.83677640179528, w1=14.624804894613247\n",
      "[-0.00180038  0.63687807]\n",
      "Gradient Descent(2546/9): loss=16.145997289314856, w0=72.83803666647121, w1=14.178990246934324\n",
      "[-0.50488509 -1.48304873]\n",
      "Gradient Descent(2547/9): loss=15.73429831759194, w0=73.19145622632061, w1=15.217124354919159\n",
      "[-0.60425841 -0.13703632]\n",
      "Gradient Descent(2548/9): loss=16.900437576190587, w0=73.61443711585784, w1=15.313049780963475\n",
      "[ 1.01108905  0.31364466]\n",
      "Gradient Descent(2549/9): loss=17.117815749973612, w0=72.90667478261359, w1=15.093498520667476\n",
      "[-2.02509197  2.34638473]\n",
      "Gradient Descent(2550/9): loss=16.763020838496036, w0=74.32423916296922, w1=13.451029209398337\n",
      "[ 0.77212859 -0.85560278]\n",
      "Gradient Descent(2551/9): loss=15.917075958530003, w0=73.78374914652652, w1=14.049951153729458\n",
      "[ 0.04316921  0.80339433]\n",
      "Gradient Descent(2552/9): loss=15.668439282710729, w0=73.75353069923912, w1=13.487575121952183\n",
      "[ 1.00636607  1.35269832]\n",
      "Gradient Descent(2553/9): loss=15.491538856993115, w0=73.04907445161709, w1=12.540686299075624\n",
      "[-2.99100423 -1.14870394]\n",
      "Gradient Descent(2554/9): loss=15.856748072283663, w0=75.14277741258019, w1=13.344779059924727\n",
      "[ 1.09861413 -1.53592838]\n",
      "Gradient Descent(2555/9): loss=17.10412454110386, w0=74.37374751941726, w1=14.419928923441573\n",
      "[ 3.30251633  2.80356369]\n",
      "Gradient Descent(2556/9): loss=16.410902965327544, w0=72.06198608533833, w1=12.457434342043607\n",
      "[-2.58789868  0.1275304 ]\n",
      "Gradient Descent(2557/9): loss=16.667247169997584, w0=73.87351515795507, w1=12.368163064572897\n",
      "[-1.21860547 -1.36465176]\n",
      "Gradient Descent(2558/9): loss=16.171622983419685, w0=74.72653898922627, w1=13.323419299676626\n",
      "[-0.6631019  -1.25935547]\n",
      "Gradient Descent(2559/9): loss=16.424297356796234, w0=75.19071031857172, w1=14.204968128723037\n",
      "[-0.62186186  2.41200802]\n",
      "Gradient Descent(2560/9): loss=17.447788738218147, w0=75.62601362308696, w1=12.516562511263533\n",
      "[ 1.52859919 -0.71081926]\n",
      "Gradient Descent(2561/9): loss=18.56904242094233, w0=74.55599418852177, w1=13.014135995511738\n",
      "[ 5.82047455  0.58682011]\n",
      "Gradient Descent(2562/9): loss=16.290681681190744, w0=70.48166200209546, w1=12.603361917268456\n",
      "[-3.5687975   2.05797253]\n",
      "Gradient Descent(2563/9): loss=19.72428613761133, w0=72.9798202512928, w1=11.162781144368674\n",
      "[-0.24429249 -3.82815162]\n",
      "Gradient Descent(2564/9): loss=18.119303126489005, w0=73.15082499220212, w1=13.842487278726006\n",
      "[ 3.43790979  3.85485711]\n",
      "Gradient Descent(2565/9): loss=15.461929039575184, w0=70.74428814150627, w1=11.144087299765236\n",
      "[-4.4323125  0.5310687]\n",
      "Gradient Descent(2566/9): loss=21.363776666530285, w0=73.84690689417532, w1=10.772339209445628\n",
      "[ 4.49525146 -4.658137  ]\n",
      "Gradient Descent(2567/9): loss=19.203718905453, w0=70.70023087242173, w1=14.033035109563514\n",
      "[-2.19793535 -0.92001814]\n",
      "Gradient Descent(2568/9): loss=18.902587698027858, w0=72.2387856184809, w1=14.677047806826124\n",
      "[-4.07582068 -0.96377371]\n",
      "Gradient Descent(2569/9): loss=16.659350259179384, w0=75.09186009458905, w1=15.351689401644313\n",
      "[ 3.15190627 -1.09571763]\n",
      "Gradient Descent(2570/9): loss=18.754327442875574, w0=72.88552570462784, w1=16.118691742600625\n",
      "[ 0.20447944  6.93570297]\n",
      "Gradient Descent(2571/9): loss=18.951387529727043, w0=72.74239009925512, w1=11.26369966407892\n",
      "[ 2.09976352 -2.13103409]\n",
      "Gradient Descent(2572/9): loss=17.9933378891785, w0=71.27255563829625, w1=12.755423527711944\n",
      "[-3.88523211 -0.92541823]\n",
      "Gradient Descent(2573/9): loss=17.691146067800805, w0=73.99221811744304, w1=13.403216292008866\n",
      "[ 0.61067017 -1.0630514 ]\n",
      "Gradient Descent(2574/9): loss=15.63262243112279, w0=73.56474899821632, w1=14.147352275344002\n",
      "[ 0.29148844  5.01282616]\n",
      "Gradient Descent(2575/9): loss=15.645432977955283, w0=73.36070708767068, w1=10.638373962538658\n",
      "[-1.26691801 -4.38882818]\n",
      "Gradient Descent(2576/9): loss=19.424720150169755, w0=74.24754969452233, w1=13.71055369026499\n",
      "[-1.16601214 -0.26008169]\n",
      "Gradient Descent(2577/9): loss=15.867234599270509, w0=75.06375819435046, w1=13.89261087088342\n",
      "[ 0.56435958 -0.10309697]\n",
      "Gradient Descent(2578/9): loss=17.037290501702024, w0=74.66870649183444, w1=13.964778753324678\n",
      "[ 4.07645505  1.09834948]\n",
      "Gradient Descent(2579/9): loss=16.4485487320213, w0=71.8151879542097, w1=13.195934116390294\n",
      "[ 0.38033121 -0.33266503]\n",
      "Gradient Descent(2580/9): loss=16.51948012808545, w0=71.54895610533002, w1=13.428799634478912\n",
      "[-1.56495613  0.40161027]\n",
      "Gradient Descent(2581/9): loss=16.909636915911477, w0=72.64442539452295, w1=13.147672442879639\n",
      "[-1.19466723  2.34010667]\n",
      "Gradient Descent(2582/9): loss=15.651936068639822, w0=73.48069245293223, w1=11.509597772952933\n",
      "[-0.49624604 -0.50575289]\n",
      "Gradient Descent(2583/9): loss=17.34400536026531, w0=73.82806467896756, w1=11.863624794110326\n",
      "[ 1.70250636 -1.29879137]\n",
      "Gradient Descent(2584/9): loss=16.83441169995273, w0=72.63631022503752, w1=12.772778751681201\n",
      "[ 0.52286074  0.39746234]\n",
      "Gradient Descent(2585/9): loss=15.85199210979605, w0=72.27030770979059, w1=12.494555113019477\n",
      "[-1.78469847  0.55251923]\n",
      "Gradient Descent(2586/9): loss=16.395048453059893, w0=73.51959663541496, w1=12.107791652239637\n",
      "[-1.88551087 -3.75466662]\n",
      "Gradient Descent(2587/9): loss=16.352435705959028, w0=74.83945424352412, w1=14.736058286144818\n",
      "[ 3.3272302   1.57457625]\n",
      "Gradient Descent(2588/9): loss=17.36942527232026, w0=72.51039310678647, w1=13.633854914391577\n",
      "[ 1.20362223  2.04007169]\n",
      "Gradient Descent(2589/9): loss=15.704726585707268, w0=71.66785754599103, w1=12.20580472807158\n",
      "[ 3.13805663 -1.66123838]\n",
      "Gradient Descent(2590/9): loss=17.51935109942018, w0=69.47121790576007, w1=13.368671597399288\n",
      "[-4.60452126 -2.06848204]\n",
      "Gradient Descent(2591/9): loss=22.698586206742593, w0=72.69438278686393, w1=14.816609022991775\n",
      "[ 1.2578997   1.60546282]\n",
      "Gradient Descent(2592/9): loss=16.459257747642123, w0=71.81385299508459, w1=13.69278505130413\n",
      "[-1.33254108 -0.90954252]\n",
      "Gradient Descent(2593/9): loss=16.503889971512553, w0=72.74663175228066, w1=14.329464814147736\n",
      "[ 0.34488994 -0.88947687]\n",
      "Gradient Descent(2594/9): loss=15.896690730548821, w0=72.50520879646258, w1=14.952098622609496\n",
      "[ 0.47552929  2.12813906]\n",
      "Gradient Descent(2595/9): loss=16.780882671954657, w0=72.17233829319613, w1=13.462401278560339\n",
      "[-2.71886081  2.87753865]\n",
      "Gradient Descent(2596/9): loss=16.015012714942944, w0=74.07554085688777, w1=11.448124224462202\n",
      "[ 3.56147196 -3.71706286]\n",
      "Gradient Descent(2597/9): loss=17.755027214481057, w0=71.58251048161334, w1=14.050068223487965\n",
      "[-0.67026855 -2.15916666]\n",
      "Gradient Descent(2598/9): loss=17.01300542780262, w0=72.0516984691762, w1=15.561484887719619\n",
      "[-0.06573685  3.59614363]\n",
      "Gradient Descent(2599/9): loss=18.32433579418462, w0=72.09771426740407, w1=13.044184350030811\n",
      "[-1.12030124 -1.32613217]\n",
      "Gradient Descent(2600/9): loss=16.196186697502483, w0=72.88192513199854, w1=13.972476871894788\n",
      "[-0.6945094   4.52889162]\n",
      "Gradient Descent(2601/9): loss=15.592166974457749, w0=73.36808171062619, w1=10.802252735933942\n",
      "[-0.36218462 -1.75832925]\n",
      "Gradient Descent(2602/9): loss=18.973032920045487, w0=73.62161094481712, w1=12.033083213072755\n",
      "[ 2.25159178 -1.35928949]\n",
      "Gradient Descent(2603/9): loss=16.4859459432683, w0=72.04549669811175, w1=12.984585855100642\n",
      "[ 1.07837333 -0.12555047]\n",
      "Gradient Descent(2604/9): loss=16.287745903710952, w0=71.29063536572575, w1=13.072471182142763\n",
      "[-5.72670548 -3.53754537]\n",
      "Gradient Descent(2605/9): loss=17.47538926158753, w0=75.29932920206186, w1=15.548752939465848\n",
      "[ 0.17214473  1.37163912]\n",
      "Gradient Descent(2606/9): loss=19.53718119223122, w0=75.17882789433452, w1=14.588605557275168\n",
      "[ 2.68322191  0.93662348]\n",
      "Gradient Descent(2607/9): loss=17.777144958436548, w0=73.30057255989526, w1=13.932969122934614\n",
      "[ 0.00614105  0.49274019]\n",
      "Gradient Descent(2608/9): loss=15.488630796372552, w0=73.29627382773246, w1=13.58805098940941\n",
      "[ 1.96166171  5.08970064]\n",
      "Gradient Descent(2609/9): loss=15.391759255558235, w0=71.9231106316104, w1=10.02526053885573\n",
      "[ 1.62380699 -4.71618078]\n",
      "Gradient Descent(2610/9): loss=22.29206872691784, w0=70.78644573651154, w1=13.326587083785395\n",
      "[-1.57860728 -1.42367243]\n",
      "Gradient Descent(2611/9): loss=18.54133016667776, w0=71.89147083507926, w1=14.323157785925515\n",
      "[ 0.41325104  3.97503854]\n",
      "Gradient Descent(2612/9): loss=16.725022536783765, w0=71.60219510878353, w1=11.540630805759099\n",
      "[-3.19987694 -0.13480848]\n",
      "Gradient Descent(2613/9): loss=18.69687659203181, w0=73.84210896803503, w1=11.634996744524042\n",
      "[-0.29246798 -0.71471619]\n",
      "Gradient Descent(2614/9): loss=17.23763033296098, w0=74.04683655741172, w1=12.135298079266374\n",
      "[ 2.27545849 -1.15623148]\n",
      "Gradient Descent(2615/9): loss=16.573053012562227, w0=72.45401561154956, w1=12.944660112624167\n",
      "[ 0.53589543 -3.49540936]\n",
      "Gradient Descent(2616/9): loss=15.881749735111516, w0=72.07888881358699, w1=15.391446667749094\n",
      "[-4.92239037  0.96144209]\n",
      "Gradient Descent(2617/9): loss=17.951404581783077, w0=75.52456207252624, w1=14.718437202481557\n",
      "[-1.11524625  1.00618059]\n",
      "Gradient Descent(2618/9): loss=18.640984955513087, w0=76.3052344459468, w1=14.014110788493166\n",
      "[ 5.08601074  2.68417174]\n",
      "Gradient Descent(2619/9): loss=20.062679986161005, w0=72.7450269259795, w1=12.135190570549181\n",
      "[ 0.89974377  2.30114469]\n",
      "Gradient Descent(2620/9): loss=16.44040029310534, w0=72.11520628707926, w1=10.524389285272502\n",
      "[-1.08342731 -0.6651411 ]\n",
      "Gradient Descent(2621/9): loss=20.447540696879205, w0=72.87360540415195, w1=10.989988058605684\n",
      "[-0.91330231 -4.55761959]\n",
      "Gradient Descent(2622/9): loss=18.57358462526566, w0=73.51291702332658, w1=14.180321771556876\n",
      "[-1.98525739  1.11421398]\n",
      "Gradient Descent(2623/9): loss=15.655293999732285, w0=74.90259719819095, w1=13.400371988874356\n",
      "[ 1.99821845  0.52974575]\n",
      "Gradient Descent(2624/9): loss=16.682953265275014, w0=73.50384428622662, w1=13.029549965385824\n",
      "[ 0.86555496 -3.55932072]\n",
      "Gradient Descent(2625/9): loss=15.509244676034417, w0=72.8979558119504, w1=15.521074465969688\n",
      "[ 1.70694219  1.94105466]\n",
      "Gradient Descent(2626/9): loss=17.547861951466956, w0=71.70309627601021, w1=14.162336205560885\n",
      "[-3.36105126  0.52645665]\n",
      "Gradient Descent(2627/9): loss=16.884238720307064, w0=74.0558321606387, w1=13.793816548125452\n",
      "[ 2.51604123  0.4513137 ]\n",
      "Gradient Descent(2628/9): loss=15.725472110612284, w0=72.29460329904428, w1=13.477896956010502\n",
      "[-0.43806895 -1.80179974]\n",
      "Gradient Descent(2629/9): loss=15.885208451955027, w0=72.60125156630959, w1=14.7391567728822\n",
      "[-0.07949954  0.74392067]\n",
      "Gradient Descent(2630/9): loss=16.418884055267593, w0=72.65690124112288, w1=14.21841230534472\n",
      "[ 1.06757596  2.2062733 ]\n",
      "Gradient Descent(2631/9): loss=15.861624343022385, w0=71.90959806599196, w1=12.674020997868086\n",
      "[-0.36772375 -2.58184036]\n",
      "Gradient Descent(2632/9): loss=16.668633594802436, w0=72.16700469006241, w1=14.481309246911191\n",
      "[-0.84302101 -1.29896605]\n",
      "Gradient Descent(2633/9): loss=16.52245726974656, w0=72.75711939593585, w1=15.39058548364683\n",
      "[-0.61528075  5.18372   ]\n",
      "Gradient Descent(2634/9): loss=17.355684291867846, w0=73.18781592352543, w1=11.761981481038255\n",
      "[-2.42172821 -3.86921407]\n",
      "Gradient Descent(2635/9): loss=16.866816933865536, w0=74.88302567267698, w1=14.470431328899178\n",
      "[-1.64293169  0.82174888]\n",
      "Gradient Descent(2636/9): loss=17.13927507011702, w0=76.03307785663306, w1=13.895207112092024\n",
      "[ 1.38471882  2.18566343]\n",
      "Gradient Descent(2637/9): loss=19.223693179876996, w0=75.06377468334443, w1=12.365242710865328\n",
      "[ 2.88753745 -0.43487018]\n",
      "Gradient Descent(2638/9): loss=17.573098508468473, w0=73.04249846612899, w1=12.669651837647805\n",
      "[-1.71943182 -2.10922392]\n",
      "Gradient Descent(2639/9): loss=15.745593851733213, w0=74.24610074298252, w1=14.146108581432216\n",
      "[-0.85585397  0.63354325]\n",
      "Gradient Descent(2640/9): loss=16.06125195811592, w0=74.84519852399211, w1=13.702628308399747\n",
      "[ 1.64698015 -0.17419941]\n",
      "Gradient Descent(2641/9): loss=16.613963035817417, w0=73.69231241965316, w1=13.824567897625917\n",
      "[ 3.14466165  2.04314849]\n",
      "Gradient Descent(2642/9): loss=15.52470797628167, w0=71.49104926554712, w1=12.39436395241742\n",
      "[ 0.87358953  1.74700739]\n",
      "Gradient Descent(2643/9): loss=17.600053585251857, w0=70.87953659647776, w1=11.171458776455223\n",
      "[-1.43754127 -1.07212222]\n",
      "Gradient Descent(2644/9): loss=20.964533788350142, w0=71.88581548230204, w1=11.921944329847037\n",
      "[-0.73637761 -1.3524483 ]\n",
      "Gradient Descent(2645/9): loss=17.59059058908434, w0=72.40127981164797, w1=12.868658138897207\n",
      "[-0.24042688 -0.78896147]\n",
      "Gradient Descent(2646/9): loss=15.970986585307683, w0=72.56957862814093, w1=13.420931169931961\n",
      "[-3.1827941   1.45210391]\n",
      "Gradient Descent(2647/9): loss=15.649952149093219, w0=74.79753449727654, w1=12.40445843515264\n",
      "[ 2.25896193 -0.22198507]\n",
      "Gradient Descent(2648/9): loss=17.094398718729213, w0=73.21626114340943, w1=12.559847981991055\n",
      "[-0.48193242 -2.37639195]\n",
      "Gradient Descent(2649/9): loss=15.811978779260732, w0=73.55361383434835, w1=14.223322346789894\n",
      "[-0.01655693  1.27388816]\n",
      "Gradient Descent(2650/9): loss=15.696085643160535, w0=73.56520368880766, w1=13.33160063336008\n",
      "[-1.52918583 -0.95092234]\n",
      "Gradient Descent(2651/9): loss=15.433653298490357, w0=74.63563377098248, w1=13.997246274159336\n",
      "[ 3.19076197  1.65791008]\n",
      "Gradient Descent(2652/9): loss=16.41990374154448, w0=72.40210039223487, w1=12.836709216146819\n",
      "[ 0.63220351 -3.52556827]\n",
      "Gradient Descent(2653/9): loss=15.990287330465977, w0=71.95955793379245, w1=15.30460700683717\n",
      "[ 2.07318565  1.33850759]\n",
      "Gradient Descent(2654/9): loss=17.94127170141183, w0=70.50832798059936, w1=14.367651694174137\n",
      "[-2.06422863  3.09059719]\n",
      "Gradient Descent(2655/9): loss=19.659872959154992, w0=71.95328802102439, w1=12.204233664395039\n",
      "[ 0.08015748  0.21875109]\n",
      "Gradient Descent(2656/9): loss=17.097960651561674, w0=71.89717778160977, w1=12.051107904424338\n",
      "[-0.78177249 -2.71114336]\n",
      "Gradient Descent(2657/9): loss=17.381790529948084, w0=72.44441852530714, w1=13.948908255818198\n",
      "[-2.28756699  1.0471935 ]\n",
      "Gradient Descent(2658/9): loss=15.856788306517155, w0=74.04571541515382, w1=13.215872802588922\n",
      "[ 2.62982656 -0.39232385]\n",
      "Gradient Descent(2659/9): loss=15.703290212593574, w0=72.20483682085913, w1=13.490499494154989\n",
      "[-2.63851611  2.01295325]\n",
      "Gradient Descent(2660/9): loss=15.978999315157008, w0=74.05179809680665, w1=12.081432216006197\n",
      "[ 1.17294123  0.07887628]\n",
      "Gradient Descent(2661/9): loss=16.650669741688734, w0=73.23073923374832, w1=12.0262188206717\n",
      "[-0.09670642 -4.62690199]\n",
      "Gradient Descent(2662/9): loss=16.444205743368673, w0=73.29843373020762, w1=15.265050215243892\n",
      "[-4.56697017  5.03422804]\n",
      "Gradient Descent(2663/9): loss=16.97961354147728, w0=76.49531284666605, w1=11.741090586509266\n",
      "[ 1.87705128  0.56870696]\n",
      "Gradient Descent(2664/9): loss=22.021742504654096, w0=75.18137694851816, w1=11.342995716208728\n",
      "[ 0.12080129 -4.61351369]\n",
      "Gradient Descent(2665/9): loss=19.449910124356474, w0=75.09681604488338, w1=14.572455299166686\n",
      "[ 1.86894065 -2.46265151]\n",
      "Gradient Descent(2666/9): loss=17.60814481717752, w0=73.7885575912936, w1=16.296311353980368\n",
      "[-0.62359778  3.06964394]\n",
      "Gradient Descent(2667/9): loss=19.47483478710682, w0=74.22507603852313, w1=14.147560597515273\n",
      "[ 3.44227201  2.59072284]\n",
      "Gradient Descent(2668/9): loss=16.042422372692933, w0=71.81548563477065, w1=12.33405460750642\n",
      "[-2.14442981 -2.25706484]\n",
      "Gradient Descent(2669/9): loss=17.135040843794176, w0=73.31658650398902, w1=13.913999993300962\n",
      "[-0.41013571  1.06049836]\n",
      "Gradient Descent(2670/9): loss=15.480447550304484, w0=73.60368150100605, w1=13.171651138746848\n",
      "[-1.77958861 -2.19493492]\n",
      "Gradient Descent(2671/9): loss=15.48131422353027, w0=74.84939352814193, w1=14.708105585000133\n",
      "[ 2.7854669   0.66262954]\n",
      "Gradient Descent(2672/9): loss=17.35010856848201, w0=72.89956669946949, w1=14.244264905586034\n",
      "[-0.79621296  2.03782756]\n",
      "Gradient Descent(2673/9): loss=15.755916161335824, w0=73.45691577137684, w1=12.817785613416946\n",
      "[ 2.82394078 -0.42865704]\n",
      "Gradient Descent(2674/9): loss=15.618244911798364, w0=71.48015722460651, w1=13.117845543287551\n",
      "[-1.18749392  1.0565826 ]\n",
      "Gradient Descent(2675/9): loss=17.09623302653166, w0=72.31140296800767, w1=12.378237721448956\n",
      "[ 0.33112043  0.70880127]\n",
      "Gradient Descent(2676/9): loss=16.47518296729547, w0=72.07961866910665, w1=11.882076833729267\n",
      "[-2.77691973 -1.53488621]\n",
      "Gradient Descent(2677/9): loss=17.39937391830143, w0=74.0234624827834, w1=12.956497179866695\n",
      "[-0.03131509 -1.00244621]\n",
      "Gradient Descent(2678/9): loss=15.788879626899874, w0=74.04538304571585, w1=13.658209528712923\n",
      "[-1.84184569 -1.33041542]\n",
      "Gradient Descent(2679/9): loss=15.684165325095549, w0=75.33467502899565, w1=14.589500322257045\n",
      "[ 2.91474396  1.497913  ]\n",
      "Gradient Descent(2680/9): loss=18.084038904574065, w0=73.29435425681098, w1=13.540961221640577\n",
      "[-1.37487868 -2.7132546 ]\n",
      "Gradient Descent(2681/9): loss=15.387763669184608, w0=74.25676933536384, w1=15.44023943884366\n",
      "[ 1.23131501  3.17420244]\n",
      "Gradient Descent(2682/9): loss=17.771258428832613, w0=73.39484882692713, w1=13.21829772880179\n",
      "[-1.02372613  1.03852374]\n",
      "Gradient Descent(2683/9): loss=15.425149805119203, w0=74.11145711561242, w1=12.491331110326907\n",
      "[ 0.34930633 -1.99931414]\n",
      "Gradient Descent(2684/9): loss=16.20851852120849, w0=73.86694268556765, w1=13.890851006467486\n",
      "[ 0.53570793  0.07947432]\n",
      "Gradient Descent(2685/9): loss=15.634581683145958, w0=73.491947133262, w1=13.835218979568912\n",
      "[-3.27630527 -0.01831477]\n",
      "Gradient Descent(2686/9): loss=15.468687296733792, w0=75.78536082125584, w1=13.848039319273965\n",
      "[ 2.75790901 -1.53018363]\n",
      "Gradient Descent(2687/9): loss=18.557353910458296, w0=73.85482451762444, w1=14.919167862483324\n",
      "[ 2.20891126  0.53605524]\n",
      "Gradient Descent(2688/9): loss=16.57920964865867, w0=72.3085866386332, w1=14.543929195950557\n",
      "[ 0.06548044  0.59981007]\n",
      "Gradient Descent(2689/9): loss=16.43760941523934, w0=72.2627503333429, w1=14.124062147838108\n",
      "[ 1.01655569  3.72549405]\n",
      "Gradient Descent(2690/9): loss=16.125138650282743, w0=71.55116134878537, w1=11.516216312029654\n",
      "[-0.65622178 -0.69919147]\n",
      "Gradient Descent(2691/9): loss=18.832153728647544, w0=72.01051659622782, w1=12.00565033926329\n",
      "[ 0.03558769 -3.1848017 ]\n",
      "Gradient Descent(2692/9): loss=17.295882117774735, w0=71.98560521087458, w1=14.23501152907379\n",
      "[ 0.72174962  1.99785792]\n",
      "Gradient Descent(2693/9): loss=16.52697264269999, w0=71.48038047691229, w1=12.836510988491495\n",
      "[-2.78497734 -1.50374353]\n",
      "Gradient Descent(2694/9): loss=17.237208351017166, w0=73.42986461534124, w1=13.889131460973012\n",
      "[-3.76530384  1.1875533 ]\n",
      "Gradient Descent(2695/9): loss=15.47894003529495, w0=76.06557730389491, w1=13.057844149048917\n",
      "[-0.50707515  0.1431333 ]\n",
      "Gradient Descent(2696/9): loss=19.315910850139975, w0=76.42052991096523, w1=12.957650835862498\n",
      "[ 3.66843347 -2.81376537]\n",
      "Gradient Descent(2697/9): loss=20.410000533343766, w0=73.85262648050025, w1=14.927286596839311\n",
      "[-1.78326116  3.61642119]\n",
      "Gradient Descent(2698/9): loss=16.589698692946993, w0=75.10090929535133, w1=12.395791766045633\n",
      "[ 0.62319865  1.5362352 ]\n",
      "Gradient Descent(2699/9): loss=17.605931416087277, w0=74.66467024196868, w1=11.320427127800741\n",
      "[-2.91296831 -0.19002019]\n",
      "Gradient Descent(2700/9): loss=18.656619756293484, w0=76.70374805908584, w1=11.453441259566643\n",
      "[ 3.54934849 -3.66196149]\n",
      "Gradient Descent(2701/9): loss=23.252232176435356, w0=74.21920411477097, w1=14.016814306050128\n",
      "[ 0.83252589  0.04554738]\n",
      "Gradient Descent(2702/9): loss=15.958200572787682, w0=73.63643598938062, w1=13.98493114090227\n",
      "[ 3.37038656  0.8254515 ]\n",
      "Gradient Descent(2703/9): loss=15.57216875497137, w0=71.27716539578098, w1=13.407115091544092\n",
      "[-3.18216276  0.29528013]\n",
      "Gradient Descent(2704/9): loss=17.422176660543197, w0=73.5046793307499, w1=13.2004190037351\n",
      "[-1.1310285  0.7744743]\n",
      "Gradient Descent(2705/9): loss=15.447099604988928, w0=74.29639928339695, w1=12.658286994454572\n",
      "[ 0.27894387 -4.42896206]\n",
      "Gradient Descent(2706/9): loss=16.22573809576109, w0=74.10113857515645, w1=15.758560437424451\n",
      "[ 2.59525054  2.20786281]\n",
      "Gradient Descent(2707/9): loss=18.308261275835626, w0=72.2844631969624, w1=14.213056470970654\n",
      "[-0.94353718  1.51564158]\n",
      "Gradient Descent(2708/9): loss=16.164288146024443, w0=72.9449392260209, w1=13.152107366624836\n",
      "[-1.44428082  0.37442106]\n",
      "Gradient Descent(2709/9): loss=15.500444898240108, w0=73.95593579913283, w1=12.890012623300859\n",
      "[ 1.56772965 -2.13945558]\n",
      "Gradient Descent(2710/9): loss=15.778891936509423, w0=72.85852504602076, w1=14.387631526489974\n",
      "[ 0.65221128 -0.1037334 ]\n",
      "Gradient Descent(2711/9): loss=15.892831661869126, w0=72.40197715249849, w1=14.460244909066603\n",
      "[-2.00239982  1.45240897]\n",
      "Gradient Descent(2712/9): loss=16.264392642559688, w0=73.80365702451887, w1=13.443558628310644\n",
      "[-3.30864284 -0.97086032]\n",
      "Gradient Descent(2713/9): loss=15.516456314235606, w0=76.11970701279762, w1=14.123160852122872\n",
      "[ 3.7832004  -3.14619985]\n",
      "Gradient Descent(2714/9): loss=19.58543126491242, w0=73.47146673164762, w1=16.32550074665662\n",
      "[ 2.73503238  5.95266553]\n",
      "Gradient Descent(2715/9): loss=19.45090449173543, w0=71.5569440633749, w1=12.158634872716501\n",
      "[-2.40671644 -0.14056096]\n",
      "Gradient Descent(2716/9): loss=17.76705701141725, w0=73.24164557406645, w1=12.257027545688265\n",
      "[-1.14714281 -0.47336727]\n",
      "Gradient Descent(2717/9): loss=16.134733450555878, w0=74.0446455379384, w1=12.588384631512978\n",
      "[ 1.24612136  0.22498324]\n",
      "Gradient Descent(2718/9): loss=16.06491340908109, w0=73.17236058454351, w1=12.430896360027921\n",
      "[ 3.13979528 -3.32764305]\n",
      "Gradient Descent(2719/9): loss=15.943284037497634, w0=70.97450388709822, w1=14.760246496277361\n",
      "[-1.38616723  1.73908567]\n",
      "Gradient Descent(2720/9): loss=18.89562180600041, w0=71.94482094806243, w1=13.542886523875264\n",
      "[-1.15785336  1.86113612]\n",
      "Gradient Descent(2721/9): loss=16.297920178592346, w0=72.75531830326992, w1=12.24009123926648\n",
      "[ 1.88965583 -4.60544543]\n",
      "Gradient Descent(2722/9): loss=16.299265195471243, w0=71.43255922380193, w1=15.463903040337488\n",
      "[-3.42038914  0.91563643]\n",
      "Gradient Descent(2723/9): loss=19.086729744232322, w0=73.82683162115251, w1=14.82295753703137\n",
      "[-4.41378524 -1.49160038]\n",
      "Gradient Descent(2724/9): loss=16.43003790194632, w0=76.91648129194714, w1=15.8670778015055\n",
      "[ 2.57666274  0.56369494]\n",
      "Gradient Descent(2725/9): loss=24.7971124696606, w0=75.11281737532224, w1=15.47249134546164\n",
      "[ 2.57051215  1.96942273]\n",
      "Gradient Descent(2726/9): loss=19.025661951196753, w0=73.31345886882086, w1=14.09389543662568\n",
      "[ 0.6240132   1.21431786]\n",
      "Gradient Descent(2727/9): loss=15.574689093159622, w0=72.87664963103398, w1=13.243872931159176\n",
      "[-1.24242494 -1.60881856]\n",
      "Gradient Descent(2728/9): loss=15.500756120442453, w0=73.74634708948003, w1=14.370045920636613\n",
      "[ 4.88910546  0.94914053]\n",
      "Gradient Descent(2729/9): loss=15.884578956505136, w0=70.32397326715639, w1=13.705647551599483\n",
      "[-6.36433023 -1.01184536]\n",
      "Gradient Descent(2730/9): loss=19.82170895140027, w0=74.77900442875544, w1=14.41393930460066\n",
      "[ 0.77334708  1.38228003]\n",
      "Gradient Descent(2731/9): loss=16.92501269775425, w0=74.237661470418, w1=13.446343284490556\n",
      "[ 4.12898633  1.42218449]\n",
      "Gradient Descent(2732/9): loss=15.831766710957575, w0=71.34737103599339, w1=12.450814140131893\n",
      "[-3.38443948 -2.65538439]\n",
      "Gradient Descent(2733/9): loss=17.80973405124477, w0=73.71647867037885, w1=14.309583215378598\n",
      "[ 1.16707614  0.42674665]\n",
      "Gradient Descent(2734/9): loss=15.819507693852849, w0=72.89952537485793, w1=14.010860562450393\n",
      "[-0.53853111  1.5108794 ]\n",
      "Gradient Descent(2735/9): loss=15.604721385274255, w0=73.27649715084257, w1=12.953244985492539\n",
      "[ 1.01001716 -1.01790083]\n",
      "Gradient Descent(2736/9): loss=15.524623669239844, w0=72.56948514092588, w1=13.665775567279633\n",
      "[-2.0685474  -0.64624255]\n",
      "Gradient Descent(2737/9): loss=15.665601996345957, w0=74.01746831759715, w1=14.118145354856718\n",
      "[ 2.37288538  0.72886939]\n",
      "Gradient Descent(2738/9): loss=15.851445800745775, w0=72.35644855143588, w1=13.607936780078536\n",
      "[-1.01933354 -0.05731209]\n",
      "Gradient Descent(2739/9): loss=15.833536845521131, w0=73.06998203284557, w1=13.648055244427894\n",
      "[ 0.63982285  1.47235532]\n",
      "Gradient Descent(2740/9): loss=15.425132074490282, w0=72.62210603809474, w1=12.617406517746574\n",
      "[ 0.28858418 -0.61070646]\n",
      "Gradient Descent(2741/9): loss=15.983341961034737, w0=72.42009711098423, w1=13.044901036679246\n",
      "[-0.94051995 -0.25027272]\n",
      "Gradient Descent(2742/9): loss=15.86220331505074, w0=73.07846107752589, w1=13.220091939064877\n",
      "[ 0.16152399  2.20700962]\n",
      "Gradient Descent(2743/9): loss=15.44280097479164, w0=72.96539428598552, w1=11.675185206855751\n",
      "[ 0.74631021 -1.35763939]\n",
      "Gradient Descent(2744/9): loss=17.06801235749602, w0=72.44297713853075, w1=12.625532777567356\n",
      "[-1.53813395 -1.29573938]\n",
      "Gradient Descent(2745/9): loss=16.11275289282778, w0=73.51967090351246, w1=13.532550340816059\n",
      "[-2.20680057  3.07679775]\n",
      "Gradient Descent(2746/9): loss=15.412765074218786, w0=75.06443130519999, w1=11.378791914947328\n",
      "[ 1.96323513 -4.90472049]\n",
      "Gradient Descent(2747/9): loss=19.16017298076821, w0=73.69016671504214, w1=14.81209625932397\n",
      "[ 1.75211797  6.01367625]\n",
      "Gradient Descent(2748/9): loss=16.35201613276932, w0=72.46368413799807, w1=10.602522886526323\n",
      "[-1.2028497  -1.76554181]\n",
      "Gradient Descent(2749/9): loss=19.869645173219546, w0=73.30567892817047, w1=11.838402150486301\n",
      "[-0.78418242 -0.14559768]\n",
      "Gradient Descent(2750/9): loss=16.7329067064919, w0=73.85460662095262, w1=11.940320523990055\n",
      "[-3.20071498 -1.98765803]\n",
      "Gradient Descent(2751/9): loss=16.727935217560013, w0=76.09510710820507, w1=13.331681146535052\n",
      "[ 2.8126395  -1.18188663]\n",
      "Gradient Descent(2752/9): loss=19.320163499327975, w0=74.12625945917917, w1=14.159001784949195\n",
      "[ 3.4431471  -0.08356447]\n",
      "Gradient Descent(2753/9): loss=15.962997700538224, w0=71.71605649030441, w1=14.217496911279694\n",
      "[-0.89841435  1.31369986]\n",
      "Gradient Descent(2754/9): loss=16.902880622222302, w0=72.34494653438769, w1=13.297907007342447\n",
      "[-0.91586693 -1.74789595]\n",
      "Gradient Descent(2755/9): loss=15.852691694755107, w0=72.98605338387692, w1=14.521434170281733\n",
      "[ 0.58325651  1.1259853 ]\n",
      "Gradient Descent(2756/9): loss=15.975871498764889, w0=72.57777382867273, w1=13.733244461693875\n",
      "[-3.33481536 -0.28220721]\n",
      "Gradient Descent(2757/9): loss=15.67446121626725, w0=74.91214458087163, w1=13.930789506970182\n",
      "[ 2.40094786  0.23667783]\n",
      "Gradient Descent(2758/9): loss=16.796945288477684, w0=73.23148107634822, w1=13.765115022907052\n",
      "[-0.53985596 -0.15180844]\n",
      "Gradient Descent(2759/9): loss=15.42856462202924, w0=73.60938025111882, w1=13.87138092885646\n",
      "[-2.20551632 -0.03063367]\n",
      "Gradient Descent(2760/9): loss=15.512346926808958, w0=75.15324167441567, w1=13.892824497579237\n",
      "[-0.38228    -1.59730206]\n",
      "Gradient Descent(2761/9): loss=17.199753478878534, w0=75.42083767415988, w1=15.010935942981067\n",
      "[ 2.51904324 -0.35244213]\n",
      "Gradient Descent(2762/9): loss=18.820095722559024, w0=73.65750740403435, w1=15.25764543250257\n",
      "[-0.82758414  4.09379022]\n",
      "Gradient Descent(2763/9): loss=17.032507912901103, w0=74.23681629985427, w1=12.39199227600173\n",
      "[-0.16918001 -1.84480009]\n",
      "Gradient Descent(2764/9): loss=16.421980269326962, w0=74.35524230903718, w1=13.683352338551396\n",
      "[ 0.38735089  2.48129246]\n",
      "Gradient Descent(2765/9): loss=15.969822870943949, w0=74.08409668510559, w1=11.94644761902106\n",
      "[ 0.97959115 -2.54767065]\n",
      "Gradient Descent(2766/9): loss=16.873526381599472, w0=73.39838288286111, w1=13.729817076401744\n",
      "[ 0.15373557  2.09857513]\n",
      "Gradient Descent(2767/9): loss=15.422620072461635, w0=73.29076798404623, w1=12.26081448859122\n",
      "[-0.18156264  0.70350193]\n",
      "Gradient Descent(2768/9): loss=16.128748944610777, w0=73.41786182962097, w1=11.768363136522444\n",
      "[-0.95108121 -2.2765013 ]\n",
      "Gradient Descent(2769/9): loss=16.85792661993279, w0=74.08361867866869, w1=13.361914047404433\n",
      "[ 2.78927535 -1.06987709]\n",
      "Gradient Descent(2770/9): loss=15.704636519375883, w0=72.13112593693863, w1=14.110828010839285\n",
      "[-0.89173942  1.56305402]\n",
      "Gradient Descent(2771/9): loss=16.261088648453203, w0=72.75534353317572, w1=13.016690198068703\n",
      "[-0.73554647  0.1723719 ]\n",
      "Gradient Descent(2772/9): loss=15.638116048367968, w0=73.27022605874402, w1=12.896029865718454\n",
      "[-1.15252709 -0.33383391]\n",
      "Gradient Descent(2773/9): loss=15.55651128853045, w0=74.0769950220813, w1=13.129713602354487\n",
      "[-1.90882064  1.3131827 ]\n",
      "Gradient Descent(2774/9): loss=15.753739137559437, w0=75.41316947353756, w1=12.210485715690455\n",
      "[ 0.67303798 -1.21843142]\n",
      "Gradient Descent(2775/9): loss=18.4369610239064, w0=74.94204288621795, w1=13.063387708648142\n",
      "[-0.49349233 -3.42295317]\n",
      "Gradient Descent(2776/9): loss=16.830702232035126, w0=75.28748751646788, w1=15.459454927072747\n",
      "[ 2.91448297 -0.55002277]\n",
      "Gradient Descent(2777/9): loss=19.332729766338364, w0=73.24734943767837, w1=15.84447086496317\n",
      "[-0.83856112 -0.97375244]\n",
      "Gradient Descent(2778/9): loss=18.183013586774884, w0=73.83434221823609, w1=16.52609757544636\n",
      "[ 1.70634313  2.69657963]\n",
      "Gradient Descent(2779/9): loss=20.17214608583044, w0=72.6399020290672, w1=14.638491831459817\n",
      "[ 1.82449811  1.3249696 ]\n",
      "Gradient Descent(2780/9): loss=16.271143776238286, w0=71.3627533501746, w1=13.711013108357697\n",
      "[ 0.52954433 -0.15370982]\n",
      "Gradient Descent(2781/9): loss=17.277344050679503, w0=70.99207232098465, w1=13.818609984942087\n",
      "[-3.92731482 -2.0148532 ]\n",
      "Gradient Descent(2782/9): loss=18.09256962074886, w0=73.74119269690624, w1=15.229007226150232\n",
      "[ 0.9461168  0.408396 ]\n",
      "Gradient Descent(2783/9): loss=17.015929539235128, w0=73.07891093764509, w1=14.943130023007754\n",
      "[-1.28754401  1.36037556]\n",
      "Gradient Descent(2784/9): loss=16.479798266210775, w0=73.98019174754369, w1=13.990867133738844\n",
      "[ 0.28192514  0.47831887]\n",
      "Gradient Descent(2785/9): loss=15.752010513608507, w0=73.78284415212464, w1=13.656043926153886\n",
      "[ 3.11530336  0.44238519]\n",
      "Gradient Descent(2786/9): loss=15.520956700607426, w0=71.6021317991455, w1=13.346374290177142\n",
      "[-3.99227493  0.43050421]\n",
      "Gradient Descent(2787/9): loss=16.82585444467554, w0=74.3967242471302, w1=13.045021340386475\n",
      "[ 2.59719373 -1.62275599]\n",
      "Gradient Descent(2788/9): loss=16.088452438508888, w0=72.57868863460956, w1=14.180950532383156\n",
      "[-1.40147177 -1.46269537]\n",
      "Gradient Descent(2789/9): loss=15.887534688437421, w0=73.55971887505312, w1=15.204837294659974\n",
      "[ 0.4507059   1.20282112]\n",
      "Gradient Descent(2790/9): loss=16.909239748391165, w0=73.24422474468447, w1=14.3628625116931\n",
      "[-0.80693709  1.66543759]\n",
      "Gradient Descent(2791/9): loss=15.777099806518157, w0=73.80908070438294, w1=13.197056199845552\n",
      "[ 1.85642104 -1.79195044]\n",
      "Gradient Descent(2792/9): loss=15.558529386728393, w0=72.50958597937988, w1=14.451421508747437\n",
      "[ 0.71724888 -0.20855085]\n",
      "Gradient Descent(2793/9): loss=16.165588629113874, w0=72.007511760142, w1=14.597407103223428\n",
      "[ 0.46683741 -1.87199031]\n",
      "Gradient Descent(2794/9): loss=16.837934209843084, w0=71.68072557275741, w1=15.90780032351273\n",
      "[ 0.24785453  4.87984648]\n",
      "Gradient Descent(2795/9): loss=19.63489462585731, w0=71.5072274040058, w1=12.491907788212112\n",
      "[-2.68175425 -0.7059648 ]\n",
      "Gradient Descent(2796/9): loss=17.469905672365215, w0=73.38445538204851, w1=12.986083147052069\n",
      "[-0.23537833  0.69788502]\n",
      "Gradient Descent(2797/9): loss=15.511820952225966, w0=73.54922021030431, w1=12.49756363155841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.04067959  1.51341879]\n",
      "Gradient Descent(2798/9): loss=15.90078459242436, w0=74.97769592028806, w1=11.43817047937444\n",
      "[ 0.63370963 -2.67410988]\n",
      "Gradient Descent(2799/9): loss=18.887381950873205, w0=74.53409917786634, w1=13.310047395534609\n",
      "[ 2.19030486 -1.42256547]\n",
      "Gradient Descent(2800/9): loss=16.169300695275385, w0=73.00088577689318, w1=14.305843227018833\n",
      "[ 4.14434586  0.02407609]\n",
      "Gradient Descent(2801/9): loss=15.770069026242528, w0=70.09984367740935, w1=14.28898996670476\n",
      "[-1.5914413  0.6738064]\n",
      "Gradient Descent(2802/9): loss=20.814421102645223, w0=71.21385258774094, w1=13.817325485873292\n",
      "[-1.28322356 -0.27111922]\n",
      "Gradient Descent(2803/9): loss=17.60622353917989, w0=72.11210908028161, w1=14.007108938312589\n",
      "[-4.25258086  3.91803471]\n",
      "Gradient Descent(2804/9): loss=16.22330229578294, w0=75.08891568430496, w1=11.264484640128677\n",
      "[ 3.37166505 -3.93812102]\n",
      "Gradient Descent(2805/9): loss=19.450506119958934, w0=72.72875015146336, w1=14.02116935631838\n",
      "[ 2.59984943  0.09311138]\n",
      "Gradient Descent(2806/9): loss=15.692185278036073, w0=70.90885554788343, w1=13.955991387713096\n",
      "[-2.9544235   0.88371315]\n",
      "Gradient Descent(2807/9): loss=18.34357968476033, w0=72.97695200091573, w1=13.337392183308035\n",
      "[-0.67381328  0.39941183]\n",
      "Gradient Descent(2808/9): loss=15.446250386675695, w0=73.44862129741686, w1=13.057803903508127\n",
      "[-0.55595797  3.36730836]\n",
      "Gradient Descent(2809/9): loss=15.486857209282558, w0=73.83779187645625, w1=10.70068805467757\n",
      "[-0.80790987 -1.73340717]\n",
      "Gradient Descent(2810/9): loss=19.39527334212552, w0=74.40332878715043, w1=11.91407307268835\n",
      "[ 0.3543709  -2.49808012]\n",
      "Gradient Descent(2811/9): loss=17.22689288257428, w0=74.15526915927961, w1=13.662729156908275\n",
      "[-0.72951864  0.15056785]\n",
      "Gradient Descent(2812/9): loss=15.773594891666658, w0=74.66593221037179, w1=13.557331659615825\n",
      "[ 3.02079654 -1.314623  ]\n",
      "Gradient Descent(2813/9): loss=16.330106246639115, w0=72.55137463229438, w1=14.477567760104458\n",
      "[ 1.38052812  1.76468933]\n",
      "Gradient Descent(2814/9): loss=16.159433791966464, w0=71.58500494731166, w1=13.242285229759183\n",
      "[ 0.58315571 -3.00600239]\n",
      "Gradient Descent(2815/9): loss=16.874272457803126, w0=71.17679594955692, w1=15.346486905510087\n",
      "[-2.9142249   2.95281943]\n",
      "Gradient Descent(2816/9): loss=19.369422691913265, w0=73.21675338128647, w1=13.279513304532552\n",
      "[-1.12813029 -1.58435789]\n",
      "Gradient Descent(2817/9): loss=15.408905212766701, w0=74.00644458245642, w1=14.388563826001468\n",
      "[ 1.42713615  2.35325787]\n",
      "Gradient Descent(2818/9): loss=16.0527375080572, w0=73.0074492796697, w1=12.741283319241104\n",
      "[-0.49986458 -0.98635813]\n",
      "Gradient Descent(2819/9): loss=15.699559958671344, w0=73.35735448468209, w1=13.431734011922874\n",
      "[-0.2219419  -2.31204308]\n",
      "Gradient Descent(2820/9): loss=15.389050673292292, w0=73.51271381322924, w1=15.050164171253595\n",
      "[ 1.69031811  0.72573058]\n",
      "Gradient Descent(2821/9): loss=16.642982125105032, w0=72.32949113641057, w1=14.542152762575018\n",
      "[ 0.12017553 -0.0660749 ]\n",
      "Gradient Descent(2822/9): loss=16.41534104102213, w0=72.24536826638281, w1=14.588405195741627\n",
      "[-0.14563617  2.71247485]\n",
      "Gradient Descent(2823/9): loss=16.550220156050667, w0=72.34731358834374, w1=12.689672798889436\n",
      "[ 1.61986555 -3.01472108]\n",
      "Gradient Descent(2824/9): loss=16.146002926635592, w0=71.21340770583993, w1=14.799977555985581\n",
      "[-1.05374581  0.41740666]\n",
      "Gradient Descent(2825/9): loss=18.42170773217147, w0=71.95102977398906, w1=14.507792897224261\n",
      "[-0.67572983 -1.32883695]\n",
      "Gradient Descent(2826/9): loss=16.816042355411632, w0=72.42404065797743, w1=15.43797876511293\n",
      "[ 1.32408939  2.59271128]\n",
      "Gradient Descent(2827/9): loss=17.681638155108587, w0=71.49717808180407, w1=13.623080872291284\n",
      "[-1.46195321  0.58386455]\n",
      "Gradient Descent(2828/9): loss=17.01030948080617, w0=72.52054533177146, w1=13.214375688484346\n",
      "[-1.76441748  0.9752471 ]\n",
      "Gradient Descent(2829/9): loss=15.720145400460495, w0=73.75563757093192, w1=12.531702717577094\n",
      "[ 5.47648533 -3.22264056]\n",
      "Gradient Descent(2830/9): loss=15.941839714231644, w0=69.92209784046926, w1=14.78755111289693\n",
      "[-3.00180645  2.34616459]\n",
      "Gradient Descent(2831/9): loss=21.92570796104123, w0=72.02336235670101, w1=13.145235901857673\n",
      "[ 0.73366461  0.30251149]\n",
      "Gradient Descent(2832/9): loss=16.248986050701983, w0=71.50979712907007, w1=12.933477857219371\n",
      "[-1.86504035  0.76622913]\n",
      "Gradient Descent(2833/9): loss=17.126624757096305, w0=72.81532537519863, w1=12.397117469636395\n",
      "[ 1.56503788 -1.73093659]\n",
      "Gradient Descent(2834/9): loss=16.086421163976024, w0=71.71979886268053, w1=13.608773080248268\n",
      "[-0.40035529 -0.47655447]\n",
      "Gradient Descent(2835/9): loss=16.633148022942834, w0=72.00004756795977, w1=13.94236121142581\n",
      "[ 1.27081251 -0.87506499]\n",
      "Gradient Descent(2836/9): loss=16.329965339666185, w0=71.1104788136141, w1=14.554906703307282\n",
      "[-0.55787521 -0.2173668 ]\n",
      "Gradient Descent(2837/9): loss=18.347621304825676, w0=71.50099145719543, w1=14.707063460772549\n",
      "[-2.40434467  1.9092239 ]\n",
      "Gradient Descent(2838/9): loss=17.746383108510518, w0=73.18403272421008, w1=13.370606727351618\n",
      "[-1.49612329 -0.21594732]\n",
      "Gradient Descent(2839/9): loss=15.397877723247086, w0=74.2313190264714, w1=13.521769854064821\n",
      "[-1.60526203  3.87639478]\n",
      "Gradient Descent(2840/9): loss=15.826128872724366, w0=75.35500244724346, w1=10.808293507594357\n",
      "[-1.18791895 -3.14037768]\n",
      "Gradient Descent(2841/9): loss=21.078153712316404, w0=76.18654571055636, w1=13.006557884014\n",
      "[ 2.56656923  0.26663374]\n",
      "Gradient Descent(2842/9): loss=19.6814614427305, w0=74.3899472464519, w1=12.81991426668901\n",
      "[ 2.15823464  0.0782184 ]\n",
      "Gradient Descent(2843/9): loss=16.204190348398075, w0=72.87918300145671, w1=12.765161383840509\n",
      "[ 2.52937226 -2.72042233]\n",
      "Gradient Descent(2844/9): loss=15.72718369050759, w0=71.10862242133751, w1=14.669457016071908\n",
      "[-3.36066954 -1.42575954]\n",
      "Gradient Descent(2845/9): loss=18.481401081789105, w0=73.46109109891154, w1=15.667488692137402\n",
      "[-0.59101364  4.01095178]\n",
      "Gradient Descent(2846/9): loss=17.793043097963956, w0=73.87480064444505, w1=12.85982244794355\n",
      "[-0.10473234 -1.25254829]\n",
      "Gradient Descent(2847/9): loss=15.746729665412332, w0=73.94811328435702, w1=13.73660625413826\n",
      "[ 0.33284205  2.30896191]\n",
      "Gradient Descent(2848/9): loss=15.63286820287508, w0=73.71512384876998, w1=12.120332915599025\n",
      "[-0.32405331  0.21811787]\n",
      "Gradient Descent(2849/9): loss=16.398549705514842, w0=73.9419611651163, w1=11.96765040695594\n",
      "[ 1.40510138 -3.44099421]\n",
      "Gradient Descent(2850/9): loss=16.739031035537266, w0=72.95839019878375, w1=14.376346352068499\n",
      "[ 1.45819736  2.20287304]\n",
      "Gradient Descent(2851/9): loss=15.844154854978088, w0=71.93765204853646, w1=12.834335226767067\n",
      "[-0.08892144 -0.71225284]\n",
      "Gradient Descent(2852/9): loss=16.513877832752254, w0=71.99989705667522, w1=13.3329122145803\n",
      "[-1.40089464 -2.37195395]\n",
      "Gradient Descent(2853/9): loss=16.233913300882943, w0=72.98052330551066, w1=14.99327998139492\n",
      "[-1.5482979   0.89599693]\n",
      "Gradient Descent(2854/9): loss=16.580440599109522, w0=74.06433183287834, w1=14.36608212814104\n",
      "[-0.84591005  2.02010303]\n",
      "Gradient Descent(2855/9): loss=16.075479138974533, w0=74.65646886737375, w1=12.952010008700324\n",
      "[ 0.41609185 -0.42687397]\n",
      "Gradient Descent(2856/9): loss=16.45338977421149, w0=74.3652045748243, w1=13.250821785958856\n",
      "[-1.46256079 -2.4722898 ]\n",
      "Gradient Descent(2857/9): loss=15.985906508741976, w0=75.38899713129277, w1=14.98142464622639\n",
      "[ 1.7346908  -3.39563916]\n",
      "Gradient Descent(2858/9): loss=18.708127549989243, w0=74.17471357404304, w1=17.35837205941073\n",
      "[-2.66302008  3.12281327]\n",
      "Gradient Descent(2859/9): loss=23.295785006487243, w0=76.0388276324037, w1=15.172402770446176\n",
      "[ 2.35095138  0.77028193]\n",
      "Gradient Descent(2860/9): loss=20.585741614326597, w0=74.39316166501872, w1=14.633205421226638\n",
      "[-0.44936958  0.35988282]\n",
      "Gradient Descent(2861/9): loss=16.655324821740184, w0=74.70772036875523, w1=14.381287444174628\n",
      "[ 2.00495332  0.7572411 ]\n",
      "Gradient Descent(2862/9): loss=16.79171952819455, w0=73.30425304299155, w1=13.851218673006256\n",
      "[ 0.01156877 -0.38367022]\n",
      "Gradient Descent(2863/9): loss=15.454949676475147, w0=73.296154902529, w1=14.119787824778282\n",
      "[ 1.25075104  0.21358735]\n",
      "Gradient Descent(2864/9): loss=15.590738614058472, w0=72.42062917525132, w1=13.970276677782529\n",
      "[-1.74380939  2.23017002]\n",
      "Gradient Descent(2865/9): loss=15.887534687700379, w0=73.64129574613926, w1=12.409157663577766\n",
      "[ 1.5924954  -3.41715282]\n",
      "Gradient Descent(2866/9): loss=16.01926588714726, w0=72.52654896751447, w1=14.801164637346739\n",
      "[-2.27835036  1.11445524]\n",
      "Gradient Descent(2867/9): loss=16.55343651749588, w0=74.12139422102773, w1=14.021045968236525\n",
      "[ 3.37366905  0.55894969]\n",
      "Gradient Descent(2868/9): loss=15.874764002482792, w0=71.75982588666318, w1=13.62978118242522\n",
      "[-3.09503291  2.28447666]\n",
      "Gradient Descent(2869/9): loss=16.57387362901506, w0=73.92634892093167, w1=12.030647523335553\n",
      "[ 0.30306857 -0.72561767]\n",
      "Gradient Descent(2870/9): loss=16.635764331750256, w0=73.71420091913696, w1=12.538579888994432\n",
      "[ 0.57124917  0.11210163]\n",
      "Gradient Descent(2871/9): loss=15.917070287445252, w0=73.31432649790497, w1=12.4601087454545\n",
      "[-2.05411309 -0.33091111]\n",
      "Gradient Descent(2872/9): loss=15.905891882410051, w0=74.75220566259577, w1=12.691746525421346\n",
      "[ 2.32951512 -1.18973705]\n",
      "Gradient Descent(2873/9): loss=16.759628623376734, w0=73.1215450812435, w1=13.524562459036396\n",
      "[ 0.60776492 -1.20662044]\n",
      "Gradient Descent(2874/9): loss=15.401750532580804, w0=72.69610963879678, w1=14.369196770372977\n",
      "[ 0.52216974  0.70071208]\n",
      "Gradient Descent(2875/9): loss=15.960168871138286, w0=72.33059082174883, w1=13.878698314299141\n",
      "[-0.51321547  0.92863746]\n",
      "Gradient Descent(2876/9): loss=15.929486216297216, w0=72.68984165151058, w1=13.2286520923139\n",
      "[-0.24569621 -0.10550627]\n",
      "Gradient Descent(2877/9): loss=15.599860051648738, w0=72.86182899662046, w1=13.302506481692715\n",
      "[ 0.21724307 -2.01403617]\n",
      "Gradient Descent(2878/9): loss=15.494941026465645, w0=72.70975885069709, w1=14.71233180003757\n",
      "[ 2.05052875  0.81395122]\n",
      "Gradient Descent(2879/9): loss=16.316186412107236, w0=71.27438872425174, w1=14.1425659487779\n",
      "[-3.3010354   0.77716133]\n",
      "Gradient Descent(2880/9): loss=17.644832589379273, w0=73.58511350214191, w1=13.598553014886512\n",
      "[-2.58838222  3.24446844]\n",
      "Gradient Descent(2881/9): loss=15.4353456553914, w0=75.39698105302526, w1=11.327425108968074\n",
      "[ 1.85674342 -0.42818252]\n",
      "Gradient Descent(2882/9): loss=19.913486921533075, w0=74.09726065658865, w1=11.627152872988713\n",
      "[ 4.74160117 -1.23140547]\n",
      "Gradient Descent(2883/9): loss=17.424552831102485, w0=70.77813983788052, w1=12.489136700407599\n",
      "[-0.66752838 -1.69455501]\n",
      "Gradient Descent(2884/9): loss=19.041087960715675, w0=71.24540970664565, w1=13.67532520743606\n",
      "[ 0.13803668 -0.41191721]\n",
      "Gradient Descent(2885/9): loss=17.503221359526073, w0=71.14878403159831, w1=13.963667254969877\n",
      "[-4.03869922 -0.34033927]\n",
      "Gradient Descent(2886/9): loss=17.803802458975923, w0=73.97587348823836, w1=14.201904740610015\n",
      "[ 2.59854602 -0.02615098]\n",
      "Gradient Descent(2887/9): loss=15.879197646698085, w0=72.1568912739669, w1=14.220210429363425\n",
      "[-0.44976233 -0.19439607]\n",
      "Gradient Descent(2888/9): loss=16.306475947030975, w0=72.47172490773094, w1=14.356287678427565\n",
      "[-0.58641528 -1.35468921]\n",
      "Gradient Descent(2889/9): loss=16.108083978532772, w0=72.88221560039001, w1=15.30457012400811\n",
      "[-2.07060569  3.34920442]\n",
      "Gradient Descent(2890/9): loss=17.135691742022026, w0=74.33163958057037, w1=12.960127031451602\n",
      "[ 2.69301401 -0.47201245]\n",
      "Gradient Descent(2891/9): loss=16.0593012509418, w0=72.4465297716328, w1=13.290535748386146\n",
      "[-0.49918372  1.63095349]\n",
      "Gradient Descent(2892/9): loss=15.762818574338915, w0=72.79595837225715, w1=12.14886830771908\n",
      "[-1.25306746 -2.94884086]\n",
      "Gradient Descent(2893/9): loss=16.3954448026996, w0=73.67310559649421, w1=14.213056908207562\n",
      "[-2.89106342  2.51938515]\n",
      "Gradient Descent(2894/9): loss=15.726675026156357, w0=75.69684999172873, w1=12.449487306339174\n",
      "[ 1.95386615 -1.9345316 ]\n",
      "Gradient Descent(2895/9): loss=18.80360123833833, w0=74.32914368993009, w1=13.803659425044057\n",
      "[-1.83929467  0.49726538]\n",
      "Gradient Descent(2896/9): loss=15.974200666483767, w0=75.61664995929101, w1=13.455573659640987\n",
      "[ 0.77628755  2.49065422]\n",
      "Gradient Descent(2897/9): loss=18.083711790613357, w0=75.07324867504865, w1=11.712115705520393\n",
      "[-1.85290635 -3.62922842]\n",
      "Gradient Descent(2898/9): loss=18.531088672367567, w0=76.37028312303498, w1=14.252575598963789\n",
      "[ 2.10404449  0.43702946]\n",
      "Gradient Descent(2899/9): loss=20.416545477128118, w0=74.89745197958602, w1=13.94665497899309\n",
      "[ 1.20759576 -0.09540413]\n",
      "Gradient Descent(2900/9): loss=16.780559732869726, w0=74.05213494856426, w1=14.013437870655185\n",
      "[ 2.34090498 -0.94433599]\n",
      "Gradient Descent(2901/9): loss=15.815762725256974, w0=72.41350146500923, w1=14.674473062590328\n",
      "[-0.54747607  1.54284651]\n",
      "Gradient Descent(2902/9): loss=16.48718450853267, w0=72.79673471198406, w1=13.59448050374582\n",
      "[-0.34766285  0.51895198]\n",
      "Gradient Descent(2903/9): loss=15.516071324361477, w0=73.0400987067967, w1=13.231214120588895\n",
      "[ 0.99041019  0.74092766]\n",
      "Gradient Descent(2904/9): loss=15.448976707579892, w0=72.34681157453025, w1=12.71256475658983\n",
      "[ 2.16562844 -1.37278173]\n",
      "Gradient Descent(2905/9): loss=16.128654730076647, w0=70.8308716677118, w1=13.673511968130017\n",
      "[ 0.3038716  -0.80613644]\n",
      "Gradient Descent(2906/9): loss=18.43797547322991, w0=70.61816154853452, w1=14.237807474751294\n",
      "[-3.16149746  2.05426394]\n",
      "Gradient Descent(2907/9): loss=19.25308891593183, w0=72.83120976723238, w1=12.799822713278555\n",
      "[-0.74838539 -2.28569195]\n",
      "Gradient Descent(2908/9): loss=15.724064191823679, w0=73.35507953946234, w1=14.399807076138691\n",
      "[-2.61989693 -0.23728257]\n",
      "Gradient Descent(2909/9): loss=15.811045065353339, w0=75.18900739391383, w1=14.56590487855034\n",
      "[ 0.90518666  2.5005454 ]\n",
      "Gradient Descent(2910/9): loss=17.771469202177467, w0=74.55537673011442, w1=12.815523095658971\n",
      "[-0.37104517 -0.33326396]\n",
      "Gradient Descent(2911/9): loss=16.40209562347768, w0=74.8151083484907, w1=13.048807870046724\n",
      "[ 4.45893989  0.27809032]\n",
      "Gradient Descent(2912/9): loss=16.63573119108831, w0=71.69385042649571, w1=12.854144648718876\n",
      "[-2.82878713  1.55464162]\n",
      "Gradient Descent(2913/9): loss=16.861669919975576, w0=73.67400141543531, w1=11.765895515727955\n",
      "[-0.29379376 -1.15194601]\n",
      "Gradient Descent(2914/9): loss=16.926702265420875, w0=73.87965704543852, w1=12.572257725384183\n",
      "[ 4.34914159  0.12879137]\n",
      "Gradient Descent(2915/9): loss=15.969167664315776, w0=70.83525793391331, w1=12.482103763148485\n",
      "[-1.62867885 -0.67309294]\n",
      "Gradient Descent(2916/9): loss=18.90601390000406, w0=71.97533312792807, w1=12.953268824446708\n",
      "[ 1.13707639 -0.22175906]\n",
      "Gradient Descent(2917/9): loss=16.39379761592167, w0=71.17937965518894, w1=13.108500163378382\n",
      "[-5.88003265 -0.13753158]\n",
      "Gradient Descent(2918/9): loss=17.690431812577618, w0=75.29540251208456, w1=13.204772270359255\n",
      "[ 1.44557809 -3.25271725]\n",
      "Gradient Descent(2919/9): loss=17.426646031806378, w0=74.2834978490397, w1=15.481674342029393\n",
      "[ 1.11978425  1.50983962]\n",
      "Gradient Descent(2920/9): loss=17.87944378586779, w0=73.49964887199516, w1=14.424786609420694\n",
      "[ 3.82039369  2.18555319]\n",
      "Gradient Descent(2921/9): loss=15.853632238915592, w0=70.82537328969057, w1=12.894899375900259\n",
      "[-2.61112768  0.36208133]\n",
      "Gradient Descent(2922/9): loss=18.60375739865172, w0=72.65316266584342, w1=12.64144244730035\n",
      "[-1.62067387 -1.2886273 ]\n",
      "Gradient Descent(2923/9): loss=15.942522418462515, w0=73.78763437375824, w1=13.54348155794196\n",
      "[ 4.0247902  2.5686081]\n",
      "Gradient Descent(2924/9): loss=15.50979707231213, w0=70.97028123098252, w1=11.745455887917778\n",
      "[-0.73261669 -3.34606545]\n",
      "Gradient Descent(2925/9): loss=19.589363970970947, w0=71.48311291723734, w1=14.087701702919443\n",
      "[-0.10655135  0.73767504]\n",
      "Gradient Descent(2926/9): loss=17.21022811470865, w0=71.55769886370318, w1=13.571329176391426\n",
      "[-4.13607975 -2.67321312]\n",
      "Gradient Descent(2927/9): loss=16.897320075643254, w0=74.45295469126484, w1=15.442578358548499\n",
      "[ 2.47694869  4.61520718]\n",
      "Gradient Descent(2928/9): loss=17.983987573035165, w0=72.71909060674092, w1=12.2119333312044\n",
      "[ 3.04104744 -1.94042418]\n",
      "Gradient Descent(2929/9): loss=16.35473536337412, w0=70.59035739679436, w1=13.570230258770952\n",
      "[-3.30246881  3.83955556]\n",
      "Gradient Descent(2930/9): loss=19.044615394585257, w0=72.90208556707164, w1=10.882541366535554\n",
      "[ 0.10612317 -3.26651319]\n",
      "Gradient Descent(2931/9): loss=18.835304544145234, w0=72.82779934813448, w1=13.169100596710408\n",
      "[ 0.31631401  0.11418219]\n",
      "Gradient Descent(2932/9): loss=15.542762890141166, w0=72.60637954388173, w1=13.089173061676304\n",
      "[ 3.1452711  1.8577706]\n",
      "Gradient Descent(2933/9): loss=15.698505685813137, w0=70.4046897732228, w1=11.788733642812566\n",
      "[-6.4118482  -1.37235156]\n",
      "Gradient Descent(2934/9): loss=20.989423942831067, w0=74.89298351474667, w1=12.749379733988595\n",
      "[ 2.67897715 -1.4268021 ]\n",
      "Gradient Descent(2935/9): loss=16.931079656510345, w0=73.01769951085372, w1=13.74814120432118\n",
      "[-0.38205378 -1.40047186]\n",
      "Gradient Descent(2936/9): loss=15.460064303268569, w0=73.28513715493273, w1=14.72847150875883\n",
      "[-4.08962427  2.6918335 ]\n",
      "Gradient Descent(2937/9): loss=16.165626067760506, w0=76.14787414275614, w1=12.844188060945516\n",
      "[ 0.70397293 -4.96297977]\n",
      "Gradient Descent(2938/9): loss=19.660354894394178, w0=75.65509308889871, w1=16.318273897696233\n",
      "[ 3.54021695  2.61814762]\n",
      "Gradient Descent(2939/9): loss=22.202167908167418, w0=73.17694122368229, w1=14.485570563046553\n",
      "[-0.41798853  2.40559347]\n",
      "Gradient Descent(2940/9): loss=15.898605406979286, w0=73.46953319466778, w1=12.801655133274972\n",
      "[-0.51991238 -1.19147502]\n",
      "Gradient Descent(2941/9): loss=15.631188366509912, w0=73.83347185814482, w1=13.635687646813832\n",
      "[-1.34933126 -0.6159196 ]\n",
      "Gradient Descent(2942/9): loss=15.543609025757487, w0=74.7780037387028, w1=14.06683137010104\n",
      "[ 2.97984465 -0.72822923]\n",
      "Gradient Descent(2943/9): loss=16.6594914912642, w0=72.69211248538565, w1=14.576591834206544\n",
      "[-0.46410853 -0.98317932]\n",
      "Gradient Descent(2944/9): loss=16.16854742425037, w0=73.01698845350832, w1=15.264817358981299\n",
      "[ 0.16684364 -0.70325385]\n",
      "Gradient Descent(2945/9): loss=17.017533758829316, w0=72.9001979020162, w1=15.757095050828228\n",
      "[-1.12801465  7.5947564 ]\n",
      "Gradient Descent(2946/9): loss=18.056632991788096, w0=73.68980815874704, w1=10.440765567493788\n",
      "[ 4.48064251 -7.07593002]\n",
      "Gradient Descent(2947/9): loss=20.081849825069362, w0=70.55335839963202, w1=15.393916579230254\n",
      "[-2.16326317  2.37161091]\n",
      "Gradient Descent(2948/9): loss=20.973321051344872, w0=72.06764262057627, w1=13.733788939643055\n",
      "[-0.54378067 -0.4382661 ]\n",
      "Gradient Descent(2949/9): loss=16.17004586471948, w0=72.44828908726483, w1=14.040575208012838\n",
      "[-0.772886   -1.35074547]\n",
      "Gradient Descent(2950/9): loss=15.900718907242073, w0=72.98930928898612, w1=14.986097034939736\n",
      "[-0.68929842  1.70820767]\n",
      "Gradient Descent(2951/9): loss=16.566879602810573, w0=73.47181818589928, w1=13.790351664628124\n",
      "[ 0.80136631  1.22238535]\n",
      "Gradient Descent(2952/9): loss=15.44995976042903, w0=72.91086176743323, w1=12.934681917231966\n",
      "[-2.12515087 -0.59720067]\n",
      "Gradient Descent(2953/9): loss=15.607784573166143, w0=74.39846737846153, w1=13.35272238274759\n",
      "[ 0.80577689  0.93907694]\n",
      "Gradient Descent(2954/9): loss=16.003961349728634, w0=73.83442355244453, w1=12.695368521339137\n",
      "[-1.49681982 -2.44984093]\n",
      "Gradient Descent(2955/9): loss=15.839556519228848, w0=74.88219742868716, w1=14.410257169630718\n",
      "[-0.10942699  1.49967084]\n",
      "Gradient Descent(2956/9): loss=17.080154035756145, w0=74.95879632451759, w1=13.36048757992642\n",
      "[ 3.38045338  0.61060171]\n",
      "Gradient Descent(2957/9): loss=16.77889840657583, w0=72.59247895987193, w1=12.933066383240229\n",
      "[-0.94096524 -2.88005755]\n",
      "Gradient Descent(2958/9): loss=15.781309992524413, w0=73.2511546286324, w1=14.949106667963694\n",
      "[-1.89875086  0.27760941]\n",
      "Gradient Descent(2959/9): loss=16.466362098895853, w0=74.5802802275979, w1=14.75478008327027\n",
      "[ 0.49485695 -1.10814182]\n",
      "Gradient Descent(2960/9): loss=17.026145364822472, w0=74.23388036131377, w1=15.530479360556647\n",
      "[-0.1625989   3.33029325]\n",
      "Gradient Descent(2961/9): loss=17.93047121885343, w0=74.34769959186104, w1=13.199274086568847\n",
      "[ 1.86922583 -0.02510171]\n",
      "Gradient Descent(2962/9): loss=15.980434306797553, w0=73.03924151352028, w1=13.216845282532587\n",
      "[ 0.12600362 -2.11249309]\n",
      "Gradient Descent(2963/9): loss=15.452868514382606, w0=72.95103897820871, w1=14.695590442835279\n",
      "[ 0.96270026  0.37811924]\n",
      "Gradient Descent(2964/9): loss=16.183851917849655, w0=72.27714879895336, w1=14.430906976369075\n",
      "[ 1.07353527  1.77686276]\n",
      "Gradient Descent(2965/9): loss=16.35518726992879, w0=71.52567411108507, w1=13.187103043505966\n",
      "[ 3.0116082   0.88094547]\n",
      "Gradient Descent(2966/9): loss=16.992048298870007, w0=69.41754837341091, w1=12.570441217348584\n",
      "[-2.76665011  1.65990526]\n",
      "Gradient Descent(2967/9): loss=23.31241119706231, w0=71.35420345060355, w1=11.408507534500128\n",
      "[ 1.17524484 -0.64553319]\n",
      "Gradient Descent(2968/9): loss=19.412086768253868, w0=70.53153206569628, w1=11.860380769664195\n",
      "[-4.20021797 -1.81486508]\n",
      "Gradient Descent(2969/9): loss=20.51240447037788, w0=73.47168464619429, w1=13.130786326136912\n",
      "[ 4.69948049  5.24955316]\n",
      "Gradient Descent(2970/9): loss=15.462562362365524, w0=70.18204829993243, w1=9.456099114657034\n",
      "[-1.72507984 -3.28663046]\n",
      "Gradient Descent(2971/9): loss=28.32249891374319, w0=71.38960419075329, w1=11.756740434846725\n",
      "[-0.6901767  -1.74575711]\n",
      "Gradient Descent(2972/9): loss=18.683417288782667, w0=71.8727278818562, w1=12.978770414512773\n",
      "[-1.61396605  1.34973423]\n",
      "Gradient Descent(2973/9): loss=16.52125568648396, w0=73.00250412003201, w1=12.033956453326912\n",
      "[-1.2472374  -0.77444159]\n",
      "Gradient Descent(2974/9): loss=16.473455239081332, w0=73.87557030069625, w1=12.576065563257313\n",
      "[ 0.32965071  1.73020335]\n",
      "Gradient Descent(2975/9): loss=15.963334074851614, w0=73.64481480471957, w1=11.36492322024111\n",
      "[ 1.97541563 -1.44825178]\n",
      "Gradient Descent(2976/9): loss=17.68361745969979, w0=72.26202386705964, w1=12.378699465193835\n",
      "[-0.74341151 -2.57401288]\n",
      "Gradient Descent(2977/9): loss=16.52440952921328, w0=72.78241192527278, w1=14.180508480079546\n",
      "[-0.33911618  0.78212566]\n",
      "Gradient Descent(2978/9): loss=15.76226669658719, w0=73.01979324872433, w1=13.633020520941846\n",
      "[-0.66253284  1.56940473]\n",
      "Gradient Descent(2979/9): loss=15.435212840153728, w0=73.48356623692652, w1=12.534437212153954\n",
      "[ 2.31163379  1.48470955]\n",
      "Gradient Descent(2980/9): loss=15.850642960182855, w0=71.86542258214446, w1=11.495140529703992\n",
      "[-3.42702975 -1.10269109]\n",
      "Gradient Descent(2981/9): loss=18.375455988866843, w0=74.26434341009008, w1=12.267024294669211\n",
      "[-1.3683668 -2.2468045]\n",
      "Gradient Descent(2982/9): loss=16.59205298620328, w0=75.2222001679295, w1=13.839787443963816\n",
      "[ 0.17884135 -1.06568443]\n",
      "Gradient Descent(2983/9): loss=17.309843217270874, w0=75.09701122069538, w1=14.585766541959531\n",
      "[ 1.9433153   3.36631639]\n",
      "Gradient Descent(2984/9): loss=17.623131077700624, w0=73.73669050726129, w1=12.229345065905278\n",
      "[-1.60912232 -0.63881243]\n",
      "Gradient Descent(2985/9): loss=16.26561912224322, w0=74.86307613022541, w1=12.676513770374637\n",
      "[ 1.19677896 -0.83866431]\n",
      "Gradient Descent(2986/9): loss=16.93957425514695, w0=74.02533086091537, w1=13.263578786652614\n",
      "[ 4.74536506  0.58883709]\n",
      "Gradient Descent(2987/9): loss=15.676724205174018, w0=70.70357532119392, w1=12.851392826681055\n",
      "[-0.30092625 -0.53490546]\n",
      "Gradient Descent(2988/9): loss=18.93822859757556, w0=70.91422369853538, w1=13.225826645877564\n",
      "[ 0.19481554 -1.46121637]\n",
      "Gradient Descent(2989/9): loss=18.249598873792294, w0=70.77785281739108, w1=14.24867810248257\n",
      "[-3.12152618 -0.77977005]\n",
      "Gradient Descent(2990/9): loss=18.846844038855245, w0=72.9629211434725, w1=14.794517137399025\n",
      "[ 0.58716831  3.23638785]\n",
      "Gradient Descent(2991/9): loss=16.305024355776883, w0=72.55190332607368, w1=12.529045644780311\n",
      "[-0.87624131 -0.67442653]\n",
      "Gradient Descent(2992/9): loss=16.113067399622064, w0=73.1652722463772, w1=13.001144215533088\n",
      "[-2.95510978 -1.02967219]\n",
      "Gradient Descent(2993/9): loss=15.508677018990461, w0=75.23384909284562, w1=13.721914745227156\n",
      "[ 3.21425678  2.08014756]\n",
      "Gradient Descent(2994/9): loss=17.296877407066045, w0=72.98386934436921, w1=12.265811451850608\n",
      "[ 0.30595757 -0.05490877]\n",
      "Gradient Descent(2995/9): loss=16.17073199254621, w0=72.76969904555659, w1=12.304247591945392\n",
      "[-0.25923598  2.27299436]\n",
      "Gradient Descent(2996/9): loss=16.2141515215315, w0=72.95116423403834, w1=10.713151542827978\n",
      "[ 0.16247101 -4.0910057 ]\n",
      "Gradient Descent(2997/9): loss=19.271558897632012, w0=72.83743453043479, w1=13.57685553519466\n",
      "[ 0.61505013 -1.35297828]\n",
      "Gradient Descent(2998/9): loss=15.494796665684195, w0=72.40689943780659, w1=14.523940332567964\n",
      "[-2.36734081  1.13665618]\n",
      "Gradient Descent(2999/9): loss=16.32449833465787, w0=74.06403800189509, w1=13.728281006188519\n",
      "[ 1.09099881  1.05570814]\n",
      "Gradient Descent(3000/9): loss=15.713320362689666, w0=73.30033883676205, w1=12.989285311344464\n",
      "[-4.38831397 -2.38267853]\n",
      "Gradient Descent(3001/9): loss=15.506167838516058, w0=76.37215861812089, w1=14.657160279078083\n",
      "[ 1.56549759  0.87817601]\n",
      "Gradient Descent(3002/9): loss=20.816849913694256, w0=75.27631030524788, w1=14.042437072844287\n",
      "[-0.10196298  0.60111526]\n",
      "Gradient Descent(3003/9): loss=17.509149070072525, w0=75.3476843914327, w1=13.62165639133275\n",
      "[ 3.60775988 -0.21397895]\n",
      "Gradient Descent(3004/9): loss=17.504931888108757, w0=72.82225247341182, w1=13.771441656970293\n",
      "[-1.77401716 -0.55643519]\n",
      "Gradient Descent(3005/9): loss=15.539676910457205, w0=74.06406448269036, w1=14.160946289296238\n",
      "[-0.69168224 -2.53175241]\n",
      "Gradient Descent(3006/9): loss=15.91448737115745, w0=74.54824204951699, w1=15.9331729776798\n",
      "[-1.71982577  3.20171761]\n",
      "Gradient Descent(3007/9): loss=19.18228157676916, w0=75.75212008556447, w1=13.691970648051191\n",
      "[ 3.7616447   1.68995664]\n",
      "Gradient Descent(3008/9): loss=18.429783552097007, w0=73.11896879235637, w1=12.509001002057728\n",
      "[ 3.42900147  5.03477077]\n",
      "Gradient Descent(3009/9): loss=15.872332524641894, w0=70.71866776004575, w1=8.984661463625372\n",
      "[-4.43429751 -4.98489609]\n",
      "Gradient Descent(3010/9): loss=28.80459669203073, w0=73.82267602007681, w1=12.474088723692217\n",
      "[ 0.06284563 -1.93391223]\n",
      "Gradient Descent(3011/9): loss=16.031317798951175, w0=73.77868407727176, w1=13.827827284912608\n",
      "[-1.06596393  0.18571004]\n",
      "Gradient Descent(3012/9): loss=15.563976977957953, w0=74.52485882525482, w1=13.697830260383478\n",
      "[ 3.12196868  0.62660647]\n",
      "Gradient Descent(3013/9): loss=16.167278292999654, w0=72.33948075048285, w1=13.259205733121794\n",
      "[-2.46473686 -2.42605416]\n",
      "Gradient Descent(3014/9): loss=15.865678523012795, w0=74.06479655013193, w1=14.957443647794717\n",
      "[ 0.63449656  2.54794438]\n",
      "Gradient Descent(3015/9): loss=16.77485642187717, w0=73.62064895992283, w1=13.173882580310616\n",
      "[ 0.67453388 -1.62486034]\n",
      "Gradient Descent(3016/9): loss=15.486029071318102, w0=73.14847524511093, w1=14.311284816816842\n",
      "[ 1.20569404  2.03320922]\n",
      "Gradient Descent(3017/9): loss=15.74222156149885, w0=72.30448941671213, w1=12.888038359892802\n",
      "[-3.09871321 -2.5815162 ]\n",
      "Gradient Descent(3018/9): loss=16.050415394918698, w0=74.4735886659079, w1=14.695099699934314\n",
      "[ 1.14727902  2.67786248]\n",
      "Gradient Descent(3019/9): loss=16.820277689568673, w0=73.6704933489267, w1=12.820595965017363\n",
      "[ 2.5709997  -2.91182074]\n",
      "Gradient Descent(3020/9): loss=15.67400811894685, w0=71.87079356235209, w1=14.858870483355743\n",
      "[-0.91439446  2.9299449 ]\n",
      "Gradient Descent(3021/9): loss=17.349573608033765, w0=72.51086968404766, w1=12.807909052368107\n",
      "[ 2.90788644 -1.28886993]\n",
      "Gradient Descent(3022/9): loss=15.918133227687504, w0=70.47534917448304, w1=13.710118002208015\n",
      "[-3.83832851 -1.81663203]\n",
      "Gradient Descent(3023/9): loss=19.3846076238371, w0=73.16217913455547, w1=14.981760423166126\n",
      "[ 2.08568602  1.40983549]\n",
      "Gradient Descent(3024/9): loss=16.522640039797913, w0=71.70219892387775, w1=13.994875580419185\n",
      "[-1.12152055  3.37994852]\n",
      "Gradient Descent(3025/9): loss=16.785375580915062, w0=72.48726330694905, w1=11.628911617764386\n",
      "[-0.36436167 -4.33441504]\n",
      "Gradient Descent(3026/9): loss=17.423968826584645, w0=72.74231647282505, w1=14.663002143982625\n",
      "[-3.69339023  0.99614542]\n",
      "Gradient Descent(3027/9): loss=16.238109466500664, w0=75.32768963604097, w1=13.965700349692602\n",
      "[ 1.6971105  -0.04212661]\n",
      "Gradient Descent(3028/9): loss=17.572085389870733, w0=74.13971228272288, w1=13.995188974218058\n",
      "[ 1.86056494  1.43936581]\n",
      "Gradient Descent(3029/9): loss=15.876426499470835, w0=72.83731682765139, w1=12.987632907989527\n",
      "[ 1.1429507  -1.27032951]\n",
      "Gradient Descent(3030/9): loss=15.611203141944424, w0=72.03725133899343, w1=13.876863567051185\n",
      "[-2.74830455  1.86775636]\n",
      "Gradient Descent(3031/9): loss=16.254362957441398, w0=73.96106452229772, w1=12.569434117091275\n",
      "[ 4.07306041 -0.01609861]\n",
      "Gradient Descent(3032/9): loss=16.02273074797122, w0=71.1099222366216, w1=12.580703146040172\n",
      "[-2.34880344  0.37473495]\n",
      "Gradient Descent(3033/9): loss=18.174924207453767, w0=72.7540846449239, w1=12.318388681741228\n",
      "[-1.04377473  0.78023795]\n",
      "Gradient Descent(3034/9): loss=16.205936484862438, w0=73.48472695825096, w1=11.772222119626527\n",
      "[-1.43417438  2.520081  ]\n",
      "Gradient Descent(3035/9): loss=16.861852723002695, w0=74.48864902234972, w1=10.008165420914363\n",
      "[ 3.43821919 -1.10778455]\n",
      "Gradient Descent(3036/9): loss=22.125393530746024, w0=72.08189559267758, w1=10.783614607864642\n",
      "[-4.48578342 -3.25733379]\n",
      "Gradient Descent(3037/9): loss=19.754863624116858, w0=75.22194398323633, w1=13.063748262576041\n",
      "[ 2.09660951 -0.20938978]\n",
      "Gradient Descent(3038/9): loss=17.33103534505744, w0=73.75431732793304, w1=13.21032110579192\n",
      "[-2.40837381 -0.84217739]\n",
      "Gradient Descent(3039/9): loss=15.528155640974763, w0=75.44017899236741, w1=13.799845280512539\n",
      "[ 3.64698986 -0.49936539]\n",
      "Gradient Descent(3040/9): loss=17.740339922345598, w0=72.88728608849287, w1=14.149401053633099\n",
      "[ 0.35022151 -2.00181638]\n",
      "Gradient Descent(3041/9): loss=15.692805674919748, w0=72.64213102863242, w1=15.550672516639551\n",
      "[-3.32194829  1.61767055]\n",
      "Gradient Descent(3042/9): loss=17.74274143527462, w0=74.9674948304561, w1=14.41830313358404\n",
      "[ 3.62324676  6.76254748]\n",
      "Gradient Descent(3043/9): loss=17.22678712447116, w0=72.43122209842427, w1=9.684519898894631\n",
      "[ 1.57814194 -1.79508128]\n",
      "Gradient Descent(3044/9): loss=22.959756623748316, w0=71.32652274279418, w1=10.94107679784382\n",
      "[-1.35708294 -3.69800198]\n",
      "Gradient Descent(3045/9): loss=20.543553240690024, w0=72.27648079741937, w1=13.529678184715602\n",
      "[-3.17404627  0.5668319 ]\n",
      "Gradient Descent(3046/9): loss=15.904729459398537, w0=74.49831318786002, w1=13.132895851936084\n",
      "[ 2.88269754 -2.06810735]\n",
      "Gradient Descent(3047/9): loss=16.17130780413163, w0=72.4804249108644, w1=14.58057099443351\n",
      "[-1.91408796 -0.85492896]\n",
      "Gradient Descent(3048/9): loss=16.322721411509082, w0=73.82028648363267, w1=15.179021263938667\n",
      "[ 1.69680242  0.4148789 ]\n",
      "Gradient Descent(3049/9): loss=16.968242900609308, w0=72.63252479233061, w1=14.88860603548487\n",
      "[ 1.716892    0.97268222]\n",
      "Gradient Descent(3050/9): loss=16.597101592137236, w0=71.4307003902271, w1=14.207728481390905\n",
      "[-0.67043763  0.77403084]\n",
      "Gradient Descent(3051/9): loss=17.386688938223486, w0=71.9000067287919, w1=13.665906892109287\n",
      "[-4.09125213  2.25146295]\n",
      "Gradient Descent(3052/9): loss=16.37472195134859, w0=74.76388322243476, w1=12.089882825969397\n",
      "[ 2.22810179 -1.97542715]\n",
      "Gradient Descent(3053/9): loss=17.432094034519658, w0=73.20421197088757, w1=13.47268183393567\n",
      "[ 1.74980056 -2.10469715]\n",
      "Gradient Descent(3054/9): loss=15.389936528355518, w0=71.97935157833389, w1=14.945969841074128\n",
      "[-2.56925575 -1.34065249]\n",
      "Gradient Descent(3055/9): loss=17.324890958806204, w0=73.77783060211496, w1=15.884426586843736\n",
      "[ 0.52291281  0.99156904]\n",
      "Gradient Descent(3056/9): loss=18.394296711476212, w0=73.41179163601637, w1=15.190328257683053\n",
      "[ 1.49908863  1.05868783]\n",
      "Gradient Descent(3057/9): loss=16.85593774055412, w0=72.36242959392531, w1=14.449246776224157\n",
      "[ 0.65810342  1.50018827]\n",
      "Gradient Descent(3058/9): loss=16.28972534149487, w0=71.90175720261796, w1=13.399114990304353\n",
      "[-1.25642144 -1.39370739]\n",
      "Gradient Descent(3059/9): loss=16.358197257339913, w0=72.78125221226487, w1=14.37471016491002\n",
      "[ 0.7068417  -0.61088436]\n",
      "Gradient Descent(3060/9): loss=15.917813493818702, w0=72.28646302225884, w1=14.80232921604375\n",
      "[ 2.02369563  2.19976806]\n",
      "Gradient Descent(3061/9): loss=16.768032241629673, w0=70.86987608192277, w1=13.2624915745414\n",
      "[-1.07897837 -4.40552805]\n",
      "Gradient Descent(3062/9): loss=18.347479631512726, w0=71.62516093846128, w1=16.346361209901044\n",
      "[ 0.75961367  0.91993577]\n",
      "Gradient Descent(3063/9): loss=20.887107211948607, w0=71.09343137121705, w1=15.70240616953006\n",
      "[ 0.28240986  2.95658135]\n",
      "Gradient Descent(3064/9): loss=20.277151095926577, w0=70.89574447188441, w1=13.63279922274606\n",
      "[-1.64930693 -0.5702497 ]\n",
      "Gradient Descent(3065/9): loss=18.273233384350192, w0=72.05025932046, w1=14.031974015979777\n",
      "[-3.20894659  0.69507399]\n",
      "Gradient Descent(3066/9): loss=16.31173272860704, w0=74.29652193019851, w1=13.545422220815054\n",
      "[-0.53491649  0.83319222]\n",
      "Gradient Descent(3067/9): loss=15.890650064712414, w0=74.67096347135207, w1=12.962187668578865\n",
      "[-0.92724181 -0.53695387]\n",
      "Gradient Descent(3068/9): loss=16.46792541476616, w0=75.32003273686607, w1=13.338055374719547\n",
      "[ 5.64616624  3.29104103]\n",
      "Gradient Descent(3069/9): loss=17.448483584948136, w0=71.36771636630009, w1=11.034326653859821\n",
      "[ 0.8404103  -2.75949764]\n",
      "Gradient Descent(3070/9): loss=20.230977753807565, w0=70.77942915542248, w1=12.965975004797468\n",
      "[-0.80881952 -1.27269831]\n",
      "Gradient Descent(3071/9): loss=18.679188080428588, w0=71.34560282067702, w1=13.856863822922357\n",
      "[-1.2091844   0.24711567]\n",
      "Gradient Descent(3072/9): loss=17.354983269899886, w0=72.19203190083165, w1=13.683882856403685\n",
      "[-1.8868593   0.48719028]\n",
      "Gradient Descent(3073/9): loss=16.013811546962017, w0=73.51283341339582, w1=13.342849663305959\n",
      "[-0.75037763 -1.8472901 ]\n",
      "Gradient Descent(3074/9): loss=15.419214680962419, w0=74.03809775453723, w1=14.635952733143906\n",
      "[ 0.27902644 -0.4904771 ]\n",
      "Gradient Descent(3075/9): loss=16.331232457621912, w0=73.84277924401587, w1=14.979286704489011\n",
      "[-0.31064386  0.79064652]\n",
      "Gradient Descent(3076/9): loss=16.66087149970147, w0=74.06022994582462, w1=14.42583413983791\n",
      "[ 3.35361892 -2.43106888]\n",
      "Gradient Descent(3077/9): loss=16.127074941326207, w0=71.712696705284, w1=16.127582356086485\n",
      "[-0.25506477  3.22559563]\n",
      "Gradient Descent(3078/9): loss=20.141632148009304, w0=71.89124204232955, w1=13.869665415445805\n",
      "[-2.90654019 -2.27336623]\n",
      "Gradient Descent(3079/9): loss=16.445675067091045, w0=73.92582017692541, w1=15.46102177914942\n",
      "[-4.26818298  0.60643878]\n",
      "Gradient Descent(3080/9): loss=17.54832887912856, w0=76.91354826387797, w1=15.03651463663271\n",
      "[ 1.40793679  2.38882141]\n",
      "Gradient Descent(3081/9): loss=23.148551553808268, w0=75.92799251054116, w1=13.364339647228949\n",
      "[ 1.98627293 -0.14189245]\n",
      "Gradient Descent(3082/9): loss=18.861707030613232, w0=74.53760146022806, w1=13.463664363030952\n",
      "[ 1.28779927  0.523532  ]\n",
      "Gradient Descent(3083/9): loss=16.159385936414584, w0=73.63614196884654, w1=13.097191962428868\n",
      "[-0.0522379  -0.96538522]\n",
      "Gradient Descent(3084/9): loss=15.517606077611457, w0=73.67270849972822, w1=13.772961617259279\n",
      "[ 2.62359211 -1.42699089]\n",
      "Gradient Descent(3085/9): loss=15.500625015671236, w0=71.8361940203381, w1=14.771855240436135\n",
      "[-2.11220335  2.09115033]\n",
      "Gradient Descent(3086/9): loss=17.283189818077116, w0=73.314736364452, w1=13.308050011031277\n",
      "[ 1.28635748  2.5688409 ]\n",
      "Gradient Descent(3087/9): loss=15.400838481568883, w0=72.41428612599653, w1=11.50986138263666\n",
      "[-1.38394709 -0.87377952]\n",
      "Gradient Descent(3088/9): loss=17.712924090325036, w0=73.38304909156956, w1=12.121507044954265\n",
      "[ 1.70193561 -1.71275981]\n",
      "Gradient Descent(3089/9): loss=16.31222062862736, w0=72.19169416154509, w1=13.320438914703601\n",
      "[-1.90506678  2.36641702]\n",
      "Gradient Descent(3090/9): loss=16.006025002214354, w0=73.52524090628033, w1=11.663946999138457\n",
      "[-2.77850921 -1.1611607 ]\n",
      "Gradient Descent(3091/9): loss=17.06114414555864, w0=75.47019735671279, w1=12.476759487648785\n",
      "[ 3.78113776 -0.437852  ]\n",
      "Gradient Descent(3092/9): loss=18.256932385654878, w0=72.82340092267997, w1=12.783255890133983\n",
      "[ 0.19549204 -2.34625627]\n",
      "Gradient Descent(3093/9): loss=15.739108771356864, w0=72.68655649645028, w1=14.42563528053489\n",
      "[-3.58761639  4.32303882]\n",
      "Gradient Descent(3094/9): loss=16.017719312421892, w0=75.19788796917972, w1=11.399508108195132\n",
      "[-0.30083303 -1.24461143]\n",
      "Gradient Descent(3095/9): loss=19.362056091324494, w0=75.40847109054559, w1=12.270736106644124\n",
      "[ 1.09923288 -0.81613417]\n",
      "Gradient Descent(3096/9): loss=18.352358673790654, w0=74.63900807268682, w1=12.84203002716074\n",
      "[ 1.18284024 -1.30601758]\n",
      "Gradient Descent(3097/9): loss=16.493835564092624, w0=73.81101990802836, w1=13.756242333167176\n",
      "[ 1.1869918   0.16453152]\n",
      "Gradient Descent(3098/9): loss=15.557817383277667, w0=72.98012564713146, w1=13.641070271699457\n",
      "[ 1.2008111   0.59785508]\n",
      "Gradient Descent(3099/9): loss=15.44814012076073, w0=72.13955787614171, w1=13.222571716731442\n",
      "[-0.64387398 -4.77553413]\n",
      "Gradient Descent(3100/9): loss=16.08522681097814, w0=72.59026965914305, w1=16.565445610726144\n",
      "[ 1.04569674  1.04544533]\n",
      "Gradient Descent(3101/9): loss=20.394325794629733, w0=71.85828194192149, w1=15.833633881235476\n",
      "[-1.44922484  4.172092  ]\n",
      "Gradient Descent(3102/9): loss=19.18689214758097, w0=72.87273933211758, w1=12.913169479545711\n",
      "[-1.58327174  0.2192936 ]\n",
      "Gradient Descent(3103/9): loss=15.635070749759587, w0=73.98102954965057, w1=12.759663961105124\n",
      "[ 0.6248155   0.08479564]\n",
      "Gradient Descent(3104/9): loss=15.881181162147602, w0=73.54365870219151, w1=12.700307015449711\n",
      "[ 2.62165788  0.7476896 ]\n",
      "Gradient Descent(3105/9): loss=15.720808482518047, w0=71.70849818663656, w1=12.176924293934617\n",
      "[-0.81827986 -1.51189731]\n",
      "Gradient Descent(3106/9): loss=17.49130067639298, w0=72.28129408697747, w1=13.23525240830505\n",
      "[ 2.00030402 -1.44571563]\n",
      "Gradient Descent(3107/9): loss=15.928475868400527, w0=70.88108127023033, w1=14.247253346025142\n",
      "[-0.05711502  0.9547893 ]\n",
      "Gradient Descent(3108/9): loss=18.591347592583677, w0=70.92106178479428, w1=13.578900836019017\n",
      "[-2.33179022 -1.29894131]\n",
      "Gradient Descent(3109/9): loss=18.20603984372724, w0=72.55331493574766, w1=14.488159750766782\n",
      "[ 1.68275165 -0.12183596]\n",
      "Gradient Descent(3110/9): loss=16.16862027654841, w0=71.3753887804792, w1=14.573444924361473\n",
      "[-1.1463988   0.12332488]\n",
      "Gradient Descent(3111/9): loss=17.824398109225104, w0=72.17786794027181, w1=14.487117505192233\n",
      "[ 3.87849029  0.86338485]\n",
      "Gradient Descent(3112/9): loss=16.516108691032336, w0=69.46292474026507, w1=13.882748111281577\n",
      "[-0.93690532  0.27770283]\n",
      "Gradient Descent(3113/9): loss=22.805376757124947, w0=70.11875846667651, w1=13.688356131169254\n",
      "[-6.72327501  2.20072195]\n",
      "Gradient Descent(3114/9): loss=20.448485703165243, w0=74.82505097304234, w1=12.14785076731357\n",
      "[-0.79887799  0.3533506 ]\n",
      "Gradient Descent(3115/9): loss=17.44499358256258, w0=75.38426556391697, w1=11.900505347107723\n",
      "[ 2.9014327  -0.58460686]\n",
      "Gradient Descent(3116/9): loss=18.81760348524069, w0=73.35326267283122, w1=12.309730151322897\n",
      "[-0.26343394 -1.07112144]\n",
      "Gradient Descent(3117/9): loss=16.072077798476837, w0=73.53766643127223, w1=13.059515158430571\n",
      "[ 3.04135782  0.14900593]\n",
      "Gradient Descent(3118/9): loss=15.503876417817963, w0=71.40871595853321, w1=12.955211004346385\n",
      "[-0.68444382  0.59887084]\n",
      "Gradient Descent(3119/9): loss=17.300439657562663, w0=71.8878266290413, w1=12.53600141847034\n",
      "[-1.32881534 -1.93302101]\n",
      "Gradient Descent(3120/9): loss=16.819735209254628, w0=72.81799736869364, w1=13.889116122154919\n",
      "[-1.34390997 -3.27454797]\n",
      "Gradient Descent(3121/9): loss=15.582945686705864, w0=73.75873434951906, w1=16.181299704599464\n",
      "[ 1.89947223  0.88906163]\n",
      "Gradient Descent(3122/9): loss=19.143200015644123, w0=72.42910379145097, w1=15.55895656096837\n",
      "[ 0.54550548  1.4030363 ]\n",
      "Gradient Descent(3123/9): loss=17.921471205278742, w0=72.04724995567425, w1=14.576831154133172\n",
      "[-3.79744926  0.87987566]\n",
      "Gradient Descent(3124/9): loss=16.764818206453782, w0=74.70546443570522, w1=13.960918194378133\n",
      "[ 2.52776667 -2.13820412]\n",
      "Gradient Descent(3125/9): loss=16.497893381190767, w0=72.9360277690573, w1=15.457661081403554\n",
      "[ 1.40518847  1.94683686]\n",
      "Gradient Descent(3126/9): loss=17.406072433780356, w0=71.95239583881167, w1=14.094875278508738\n",
      "[-1.96338814 -0.60170572]\n",
      "Gradient Descent(3127/9): loss=16.474946754253533, w0=73.32676753420742, w1=14.516069280658447\n",
      "[-1.3247001   1.33713445]\n",
      "Gradient Descent(3128/9): loss=15.923445039101853, w0=74.25405760481583, w1=13.580075163785148\n",
      "[-0.06619849  2.7651691 ]\n",
      "Gradient Descent(3129/9): loss=15.85185439529136, w0=74.30039654790396, w1=11.644456791509405\n",
      "[-4.23282024 -1.19270216]\n",
      "Gradient Descent(3130/9): loss=17.57646501296176, w0=77.2633707167607, w1=12.47934830402871\n",
      "[ 6.02514436 -2.37213327]\n",
      "Gradient Descent(3131/9): loss=23.76451361522558, w0=73.04576966286807, w1=14.139841590939126\n",
      "[-2.68003456 -0.90585287]\n",
      "Gradient Descent(3132/9): loss=15.634562911831509, w0=74.9217938536718, w1=14.773938598930908\n",
      "[-0.69176679 -0.95953694]\n",
      "Gradient Descent(3133/9): loss=17.54838193310668, w0=75.40603060917755, w1=15.445614458912022\n",
      "[-0.27650677  3.44506173]\n",
      "Gradient Descent(3134/9): loss=19.548774636696187, w0=75.59958534617914, w1=13.034071248849889\n",
      "[ 1.86592952 -1.79504159]\n",
      "Gradient Descent(3135/9): loss=18.143227630324294, w0=74.29343468145242, w1=14.29060035870483\n",
      "[-0.81698379  0.35546488]\n",
      "Gradient Descent(3136/9): loss=16.21417027933139, w0=74.8653233340277, w1=14.041774939846238\n",
      "[-0.6661447   0.09243282]\n",
      "Gradient Descent(3137/9): loss=16.7784960714965, w0=75.33162462310626, w1=13.9770719661504\n",
      "[-0.23450742  2.10519869]\n",
      "Gradient Descent(3138/9): loss=17.585687106265244, w0=75.49577981491377, w1=12.503432885521592\n",
      "[ 0.95107709 -1.97636463]\n",
      "Gradient Descent(3139/9): loss=18.286537662096695, w0=74.8300258533606, w1=13.886888126529556\n",
      "[ 0.44875069  1.91813553]\n",
      "Gradient Descent(3140/9): loss=16.64859141164099, w0=74.51590037298183, w1=12.544193253073102\n",
      "[ 1.72274857 -1.70519386]\n",
      "Gradient Descent(3141/9): loss=16.57010150814091, w0=73.30997637513931, w1=13.737828953212704\n",
      "[ 1.85744162  3.09417114]\n",
      "Gradient Descent(3142/9): loss=15.419328808766108, w0=72.00976724217527, w1=11.57190915363841\n",
      "[ 0.96060797 -0.37080516]\n",
      "Gradient Descent(3143/9): loss=18.03027127272086, w0=71.33734166028378, w1=11.831472766992153\n",
      "[-5.0326231  -4.34345208]\n",
      "Gradient Descent(3144/9): loss=18.65833818740964, w0=74.86017783371702, w1=14.87188921955826\n",
      "[ 0.96687493  2.36312605]\n",
      "Gradient Descent(3145/9): loss=17.581544633605162, w0=74.18336538037039, w1=13.217700982159844\n",
      "[ 0.01587052 -0.55207234]\n",
      "Gradient Descent(3146/9): loss=15.815767631106143, w0=74.17225601480861, w1=13.604151621751711\n",
      "[-2.21452465 -0.66601155]\n",
      "Gradient Descent(3147/9): loss=15.779365743366323, w0=75.72242326985337, w1=14.070359706859563\n",
      "[ 4.07722279 -1.23942125]\n",
      "Gradient Descent(3148/9): loss=18.509129172440687, w0=72.86836731551652, w1=14.937954581824483\n",
      "[-1.49150606  2.36632172]\n",
      "Gradient Descent(3149/9): loss=16.5396713438718, w0=73.91242156019516, w1=13.281529380251806\n",
      "[ 2.72067059 -0.52968083]\n",
      "Gradient Descent(3150/9): loss=15.596796982100635, w0=72.00795214871746, w1=13.652305960178039\n",
      "[-4.69822954  0.21067415]\n",
      "Gradient Descent(3151/9): loss=16.227641363209003, w0=75.29671282624388, w1=13.504834054293283\n",
      "[ 0.06858406  0.75064353]\n",
      "Gradient Descent(3152/9): loss=17.391788959334693, w0=75.2487039857724, w1=12.979383583848948\n",
      "[ 1.03402087  3.13621288]\n",
      "Gradient Descent(3153/9): loss=17.42163865030593, w0=74.52488937644554, w1=10.784034564667115\n",
      "[ 1.55516201 -5.05931313]\n",
      "Gradient Descent(3154/9): loss=19.776867797446297, w0=73.4362759721851, w1=14.32555375685129\n",
      "[ 0.20405624  2.01913945]\n",
      "Gradient Descent(3155/9): loss=15.753743966112987, w0=73.29343660597584, w1=12.912156145171783\n",
      "[ 2.3772754  -2.38933669]\n",
      "Gradient Descent(3156/9): loss=15.546948057689672, w0=71.62934382388177, w1=14.584691825095945\n",
      "[ 1.80390529  3.14063057]\n",
      "Gradient Descent(3157/9): loss=17.381787850818707, w0=70.36661011891144, w1=12.386250429593726\n",
      "[-5.62180836 -1.20219207]\n",
      "Gradient Descent(3158/9): loss=20.26829487819465, w0=74.30187597145436, w1=13.227784879347892\n",
      "[-0.19759155  0.93027205]\n",
      "Gradient Descent(3159/9): loss=15.925607217638445, w0=74.44019005534946, w1=12.576594442210311\n",
      "[ 0.54299522 -0.0671186 ]\n",
      "Gradient Descent(3160/9): loss=16.450664148213953, w0=74.06009340366157, w1=12.623577462439854\n",
      "[ 2.98969197 -1.61560193]\n",
      "Gradient Descent(3161/9): loss=16.04588072272174, w0=71.96730902548605, w1=13.754498815148118\n",
      "[-3.16630633 -0.35536262]\n",
      "Gradient Descent(3162/9): loss=16.303592641057016, w0=74.1837234562673, w1=14.00325264774044\n",
      "[-0.02504945  0.43071875]\n",
      "Gradient Descent(3163/9): loss=15.918808359927791, w0=74.20125807381625, w1=13.70174952186553\n",
      "[ 2.33203611  0.08945967]\n",
      "Gradient Descent(3164/9): loss=15.822167476317723, w0=72.56883279673696, w1=13.639127750189703\n",
      "[-3.34094087 -0.97001237]\n",
      "Gradient Descent(3165/9): loss=15.661471668060429, w0=74.90749140580687, w1=14.318136408143896\n",
      "[ 4.02887724 -0.20218681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3166/9): loss=17.039168358490873, w0=72.08727734104387, w1=14.45966717540687\n",
      "[ 0.55413773  1.26165586]\n",
      "Gradient Descent(3167/9): loss=16.594039184496975, w0=71.69938093036079, w1=13.576508072187574\n",
      "[-2.1176898   1.31981124]\n",
      "Gradient Descent(3168/9): loss=16.661853181259627, w0=73.18176378744329, w1=12.652640206760195\n",
      "[-2.69631162 -2.06939304]\n",
      "Gradient Descent(3169/9): loss=15.734201836741194, w0=75.06918192305329, w1=14.101215335982927\n",
      "[ 1.13343365  2.86878285]\n",
      "Gradient Descent(3170/9): loss=17.154794690263635, w0=74.27577837107214, w1=12.093067342444847\n",
      "[-2.57899105 -1.44536212]\n",
      "Gradient Descent(3171/9): loss=16.82930113980834, w0=76.08107210489415, w1=13.104820827174299\n",
      "[ 3.62653866  1.71104504]\n",
      "Gradient Descent(3172/9): loss=19.340262575372613, w0=73.5424950461239, w1=11.90708930048201\n",
      "[-1.44347469  0.13122782]\n",
      "Gradient Descent(3173/9): loss=16.653353909529137, w0=74.55292732690178, w1=11.81522982575313\n",
      "[ 2.1052707  -2.61289016]\n",
      "Gradient Descent(3174/9): loss=17.56368625098689, w0=73.07923783913773, w1=13.644252935897217\n",
      "[-0.06618332  0.6674601 ]\n",
      "Gradient Descent(3175/9): loss=15.422469301963476, w0=73.12556616092604, w1=13.17703086915489\n",
      "[-1.53229953  0.86547631]\n",
      "Gradient Descent(3176/9): loss=15.445867778606878, w0=74.19817583512601, w1=12.571197451969642\n",
      "[-0.1567986 -1.3406553]\n",
      "Gradient Descent(3177/9): loss=16.2074251032812, w0=74.30793485470438, w1=13.509656163315844\n",
      "[ 2.34377613 -0.11242125]\n",
      "Gradient Descent(3178/9): loss=15.900447214880636, w0=72.6672915624544, w1=13.588351041080639\n",
      "[-0.03205535  1.13990137]\n",
      "Gradient Descent(3179/9): loss=15.588121896144637, w0=72.68973030823547, w1=12.790420078713208\n",
      "[-3.21981446  0.64061368]\n",
      "Gradient Descent(3180/9): loss=15.805973646510134, w0=74.94360042705117, w1=12.341990500031551\n",
      "[ 3.62000905 -0.10654873]\n",
      "Gradient Descent(3181/9): loss=17.393812922337236, w0=72.40959408962345, w1=12.416574614478622\n",
      "[-0.16955424 -1.69698364]\n",
      "Gradient Descent(3182/9): loss=16.34203680992638, w0=72.52828205528182, w1=13.604463164291355\n",
      "[-2.39444336  0.60608858]\n",
      "Gradient Descent(3183/9): loss=15.686771505145977, w0=74.20439240519947, w1=13.180201160178935\n",
      "[ 4.73178126 -3.22015452]\n",
      "Gradient Descent(3184/9): loss=15.845219548153919, w0=70.89214552341286, w1=15.434309323203898\n",
      "[-0.88405717  3.52311339]\n",
      "Gradient Descent(3185/9): loss=20.18037749333885, w0=71.51098554479456, w1=12.968129948485444\n",
      "[-3.71893523 -2.41085166]\n",
      "Gradient Descent(3186/9): loss=17.106177394481687, w0=74.11424020590508, w1=14.655726107652237\n",
      "[-1.19898485  1.11128176]\n",
      "Gradient Descent(3187/9): loss=16.413852925717517, w0=74.95352960376093, w1=13.877828878150499\n",
      "[ 1.02437322 -0.77949007]\n",
      "Gradient Descent(3188/9): loss=16.842284915723916, w0=74.23646835001045, w1=14.423471928307656\n",
      "[ 1.4769554 -0.4222279]\n",
      "Gradient Descent(3189/9): loss=16.275425668418674, w0=73.20259956771265, w1=14.719031456733122\n",
      "[-0.657556    2.48645915]\n",
      "Gradient Descent(3190/9): loss=16.15801358116943, w0=73.66288877098526, w1=12.97851005142985\n",
      "[-0.15427724 -0.75961331]\n",
      "Gradient Descent(3191/9): loss=15.579558021741008, w0=73.77088284151225, w1=13.510239370711385\n",
      "[ 3.59908374 -1.04751149]\n",
      "Gradient Descent(3192/9): loss=15.50009963689564, w0=71.25152422473482, w1=14.24349741287528\n",
      "[-3.34484542  1.38439761]\n",
      "Gradient Descent(3193/9): loss=17.763265955555443, w0=73.59291601846178, w1=13.274419083607793\n",
      "[ 1.71185357  1.61652111]\n",
      "Gradient Descent(3194/9): loss=15.451659259798596, w0=72.3946185208015, w1=12.14285430869576\n",
      "[-2.58545718  0.1507837 ]\n",
      "Gradient Descent(3195/9): loss=16.68385606949007, w0=74.20443854337827, w1=12.03730571635037\n",
      "[ 2.69005045 -2.60684693]\n",
      "Gradient Descent(3196/9): loss=16.840676625782354, w0=72.32140323039295, w1=13.86209856850043\n",
      "[-1.2717949  -1.45034704]\n",
      "Gradient Descent(3197/9): loss=15.931893827046638, w0=73.21165966042177, w1=14.87734149870346\n",
      "[-2.11913961  6.33321614]\n",
      "Gradient Descent(3198/9): loss=16.36595491512863, w0=74.69505738953968, w1=10.444090203090184\n",
      "[ 2.21012897 -1.30128744]\n",
      "Gradient Descent(3199/9): loss=20.974979223189333, w0=73.14796710709764, w1=11.354991408947782\n",
      "[ 0.08695201 -3.71487255]\n",
      "Gradient Descent(3200/9): loss=17.65375900376866, w0=73.08710070253416, w1=13.95540219129387\n",
      "[-2.27980582  2.41763781]\n",
      "Gradient Descent(3201/9): loss=15.5204157659342, w0=74.68296477756796, w1=12.263055725708028\n",
      "[ 3.11355239 -2.92376753]\n",
      "Gradient Descent(3202/9): loss=17.09073455898132, w0=72.50347810145897, w1=14.309692995321077\n",
      "[-0.62095152  0.70911976]\n",
      "Gradient Descent(3203/9): loss=16.042722514128343, w0=72.93814416476665, w1=13.813309160797646\n",
      "[ 0.49112137  2.20425164]\n",
      "Gradient Descent(3204/9): loss=15.504820191335149, w0=72.59435920697325, w1=12.270333009899778\n",
      "[-1.94995943 -1.96624537]\n",
      "Gradient Descent(3205/9): loss=16.36188121791043, w0=73.95933080818031, w1=13.646704768736045\n",
      "[ 1.83545463 -0.69296621]\n",
      "Gradient Descent(3206/9): loss=15.621215528195691, w0=72.67451256488329, w1=14.131781119201424\n",
      "[-0.2238054  -1.56467318]\n",
      "Gradient Descent(3207/9): loss=15.790318678754408, w0=72.83117634591216, w1=15.227052342143068\n",
      "[-1.64747163  2.00746532]\n",
      "Gradient Descent(3208/9): loss=17.01955301555867, w0=73.98440648831073, w1=13.821826615002191\n",
      "[-1.5274434   0.68687568]\n",
      "Gradient Descent(3209/9): loss=15.682793337757694, w0=75.05361686897362, w1=13.341013640666402\n",
      "[ 1.04685522  1.60913957]\n",
      "Gradient Descent(3210/9): loss=16.94376955884423, w0=74.32081821584023, w1=12.214615938186165\n",
      "[ 3.20623544 -0.93787917]\n",
      "Gradient Descent(3211/9): loss=16.713380358832552, w0=72.07645340557102, w1=12.87113135735203\n",
      "[-0.12562227 -1.39240463]\n",
      "Gradient Descent(3212/9): loss=16.312188224631765, w0=72.16438899779813, w1=13.845814597338187\n",
      "[-2.83117013 -1.5168623 ]\n",
      "Gradient Descent(3213/9): loss=16.090825669377224, w0=74.14620809107578, w1=14.907618208124491\n",
      "[ 1.4469587   2.77913313]\n",
      "Gradient Descent(3214/9): loss=16.768541106032554, w0=73.1333370040997, w1=12.962225020288248\n",
      "[-1.29023603 -2.82613314]\n",
      "Gradient Descent(3215/9): loss=15.532678251808472, w0=74.03650222296896, w1=14.940518216951762\n",
      "[ 1.50458779  0.62726524]\n",
      "Gradient Descent(3216/9): loss=16.728577327346294, w0=72.98329076953706, w1=14.501432549368742\n",
      "[ 2.30038887  0.69986238]\n",
      "Gradient Descent(3217/9): loss=15.956089746216824, w0=71.37301855767478, w1=14.011528881752772\n",
      "[-1.98032626  2.48086657]\n",
      "Gradient Descent(3218/9): loss=17.3722372567659, w0=72.75924694065662, w1=12.274922280103983\n",
      "[-1.84972067 -0.46148941]\n",
      "Gradient Descent(3219/9): loss=16.254586238150903, w0=74.05405141007151, w1=12.597964870249164\n",
      "[ 0.70731361  1.60106567]\n",
      "Gradient Descent(3220/9): loss=16.063525611219376, w0=73.55893188571581, w1=11.477218900995622\n",
      "[-4.22761553 -1.68555442]\n",
      "Gradient Descent(3221/9): loss=17.4259931648778, w0=76.51826275572085, w1=12.657106992324252\n",
      "[ 1.67649797 -0.42643163]\n",
      "Gradient Descent(3222/9): loss=20.922414373693698, w0=75.34471417461614, w1=12.95560913395763\n",
      "[ 2.40617663 -0.68709732]\n",
      "Gradient Descent(3223/9): loss=17.626104271321516, w0=73.66039053023285, w1=13.436577258838224\n",
      "[-0.83401081 -1.11206527]\n",
      "Gradient Descent(3224/9): loss=15.45396778159421, w0=74.24419809703242, w1=14.215022947963538\n",
      "[ 2.51919699  2.41248869]\n",
      "Gradient Descent(3225/9): loss=16.10774097236988, w0=72.48076020282296, w1=12.52628086253123\n",
      "[ 1.80280908 -1.96605147]\n",
      "Gradient Descent(3226/9): loss=16.171019806415046, w0=71.21879384856175, w1=13.902516888311862\n",
      "[-1.36706759 -0.37935367]\n",
      "Gradient Descent(3227/9): loss=17.62834809851851, w0=72.17574116420569, w1=14.168064457426427\n",
      "[-0.51821736  1.37748799]\n",
      "Gradient Descent(3228/9): loss=16.24796631534903, w0=72.53849331961584, w1=13.203822863065614\n",
      "[-0.02473906  1.1538071 ]\n",
      "Gradient Descent(3229/9): loss=15.709281643941244, w0=72.55581065964914, w1=12.396157891045387\n",
      "[ 2.84767349  0.55098639]\n",
      "Gradient Descent(3230/9): loss=16.245337270611014, w0=70.56243921907327, w1=12.010467421280845\n",
      "[-3.44297022 -2.76679456]\n",
      "Gradient Descent(3231/9): loss=20.195727420982504, w0=72.97251836961141, w1=13.947223614038007\n",
      "[-0.41656939 -1.26928474]\n",
      "Gradient Descent(3232/9): loss=15.546821367587377, w0=73.26411693994487, w1=14.835722931956267\n",
      "[ 2.04442393 -0.34977291]\n",
      "Gradient Descent(3233/9): loss=16.305714273637232, w0=71.8330201877427, w1=15.080563967057305\n",
      "[ 0.05764638  0.94212357]\n",
      "Gradient Descent(3234/9): loss=17.73436773829585, w0=71.79266772164709, w1=14.421077471069738\n",
      "[-3.21399096 -0.43918666]\n",
      "Gradient Descent(3235/9): loss=16.955854141703878, w0=74.04246139605185, w1=14.728508130008308\n",
      "[-0.05117569  1.2385164 ]\n",
      "Gradient Descent(3236/9): loss=16.445788824923735, w0=74.07828438069502, w1=13.861546651263694\n",
      "[ 1.05135821  0.36902144]\n",
      "Gradient Descent(3237/9): loss=15.766398723661986, w0=73.34233363576203, w1=13.60323164244975\n",
      "[-2.23165144 -1.95887776]\n",
      "Gradient Descent(3238/9): loss=15.39468820927192, w0=74.9044896411893, w1=14.974446073866952\n",
      "[-1.15763617  1.51393146]\n",
      "Gradient Descent(3239/9): loss=17.79996625445847, w0=75.71483496134711, w1=13.914694049693935\n",
      "[ 2.63042288 -1.10237931]\n",
      "Gradient Descent(3240/9): loss=18.410902149507766, w0=73.87353894437729, w1=14.686359564901885\n",
      "[ 0.20673129  3.2736149 ]\n",
      "Gradient Descent(3241/9): loss=16.28186441677727, w0=73.72882704043106, w1=12.394829134139892\n",
      "[-2.0962084  -0.22416814]\n",
      "Gradient Descent(3242/9): loss=16.068944953240692, w0=75.19617291945376, w1=12.551746830068192\n",
      "[ 5.56878468  0.22663362]\n",
      "Gradient Descent(3243/9): loss=17.625727227064203, w0=71.29802364194849, w1=12.393103297312594\n",
      "[-4.21488506 -1.05070345]\n",
      "Gradient Descent(3244/9): loss=17.96805270990848, w0=74.24844318238273, w1=13.12859571202347\n",
      "[ 0.29570934 -1.88021551]\n",
      "Gradient Descent(3245/9): loss=15.903084687201659, w0=74.04144664443879, w1=14.44474656936178\n",
      "[ 0.41078872  0.7650818 ]\n",
      "Gradient Descent(3246/9): loss=16.130929854529654, w0=73.75389454305918, w1=13.909189306893454\n",
      "[-0.77730144  0.51892297]\n",
      "Gradient Descent(3247/9): loss=15.58390042979563, w0=74.29800555200711, w1=13.545943230748518\n",
      "[-0.40971799  0.89036671]\n",
      "Gradient Descent(3248/9): loss=15.892173015574691, w0=74.58480814433015, w1=12.922686530991996\n",
      "[ 0.36586153 -2.8434393 ]\n",
      "Gradient Descent(3249/9): loss=16.374220313785493, w0=74.32870507355928, w1=14.913094039197777\n",
      "[ 0.75125506  1.21224318]\n",
      "Gradient Descent(3250/9): loss=16.948567282955374, w0=73.802826534474, w1=14.064523816410938\n",
      "[-1.18183678  1.03960531]\n",
      "Gradient Descent(3251/9): loss=15.686381956282444, w0=74.63011228100096, w1=13.336800097835242\n",
      "[-0.2424048 -0.8180166]\n",
      "Gradient Descent(3252/9): loss=16.288802067592563, w0=74.79979564067071, w1=13.909411719444956\n",
      "[ 2.52862996  3.47295817]\n",
      "Gradient Descent(3253/9): loss=16.612036314023637, w0=73.02975467161649, w1=11.478340999377584\n",
      "[-1.34987767 -3.49162824]\n",
      "Gradient Descent(3254/9): loss=17.42352386971891, w0=73.97466903857358, w1=13.922480764940184\n",
      "[ 3.7047505 -1.7265022]\n",
      "Gradient Descent(3255/9): loss=15.715618029663505, w0=71.38134369138827, w1=15.131032306614724\n",
      "[ 0.42722607  3.13540993]\n",
      "Gradient Descent(3256/9): loss=18.578294425354724, w0=71.08228544380455, w1=12.936245354588653\n",
      "[-5.20652757 -0.93712501]\n",
      "Gradient Descent(3257/9): loss=17.97923423557481, w0=74.72685474221167, w1=13.592232860178758\n",
      "[ 1.10123478  1.14999746]\n",
      "Gradient Descent(3258/9): loss=16.41886641070637, w0=73.95599039858611, w1=12.787234637168119\n",
      "[ 3.46600172 -3.07413459]\n",
      "Gradient Descent(3259/9): loss=15.844817899876272, w0=71.52978919316253, w1=14.939128851224677\n",
      "[-0.53348522  1.83487127]\n",
      "Gradient Descent(3260/9): loss=18.006918290612386, w0=71.90322884593765, w1=13.654718959992145\n",
      "[-3.03010451  0.29041   ]\n",
      "Gradient Descent(3261/9): loss=16.36821523803185, w0=74.02430200410544, w1=13.451431963273295\n",
      "[ 1.23468999  0.71996643]\n",
      "Gradient Descent(3262/9): loss=15.653015235030576, w0=73.16001900766962, w1=12.947455464601575\n",
      "[-3.16743915 -3.05533451]\n",
      "Gradient Descent(3263/9): loss=15.536501616051831, w0=75.3772264134122, w1=15.086189621185216\n",
      "[-2.3076485   1.33077963]\n",
      "Gradient Descent(3264/9): loss=18.846350978799403, w0=76.99258036115668, w1=14.154643882740872\n",
      "[ 3.40418832 -2.21310717]\n",
      "Gradient Descent(3265/9): loss=22.453690926902333, w0=74.60964853419824, w1=15.70381890111996\n",
      "[ 1.14187736  2.99376709]\n",
      "Gradient Descent(3266/9): loss=18.72478080879887, w0=73.81033437902714, w1=13.608181939821208\n",
      "[ 0.64095456 -0.92553711]\n",
      "Gradient Descent(3267/9): loss=15.527480947184397, w0=73.36166618897995, w1=14.256057917650411\n",
      "[-1.96651061 -0.66284885]\n",
      "Gradient Descent(3268/9): loss=15.689538660481414, w0=74.7382236154916, w1=14.720052110005888\n",
      "[ 3.59019485  2.98181117]\n",
      "Gradient Descent(3269/9): loss=17.19811269875514, w0=72.22508722281493, w1=12.63278429197609\n",
      "[ 0.12342624 -1.7003435 ]\n",
      "Gradient Descent(3270/9): loss=16.31573540125331, w0=72.1386888574992, w1=13.823024742746394\n",
      "[-2.3726023   0.76532717]\n",
      "Gradient Descent(3271/9): loss=16.112101348356354, w0=73.79951046938668, w1=13.287295723482664\n",
      "[ 1.00383205  0.11436453]\n",
      "Gradient Descent(3272/9): loss=15.532209813386888, w0=73.09682803543028, w1=13.207240550730061\n",
      "[-2.18909862 -1.17463884]\n",
      "Gradient Descent(3273/9): loss=15.442431348535045, w0=74.62919707018949, w1=14.029487736729017\n",
      "[ 1.50829944  0.4698102 ]\n",
      "Gradient Descent(3274/9): loss=16.428494063754805, w0=73.57338746423436, w1=13.700620597526248\n",
      "[ 2.35350077 -0.1434424 ]\n",
      "Gradient Descent(3275/9): loss=15.449338549228717, w0=71.92593692243003, w1=13.801030275956416\n",
      "[-1.73367639 -1.52615185]\n",
      "Gradient Descent(3276/9): loss=16.373202035398297, w0=73.13951039783328, w1=14.869336572579664\n",
      "[-0.28403687  0.3570677 ]\n",
      "Gradient Descent(3277/9): loss=16.363336962483547, w0=73.33833620458147, w1=14.619389185405279\n",
      "[-0.45674506 -0.49185561]\n",
      "Gradient Descent(3278/9): loss=16.036305727239853, w0=73.65805774839754, w1=14.963688113410477\n",
      "[-3.50629796  3.37132275]\n",
      "Gradient Descent(3279/9): loss=16.553277196766512, w0=76.11246632045761, w1=12.603762185619852\n",
      "[ 6.78312755  1.66904924]\n",
      "Gradient Descent(3280/9): loss=19.741628325772744, w0=71.36427703304058, w1=11.43542772074762\n",
      "[-3.96534236 -2.9170371 ]\n",
      "Gradient Descent(3281/9): loss=19.337202718588156, w0=74.14001668689147, w1=13.477353691894645\n",
      "[ 2.15993655 -1.33142799]\n",
      "Gradient Descent(3282/9): loss=15.74382875847569, w0=72.6280610996608, w1=14.409353284620074\n",
      "[ 3.27227101 -0.23462991]\n",
      "Gradient Descent(3283/9): loss=16.03968929418278, w0=70.33747139041255, w1=14.57359422098045\n",
      "[-4.5112155   3.49484107]\n",
      "Gradient Descent(3284/9): loss=20.354476659380175, w0=73.49532223948881, w1=12.127205473897227\n",
      "[ 1.37603451 -0.07500453]\n",
      "Gradient Descent(3285/9): loss=16.32080643653941, w0=72.53209808051933, w1=12.179708641480792\n",
      "[-2.63801973  0.61535538]\n",
      "Gradient Descent(3286/9): loss=16.521080644147556, w0=74.37871188846948, w1=11.74895987827006\n",
      "[ 2.91985749 -1.63606902]\n",
      "Gradient Descent(3287/9): loss=17.472024623903188, w0=72.33481164396646, w1=12.89420819386977\n",
      "[ 0.29018688 -0.95036489]\n",
      "Gradient Descent(3288/9): loss=16.01724181655823, w0=72.1316808267164, w1=13.559463616967781\n",
      "[ 0.21800842  0.30746087]\n",
      "Gradient Descent(3289/9): loss=16.064470269227453, w0=71.97907493369263, w1=13.344241009203042\n",
      "[ 1.23994477 -0.2485901 ]\n",
      "Gradient Descent(3290/9): loss=16.259475529088196, w0=71.11111359215826, w1=13.518254076305801\n",
      "[-0.10174899  0.08129547]\n",
      "Gradient Descent(3291/9): loss=17.768956875154615, w0=71.18233788546829, w1=13.461347250639712\n",
      "[-2.10773425 -0.68025993]\n",
      "Gradient Descent(3292/9): loss=17.61545024964412, w0=72.65775185754053, w1=13.937529198549122\n",
      "[-1.40808426 -0.03865371]\n",
      "Gradient Descent(3293/9): loss=15.69304218974542, w0=73.64341083669862, w1=13.964586798910272\n",
      "[ 0.28393909 -0.0892636 ]\n",
      "Gradient Descent(3294/9): loss=15.56451066597614, w0=73.44465347028364, w1=14.027071319994056\n",
      "[-0.21880448  0.3891994 ]\n",
      "Gradient Descent(3295/9): loss=15.547048731075979, w0=73.59781660565541, w1=13.754631742827696\n",
      "[-1.57817927 -0.79589394]\n",
      "Gradient Descent(3296/9): loss=15.469854146774113, w0=74.70254209163252, w1=14.311757500836794\n",
      "[-1.83321615  3.8034369 ]\n",
      "Gradient Descent(3297/9): loss=16.724142642940183, w0=75.98579339873312, w1=11.649351670384522\n",
      "[ 1.62394302 -4.12656423]\n",
      "Gradient Descent(3298/9): loss=20.68408394112305, w0=74.84903328278301, w1=14.53794663012156\n",
      "[ 2.02367774 -1.69676851]\n",
      "Gradient Descent(3299/9): loss=17.155003222348984, w0=73.43245886296195, w1=15.725684586091287\n",
      "[-1.2217455   1.26671602]\n",
      "Gradient Descent(3300/9): loss=17.91767955150083, w0=74.28768071348858, w1=14.838983370831771\n",
      "[-0.47643499  1.91303287]\n",
      "Gradient Descent(3301/9): loss=16.803474795567965, w0=74.62118520367876, w1=13.499860360582952\n",
      "[ 2.47723207 -3.27260521]\n",
      "Gradient Descent(3302/9): loss=16.266904641407937, w0=72.8871227520277, w1=15.790684009571155\n",
      "[ 0.31973367 -0.9810052 ]\n",
      "Gradient Descent(3303/9): loss=18.13892549302446, w0=72.66330918472174, w1=16.47738765227527\n",
      "[ 0.0349943   2.87938507]\n",
      "Gradient Descent(3304/9): loss=20.077752485719543, w0=72.63881317790546, w1=14.46181810107075\n",
      "[ 1.32690556 -0.50918632]\n",
      "Gradient Descent(3305/9): loss=16.082737424276466, w0=71.70997928367493, w1=14.818248527170903\n",
      "[-1.92955108  0.75146007]\n",
      "Gradient Descent(3306/9): loss=17.53616457150026, w0=73.06066503907672, w1=14.292226480431225\n",
      "[-1.01133523 -1.71345335]\n",
      "Gradient Descent(3307/9): loss=15.743181811250437, w0=73.76859969785275, w1=15.491643828131739\n",
      "[-1.17318289  1.10662793]\n",
      "Gradient Descent(3308/9): loss=17.52248129160605, w0=74.58982771835416, w1=14.71700427781486\n",
      "[ 0.51944943 -0.32028059]\n",
      "Gradient Descent(3309/9): loss=16.991019233694423, w0=74.22621311473732, w1=14.941200687842091\n",
      "[ 1.72115719 -0.47946388]\n",
      "Gradient Descent(3310/9): loss=16.888445184789546, w0=73.02140308422713, w1=15.276825401959854\n",
      "[ 1.67834865  6.24751826]\n",
      "Gradient Descent(3311/9): loss=17.037828657157423, w0=71.84655902883021, w1=10.903562622734938\n",
      "[-2.1072115  -4.00880555]\n",
      "Gradient Descent(3312/9): loss=19.75159158462153, w0=73.32160708046392, w1=13.709726508303504\n",
      "[-0.55642992  2.41667159]\n",
      "Gradient Descent(3313/9): loss=15.41272433757262, w0=73.7111080229604, w1=12.018056396049902\n",
      "[ 1.79180207 -0.62555933]\n",
      "Gradient Descent(3314/9): loss=16.54112914491154, w0=72.4568465754368, w1=12.455947927538455\n",
      "[-1.13656062 -2.42303079]\n",
      "Gradient Descent(3315/9): loss=16.26028238715321, w0=73.25243901186198, w1=14.152069481909129\n",
      "[ 1.02576611 -0.93310939]\n",
      "Gradient Descent(3316/9): loss=15.612780287340705, w0=72.53440273163402, w1=14.805246051980529\n",
      "[ 0.11881359  3.32731206]\n",
      "Gradient Descent(3317/9): loss=16.552842314825188, w0=72.4512332176856, w1=12.47612760732586\n",
      "[ 1.35537777 -2.06504839]\n",
      "Gradient Descent(3318/9): loss=16.244541315680557, w0=71.5024687755557, w1=13.921661481888746\n",
      "[-0.75736956  0.60120894]\n",
      "Gradient Descent(3319/9): loss=17.088199680314467, w0=72.03262746959865, w1=13.50081522370042\n",
      "[-4.99980999 -0.30347988]\n",
      "Gradient Descent(3320/9): loss=16.18154248154054, w0=75.53249446454384, w1=13.713251136643843\n",
      "[ 2.05435694  0.89761384]\n",
      "Gradient Descent(3321/9): loss=17.918761366209036, w0=74.09444460461187, w1=13.084921446853421\n",
      "[ 2.47099068 -0.92722178]\n",
      "Gradient Descent(3322/9): loss=15.784236049547983, w0=72.36475112816588, w1=13.733976696300491\n",
      "[-4.14319939  0.5310025 ]\n",
      "Gradient Descent(3323/9): loss=15.8498922826081, w0=75.2649907023224, w1=13.362274944194667\n",
      "[ 3.3579124  -0.66943179]\n",
      "Gradient Descent(3324/9): loss=17.33533956143941, w0=72.91445202470085, w1=13.830877194936448\n",
      "[ 4.55605731 -2.39358828]\n",
      "Gradient Descent(3325/9): loss=15.519544945019483, w0=69.72521190678751, w1=15.506388989468128\n",
      "[-1.83970739  1.72353763]\n",
      "Gradient Descent(3326/9): loss=23.807442669278174, w0=71.01300708309735, w1=14.299912646827307\n",
      "[-3.96221125  1.26230749]\n",
      "Gradient Descent(3327/9): loss=18.323538496455434, w0=73.78655495686547, w1=13.416297401580898\n",
      "[ 0.90281266  1.07329522]\n",
      "Gradient Descent(3328/9): loss=15.509242216118402, w0=73.15458609532138, w1=12.664990747248194\n",
      "[-2.07536739 -2.30436185]\n",
      "Gradient Descent(3329/9): loss=15.727480830526684, w0=74.60734327020651, w1=14.278044044985204\n",
      "[ 2.57427544 -1.26918248]\n",
      "Gradient Descent(3330/9): loss=16.567092262339372, w0=72.80535046296097, w1=15.166471779143222\n",
      "[ 0.55594747  2.31007401]\n",
      "Gradient Descent(3331/9): loss=16.927817485805985, w0=72.41618723505505, w1=13.54941997135501\n",
      "[-0.34470668 -2.37610304]\n",
      "Gradient Descent(3332/9): loss=15.773526599786788, w0=72.65748191018189, w1=15.21269209585801\n",
      "[ 0.44977508  1.69949343]\n",
      "Gradient Descent(3333/9): loss=17.090025116625824, w0=72.34263935218625, w1=14.023046696171695\n",
      "[-1.79137055  2.76375321]\n",
      "Gradient Descent(3334/9): loss=15.985963268535244, w0=73.59659873637918, w1=12.088419452308614\n",
      "[ 1.34258362 -3.96215754]\n",
      "Gradient Descent(3335/9): loss=16.399542553392696, w0=72.65679020021362, w1=14.861929732450697\n",
      "[ 2.85810067  3.7285005 ]\n",
      "Gradient Descent(3336/9): loss=16.544118664021294, w0=70.65611972787167, w1=12.251979383815279\n",
      "[ 0.18336723 -3.27319747]\n",
      "Gradient Descent(3337/9): loss=19.618552510277393, w0=70.52776266527654, w1=14.54321761339028\n",
      "[-3.99373356  3.37516422]\n",
      "Gradient Descent(3338/9): loss=19.777228239434677, w0=73.32337615554353, w1=12.18060266056365\n",
      "[-0.3824431  -3.06183543]\n",
      "Gradient Descent(3339/9): loss=16.23016474541059, w0=73.59108632560583, w1=14.323887459520785\n",
      "[ 0.52698821  6.13679799]\n",
      "Gradient Descent(3340/9): loss=15.786356922431773, w0=73.22219457920662, w1=10.028128865829132\n",
      "[-1.82393434 -3.18579486]\n",
      "Gradient Descent(3341/9): loss=21.34517484787459, w0=74.49894861977413, w1=12.258185267653014\n",
      "[ 2.08780213 -2.50581272]\n",
      "Gradient Descent(3342/9): loss=16.857996753744718, w0=73.03748713221496, w1=14.012254171813407\n",
      "[-0.02887452 -1.23217951]\n",
      "Gradient Descent(3343/9): loss=15.56056764080716, w0=73.05769929702726, w1=14.874779831235251\n",
      "[-1.88359446  1.15915096]\n",
      "Gradient Descent(3344/9): loss=16.38689497206115, w0=74.37621542036939, w1=14.063374159144795\n",
      "[ 3.43023334  2.85791877]\n",
      "Gradient Descent(3345/9): loss=16.141897894560632, w0=71.9750520809572, w1=12.062831018030924\n",
      "[-2.64157227 -1.86456365]\n",
      "Gradient Descent(3346/9): loss=17.259373278144484, w0=73.82415266890544, w1=13.368025573590796\n",
      "[ 3.52853835  1.88815043]\n",
      "Gradient Descent(3347/9): loss=15.532697126341617, w0=71.35417582320987, w1=12.046320274843671\n",
      "[-4.3755789   2.58006671]\n",
      "Gradient Descent(3348/9): loss=18.294502030482004, w0=74.41708104978099, w1=10.240273578044347\n",
      "[ 4.00716203 -3.66815583]\n",
      "Gradient Descent(3349/9): loss=21.26361304595901, w0=71.61206763084583, w1=12.807982656036428\n",
      "[-1.22389021 -1.32238763]\n",
      "Gradient Descent(3350/9): loss=17.025815379857377, w0=72.46879077701367, w1=13.733653993846977\n",
      "[-1.24530418  0.15937277]\n",
      "Gradient Descent(3351/9): loss=15.758551795797516, w0=73.3405037038572, w1=13.622093055867396\n",
      "[ 1.40976114 -1.02762423]\n",
      "Gradient Descent(3352/9): loss=15.397108916899306, w0=72.3536709082471, w1=14.34143001940814\n",
      "[ 2.76389585  2.0424039 ]\n",
      "Gradient Descent(3353/9): loss=16.199202526228607, w0=70.41894381245872, w1=12.91174728801195\n",
      "[-1.81754903  1.11736809]\n",
      "Gradient Descent(3354/9): loss=19.6799298683912, w0=71.69122813572754, w1=12.129589625847872\n",
      "[-3.47093085 -1.0800392 ]\n",
      "Gradient Descent(3355/9): loss=17.581617483373297, w0=74.1208797308664, w1=12.885617065011074\n",
      "[ 0.05339491 -0.85229682]\n",
      "Gradient Descent(3356/9): loss=15.904292065722988, w0=74.0835032948549, w1=13.48222483632441\n",
      "[ 2.43072202 -0.37182735]\n",
      "Gradient Descent(3357/9): loss=15.697610333839796, w0=72.38199787972584, w1=13.742503984033563\n",
      "[-5.17441592  1.01376486]\n",
      "Gradient Descent(3358/9): loss=15.836220370442676, w0=76.00408902713978, w1=13.032868584512572\n",
      "[ 2.2076017   2.14141297]\n",
      "Gradient Descent(3359/9): loss=19.158225233976157, w0=74.4587678342722, w1=11.533879505483952\n",
      "[ 0.3095051   0.62553063]\n",
      "Gradient Descent(3360/9): loss=17.95745366996102, w0=74.24211426613618, w1=11.096008067464435\n",
      "[ 1.12141383  0.02948502]\n",
      "Gradient Descent(3361/9): loss=18.676445409491475, w0=73.45712458516414, w1=11.075368551872494\n",
      "[ 3.37398094 -6.71709754]\n",
      "Gradient Descent(3362/9): loss=18.289640164527956, w0=71.09533792685652, w1=15.777336832840124\n",
      "[ 3.16888091  2.99917714]\n",
      "Gradient Descent(3363/9): loss=20.442312773598196, w0=68.87712128882347, w1=13.677912834206516\n",
      "[-3.6905368  -0.02704688]\n",
      "Gradient Descent(3364/9): loss=25.15959383837732, w0=71.46049704778478, w1=13.696845648361226\n",
      "[-2.59151191  0.91566141]\n",
      "Gradient Descent(3365/9): loss=17.090184816566452, w0=73.2745553869953, w1=13.055882663496206\n",
      "[ 1.25433148 -1.27182019]\n",
      "Gradient Descent(3366/9): loss=15.475891239321646, w0=72.39652335251257, w1=13.946156793178272\n",
      "[-2.00244445 -1.51500464]\n",
      "Gradient Descent(3367/9): loss=15.897335206618003, w0=73.7982344668067, w1=15.006660039943903\n",
      "[-2.07803193  1.97792911]\n",
      "Gradient Descent(3368/9): loss=16.67883789399474, w0=75.25285681766196, w1=13.622109661432912\n",
      "[ 3.60401745  2.38276804]\n",
      "Gradient Descent(3369/9): loss=17.314739159679082, w0=72.73004460502253, w1=11.954172035281163\n",
      "[ 0.00856507 -1.00546444]\n",
      "Gradient Descent(3370/9): loss=16.708503483870206, w0=72.72404905949291, w1=12.65799714089572\n",
      "[-1.78407783 -1.3737457 ]\n",
      "Gradient Descent(3371/9): loss=15.885873466463629, w0=73.97290354147731, w1=13.619619134221868\n",
      "[ 0.59015203  0.95191659]\n",
      "Gradient Descent(3372/9): loss=15.626182776478576, w0=73.55979712384718, w1=12.953277518502107\n",
      "[-2.22979147  2.37626218]\n",
      "Gradient Descent(3373/9): loss=15.559799519658366, w0=75.12065115481897, w1=11.289893990694372\n",
      "[ 1.31765258 -3.35921835]\n",
      "Gradient Descent(3374/9): loss=19.45200997700319, w0=74.19829434690867, w1=13.641346832467153\n",
      "[ 4.56224054  3.44840218]\n",
      "Gradient Descent(3375/9): loss=15.807895377076127, w0=71.00472597075591, w1=11.227465307186506\n",
      "[-2.19378917 -3.10906954]\n",
      "Gradient Descent(3376/9): loss=20.542405666149435, w0=72.54037839002002, w1=13.40381398170446\n",
      "[-0.18496312 -1.8447105 ]\n",
      "Gradient Descent(3377/9): loss=15.672682144092075, w0=72.66985257309103, w1=14.695111330975445\n",
      "[ 3.08246415  1.53695118]\n",
      "Gradient Descent(3378/9): loss=16.3192164331269, w0=70.51212766499029, w1=13.619245503932973\n",
      "[-4.08848942  0.78213901]\n",
      "Gradient Descent(3379/9): loss=19.26481247449612, w0=73.3740702573146, w1=13.07174819971255\n",
      "[ 2.30314344 -0.00540539]\n",
      "Gradient Descent(3380/9): loss=15.472317148868326, w0=71.76186984615343, w1=13.07553197274631\n",
      "[ 1.20251503  1.02216592]\n",
      "Gradient Descent(3381/9): loss=16.641160696136993, w0=70.92010932450923, w1=12.360015832187129\n",
      "[-0.34646275  0.46001786]\n",
      "Gradient Descent(3382/9): loss=18.830241424150135, w0=71.16263325022264, w1=12.038003326767187\n",
      "[-2.39645815  0.44201044]\n",
      "Gradient Descent(3383/9): loss=18.69634631714487, w0=72.8401539552407, w1=11.728596016733833\n",
      "[-3.37146433 -3.56806651]\n",
      "Gradient Descent(3384/9): loss=17.022044944148497, w0=75.20017898453479, w1=14.226242573989161\n",
      "[ 0.57039961  1.77366536]\n",
      "Gradient Descent(3385/9): loss=17.481449334577956, w0=74.80089925984602, w1=12.984676822104136\n",
      "[ 2.70536345  0.41448816]\n",
      "Gradient Descent(3386/9): loss=16.643908225515606, w0=72.90714484227462, w1=12.694535112518583\n",
      "[-0.39327015  0.09594694]\n",
      "Gradient Descent(3387/9): loss=15.768937868373646, w0=73.18243394864255, w1=12.62737225685246\n",
      "[ 0.83583307 -2.50619991]\n",
      "Gradient Descent(3388/9): loss=15.7553445514948, w0=72.59735080233531, w1=14.381712193874986\n",
      "[ 2.10552879 -0.18216964]\n",
      "Gradient Descent(3389/9): loss=16.035295369518966, w0=71.12348065217874, w1=14.509230943792087\n",
      "[-1.94698989  3.47462185]\n",
      "Gradient Descent(3390/9): loss=18.271249875548687, w0=72.48637357584919, w1=12.076995647508902\n",
      "[ 0.44541586 -1.11192094]\n",
      "Gradient Descent(3391/9): loss=16.695762292142984, w0=72.17458247546185, w1=12.855340308979631\n",
      "[-2.28133317 -3.64461238]\n",
      "Gradient Descent(3392/9): loss=16.207268632651232, w0=73.7715156921702, w1=15.406568976653244\n",
      "[-2.59442821 -1.35304907]\n",
      "Gradient Descent(3393/9): loss=17.356323801301404, w0=75.58761543878805, w1=16.353703323017488\n",
      "[-0.87814077  5.7452503 ]\n",
      "Gradient Descent(3394/9): loss=22.146314471805656, w0=76.20231397868127, w1=12.3320281130828\n",
      "[ 2.24177588  0.16518532]\n",
      "Gradient Descent(3395/9): loss=20.27384946491018, w0=74.63307086061864, w1=12.216398390308079\n",
      "[ 2.54397666  1.30633075]\n",
      "Gradient Descent(3396/9): loss=17.080528889202444, w0=72.85228719519961, w1=11.301966866401852\n",
      "[-2.11052857 -2.44515587]\n",
      "Gradient Descent(3397/9): loss=17.85469640091525, w0=74.32965719173092, w1=13.013575975746566\n",
      "[-0.21177694  1.28759126]\n",
      "Gradient Descent(3398/9): loss=16.03090315966148, w0=74.4779010499254, w1=12.112262095235558\n",
      "[ 0.16002419 -0.64755417]\n",
      "Gradient Descent(3399/9): loss=17.021751277514007, w0=74.36588411713807, w1=12.565550011782367\n",
      "[-0.09171018  0.7375513 ]\n",
      "Gradient Descent(3400/9): loss=16.378285724863836, w0=74.43008124236903, w1=12.049264103524688\n",
      "[-2.33546872 -4.41757143]\n",
      "Gradient Descent(3401/9): loss=17.05440799294244, w0=76.06490934680568, w1=15.141564104906644\n",
      "[ 6.7245306   6.10518872]\n",
      "Gradient Descent(3402/9): loss=20.605948787478475, w0=71.35773792886192, w1=10.867931999960199\n",
      "[-3.08593906 -4.66469556]\n",
      "Gradient Descent(3403/9): loss=20.67099077196958, w0=73.5178952716963, w1=14.133218893735348\n",
      "[ 0.07185096  1.46043725]\n",
      "Gradient Descent(3404/9): loss=15.624505227386628, w0=73.4675996028842, w1=13.110912815855464\n",
      "[-0.10303818  2.21422919]\n",
      "Gradient Descent(3405/9): loss=15.46897640287212, w0=73.53972632753143, w1=11.560952382533408\n",
      "[-0.99200312 -0.96342848]\n",
      "Gradient Descent(3406/9): loss=17.25691782147831, w0=74.23412850873551, w1=12.235352319105262\n",
      "[ 2.57975424 -3.35621984]\n",
      "Gradient Descent(3407/9): loss=16.602098055385554, w0=72.42830053920333, w1=14.584706208647685\n",
      "[-1.69483015  1.15311929]\n",
      "Gradient Descent(3408/9): loss=16.37104374725975, w0=73.61468164110237, w1=13.777522705539239\n",
      "[ 2.29750722  1.24191553]\n",
      "Gradient Descent(3409/9): loss=15.481676720456788, w0=72.00642658915345, w1=12.908181834123338\n",
      "[ 0.7505399   0.98035799]\n",
      "Gradient Descent(3410/9): loss=16.37803370187825, w0=71.48104865812395, w1=12.221931238683903\n",
      "[-0.77830543 -0.24389022]\n",
      "Gradient Descent(3411/9): loss=17.82014951837765, w0=72.02586245743389, w1=12.392654395759395\n",
      "[ 1.13350306  0.46840096]\n",
      "Gradient Descent(3412/9): loss=16.780722963572302, w0=71.23241031251045, w1=12.06477372075283\n",
      "[-3.80417321 -0.37206679]\n",
      "Gradient Descent(3413/9): loss=18.511828874519487, w0=73.89533155620181, w1=12.325220477084557\n",
      "[-0.2760201  -0.53969829]\n",
      "Gradient Descent(3414/9): loss=16.23316043514182, w0=74.08854562860245, w1=12.703009278530704\n",
      "[ 1.63509271  0.17539499]\n",
      "Gradient Descent(3415/9): loss=16.003235119349405, w0=72.94398073252076, w1=12.580232785592747\n",
      "[-2.23668574 -2.19746166]\n",
      "Gradient Descent(3416/9): loss=15.851649134747632, w0=74.50966075002289, w1=14.118455950647565\n",
      "[ 2.17998027  2.03777695]\n",
      "Gradient Descent(3417/9): loss=16.328894859821546, w0=72.98367456132657, w1=12.692012087031305\n",
      "[-0.85106444 -1.68644596]\n",
      "Gradient Descent(3418/9): loss=15.744250525170617, w0=73.57941967037847, w1=13.872524258958943\n",
      "[ 2.59332959 -1.54149313]\n",
      "Gradient Descent(3419/9): loss=15.503792892649418, w0=71.76408895564835, w1=14.95156944912071\n",
      "[-4.88952382  0.62336139]\n",
      "Gradient Descent(3420/9): loss=17.639263978869398, w0=75.18675562692889, w1=14.515216476485069\n",
      "[ 1.63520056  1.05913829]\n",
      "Gradient Descent(3421/9): loss=17.7134317444383, w0=74.04211523484638, w1=13.77381967402057\n",
      "[-2.23452138  0.16450246]\n",
      "Gradient Descent(3422/9): loss=15.709033959614628, w0=75.60628020184615, w1=13.65866795098314\n",
      "[ 4.00617003  0.41383984]\n",
      "Gradient Descent(3423/9): loss=18.075400629136375, w0=72.80196118326721, w1=13.36898006376049\n",
      "[-3.45795572 -0.23686785]\n",
      "Gradient Descent(3424/9): loss=15.513031421484216, w0=75.22253018670723, w1=13.534787562097359\n",
      "[ 3.61310829 -1.32252806]\n",
      "Gradient Descent(3425/9): loss=17.24716926849939, w0=72.69335438222787, w1=14.460557204558055\n",
      "[-1.67519646  0.86907002]\n",
      "Gradient Descent(3426/9): loss=16.0472568328474, w0=73.86599190767258, w1=13.852208190168954\n",
      "[ 2.45571918 -1.25819578]\n",
      "Gradient Descent(3427/9): loss=15.618896401070865, w0=72.14698848020136, w1=14.732945236858175\n",
      "[-2.52498183  2.95036192]\n",
      "Gradient Descent(3428/9): loss=16.828912348503135, w0=73.91447575849269, w1=12.66769189128815\n",
      "[-0.89318498  1.49670458]\n",
      "Gradient Descent(3429/9): loss=15.908120032808872, w0=74.53970524298832, w1=11.619998682337412\n",
      "[ 1.62190869 -1.09313996]\n",
      "Gradient Descent(3430/9): loss=17.891143431362853, w0=73.40436916282754, w1=12.385196655185897\n",
      "[-1.44417098 -1.5734226 ]\n",
      "Gradient Descent(3431/9): loss=15.990969552604263, w0=74.41528885174077, w1=13.48659247749233\n",
      "[ 0.42653613 -0.05962203]\n",
      "Gradient Descent(3432/9): loss=16.014643342052633, w0=74.11671355866055, w1=13.528327900234592\n",
      "[-2.31690496  0.05951564]\n",
      "Gradient Descent(3433/9): loss=15.725562573329318, w0=75.73854703127688, w1=13.486666954739485\n",
      "[ 3.67832706  0.38096226]\n",
      "Gradient Descent(3434/9): loss=18.374007818128224, w0=73.1637180893546, w1=13.219993371693032\n",
      "[-0.57990954 -3.15071991]\n",
      "Gradient Descent(3435/9): loss=15.42809139419686, w0=73.56965476875742, w1=15.425497306566625\n",
      "[-1.18454409  0.03906795]\n",
      "Gradient Descent(3436/9): loss=17.316941531362325, w0=74.39883562902271, w1=15.398149744680689\n",
      "[-0.14412089  0.21968802]\n",
      "Gradient Descent(3437/9): loss=17.836505785911864, w0=74.49972025336191, w1=15.244368133017723\n",
      "[ 0.06583411 -0.67539517]\n",
      "Gradient Descent(3438/9): loss=17.669867446488816, w0=74.45363637532947, w1=15.71714475388552\n",
      "[-1.69103748  0.39174897]\n",
      "Gradient Descent(3439/9): loss=18.561408273382114, w0=75.63736261042469, w1=15.442920477193518\n",
      "[ 3.13626244 -0.30720879]\n",
      "Gradient Descent(3440/9): loss=20.05883771967798, w0=73.4419789047256, w1=15.657966631997027\n",
      "[ 2.50465105  9.23848257]\n",
      "Gradient Descent(3441/9): loss=17.769243965427613, w0=71.68872316716815, w1=9.191028835331707\n",
      "[-1.43637779 -7.52362719]\n",
      "Gradient Descent(3442/9): loss=25.870623027655935, w0=72.6941876212701, w1=14.457567869001391\n",
      "[-1.33270454  2.67772384]\n",
      "Gradient Descent(3443/9): loss=16.043829157520957, w0=73.62708079971614, w1=12.583161179452986\n",
      "[-0.59729764  0.75322448]\n",
      "Gradient Descent(3444/9): loss=15.843287337943833, w0=74.04518914817383, w1=12.055904042563098\n",
      "[ 2.88974738 -1.31235155]\n",
      "Gradient Descent(3445/9): loss=16.68170420038175, w0=72.02236598199013, w1=12.974550130724117\n",
      "[ 1.12707583 -0.23135414]\n",
      "Gradient Descent(3446/9): loss=16.321909701799946, w0=71.23341289897392, w1=13.136498025489825\n",
      "[-4.00913755  0.90900556]\n",
      "Gradient Descent(3447/9): loss=17.567634816316755, w0=74.03980918236076, w1=12.50019413122285\n",
      "[-1.31025056 -0.86126713]\n",
      "Gradient Descent(3448/9): loss=16.143789765370705, w0=74.95698457551354, w1=13.103081120357233\n",
      "[-0.01528347 -1.56736937]\n",
      "Gradient Descent(3449/9): loss=16.839702003945856, w0=74.96768300237606, w1=14.200239680795889\n",
      "[-2.48315844  4.60418513]\n",
      "Gradient Descent(3450/9): loss=17.046205567818273, w0=76.70589391238663, w1=10.977310088486504\n",
      "[ 0.36550243 -1.15165404]\n",
      "Gradient Descent(3451/9): loss=24.337672778994914, w0=76.45004221073093, w1=11.78346791326745\n",
      "[ 3.57089945 -4.91543226]\n",
      "Gradient Descent(3452/9): loss=21.805057993212564, w0=73.9504125977954, w1=15.22427049628398\n",
      "[ 2.0667452   1.45365017]\n",
      "Gradient Descent(3453/9): loss=17.12311923455881, w0=72.50369095918673, w1=14.206715377187992\n",
      "[ 0.55076679  0.14485185]\n",
      "Gradient Descent(3454/9): loss=15.96238705840836, w0=72.11815420850084, w1=14.105319079809345\n",
      "[ 0.88717494 -0.53134418]\n",
      "Gradient Descent(3455/9): loss=16.272794658089676, w0=71.49713175382173, w1=14.477260005511182\n",
      "[-2.21259494 -0.45540423]\n",
      "Gradient Descent(3456/9): loss=17.497666044719967, w0=73.04594820967533, w1=14.796042966818552\n",
      "[-1.72421355  2.6055094 ]\n",
      "Gradient Descent(3457/9): loss=16.282996404208696, w0=74.25289769563125, w1=12.972186385039794\n",
      "[ 1.85465187  0.37353718]\n",
      "Gradient Descent(3458/9): loss=15.97449640490484, w0=72.95464138326822, w1=12.710710361260404\n",
      "[ 0.68486597 -2.25260004]\n",
      "Gradient Descent(3459/9): loss=15.739125632688078, w0=72.47523520351228, w1=14.287530388430147\n",
      "[-1.94772932  1.82248727]\n",
      "Gradient Descent(3460/9): loss=16.047296828875435, w0=73.83864573067844, w1=13.01178930265488\n",
      "[-0.51475411 -0.87855399]\n",
      "Gradient Descent(3461/9): loss=15.643725867951481, w0=74.19897360692867, w1=13.626777093656488\n",
      "[ 0.34503066 -0.87007259]\n",
      "Gradient Descent(3462/9): loss=15.806261079440668, w0=73.95745214698984, w1=14.235827909738225\n",
      "[-1.04955008  0.07885254]\n",
      "Gradient Descent(3463/9): loss=15.891879300992308, w0=74.69213720406071, w1=14.180631132727608\n",
      "[ 2.4662813   1.31359364]\n",
      "Gradient Descent(3464/9): loss=16.609034254738923, w0=72.9657402932999, w1=13.26111558514064\n",
      "[ 1.37314091  2.4218857 ]\n",
      "Gradient Descent(3465/9): loss=15.463631777208404, w0=72.0045416549889, w1=11.565795595660404\n",
      "[-2.59444526 -2.27704476]\n",
      "Gradient Descent(3466/9): loss=18.048677542527138, w0=73.82065333848507, w1=13.159726929185952\n",
      "[ 2.7200491  -2.50077298]\n",
      "Gradient Descent(3467/9): loss=15.575806181153702, w0=71.91661896725805, w1=14.910268012427112\n",
      "[ 0.76213572  4.36650893]\n",
      "Gradient Descent(3468/9): loss=17.357614323798554, w0=71.38312395992243, w1=11.853711759956704\n",
      "[-1.9504089  -0.23560014]\n",
      "Gradient Descent(3469/9): loss=18.533401545436952, w0=72.74841019201979, w1=12.018631855355231\n",
      "[-0.78375628  1.04745559]\n",
      "Gradient Descent(3470/9): loss=16.60205766639227, w0=73.29703958832474, w1=11.28541294194111\n",
      "[-1.6958572  -4.68935302]\n",
      "Gradient Descent(3471/9): loss=17.793367861096538, w0=74.48413962663857, w1=14.56796005461237\n",
      "[ 0.34869575  3.76226717]\n",
      "Gradient Descent(3472/9): loss=16.686338306512248, w0=74.24005260217473, w1=11.934373037306262\n",
      "[-1.61401179 -3.10627971]\n",
      "Gradient Descent(3473/9): loss=17.027506352038674, w0=75.36986085757404, w1=14.108768833509291\n",
      "[ 1.33181274 -0.77988153]\n",
      "Gradient Descent(3474/9): loss=17.73850491091169, w0=74.43759193822835, w1=14.654685904516942\n",
      "[-1.65281298  2.28146444]\n",
      "Gradient Descent(3475/9): loss=16.730159657272587, w0=75.59456102430109, w1=13.057660795471053\n",
      "[ 4.27892858 -0.0461243 ]\n",
      "Gradient Descent(3476/9): loss=18.121421617264563, w0=72.59931102129224, w1=13.089947805612702\n",
      "[-2.59987124 -2.96815679]\n",
      "Gradient Descent(3477/9): loss=15.703088309318805, w0=74.41922088887607, w1=15.167657556996511\n",
      "[-0.57516495  1.15382684]\n",
      "Gradient Descent(3478/9): loss=17.443616028567682, w0=74.82183635410134, w1=14.359978770357436\n",
      "[ 0.98509004  1.40510938]\n",
      "Gradient Descent(3479/9): loss=16.940583412938746, w0=74.13227332577556, w1=13.376402206647425\n",
      "[ 1.48549768 -3.24538361]\n",
      "Gradient Descent(3480/9): loss=15.742640841419329, w0=73.09242495105377, w1=15.648170736311096\n",
      "[ 2.95104166  2.43081448]\n",
      "Gradient Descent(3481/9): loss=17.757294101906858, w0=71.02669578897172, w1=13.946600598257438\n",
      "[-5.19857936 -0.16284976]\n",
      "Gradient Descent(3482/9): loss=18.06503749808923, w0=74.66570134310835, w1=14.060595430822008\n",
      "[-0.49905889  0.06677236]\n",
      "Gradient Descent(3483/9): loss=16.495489676454874, w0=75.01504256504269, w1=14.013854776138972\n",
      "[ 3.36868536 -3.04601792]\n",
      "Gradient Descent(3484/9): loss=17.009669885217107, w0=72.65696281621082, w1=16.146067317741288\n",
      "[ 0.9232576   0.89876396]\n",
      "Gradient Descent(3485/9): loss=19.143470551465363, w0=72.01068249824658, w1=15.516932543938635\n",
      "[-0.11422865  1.93824055]\n",
      "Gradient Descent(3486/9): loss=18.28437256711533, w0=72.09064255038693, w1=14.160164159144534\n",
      "[ 1.68068394  0.26881449]\n",
      "Gradient Descent(3487/9): loss=16.34133586274618, w0=70.91416379551505, w1=13.971994015597568\n",
      "[-2.93000244 -1.35871287]\n",
      "Gradient Descent(3488/9): loss=18.338683007049166, w0=72.9651655066687, w1=14.923093022413228\n",
      "[-4.97021547  2.57339934]\n",
      "Gradient Descent(3489/9): loss=16.481602045551725, w0=76.44431633486765, w1=13.121713481258105\n",
      "[ 4.7141692  -2.08066487]\n",
      "Gradient Descent(3490/9): loss=20.41246172021654, w0=73.14439789556732, w1=14.578178891759993\n",
      "[-0.2283995   1.65914664]\n",
      "Gradient Descent(3491/9): loss=16.00038087637283, w0=73.30427754345864, w1=13.416776243961428\n",
      "[ 3.57053196 -2.72494848]\n",
      "Gradient Descent(3492/9): loss=15.387921969518295, w0=70.80490517055354, w1=15.324240182123459\n",
      "[-5.39123611  2.79716938]\n",
      "Gradient Descent(3493/9): loss=20.184631567677485, w0=74.57877045021034, w1=13.366221617753206\n",
      "[ 0.87111217  3.02930646]\n",
      "Gradient Descent(3494/9): loss=16.21774571892694, w0=73.96899193279758, w1=11.245707097736522\n",
      "[-1.41821929 -4.16080152]\n",
      "Gradient Descent(3495/9): loss=18.109137497928305, w0=74.96174543273229, w1=14.158268160683567\n",
      "[ 1.18706999 -0.12295285]\n",
      "Gradient Descent(3496/9): loss=17.006924303140178, w0=74.13079644265886, w1=14.244335152711628\n",
      "[-1.15630334  1.4022192 ]\n",
      "Gradient Descent(3497/9): loss=16.02839123368414, w0=74.94020878169665, w1=13.2627817124285\n",
      "[ 3.78232719 -0.72427994]\n",
      "Gradient Descent(3498/9): loss=16.764547418353537, w0=72.29257974871366, w1=13.769777667482728\n",
      "[-1.52079214  0.23732589]\n",
      "Gradient Descent(3499/9): loss=15.929299942593815, w0=73.35713424730774, w1=13.603649541696546\n",
      "[-2.01399307  0.94393724]\n",
      "Gradient Descent(3500/9): loss=15.395565966010688, w0=74.76692939383749, w1=12.942893471532278\n",
      "[ 3.70750468  1.04402835]\n",
      "Gradient Descent(3501/9): loss=16.614850556641795, w0=72.1716761166038, w1=12.212073626621782\n",
      "[-2.61500211 -0.76958586]\n",
      "Gradient Descent(3502/9): loss=16.819059856831185, w0=74.00217759036052, w1=12.750783727350663\n",
      "[ 2.88949105  0.36825739]\n",
      "Gradient Descent(3503/9): loss=15.902369388386534, w0=71.97953385346881, w1=12.493003556543112\n",
      "[-0.7692788   0.41360603]\n",
      "Gradient Descent(3504/9): loss=16.736493176869303, w0=72.51802901034152, w1=12.203479335167428\n",
      "[-0.66968178 -1.22443044]\n",
      "Gradient Descent(3505/9): loss=16.50127829870354, w0=72.98680625593181, w1=13.060580640739381\n",
      "[-1.88439465 -0.55132995]\n",
      "Gradient Descent(3506/9): loss=15.520883640078688, w0=74.30588250926975, w1=13.446511608498428\n",
      "[-0.24902637  0.42041209]\n",
      "Gradient Descent(3507/9): loss=15.898471050299609, w0=74.48020096844978, w1=13.152223142167443\n",
      "[ 4.25948174  1.0584869 ]\n",
      "Gradient Descent(3508/9): loss=16.14314138028159, w0=71.49856374895282, w1=12.41128230916099\n",
      "[-3.44539559  1.8658416 ]\n",
      "Gradient Descent(3509/9): loss=17.568314964299045, w0=73.9103406592219, w1=11.105193189990464\n",
      "[ 2.38999001 -0.80949894]\n",
      "Gradient Descent(3510/9): loss=18.395044671684513, w0=72.23734765406708, w1=11.671842449345434\n",
      "[-2.68327129 -0.4739506 ]\n",
      "Gradient Descent(3511/9): loss=17.578259487790998, w0=74.1156375598627, w1=12.00360786941274\n",
      "[-0.27333848 -2.53050737]\n",
      "Gradient Descent(3512/9): loss=16.812938442017387, w0=74.30697449426259, w1=13.774963028381364\n",
      "[-0.51952486  0.5586896 ]\n",
      "Gradient Descent(3513/9): loss=15.942612001211819, w0=74.67064189410873, w1=13.383880310652104\n",
      "[ 1.08009424 -1.50657386]\n",
      "Gradient Descent(3514/9): loss=16.33815859737598, w0=73.91457592725985, w1=14.438482009905508\n",
      "[ 1.25933476  0.0858808 ]\n",
      "Gradient Descent(3515/9): loss=16.038113065127085, w0=73.03304159474382, w1=14.378365448537416\n",
      "[ 1.02986925  0.4278749 ]\n",
      "Gradient Descent(3516/9): loss=15.823705781681696, w0=72.31213311728031, w1=14.078853015193147\n",
      "[-1.22010773  0.11374889]\n",
      "Gradient Descent(3517/9): loss=16.04732729343589, w0=73.1662085276783, w1=13.999228791447203\n",
      "[-0.65878398 -0.35555436]\n",
      "Gradient Descent(3518/9): loss=15.528991856918275, w0=73.62735731362939, w1=14.24811684398545\n",
      "[-2.82862763 -0.61098677]\n",
      "Gradient Descent(3519/9): loss=15.736700090197573, w0=75.60739665652535, w1=14.675807581500607\n",
      "[ 2.04747784 -3.5344489 ]\n",
      "Gradient Descent(3520/9): loss=18.777292156905894, w0=74.17416216644574, w1=17.149921809474417\n",
      "[ 3.67632268  7.60431377]\n",
      "Gradient Descent(3521/9): loss=22.50851766856868, w0=71.60073628717072, w1=11.826902168136447\n",
      "[-3.18094742 -1.31701177]\n",
      "Gradient Descent(3522/9): loss=18.185217690565157, w0=73.8273994841694, w1=12.74881040381966\n",
      "[ 2.69508479  1.31224001]\n",
      "Gradient Descent(3523/9): loss=15.795295870347958, w0=71.94084012834679, w1=11.830242397362449\n",
      "[-0.5205736  -2.39383804]\n",
      "Gradient Descent(3524/9): loss=17.66167884989011, w0=72.30524165173622, w1=13.50592902341908\n",
      "[-0.08185795  3.13230441]\n",
      "Gradient Descent(3525/9): loss=15.874975941186701, w0=72.36254221668631, w1=11.313315936187456\n",
      "[ 0.73743803 -4.66846157]\n",
      "Gradient Descent(3526/9): loss=18.16625891618276, w0=71.84633559629589, w1=14.581239036212622\n",
      "[ 0.25074051  2.99977002]\n",
      "Gradient Descent(3527/9): loss=17.04032149657293, w0=71.670817236789, w1=12.481400023143825\n",
      "[-3.48557804 -3.14328767]\n",
      "Gradient Descent(3528/9): loss=17.201436244247567, w0=74.11072186530566, w1=14.68170139490042\n",
      "[ 3.10135606  1.32265772]\n",
      "Gradient Descent(3529/9): loss=16.441857606965968, w0=71.93977262557055, w1=13.75584098990011\n",
      "[-0.37654052  1.74276621]\n",
      "Gradient Descent(3530/9): loss=16.340871625232605, w0=72.20335099112647, w1=12.535904641230957\n",
      "[-3.58939453 -5.55565389]\n",
      "Gradient Descent(3531/9): loss=16.42594700960223, w0=74.71592715877614, w1=16.42486236380655\n",
      "[ 0.28948456  1.30262525]\n",
      "Gradient Descent(3532/9): loss=20.733891253235647, w0=74.51328796725248, w1=15.513024690091337\n",
      "[ 3.53495552 -0.58986256]\n",
      "Gradient Descent(3533/9): loss=18.196493910683763, w0=72.03881910565353, w1=15.925928482927956\n",
      "[ 1.16063554  1.40031557]\n",
      "Gradient Descent(3534/9): loss=19.165515985766998, w0=71.22637422951243, w1=14.945707585468606\n",
      "[-2.21733398  0.80831001]\n",
      "Gradient Descent(3535/9): loss=18.597835655420834, w0=72.77850801655458, w1=14.379890581401002\n",
      "[ 0.7758474  -0.04714354]\n",
      "Gradient Descent(3536/9): loss=15.923874004718813, w0=72.23541483876653, w1=14.412891060113388\n",
      "[-2.61263225  0.75917432]\n",
      "Gradient Descent(3537/9): loss=16.381517749443503, w0=74.06425741502574, w1=13.881469036037203\n",
      "[-0.15825424  0.52771103]\n",
      "Gradient Descent(3538/9): loss=15.763300376272015, w0=74.17503538311762, w1=13.512071312329414\n",
      "[-0.90588736 -1.56319741]\n",
      "Gradient Descent(3539/9): loss=15.774591812400342, w0=74.80915653251766, w1=14.606309502209653\n",
      "[-0.98415267  1.29561847]\n",
      "Gradient Descent(3540/9): loss=17.168466185841584, w0=75.49806340474643, w1=13.699376574754245\n",
      "[ 2.12313338  2.07321115]\n",
      "Gradient Descent(3541/9): loss=17.839133697397447, w0=74.01187004170522, w1=12.248128770388526\n",
      "[ 2.25013552  0.35906316]\n",
      "Gradient Descent(3542/9): loss=16.402011724067595, w0=72.4367751780581, w1=11.996784560573495\n",
      "[-2.12781826 -0.4525153 ]\n",
      "Gradient Descent(3543/9): loss=16.85277574817572, w0=73.92624796141986, w1=12.313545269182958\n",
      "[ 1.00019503 -2.41635413]\n",
      "Gradient Descent(3544/9): loss=16.265778857543115, w0=73.22611143869402, w1=14.004993162336403\n",
      "[ 0.3567498   1.48493432]\n",
      "Gradient Descent(3545/9): loss=15.526146926345755, w0=72.97638658105535, w1=12.965539139843772\n",
      "[ 1.94244274 -1.16311212]\n",
      "Gradient Descent(3546/9): loss=15.568489329360327, w0=71.61667666447698, w1=13.779717622796312\n",
      "[-3.07869611 -0.57877156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3547/9): loss=16.837465386482815, w0=73.77176394241793, w1=14.184857717832925\n",
      "[ 4.95116401 -0.35029256]\n",
      "Gradient Descent(3548/9): loss=15.74866926374881, w0=70.30594913355286, w1=14.430062509213025\n",
      "[-3.6456096  -3.09968244]\n",
      "Gradient Descent(3549/9): loss=20.301461432220588, w0=72.85787585581326, w1=16.599840214426248\n",
      "[-0.84611325  1.19144405]\n",
      "Gradient Descent(3550/9): loss=20.348554669685278, w0=73.45015513169264, w1=15.765829381674193\n",
      "[ 3.42088898  0.97839789]\n",
      "Gradient Descent(3551/9): loss=18.01125761118025, w0=71.05553284505781, w1=15.080950861196497\n",
      "[ 1.28263287 -1.31765855]\n",
      "Gradient Descent(3552/9): loss=19.173063126804692, w0=70.15768983505599, w1=16.003311844433803\n",
      "[-5.70683914  1.42820173]\n",
      "Gradient Descent(3553/9): loss=23.488140961321427, w0=74.15247723624077, w1=15.003570636397248\n",
      "[-1.50410833  1.51620664]\n",
      "Gradient Descent(3554/9): loss=16.915518322859715, w0=75.20535306980734, w1=13.942225985349713\n",
      "[ 0.26301449  0.79547296]\n",
      "Gradient Descent(3555/9): loss=17.31963162425151, w0=75.02124292431262, w1=13.385394913400509\n",
      "[ 0.84308757 -0.27522467]\n",
      "Gradient Descent(3556/9): loss=16.88215455041646, w0=74.43108162339679, w1=13.57805217999643\n",
      "[-0.71381402 -2.47039325]\n",
      "Gradient Descent(3557/9): loss=16.037289223701492, w0=74.93075143464633, w1=15.307327454566067\n",
      "[ 0.71696998 -0.85456736]\n",
      "Gradient Descent(3558/9): loss=18.395581494337616, w0=74.42887244868317, w1=15.905524605701352\n",
      "[ 2.61211496  1.5859189 ]\n",
      "Gradient Descent(3559/9): loss=18.97222647071114, w0=72.60039197420267, w1=14.795381379200462\n",
      "[-1.71108413  1.2384831 ]\n",
      "Gradient Descent(3560/9): loss=16.491872204011823, w0=73.79815086761774, w1=13.92844320649885\n",
      "[ 0.87794681 -0.04672117]\n",
      "Gradient Descent(3561/9): loss=15.613690895887325, w0=73.1835881013261, w1=13.961148022652585\n",
      "[ 0.38328742  3.69773836]\n",
      "Gradient Descent(3562/9): loss=15.507864766194432, w0=72.91528690887635, w1=11.372731173434511\n",
      "[ 1.31044669 -2.47980253]\n",
      "Gradient Descent(3563/9): loss=17.67725515401258, w0=71.99797422287618, w1=13.10859294517669\n",
      "[-0.62974426 -1.61950276]\n",
      "Gradient Descent(3564/9): loss=16.29449302993299, w0=72.43879520300418, w1=14.24224487426539\n",
      "[ 0.08356814 -1.88007853]\n",
      "Gradient Descent(3565/9): loss=16.042236650574136, w0=72.38029750553525, w1=15.558299843992133\n",
      "[-1.17762989 -0.16210235]\n",
      "Gradient Descent(3566/9): loss=17.963505537628812, w0=73.20463842861508, w1=15.671771486693444\n",
      "[ 0.7288997   1.45468853]\n",
      "Gradient Descent(3567/9): loss=17.792435090156573, w0=72.69440863589035, w1=14.653489516559562\n",
      "[-1.33872465  0.10607879]\n",
      "Gradient Descent(3568/9): loss=16.254472325574618, w0=73.63151589320262, w1=14.57923436074094\n",
      "[ 1.26156432  1.61531331]\n",
      "Gradient Descent(3569/9): loss=16.047346919087126, w0=72.74842087224691, w1=13.448515045846692\n",
      "[-2.05268764 -1.9870115 ]\n",
      "Gradient Descent(3570/9): loss=15.53516024871238, w0=74.18530221899152, w1=14.839423097881227\n",
      "[ 2.18345047 -0.88419058]\n",
      "Gradient Descent(3571/9): loss=16.707573757748904, w0=72.6568868927253, w1=15.458356503511366\n",
      "[-0.64662814  0.54622648]\n",
      "Gradient Descent(3572/9): loss=17.546310909070005, w0=73.10952658871095, w1=15.075997966905696\n",
      "[ 0.42570003  0.19490016]\n",
      "Gradient Descent(3573/9): loss=16.676952452773026, w0=72.81153656867802, w1=14.939567853334731\n",
      "[-0.64419547 -0.83472563]\n",
      "Gradient Descent(3574/9): loss=16.56782464325738, w0=73.2624733943666, w1=15.523875796962384\n",
      "[-2.10150929  6.35197706]\n",
      "Gradient Descent(3575/9): loss=17.475684301510814, w0=74.73352990036165, w1=11.077491852766517\n",
      "[ 2.88300902 -4.36719013]\n",
      "Gradient Descent(3576/9): loss=19.307455182017367, w0=72.71542358320579, w1=14.134524941581855\n",
      "[ 0.45749773  0.51395042]\n",
      "Gradient Descent(3577/9): loss=15.76760778855913, w0=72.39517516896731, w1=13.774759645536058\n",
      "[ 0.24259181 -1.02195081]\n",
      "Gradient Descent(3578/9): loss=15.833287232092866, w0=72.22536089983512, w1=14.49012521468111\n",
      "[ 2.34581138 -1.06766072]\n",
      "Gradient Descent(3579/9): loss=16.467266276154227, w0=70.58329293353601, w1=15.237487715678718\n",
      "[-3.38227236  1.56819888]\n",
      "Gradient Descent(3580/9): loss=20.604529811217326, w0=72.95088358457201, w1=14.139748502922627\n",
      "[-1.30732813  2.44564796]\n",
      "Gradient Descent(3581/9): loss=15.662549352267844, w0=73.86601327461054, w1=12.427794932414411\n",
      "[ 1.07090421 -0.62315009]\n",
      "Gradient Descent(3582/9): loss=16.102797296979226, w0=73.11638032852359, w1=12.863999993475602\n",
      "[ 3.81249467  1.76470742]\n",
      "Gradient Descent(3583/9): loss=15.5911992970757, w0=70.44763406178595, w1=11.628704796443884\n",
      "[-2.77467882 -2.23914974]\n",
      "Gradient Descent(3584/9): loss=21.149680027409044, w0=72.38990923672439, w1=13.196109615701022\n",
      "[-5.03328686  1.13023685]\n",
      "Gradient Descent(3585/9): loss=15.834722688369176, w0=75.9132100370995, w1=12.404943821057204\n",
      "[ 1.55717946 -0.58964791]\n",
      "Gradient Descent(3586/9): loss=19.39378656070808, w0=74.8231844171167, w1=12.817697359313486\n",
      "[ 1.66544705 -4.66090733]\n",
      "Gradient Descent(3587/9): loss=16.774341616023694, w0=73.65737148324033, w1=16.080332487384428\n",
      "[ 2.06865752  1.22719002]\n",
      "Gradient Descent(3588/9): loss=18.833547959958576, w0=72.20931121875473, w1=15.221299469925242\n",
      "[-0.97745554  0.79949282]\n",
      "Gradient Descent(3589/9): loss=17.49064084463837, w0=72.89353009536374, w1=14.661654493239514\n",
      "[-0.92812188  0.96409194]\n",
      "Gradient Descent(3590/9): loss=16.164538222852098, w0=73.54321540861078, w1=13.986790136012365\n",
      "[ 3.92222774  1.67684453]\n",
      "Gradient Descent(3591/9): loss=15.545525367530525, w0=70.7976559897003, w1=12.812998964071527\n",
      "[-1.10633763  0.35777472]\n",
      "Gradient Descent(3592/9): loss=18.72381329732474, w0=71.57209233275532, w1=12.56255666123353\n",
      "[-2.30674631 -2.43752587]\n",
      "Gradient Descent(3593/9): loss=17.288823930622684, w0=73.18681474699089, w1=14.268824772086415\n",
      "[ 2.40424732  4.02966135]\n",
      "Gradient Descent(3594/9): loss=15.702972991158095, w0=71.50384162501707, w1=11.448061827136295\n",
      "[-0.32106047 -6.55512002]\n",
      "Gradient Descent(3595/9): loss=19.051883843241594, w0=71.7285839524285, w1=16.036645841676993\n",
      "[-1.55029194  4.04937035]\n",
      "Gradient Descent(3596/9): loss=19.879983696830635, w0=72.81378831389144, w1=13.202086596395082\n",
      "[-2.11891736  0.84949239]\n",
      "Gradient Descent(3597/9): loss=15.539690101235774, w0=74.29703046880607, w1=12.607441924382833\n",
      "[ 0.92099597 -1.27810753]\n",
      "Gradient Descent(3598/9): loss=16.26942908864951, w0=73.65233329122816, w1=13.502117192569338\n",
      "[ 0.93068255  0.89797463]\n",
      "Gradient Descent(3599/9): loss=15.450368181495909, w0=73.00085550836545, w1=12.873534952545455\n",
      "[-1.98119247 -1.73949259]\n",
      "Gradient Descent(3600/9): loss=15.61255742381668, w0=74.38769023847456, w1=14.091179766794882\n",
      "[ 1.32560939  3.35112447]\n",
      "Gradient Descent(3601/9): loss=16.17099849520756, w0=73.45976366506065, w1=11.745392636014161\n",
      "[ 0.88069684 -1.31099533]\n",
      "Gradient Descent(3602/9): loss=16.903572179973462, w0=72.84327587838474, w1=12.663089368276268\n",
      "[-1.71819598 -0.43300889]\n",
      "Gradient Descent(3603/9): loss=15.820865449785225, w0=74.04601306155153, w1=12.966195593956877\n",
      "[ 0.63459106 -0.47032778]\n",
      "Gradient Descent(3604/9): loss=15.800558122690797, w0=73.60179931687772, w1=13.295425039865009\n",
      "[-0.6697875  -0.17072715]\n",
      "Gradient Descent(3605/9): loss=15.450263011305971, w0=74.07065056504761, w1=13.414934045622866\n",
      "[ 4.63825007  0.76826335]\n",
      "Gradient Descent(3606/9): loss=15.689639618939141, w0=70.82387551537254, w1=12.877149697831813\n",
      "[-4.59804907 -1.16983177]\n",
      "Gradient Descent(3607/9): loss=18.617993618244775, w0=74.0425098666839, w1=13.69603193937648\n",
      "[-0.24999973 -1.98526194]\n",
      "Gradient Descent(3608/9): loss=15.689476828315868, w0=74.21750967848045, w1=15.085715294392251\n",
      "[ 4.01396487  7.85453173]\n",
      "Gradient Descent(3609/9): loss=17.102017559011163, w0=71.40773427061016, w1=9.587543081946027\n",
      "[-1.38148147 -3.83679241]\n",
      "Gradient Descent(3610/9): loss=24.739231084434234, w0=72.37477130288292, w1=12.273297771040472\n",
      "[-3.4301938  -2.61705244]\n",
      "Gradient Descent(3611/9): loss=16.53602504346487, w0=74.77590696351916, w1=14.105234479636524\n",
      "[ 3.04580484 -0.07524124]\n",
      "Gradient Descent(3612/9): loss=16.679666495927965, w0=72.64384357738963, w1=14.15790334697358\n",
      "[-1.20332931  4.07524546]\n",
      "Gradient Descent(3613/9): loss=15.827160304518934, w0=73.4861740966688, w1=11.305231527070418\n",
      "[-0.09357845 -1.92995535]\n",
      "Gradient Descent(3614/9): loss=17.768551912212764, w0=73.55167901440244, w1=12.65620027463023\n",
      "[ 2.39891763 -1.88226367]\n",
      "Gradient Descent(3615/9): loss=15.758193346653028, w0=71.8724366707321, w1=13.973784845676054\n",
      "[-3.71769541  2.50517279]\n",
      "Gradient Descent(3616/9): loss=16.518251915984866, w0=74.47482345755677, w1=12.220163892321816\n",
      "[-0.70157417 -0.98749547]\n",
      "Gradient Descent(3617/9): loss=16.876383258240804, w0=74.96592537965469, w1=12.911410723462264\n",
      "[-2.34722307  0.60386883]\n",
      "Gradient Descent(3618/9): loss=16.94516893376001, w0=76.60898153060224, w1=12.488702540746962\n",
      "[ 1.36228364 -1.8804558 ]\n",
      "Gradient Descent(3619/9): loss=21.37174801281179, w0=75.65538298442125, w1=13.805021601973076\n",
      "[ 1.50726935 -1.21647488]\n",
      "Gradient Descent(3620/9): loss=18.227049881391892, w0=74.60029443796118, w1=14.656554014906368\n",
      "[ 4.4756514   1.84013243]\n",
      "Gradient Descent(3621/9): loss=16.931670391522704, w0=71.46733846069176, w1=13.368461310942587\n",
      "[-2.93448281  3.57067312]\n",
      "Gradient Descent(3622/9): loss=17.06027999201141, w0=73.52147642866812, w1=10.868990124902933\n",
      "[-0.47135529 -3.27677591]\n",
      "Gradient Descent(3623/9): loss=18.81971386754429, w0=73.85142513307116, w1=13.162733261078358\n",
      "[ 1.52224138  1.03054149]\n",
      "Gradient Descent(3624/9): loss=15.591530637694383, w0=72.7858561655237, w1=12.441354215144093\n",
      "[ 2.02208266 -3.09634385]\n",
      "Gradient Descent(3625/9): loss=16.054047212339814, w0=71.37039830093337, w1=14.6087949109811\n",
      "[-2.54724363  1.48263259]\n",
      "Gradient Descent(3626/9): loss=17.873273202110433, w0=73.15346884370419, w1=13.57095209627824\n",
      "[ 1.1249712   0.81873573]\n",
      "Gradient Descent(3627/9): loss=15.399913751577891, w0=72.36598900223233, w1=12.997837082385129\n",
      "[ 2.33116737  1.91419707]\n",
      "Gradient Descent(3628/9): loss=15.932519622679504, w0=70.73417184668678, w1=11.657899132019235\n",
      "[-6.04633346 -4.72706715]\n",
      "Gradient Descent(3629/9): loss=20.321550153350575, w0=74.96660527157938, w1=14.966846135518226\n",
      "[-0.13187959  0.13334754]\n",
      "Gradient Descent(3630/9): loss=17.890605850443638, w0=75.05892098558536, w1=14.873502855579643\n",
      "[ 4.97450917  1.17235872]\n",
      "Gradient Descent(3631/9): loss=17.91482444293747, w0=71.57676456353458, w1=14.052851750246237\n",
      "[-0.52050628  4.11238283]\n",
      "Gradient Descent(3632/9): loss=17.024447040595327, w0=71.94111895766846, w1=11.174183768743543\n",
      "[-1.09200237 -2.56381844]\n",
      "Gradient Descent(3633/9): loss=18.95865712278792, w0=72.70552061474388, w1=12.968856675876774\n",
      "[ 3.11040004 -0.66697166]\n",
      "Gradient Descent(3634/9): loss=15.689482768462845, w0=70.52824058880203, w1=13.435736840704386\n",
      "[-1.10301076 -0.88305732]\n",
      "Gradient Descent(3635/9): loss=19.211351635221014, w0=71.3003481189607, w1=14.053876964371842\n",
      "[ 0.08798665 -0.85334871]\n",
      "Gradient Descent(3636/9): loss=17.537888736007986, w0=71.23875746593654, w1=14.651221061509222\n",
      "[-4.02884377  0.49859331]\n",
      "Gradient Descent(3637/9): loss=18.183954735197638, w0=74.05894810278093, w1=14.302205743313271\n",
      "[-3.21209916  3.02360545]\n",
      "Gradient Descent(3638/9): loss=16.016767957306026, w0=76.30741751161973, w1=12.185681928461879\n",
      "[ 5.69734938 -2.94523078]\n",
      "Gradient Descent(3639/9): loss=20.763722937673045, w0=72.31927294666988, w1=14.247343475375216\n",
      "[-0.35093657 -0.28051311]\n",
      "Gradient Descent(3640/9): loss=16.155486966542043, w0=72.56492854409285, w1=14.443702651133403\n",
      "[ 2.0918661   0.02667012]\n",
      "Gradient Descent(3641/9): loss=16.116242168152816, w0=71.10062227332685, w1=14.425033564310239\n",
      "[-3.72016223  1.45067442]\n",
      "Gradient Descent(3642/9): loss=18.23798573772952, w0=73.70473583553503, w1=13.409561470685354\n",
      "[ 1.28787925  5.21576389]\n",
      "Gradient Descent(3643/9): loss=15.472732450594433, w0=72.80322035777903, w1=9.75852674455029\n",
      "[ -4.48340835e-04  -6.42874994e+00]\n",
      "Gradient Descent(3644/9): loss=22.42989339206469, w0=72.80353419636356, w1=14.258651703578407\n",
      "[-1.74066107  0.36325666]\n",
      "Gradient Descent(3645/9): loss=15.80950116091471, w0=74.02199694396349, w1=14.00437203857\n",
      "[ 0.34109521  2.78421026]\n",
      "Gradient Descent(3646/9): loss=15.788568279125245, w0=73.78323029787624, w1=12.055424858391389\n",
      "[ 1.49369925 -2.36379587]\n",
      "Gradient Descent(3647/9): loss=16.519896723409808, w0=72.7376408245794, w1=13.710081966773922\n",
      "[-1.1354103  -1.32062363]\n",
      "Gradient Descent(3648/9): loss=15.567147303651529, w0=73.53242803275685, w1=14.6345185043617\n",
      "[ 3.24407289  0.46938953]\n",
      "Gradient Descent(3649/9): loss=16.081118961087963, w0=71.2615770105729, w1=14.305945832505063\n",
      "[-2.61431757  4.78035631]\n",
      "Gradient Descent(3650/9): loss=17.792431764718007, w0=73.09159930724798, w1=10.959696414587984\n",
      "[-0.86410816 -2.96400677]\n",
      "Gradient Descent(3651/9): loss=18.581595476795552, w0=73.69647501935471, w1=13.03450115671761\n",
      "[ 2.68716718 -2.18729929]\n",
      "Gradient Descent(3652/9): loss=15.566018875827789, w0=71.81545799234691, w1=14.565610658723102\n",
      "[-1.68808852  1.03609898]\n",
      "Gradient Descent(3653/9): loss=17.068403259059057, w0=72.99711995648143, w1=13.840341370736468\n",
      "[ 3.36895051 -2.91806962]\n",
      "Gradient Descent(3654/9): loss=15.494960210621782, w0=70.63885460249449, w1=15.882990105753063\n",
      "[-0.70659934 -1.15208372]\n",
      "Gradient Descent(3655/9): loss=21.79845109846362, w0=71.13347414371292, w1=16.68944871128438\n",
      "[-3.53758546  3.17046721]\n",
      "Gradient Descent(3656/9): loss=22.870858824928398, w0=73.60978396383652, w1=14.470121667164502\n",
      "[ 0.78667234  2.98097145]\n",
      "Gradient Descent(3657/9): loss=15.926227481852969, w0=73.05911332264027, w1=12.383441651074602\n",
      "[ 0.72612196  0.5586506 ]\n",
      "Gradient Descent(3658/9): loss=16.01436024263763, w0=72.55082795020665, w1=11.99238623343283\n",
      "[-1.63496156  0.13795741]\n",
      "Gradient Descent(3659/9): loss=16.768051868730716, w0=73.69530104199072, w1=11.895816043220574\n",
      "[ 7.06727807 -4.25622521]\n",
      "Gradient Descent(3660/9): loss=16.720804325587714, w0=68.74820639443828, w1=14.875173693226278\n",
      "[-4.98928041 -1.37610874]\n",
      "Gradient Descent(3661/9): loss=26.69130912334317, w0=72.24070268385441, w1=15.838449811506898\n",
      "[-2.89941728  1.74064301]\n",
      "Gradient Descent(3662/9): loss=18.722344340688874, w0=74.27029478276037, w1=14.619999704530322\n",
      "[-0.68025855  2.30400408]\n",
      "Gradient Descent(3663/9): loss=16.512667300770513, w0=74.74647577028965, w1=13.00719684665101\n",
      "[ 5.82211548  2.49802872]\n",
      "Gradient Descent(3664/9): loss=16.552479584174062, w0=70.67099493662609, w1=11.258576741063376\n",
      "[-0.95455554 -0.48595605]\n",
      "Gradient Descent(3665/9): loss=21.29248294965614, w0=71.33918381807915, w1=11.598745977634458\n",
      "[-1.58739241 -0.93419588]\n",
      "Gradient Descent(3666/9): loss=19.06540595972065, w0=72.450358507232, w1=12.252683094139503\n",
      "[-0.22619321 -2.09583681]\n",
      "Gradient Descent(3667/9): loss=16.494488055423574, w0=72.60869375386895, w1=13.719768859879705\n",
      "[-0.11853028 -0.32039523]\n",
      "Gradient Descent(3668/9): loss=15.649470288485496, w0=72.69166494769243, w1=13.944045521241028\n",
      "[-0.97561486 -0.78876977]\n",
      "Gradient Descent(3669/9): loss=15.675047256118512, w0=73.37459535080812, w1=14.496184357953702\n",
      "[-0.56131926 -0.85196836]\n",
      "Gradient Descent(3670/9): loss=15.905749548512604, w0=73.76751882965378, w1=15.092562211237516\n",
      "[ 1.37568033 -1.41719624]\n",
      "Gradient Descent(3671/9): loss=16.79867704673381, w0=72.8045425956428, w1=16.08459957942383\n",
      "[-0.61904776  2.03259215]\n",
      "Gradient Descent(3672/9): loss=18.89835248818494, w0=73.23787602553482, w1=14.661785076644232\n",
      "[-1.03953052  1.45851713]\n",
      "Gradient Descent(3673/9): loss=16.086106309649097, w0=73.96554738965338, w1=13.640823086380799\n",
      "[ 1.13381947  0.63325628]\n",
      "Gradient Descent(3674/9): loss=15.624406520424966, w0=73.17187375902688, w1=13.197543690947123\n",
      "[ 0.09964579  0.63440216]\n",
      "Gradient Descent(3675/9): loss=15.43314535570575, w0=73.10212170785685, w1=12.753462177593185\n",
      "[-1.27735926  0.17791967]\n",
      "Gradient Descent(3676/9): loss=15.668001263450057, w0=73.99627319245847, w1=12.62891840928427\n",
      "[-2.54108973  1.81635768]\n",
      "Gradient Descent(3677/9): loss=15.99446170321221, w0=75.77503600205308, w1=11.357468030651058\n",
      "[ 2.02395383 -0.4541527 ]\n",
      "Gradient Descent(3678/9): loss=20.715811865070087, w0=74.35826831920728, w1=11.675374918886181\n",
      "[ 0.25037644 -0.7794985 ]\n",
      "Gradient Descent(3679/9): loss=17.58012134620193, w0=74.18300481398204, w1=12.221023868022284\n",
      "[ 0.6449401  -4.35852395]\n",
      "Gradient Descent(3680/9): loss=16.57327044632325, w0=73.73154674273735, w1=15.271990636500595\n",
      "[-0.82384243 -0.40412028]\n",
      "Gradient Descent(3681/9): loss=17.087776151442814, w0=74.30823644343553, w1=15.554874833034908\n",
      "[ 0.18563347  6.30029521]\n",
      "Gradient Descent(3682/9): loss=18.053454250906764, w0=74.17829301228608, w1=11.144668188401658\n",
      "[-0.43921446 -1.82035751]\n",
      "Gradient Descent(3683/9): loss=18.503159727414022, w0=74.48574313281982, w1=12.418918445771654\n",
      "[ 1.81633662  1.39183458]\n",
      "Gradient Descent(3684/9): loss=16.65874861641823, w0=73.2143074970909, w1=11.444634243217726\n",
      "[-1.18333374  0.70803486]\n",
      "Gradient Descent(3685/9): loss=17.459828726845352, w0=74.04264111304568, w1=10.949009844609336\n",
      "[-0.4901602  -2.02115321]\n",
      "Gradient Descent(3686/9): loss=18.868405822850452, w0=74.38575325047424, w1=12.363817092171269\n",
      "[ 2.60192801 -1.78097711]\n",
      "Gradient Descent(3687/9): loss=16.604546814348165, w0=72.56440364275676, w1=13.61050106879316\n",
      "[-1.2903224  -0.46725905]\n",
      "Gradient Descent(3688/9): loss=15.660539220508785, w0=73.46762932562676, w1=13.93758240395767\n",
      "[ 0.97663029 -0.25370894]\n",
      "Gradient Descent(3689/9): loss=15.505797440193575, w0=72.78398812322614, w1=14.115178660974948\n",
      "[-2.56257136  5.39185421]\n",
      "Gradient Descent(3690/9): loss=15.717812811428098, w0=74.57778807405691, w1=10.340880717334747\n",
      "[ 1.2484429  -4.90885628]\n",
      "Gradient Descent(3691/9): loss=21.136176190060105, w0=73.70387804220593, w1=13.777080112087289\n",
      "[-1.20051146  0.96783564]\n",
      "Gradient Descent(3692/9): loss=15.51413361392834, w0=74.54423606687178, w1=13.099595162529305\n",
      "[-0.23688961 -2.18742251]\n",
      "Gradient Descent(3693/9): loss=16.23977506951709, w0=74.71005879472146, w1=14.630790922934935\n",
      "[ 2.38340224  1.64097635]\n",
      "Gradient Descent(3694/9): loss=17.051100419236043, w0=73.04167722511353, w1=13.482107480104427\n",
      "[-0.39379077 -0.57868546]\n",
      "Gradient Descent(3695/9): loss=15.417704450709737, w0=73.31733076374837, w1=13.887187300418342\n",
      "[ 2.61413908  0.17805313]\n",
      "Gradient Descent(3696/9): loss=15.469179736868545, w0=71.48743341078345, w1=13.762550111796298\n",
      "[-5.25125148  0.70187384]\n",
      "Gradient Descent(3697/9): loss=17.057586959828058, w0=75.16330944673511, w1=13.27123842572013\n",
      "[ 4.40243797 -1.77138598]\n",
      "Gradient Descent(3698/9): loss=17.154923284169726, w0=72.08160286723545, w1=14.511208611591508\n",
      "[ 1.63083415  0.04839668]\n",
      "Gradient Descent(3699/9): loss=16.65273889238781, w0=70.94001896471389, w1=14.477330936778888\n",
      "[-4.35863142  2.71621358]\n",
      "Gradient Descent(3700/9): loss=18.653938961106107, w0=73.99106095737491, w1=12.575981433098017\n",
      "[-1.95481649  0.33929934]\n",
      "Gradient Descent(3701/9): loss=16.037254092196164, w0=75.35943250377491, w1=12.338471895788242\n",
      "[ 5.82049405 -0.47357073]\n",
      "Gradient Descent(3702/9): loss=18.17026966924103, w0=71.28508666622571, w1=12.669971407515744\n",
      "[-0.72911493  2.99136877]\n",
      "Gradient Descent(3703/9): loss=17.731437837955177, w0=71.7954671157696, w1=10.576013265566957\n",
      "[ 0.88351305 -2.52888469]\n",
      "Gradient Descent(3704/9): loss=20.72430582527218, w0=71.17700798262959, w1=12.346232547264284\n",
      "[-3.51405779 -0.73367621]\n",
      "Gradient Descent(3705/9): loss=18.268938679693836, w0=73.63684843619993, w1=12.859805891884486\n",
      "[ 1.72058555 -1.52079556]\n",
      "Gradient Descent(3706/9): loss=15.63682919952179, w0=72.43243855201888, w1=13.924362784786656\n",
      "[ 0.30949153  0.22381728]\n",
      "Gradient Descent(3707/9): loss=15.855821703003274, w0=72.21579447942102, w1=13.767690691204074\n",
      "[-3.00123569  4.10505063]\n",
      "Gradient Descent(3708/9): loss=16.008533084440277, w0=74.31665946348637, w1=10.894155248018972\n",
      "[ 0.10671753 -4.38116554]\n",
      "Gradient Descent(3709/9): loss=19.251436809831926, w0=74.2419571924351, w1=13.960971123890912\n",
      "[ 1.23249063  4.52256916]\n",
      "Gradient Descent(3710/9): loss=15.9510781927031, w0=73.37921375206999, w1=10.795172709807462\n",
      "[-0.3155243  -0.85258188]\n",
      "Gradient Descent(3711/9): loss=18.992901978174444, w0=73.60008075957064, w1=11.391980022723281\n",
      "[ 0.39043745 -3.94868651]\n",
      "Gradient Descent(3712/9): loss=17.61206777382832, w0=73.32677454503818, w1=14.156060579222565\n",
      "[-2.38585461  1.00369176]\n",
      "Gradient Descent(3713/9): loss=15.61515091972204, w0=74.99687277418327, w1=13.453476346629795\n",
      "[ 3.2193384   1.55274437]\n",
      "Gradient Descent(3714/9): loss=16.836252701056267, w0=72.74333589532931, w1=12.366555286287694\n",
      "[ 2.39675984 -0.62378394]\n",
      "Gradient Descent(3715/9): loss=16.157019818169175, w0=71.06560400665441, w1=12.803204045876436\n",
      "[-3.21465585  0.02564103]\n",
      "Gradient Descent(3716/9): loss=18.09742021352416, w0=73.31586310464327, w1=12.785255327178985\n",
      "[ 0.53224731 -2.25034764]\n",
      "Gradient Descent(3717/9): loss=15.627263912113651, w0=72.94328998774296, w1=14.360498675976904\n",
      "[-0.23901964  0.12844258]\n",
      "Gradient Descent(3718/9): loss=15.835251474734017, w0=73.11060373235333, w1=14.270588869057802\n",
      "[-1.70744894 -0.25157776]\n",
      "Gradient Descent(3719/9): loss=15.715433429824461, w0=74.30581798824235, w1=14.446693304153516\n",
      "[-2.30730528 -1.04392044]\n",
      "Gradient Descent(3720/9): loss=16.365380612874684, w0=75.92093168598296, w1=15.177437615481816\n",
      "[ 2.39672958  0.8924508 ]\n",
      "Gradient Descent(3721/9): loss=20.2776132026628, w0=74.24322098228896, w1=14.552722055195208\n",
      "[ 1.02252593  2.38202969]\n",
      "Gradient Descent(3722/9): loss=16.412146968245857, w0=73.52745283231232, w1=12.885301269569554\n",
      "[-1.60321696 -0.70267542]\n",
      "Gradient Descent(3723/9): loss=15.5898185099457, w0=74.6497047008304, w1=13.377174061728082\n",
      "[ 3.33329101  0.97459515]\n",
      "Gradient Descent(3724/9): loss=16.310218290906207, w0=72.3164009970036, w1=12.69495745675968\n",
      "[ 0.07992343  0.19540561]\n",
      "Gradient Descent(3725/9): loss=16.1715817144647, w0=72.26045459874787, w1=12.558173528867119\n",
      "[-2.77783902  0.3271135 ]\n",
      "Gradient Descent(3726/9): loss=16.344532283478664, w0=74.20494191003128, w1=12.329194075823844\n",
      "[ 0.07077528  0.75090758]\n",
      "Gradient Descent(3727/9): loss=16.462712752536333, w0=74.15539921221854, w1=11.80355877140861\n",
      "[ 1.66405112 -0.45639284]\n",
      "Gradient Descent(3728/9): loss=17.161704912568805, w0=72.99056342631043, w1=12.123033760316217\n",
      "[-3.46461996  2.06008333]\n",
      "Gradient Descent(3729/9): loss=16.35218959473953, w0=75.4157974002878, w1=10.680975431107155\n",
      "[ 4.36178146 -2.98542127]\n",
      "Gradient Descent(3730/9): loss=21.553529879984616, w0=72.36255037672788, w1=12.770770323221498\n",
      "[ 0.03185712  0.10835239]\n",
      "Gradient Descent(3731/9): loss=16.070913880027103, w0=72.3402503956863, w1=12.694923653017032\n",
      "[-0.2478549  -2.27997517]\n",
      "Gradient Descent(3732/9): loss=16.148579351428758, w0=72.51374882398116, w1=14.290906270887113\n",
      "[-3.02392798  1.12017593]\n",
      "Gradient Descent(3733/9): loss=16.019240682460985, w0=74.63049840793798, w1=13.50678311860167\n",
      "[-0.80961962 -0.89775995]\n",
      "Gradient Descent(3734/9): loss=16.279472524099475, w0=75.19723214193384, w1=14.135215086309028\n",
      "[ 2.29505565  3.09042049]\n",
      "Gradient Descent(3735/9): loss=17.412024475960433, w0=73.59069318838391, w1=11.971920744847354\n",
      "[ 0.31168675 -3.2938476 ]\n",
      "Gradient Descent(3736/9): loss=16.56664232776221, w0=73.37251246279057, w1=14.2776140656091\n",
      "[-0.78820002  4.1351342 ]\n",
      "Gradient Descent(3737/9): loss=15.707299605157838, w0=73.92425247777437, w1=11.383020126257524\n",
      "[ 3.60969183 -5.06425568]\n",
      "Gradient Descent(3738/9): loss=17.78260544185504, w0=71.39746819701492, w1=14.927999103922042\n",
      "[ 1.52708985  0.50519429]\n",
      "Gradient Descent(3739/9): loss=18.232923523954604, w0=70.32850529854235, w1=14.574363099778939\n",
      "[-1.62800042  1.31766668]\n",
      "Gradient Descent(3740/9): loss=20.381866020676615, w0=71.46810558966838, w1=13.651996423274563\n",
      "[-1.81513516  0.93358427]\n",
      "Gradient Descent(3741/9): loss=17.067531541100987, w0=72.73870020021708, w1=12.998487434732603\n",
      "[-3.26575499 -1.55112454]\n",
      "Gradient Descent(3742/9): loss=15.655812243911246, w0=75.02472869141839, w1=14.084274610105862\n",
      "[ 3.32662839 -1.13841871]\n",
      "Gradient Descent(3743/9): loss=17.066481478506045, w0=72.69608882184289, w1=14.881167709572981\n",
      "[-0.48094198 -2.34076706]\n",
      "Gradient Descent(3744/9): loss=16.54662856787023, w0=73.03274820507896, w1=16.519704648920715\n",
      "[ 1.07005043  3.1658785 ]\n",
      "Gradient Descent(3745/9): loss=20.04077007533853, w0=72.2837129043011, w1=14.303589701591381\n",
      "[-0.50830164 -0.82949038]\n",
      "Gradient Descent(3746/9): loss=16.235535954684543, w0=72.63952405091649, w1=14.88423296552204\n",
      "[ 0.37945595  0.90955855]\n",
      "Gradient Descent(3747/9): loss=16.586345168433724, w0=72.37390488341589, w1=14.247541982780618\n",
      "[ 2.0114602   3.19732771]\n",
      "Gradient Descent(3748/9): loss=16.103884725400984, w0=70.96588274266173, w1=12.009412588481581\n",
      "[-0.9246128  -2.29121064]\n",
      "Gradient Descent(3749/9): loss=19.176662084904358, w0=71.6131117043232, w1=13.613260037507516\n",
      "[-3.83501814 -2.79976615]\n",
      "Gradient Descent(3750/9): loss=16.8073669784636, w0=74.2976243995497, w1=15.573096344037747\n",
      "[ 4.98619689  3.16359928]\n",
      "Gradient Descent(3751/9): loss=18.080725215479337, w0=70.80728657465026, w1=13.358576849559403\n",
      "[-0.94338047 -1.40530142]\n",
      "Gradient Descent(3752/9): loss=18.484902658395065, w0=71.46765290602923, w1=14.342287843753613\n",
      "[-2.66638818  1.56772441]\n",
      "Gradient Descent(3753/9): loss=17.425535442373132, w0=73.3341246322645, w1=13.244880759896345\n",
      "[ 0.31008984 -1.49329429]\n",
      "Gradient Descent(3754/9): loss=15.414268952378686, w0=73.11706174667682, w1=14.290186763166394\n",
      "[-0.0301968  -0.91750415]\n",
      "Gradient Descent(3755/9): loss=15.72996196212175, w0=73.13819950344433, w1=14.932439670554817\n",
      "[ 0.5044881   0.88166165]\n",
      "Gradient Descent(3756/9): loss=16.453220827601275, w0=72.7850578306567, w1=14.315276518379846\n",
      "[ 2.29541388  2.72706596]\n",
      "Gradient Descent(3757/9): loss=15.864442910047734, w0=71.17826811200882, w1=12.406330346514626\n",
      "[-5.2801904   0.76791536]\n",
      "Gradient Descent(3758/9): loss=18.199958114098198, w0=74.87440139066243, w1=11.868789592860102\n",
      "[ 0.75317901 -0.68496765]\n",
      "Gradient Descent(3759/9): loss=17.932381619302937, w0=74.34717608486372, w1=12.348266944478787\n",
      "[ 4.18229128  1.20210511]\n",
      "Gradient Descent(3760/9): loss=16.58064439925116, w0=71.41957218963127, w1=11.506793368439961\n",
      "[-0.72478412 -2.75181678]\n",
      "Gradient Descent(3761/9): loss=19.08868630016637, w0=71.9269210707902, w1=13.433065113918769\n",
      "[-0.27538312 -0.35377681]\n",
      "Gradient Descent(3762/9): loss=16.321321628218943, w0=72.11968925410224, w1=13.680708880270842\n",
      "[-4.52606193  2.28182843]\n",
      "Gradient Descent(3763/9): loss=16.09549892757864, w0=75.28793260396687, w1=12.083428976930549\n",
      "[ 3.12758735 -2.07758395]\n",
      "Gradient Descent(3764/9): loss=18.348730756621684, w0=73.09862145772875, w1=13.537737745260062\n",
      "[ 0.56953049  3.47909932]\n",
      "Gradient Descent(3765/9): loss=15.40664248846229, w0=72.69995011355589, w1=11.102368219469414\n",
      "[-1.6874563  -3.31938841]\n",
      "Gradient Descent(3766/9): loss=18.388171930555146, w0=73.88116952382843, w1=13.42594010789638\n",
      "[ 0.03528765 -2.35819582]\n",
      "Gradient Descent(3767/9): loss=15.559763426294925, w0=73.85646816893484, w1=15.076677184447451\n",
      "[-2.17467407  0.45379263]\n",
      "Gradient Descent(3768/9): loss=16.819265169243135, w0=75.37874001478924, w1=14.759022341486608\n",
      "[ 1.52779542 -1.43248184]\n",
      "Gradient Descent(3769/9): loss=18.377437860266742, w0=74.30928321916697, w1=15.76175963006457\n",
      "[ 0.70102357  1.47852844]\n",
      "Gradient Descent(3770/9): loss=18.505236769662016, w0=73.81856671927132, w1=14.726789720768416\n",
      "[-0.84726606  2.20943202]\n",
      "Gradient Descent(3771/9): loss=16.30111478680798, w0=74.41165296280424, w1=13.180187306881082\n",
      "[ 1.90272251  1.82371226]\n",
      "Gradient Descent(3772/9): loss=16.05540677026606, w0=73.07974720249871, w1=11.90358872763552\n",
      "[ 1.36454409 -2.65636585]\n",
      "Gradient Descent(3773/9): loss=16.65090626166355, w0=72.12456633838721, w1=13.763044821241712\n",
      "[-2.34374778  0.2278928 ]\n",
      "Gradient Descent(3774/9): loss=16.10972282351387, w0=73.76518978421811, w1=13.603519858167987\n",
      "[ 2.79904318 -1.11845009]\n",
      "Gradient Descent(3775/9): loss=15.50459866907532, w0=71.80585955738466, w1=14.38643492032719\n",
      "[-1.86772535  3.10710228]\n",
      "Gradient Descent(3776/9): loss=16.9041256212321, w0=73.1132673057164, w1=12.211463327071204\n",
      "[-0.70689042 -3.69346042]\n",
      "Gradient Descent(3777/9): loss=16.206433828360268, w0=73.60809060142675, w1=14.796885624344679\n",
      "[ 1.1597749  4.2366118]\n",
      "Gradient Descent(3778/9): loss=16.30271142860788, w0=72.796248168091, w1=11.8312573651517\n",
      "[ 1.78996539 -0.3402035 ]\n",
      "Gradient Descent(3779/9): loss=16.868429549996822, w0=71.54327239527812, w1=12.069399818473885\n",
      "[-3.25556829 -3.09898772]\n",
      "Gradient Descent(3780/9): loss=17.912765729922203, w0=73.82217019742765, w1=14.238691219543632\n",
      "[ 2.27004822  1.47116389]\n",
      "Gradient Descent(3781/9): loss=15.813435344462096, w0=72.23313644114391, w1=13.208876499621192\n",
      "[-2.22318614  2.89030468]\n",
      "Gradient Descent(3782/9): loss=15.985196923944656, w0=73.78936673997426, w1=11.185663223786857\n",
      "[ 0.13711508 -2.68169889]\n",
      "Gradient Descent(3783/9): loss=18.139951504679175, w0=73.69338618296243, w1=13.062852447570382\n",
      "[ 1.93811644 -0.04130703]\n",
      "Gradient Descent(3784/9): loss=15.552559809278717, w0=72.3367046760401, w1=13.09176736696728\n",
      "[-1.13402405  0.02029362]\n",
      "Gradient Descent(3785/9): loss=15.919271061390207, w0=73.13052150764518, w1=13.077561833216093\n",
      "[-1.01922506 -0.42063856]\n",
      "Gradient Descent(3786/9): loss=15.480100282877462, w0=73.8439790501236, w1=13.372008826138496\n",
      "[-0.56599402 -1.54670245]\n",
      "Gradient Descent(3787/9): loss=15.542969280546478, w0=74.2401748614193, w1=14.454700544028597\n",
      "[ 0.6959692   1.06803201]\n",
      "Gradient Descent(3788/9): loss=16.30888601209373, w0=73.75299642075905, w1=13.707078137116788\n",
      "[ 2.73852469  1.27523054]\n",
      "Gradient Descent(3789/9): loss=15.51711011101261, w0=71.83602913684946, w1=12.814416756812225\n",
      "[-2.79298366 -2.30429802]\n",
      "Gradient Descent(3790/9): loss=16.66992284181156, w0=73.79111769995103, w1=14.427425367442664\n",
      "[ 1.82370318 -0.6059484 ]\n",
      "Gradient Descent(3791/9): loss=15.958569550977526, w0=72.51452547539928, w1=14.85158924783254\n",
      "[-3.1736233   1.64742809]\n",
      "Gradient Descent(3792/9): loss=16.630640336558827, w0=74.73606178746915, w1=13.698389588078776\n",
      "[ 1.97904309  0.40467389]\n",
      "Gradient Descent(3793/9): loss=16.44968129773592, w0=73.35073162251201, w1=13.415117867601143\n",
      "[-1.11380344 -2.4977925 ]\n",
      "Gradient Descent(3794/9): loss=15.3895877643828, w0=74.13039403119683, w1=15.163572616459897\n",
      "[-0.97243067 -0.45327516]\n",
      "Gradient Descent(3795/9): loss=17.15342315192726, w0=74.81109549972656, w1=15.480865225016796\n",
      "[ 1.97603127  2.42915481]\n",
      "Gradient Descent(3796/9): loss=18.539101824289652, w0=73.4278736096051, w1=13.78045685883161\n",
      "[ 2.13738882  0.72080863]\n",
      "Gradient Descent(3797/9): loss=15.4400829896415, w0=71.93170143909529, w1=13.275890820830513\n",
      "[-2.30985751  0.22759574]\n",
      "Gradient Descent(3798/9): loss=16.334481925171996, w0=73.54860169328181, w1=13.11657380283748\n",
      "[-3.29769117  1.69314017]\n",
      "Gradient Descent(3799/9): loss=15.484253574458766, w0=75.85698551246323, w1=11.931375683559251\n",
      "[ 3.20797683 -2.49199749]\n",
      "Gradient Descent(3800/9): loss=19.86920849580792, w0=73.61140172966283, w1=13.675773926815182\n",
      "[-2.30325041  0.08366041]\n",
      "Gradient Descent(3801/9): loss=15.455504611822983, w0=75.223677016766, w1=13.617211642508124\n",
      "[ 5.42224762  4.95537738]\n",
      "Gradient Descent(3802/9): loss=17.257318093167868, w0=71.4281036825927, w1=10.148447478923762\n",
      "[-4.02009087 -4.44627874]\n",
      "Gradient Descent(3803/9): loss=22.675189973297915, w0=74.2421672913045, w1=13.26084259821174\n",
      "[-1.6641188   1.68343367]\n",
      "Gradient Descent(3804/9): loss=15.859424435799202, w0=75.407050450031, w1=12.082439029177639\n",
      "[ 2.77039663 -0.37607077]\n",
      "Gradient Descent(3805/9): loss=18.59473027283997, w0=73.46777280877087, w1=12.345688569451108\n",
      "[-2.0068789  -0.51576338]\n",
      "Gradient Descent(3806/9): loss=16.04400498412336, w0=74.87258803690891, w1=12.706722933163858\n",
      "[ 1.52341219 -2.74368664]\n",
      "Gradient Descent(3807/9): loss=16.93073747851683, w0=73.80619950307438, w1=14.627303578835626\n",
      "[ 2.96176835  5.13545969]\n",
      "Gradient Descent(3808/9): loss=16.175584704546566, w0=71.73296166093819, w1=11.032481796524271\n",
      "[ 0.55784978 -2.70532867]\n",
      "Gradient Descent(3809/9): loss=19.598655361097858, w0=71.34246681827373, w1=12.926211866804376\n",
      "[-1.39949814 -0.39783723]\n",
      "Gradient Descent(3810/9): loss=17.4431579755711, w0=72.32211551410053, w1=13.204697927742142\n",
      "[-0.52980509 -0.48348439]\n",
      "Gradient Descent(3811/9): loss=15.895908283491506, w0=72.69297907700488, w1=13.543136997935752\n",
      "[-3.24279956  0.04863288]\n",
      "Gradient Descent(3812/9): loss=15.56846540603595, w0=74.96293877226641, w1=13.509093982946665\n",
      "[ 1.52413538 -0.9808568 ]\n",
      "Gradient Descent(3813/9): loss=16.779127996049294, w0=73.89604400629719, w1=14.195693741843048\n",
      "[-1.25155408  0.23790443]\n",
      "Gradient Descent(3814/9): loss=15.823477938677676, w0=74.7721318595799, w1=14.029160641403754\n",
      "[ 2.11435402 -0.06930101]\n",
      "Gradient Descent(3815/9): loss=16.629386725963272, w0=73.29208404457773, w1=14.077671351315637\n",
      "[-0.17778095  0.33234767]\n",
      "Gradient Descent(3816/9): loss=15.564666990680571, w0=73.41653070969396, w1=13.845027983090654\n",
      "[-1.59528493  0.3502941 ]\n",
      "Gradient Descent(3817/9): loss=15.460132041260081, w0=74.5332301603279, w1=13.599822110360094\n",
      "[ 1.93520686  0.94171737]\n",
      "Gradient Descent(3818/9): loss=16.16104339140696, w0=73.17858535815715, w1=12.940619953343187\n",
      "[ 1.65722707  0.44076047]\n",
      "Gradient Descent(3819/9): loss=15.537849491431547, w0=72.018526410644, w1=12.632087623467784\n",
      "[-2.17056695  0.37614452]\n",
      "Gradient Descent(3820/9): loss=16.558438736741945, w0=73.53792327623047, w1=12.368786457643733\n",
      "[ 2.90687428  0.1291605 ]\n",
      "Gradient Descent(3821/9): loss=16.032734443287104, w0=71.50311128154144, w1=12.278374105699907\n",
      "[ 0.75716544 -1.11700186]\n",
      "Gradient Descent(3822/9): loss=17.710996277982048, w0=70.97309547246849, w1=13.060275406858354\n",
      "[-0.60620497 -0.35565313]\n",
      "Gradient Descent(3823/9): loss=18.166969469445725, w0=71.39743895006544, w1=13.309232598835885\n",
      "[ 0.50207927 -0.97755283]\n",
      "Gradient Descent(3824/9): loss=17.198743539433814, w0=71.04598346348462, w1=13.993519581481062\n",
      "[-2.12804592  1.53755093]\n",
      "Gradient Descent(3825/9): loss=18.04450059743033, w0=72.53561560891143, w1=12.91723392921854\n",
      "[ 0.97530876 -0.48263001]\n",
      "Gradient Descent(3826/9): loss=15.831593196535577, w0=71.85289947807537, w1=13.255074939676028\n",
      "[-1.25789929  0.1995349 ]\n",
      "Gradient Descent(3827/9): loss=16.44939182836029, w0=72.7334289841352, w1=13.115400508861175\n",
      "[ 1.50520656  0.31061629]\n",
      "Gradient Descent(3828/9): loss=15.609325670185452, w0=71.6797843915753, w1=12.897969108001462\n",
      "[-0.79304379 -1.50298779]\n",
      "Gradient Descent(3829/9): loss=16.857820630940267, w0=72.23491504697972, w1=13.950060559261255\n",
      "[-1.56788457  1.83364418]\n",
      "Gradient Descent(3830/9): loss=16.05724941333465, w0=73.33243424317482, w1=12.66650963379334\n",
      "[-2.87251345 -2.08383265]\n",
      "Gradient Descent(3831/9): loss=15.717278863121779, w0=75.34319365659303, w1=14.12519249118306\n",
      "[-1.27591442 -0.67659665]\n",
      "Gradient Descent(3832/9): loss=17.69396727724518, w0=76.23633375037052, w1=14.59881014902776\n",
      "[ 1.14590253  3.18494253]\n",
      "Gradient Descent(3833/9): loss=20.340971163777645, w0=75.4342019798174, w1=12.369350375221305\n",
      "[-2.16588367 -1.17719813]\n",
      "Gradient Descent(3834/9): loss=18.29273901221313, w0=76.95032055145808, w1=13.193389069083183\n",
      "[ 2.50982618 -0.02144184]\n",
      "Gradient Descent(3835/9): loss=22.111503579616187, w0=75.19344222352272, w1=13.208398357951852\n",
      "[ 2.63389784 -2.07088264]\n",
      "Gradient Descent(3836/9): loss=17.22678206881573, w0=73.34971373820811, w1=14.658016209035601\n",
      "[ 1.59015539 -0.16552669]\n",
      "Gradient Descent(3837/9): loss=16.081644119704265, w0=72.23660496537273, w1=14.773884890489347\n",
      "[-3.53603259  0.8083402 ]\n",
      "Gradient Descent(3838/9): loss=16.782288699199587, w0=74.71182777586931, w1=14.208046750815141\n",
      "[-1.66346184  0.11851711]\n",
      "Gradient Descent(3839/9): loss=16.656351698271145, w0=75.87625106381357, w1=14.125084772167407\n",
      "[ 0.11754889  0.86435573]\n",
      "Gradient Descent(3840/9): loss=18.928352287098765, w0=75.79396684049635, w1=13.520035761556736\n",
      "[ 3.04337726  2.09075558]\n",
      "Gradient Descent(3841/9): loss=18.51181295114529, w0=73.66360276048908, w1=12.056506852586217\n",
      "[-1.16685073 -4.04407015]\n",
      "Gradient Descent(3842/9): loss=16.466976865280337, w0=74.48039827074423, w1=14.88735595668438\n",
      "[ 0.52737765  1.3328835 ]\n",
      "Gradient Descent(3843/9): loss=17.080480978936627, w0=74.11123391654975, w1=13.954337507175833\n",
      "[ 0.67440527  0.68343667]\n",
      "Gradient Descent(3844/9): loss=15.832521731150072, w0=73.63915022467997, w1=13.475931839198683\n",
      "[-2.73993921 -1.5686746 ]\n",
      "Gradient Descent(3845/9): loss=15.445486278112734, w0=75.55710767452409, w1=14.574004058324032\n",
      "[ 1.73872696 -0.01754616]\n",
      "Gradient Descent(3846/9): loss=18.545629641201046, w0=74.33999880097019, w1=14.586286370128178\n",
      "[ 1.33230256  0.43073009]\n",
      "Gradient Descent(3847/9): loss=16.545279140355873, w0=73.40738700818233, w1=14.284775309257071\n",
      "[ 0.36098403  4.04496581]\n",
      "Gradient Descent(3848/9): loss=15.716388138393784, w0=73.15469818648018, w1=11.453299239872823\n",
      "[-1.17614434  0.5383387 ]\n",
      "Gradient Descent(3849/9): loss=17.44875472291857, w0=73.97799922315329, w1=11.076462150030222\n",
      "[ 0.68746712  0.0756771 ]\n",
      "Gradient Descent(3850/9): loss=18.50767465708519, w0=73.49677224110059, w1=11.023488178685716\n",
      "[-1.74745828 -1.03541208]\n",
      "Gradient Descent(3851/9): loss=18.422980777186073, w0=74.71999303807647, w1=11.74827663153041\n",
      "[-1.50388578 -1.20715667]\n",
      "Gradient Descent(3852/9): loss=17.90166213939673, w0=75.77271308323935, w1=12.593286302157333\n",
      "[ 3.0442759   0.82919024]\n",
      "Gradient Descent(3853/9): loss=18.850966125268034, w0=73.64171995094652, w1=12.012853131154696\n",
      "[ 2.93620121 -2.07426072]\n",
      "Gradient Descent(3854/9): loss=16.522207684061225, w0=71.58637910680388, w1=13.464835636050319\n",
      "[-4.15769427 -4.56392317]\n",
      "Gradient Descent(3855/9): loss=16.843849898049726, w0=74.49676509285811, w1=16.659581853091872\n",
      "[-2.89694753  2.23487407]\n",
      "Gradient Descent(3856/9): loss=21.16508837740821, w0=76.52462836682996, w1=15.095170004367844\n",
      "[ 2.85094314 -1.72732164]\n",
      "Gradient Descent(3857/9): loss=21.90947125559758, w0=74.52896816537636, w1=16.30429515445949\n",
      "[ 3.79538447  2.99681434]\n",
      "Gradient Descent(3858/9): loss=20.137691151100327, w0=71.8721990387879, w1=14.206525114060504\n",
      "[-2.8348748  -2.56991095]\n",
      "Gradient Descent(3859/9): loss=16.660664296270767, w0=73.8566114022868, w1=16.005462781969545\n",
      "[ 1.05589358  1.43845796]\n",
      "Gradient Descent(3860/9): loss=18.73390495700382, w0=73.11748589945294, w1=14.998542210404214\n",
      "[ 1.37076394  2.09087023]\n",
      "Gradient Descent(3861/9): loss=16.554874661332796, w0=72.15795113932862, w1=13.534933049796585\n",
      "[-4.19748681  0.56916397]\n",
      "Gradient Descent(3862/9): loss=16.032627427517937, w0=75.09619190732539, w1=13.13651827097994\n",
      "[-3.17163855  3.90113492]\n",
      "Gradient Descent(3863/9): loss=17.068867391565572, w0=77.3163388925609, w1=10.405723826977988\n",
      "[ 4.50764179 -3.70012371]\n",
      "Gradient Descent(3864/9): loss=28.200509670232027, w0=74.16098964023452, w1=12.995810420754417\n",
      "[-2.00059819  1.58465921]\n",
      "Gradient Descent(3865/9): loss=15.878871593065153, w0=75.56140837507421, w1=11.886548972002101\n",
      "[ 0.28909995 -1.49454179]\n",
      "Gradient Descent(3866/9): loss=19.225720004527783, w0=75.35903841095293, w1=12.932728227201299\n",
      "[ 0.92856108 -0.38937126]\n",
      "Gradient Descent(3867/9): loss=17.6678366216601, w0=74.70904565394527, w1=13.205288111272175\n",
      "[-0.01274982 -3.45886798]\n",
      "Gradient Descent(3868/9): loss=16.424829698551733, w0=74.71797052813473, w1=15.626495694625186\n",
      "[-1.73801654  0.36432899]\n",
      "Gradient Descent(3869/9): loss=18.70418415299983, w0=75.9345821041469, w1=15.37146540465158\n",
      "[-0.34820787 -0.47110678]\n",
      "Gradient Descent(3870/9): loss=20.66179540520037, w0=76.1783276115002, w1=15.70124015150872\n",
      "[ 2.81367836 -0.59380273]\n",
      "Gradient Descent(3871/9): loss=22.013378426216633, w0=74.20875276072576, w1=16.11690206131303\n",
      "[-0.64144232  0.83856885]\n",
      "Gradient Descent(3872/9): loss=19.281730089883954, w0=74.65776238176657, w1=15.529903865106265\n",
      "[ 4.39760991 -1.401802  ]\n",
      "Gradient Descent(3873/9): loss=18.41756060948989, w0=71.57943544593694, w1=16.511165261877064\n",
      "[ 1.59270769  2.23676094]\n",
      "Gradient Descent(3874/9): loss=21.450473065293917, w0=70.46454006395707, w1=14.945432604514545\n",
      "[-4.73381777  2.9777374 ]\n",
      "Gradient Descent(3875/9): loss=20.462756752465722, w0=73.7782125003753, w1=12.861016425621274\n",
      "[ 0.90377076  2.21900694]\n",
      "Gradient Descent(3876/9): loss=15.694548888190562, w0=73.14557296655012, w1=11.30771156706067\n",
      "[ 0.39969634 -2.7064426 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3877/9): loss=17.755685472145274, w0=72.86578552725994, w1=13.202221383846808\n",
      "[ 0.77255971 -0.1301504 ]\n",
      "Gradient Descent(3878/9): loss=15.51603893110787, w0=72.32499373317806, w1=13.293326665849401\n",
      "[-3.40228386 -0.11913512]\n",
      "Gradient Descent(3879/9): loss=15.87266869146135, w0=74.70659243466929, w1=13.376721251583518\n",
      "[ 0.92217007 -1.08939122]\n",
      "Gradient Descent(3880/9): loss=16.389010336279455, w0=74.06107338492178, w1=14.139295109005296\n",
      "[ 1.56534159 -0.97975888]\n",
      "Gradient Descent(3881/9): loss=15.897673142839318, w0=72.9653342727035, w1=14.825126323788107\n",
      "[ 0.92728517 -0.17386854]\n",
      "Gradient Descent(3882/9): loss=16.34494208287278, w0=72.31623465139351, w1=14.9468343047452\n",
      "[-1.99105836  1.7932897 ]\n",
      "Gradient Descent(3883/9): loss=16.940047437058606, w0=73.70997550531219, w1=13.69153151433355\n",
      "[-0.16691019  0.48603374]\n",
      "Gradient Descent(3884/9): loss=15.494871788781985, w0=73.8268126387137, w1=13.351307895268624\n",
      "[-0.63008184  0.43835875]\n",
      "Gradient Descent(3885/9): loss=15.536117947032324, w0=74.2678699252605, w1=13.044456767270136\n",
      "[ 0.41968674 -0.16324676]\n",
      "Gradient Descent(3886/9): loss=15.95489889547939, w0=73.97408920608773, w1=13.158729499852013\n",
      "[-0.36056634 -0.78432926]\n",
      "Gradient Descent(3887/9): loss=15.668716603840704, w0=74.22648564692653, w1=13.707759982295626\n",
      "[ 0.25416379  2.36755999]\n",
      "Gradient Descent(3888/9): loss=15.84672818656691, w0=74.04857099483493, w1=12.050467990788032\n",
      "[-0.51462282  1.0843192 ]\n",
      "Gradient Descent(3889/9): loss=16.692005260583144, w0=74.40880696663048, w1=11.291444550613802\n",
      "[ 4.8033681  -4.62886341]\n",
      "Gradient Descent(3890/9): loss=18.401630277785742, w0=71.04644929690073, w1=14.531648934756227\n",
      "[-0.10340497  1.47374121]\n",
      "Gradient Descent(3891/9): loss=18.464739848920136, w0=71.11883277422025, w1=13.500030084581528\n",
      "[-2.53640455  0.01462604]\n",
      "Gradient Descent(3892/9): loss=17.75160084690243, w0=72.89431595804152, w1=13.489791856024796\n",
      "[ 3.25915397 -3.88739916]\n",
      "Gradient Descent(3893/9): loss=15.465781161419715, w0=70.61290817977435, w1=16.21097126975754\n",
      "[-4.30228621  2.01472308]\n",
      "Gradient Descent(3894/9): loss=22.709692837844383, w0=73.62450852350948, w1=14.800665113258894\n",
      "[ 5.42749267  1.70601854]\n",
      "Gradient Descent(3895/9): loss=16.312989582010633, w0=69.8252636546375, w1=13.606452138261634\n",
      "[-4.93846726 -0.95430808]\n",
      "Gradient Descent(3896/9): loss=21.40971471075087, w0=73.28219073629727, w1=14.274467792976976\n",
      "[-0.06459781  0.35801179]\n",
      "Gradient Descent(3897/9): loss=15.701774719653386, w0=73.32740920643647, w1=14.023859540750747\n",
      "[-3.97250608  2.56020391]\n",
      "Gradient Descent(3898/9): loss=15.53449660161078, w0=76.10816346276981, w1=12.231716807044537\n",
      "[ 3.34431771 -3.40815109]\n",
      "Gradient Descent(3899/9): loss=20.12461191197558, w0=73.76714106695302, w1=14.617422568681263\n",
      "[-1.93270252  1.70835275]\n",
      "Gradient Descent(3900/9): loss=16.145048184650108, w0=75.12003283209346, w1=13.421575645200777\n",
      "[ 1.16717667  1.90460438]\n",
      "Gradient Descent(3901/9): loss=17.054918193693062, w0=74.30300916439491, w1=12.088352579102581\n",
      "[ 0.5061682  -0.51229294]\n",
      "Gradient Descent(3902/9): loss=16.862957443664566, w0=73.94869142129997, w1=12.44695763786567\n",
      "[ 1.11736796 -2.45641691]\n",
      "Gradient Descent(3903/9): loss=16.13354060047641, w0=73.16653385153639, w1=14.166449475696211\n",
      "[-2.63377041 -0.17516077]\n",
      "Gradient Descent(3904/9): loss=15.629805620821685, w0=75.01017313571664, w1=14.289062016166094\n",
      "[ 0.39752634  2.67864297]\n",
      "Gradient Descent(3905/9): loss=17.186170217916423, w0=74.73190469670764, w1=12.414011934909118\n",
      "[ 1.22323166  0.22616142]\n",
      "Gradient Descent(3906/9): loss=16.98764376175278, w0=73.87564253755615, w1=12.255698943330026\n",
      "[ 1.10292639 -0.00759285]\n",
      "Gradient Descent(3907/9): loss=16.304191773393725, w0=73.10359406344554, w1=12.261013936403474\n",
      "[-2.28327359 -1.38931218]\n",
      "Gradient Descent(3908/9): loss=16.14661324617398, w0=74.7018855781328, w1=13.233532461711457\n",
      "[ 2.73944352 -1.91169546]\n",
      "Gradient Descent(3909/9): loss=16.407370874161114, w0=72.78427511469721, w1=14.571719280934918\n",
      "[-4.72182965  0.77355841]\n",
      "Gradient Descent(3910/9): loss=16.111997319548045, w0=76.08955586793155, w1=14.030228391064488\n",
      "[ 6.80821323  3.64636489]\n",
      "Gradient Descent(3911/9): loss=19.445206133653848, w0=71.32380660895099, w1=11.477772968195811\n",
      "[-4.42502747 -3.92288802]\n",
      "Gradient Descent(3912/9): loss=19.330446014353157, w0=74.42132583712163, w1=14.223794580189843\n",
      "[ 2.61209346 -0.1049251 ]\n",
      "Gradient Descent(3913/9): loss=16.298236691837594, w0=72.59286041430038, w1=14.297242147003248\n",
      "[ 0.04455954 -0.56182116]\n",
      "Gradient Descent(3914/9): loss=15.965808958790113, w0=72.56166873880413, w1=14.690516962113948\n",
      "[-1.5801501   4.87263159]\n",
      "Gradient Descent(3915/9): loss=16.387009091090004, w0=73.66777380794298, w1=11.279674851683449\n",
      "[-1.73120717 -1.53879587]\n",
      "Gradient Descent(3916/9): loss=17.875853139172058, w0=74.87961882709023, w1=12.356831957907398\n",
      "[ 3.35322822 -1.47053417]\n",
      "Gradient Descent(3917/9): loss=17.273535362118775, w0=72.53235907148769, w1=13.386205878929704\n",
      "[-1.60672324  1.08487587]\n",
      "Gradient Descent(3918/9): loss=15.680248655487794, w0=73.6570653368653, w1=12.6267927682504\n",
      "[-0.674787    0.26143844]\n",
      "Gradient Descent(3919/9): loss=15.81556038857453, w0=74.12941623667085, w1=12.443785857969896\n",
      "[-0.98714982 -1.54774546]\n",
      "Gradient Descent(3920/9): loss=16.271485113312938, w0=74.8204211124929, w1=13.52720768106627\n",
      "[ 2.22908725 -1.30418708]\n",
      "Gradient Descent(3921/9): loss=16.552115535036595, w0=73.2600600394992, w1=14.440138636804091\n",
      "[-1.37648386 -1.06507828]\n",
      "Gradient Descent(3922/9): loss=15.847670429651597, w0=74.22359874261018, w1=15.185693432262681\n",
      "[ 0.81205508  4.66797295]\n",
      "Gradient Descent(3923/9): loss=17.273222871276758, w0=73.65516018429602, w1=11.918112368427366\n",
      "[-1.34290771 -3.75584853]\n",
      "Gradient Descent(3924/9): loss=16.670431764908397, w0=74.59519558089252, w1=14.547206340036633\n",
      "[ 1.92083518 -0.88261355]\n",
      "Gradient Descent(3925/9): loss=16.802315950911265, w0=73.25061095654495, w1=15.165035823931658\n",
      "[ 2.52817381  1.66861264]\n",
      "Gradient Descent(3926/9): loss=16.806983254821667, w0=71.48088929064849, w1=13.997006977620098\n",
      "[-1.80380544 -0.04442813]\n",
      "Gradient Descent(3927/9): loss=17.163228497153344, w0=72.7435531003768, w1=14.028106671677218\n",
      "[-2.15409203  1.19514156]\n",
      "Gradient Descent(3928/9): loss=15.687708952240657, w0=74.25141752474798, w1=13.191507577322657\n",
      "[ 3.15573494  1.97431589]\n",
      "Gradient Descent(3929/9): loss=15.885817726761147, w0=72.04240306635083, w1=11.809486451277808\n",
      "[-1.91412781 -2.48993302]\n",
      "Gradient Descent(3930/9): loss=17.56386511043735, w0=73.38229253458042, w1=13.552439565023818\n",
      "[ 0.3818319  -0.54771266]\n",
      "Gradient Descent(3931/9): loss=15.392437162055925, w0=73.11501020614241, w1=13.93583842801184\n",
      "[-0.50777531  0.31141521]\n",
      "Gradient Descent(3932/9): loss=15.505918044952224, w0=73.47045292093665, w1=13.717847784152902\n",
      "[ 1.89344982 -1.06298932]\n",
      "Gradient Descent(3933/9): loss=15.429823673741835, w0=72.14503804586282, w1=14.461940308407069\n",
      "[-3.9584448   5.16872787]\n",
      "Gradient Descent(3934/9): loss=16.528240838944605, w0=74.91594940545107, w1=10.843830802031224\n",
      "[ 0.27542517 -1.64914056]\n",
      "Gradient Descent(3935/9): loss=20.175310308914106, w0=74.72315178835284, w1=11.998229197384028\n",
      "[ 0.40417687 -1.26799273]\n",
      "Gradient Descent(3936/9): loss=17.504633051430478, w0=74.44022797839257, w1=12.885824107337791\n",
      "[ 2.95763787 -2.98180602]\n",
      "Gradient Descent(3937/9): loss=16.219248237325687, w0=72.36988146860412, w1=14.97308831861082\n",
      "[-1.31648895  4.94381425]\n",
      "Gradient Descent(3938/9): loss=16.927899087497426, w0=73.29142373255208, w1=11.512418343963029\n",
      "[-1.09884197 -2.43063754]\n",
      "Gradient Descent(3939/9): loss=17.321014009797725, w0=74.06061311086445, w1=13.213864623962559\n",
      "[-1.0968496   0.00536903]\n",
      "Gradient Descent(3940/9): loss=15.715133026268443, w0=74.82840783142946, w1=13.210106303967649\n",
      "[ 0.73513647 -0.72060391]\n",
      "Gradient Descent(3941/9): loss=16.59955498197006, w0=74.31381230422907, w1=13.714529041655373\n",
      "[-0.3149205   2.36721941]\n",
      "Gradient Descent(3942/9): loss=15.933545402395717, w0=74.53425665710007, w1=12.057475452103615\n",
      "[ 2.07361952 -2.25298513]\n",
      "Gradient Descent(3943/9): loss=17.166481914763573, w0=73.08272299340122, w1=13.634565040711378\n",
      "[ 1.99097452 -1.07230003]\n",
      "Gradient Descent(3944/9): loss=15.42018004421767, w0=71.68904083092048, w1=14.385175058878437\n",
      "[-1.6628345   0.05590798]\n",
      "Gradient Descent(3945/9): loss=17.08364093727134, w0=72.85302498125418, w1=14.346039471052153\n",
      "[ 0.66140811  1.20420996]\n",
      "Gradient Descent(3946/9): loss=15.85834422703399, w0=72.39003930661418, w1=13.503092500857273\n",
      "[ 1.43697882  2.84283699]\n",
      "Gradient Descent(3947/9): loss=15.794663146173441, w0=71.3841541341183, w1=11.51310660454319\n",
      "[-1.00160334 -1.42618736]\n",
      "Gradient Descent(3948/9): loss=19.143263769798804, w0=72.08527646926099, w1=12.511437755545748\n",
      "[-2.29766824 -3.77041336]\n",
      "Gradient Descent(3949/9): loss=16.58507780828703, w0=73.69364423714443, w1=15.15072710595669\n",
      "[ 0.41599998  1.68449366]\n",
      "Gradient Descent(3950/9): loss=16.861921816716333, w0=73.4024442506268, w1=13.971581542822056\n",
      "[-3.02159723  3.51447195]\n",
      "Gradient Descent(3951/9): loss=15.512744017661715, w0=75.51756231071491, w1=11.511451177605883\n",
      "[ 2.36285446 -1.95448844]\n",
      "Gradient Descent(3952/9): loss=19.79520216852415, w0=73.86356418747086, w1=12.87959308505279\n",
      "[ 2.00556692 -0.08777673]\n",
      "Gradient Descent(3953/9): loss=15.728205595587447, w0=72.45966734474804, w1=12.941036796677006\n",
      "[-1.662988    0.63011421]\n",
      "Gradient Descent(3954/9): loss=15.878964007145887, w0=73.62375894298282, w1=12.499956846691656\n",
      "[ 1.44426051 -0.19181805]\n",
      "Gradient Descent(3955/9): loss=15.920244579013143, w0=72.61277658575926, w1=12.634229480632328\n",
      "[ 0.70856391  0.009216  ]\n",
      "Gradient Descent(3956/9): loss=15.975288120987816, w0=72.11678184990308, w1=12.62777827904329\n",
      "[-1.05116431 -2.4635738 ]\n",
      "Gradient Descent(3957/9): loss=16.44161324082611, w0=72.85259686678556, w1=14.352279937137233\n",
      "[ 1.08643638 -1.14452861]\n",
      "Gradient Descent(3958/9): loss=15.863958829264403, w0=72.0920914033399, w1=15.153449961403378\n",
      "[ 1.1864441   3.37501349]\n",
      "Gradient Descent(3959/9): loss=17.508784916557456, w0=71.26158053391245, w1=12.790940514958375\n",
      "[-3.31928963  2.2420927 ]\n",
      "Gradient Descent(3960/9): loss=17.688297169408678, w0=73.58508327552606, w1=11.221475623592424\n",
      "[ 2.24893841 -1.49421592]\n",
      "Gradient Descent(3961/9): loss=17.978092060572827, w0=72.01082638986543, w1=12.267426764741513\n",
      "[-0.96866685 -3.39640124]\n",
      "Gradient Descent(3962/9): loss=16.943873317047622, w0=72.68889318616402, w1=14.644907630092955\n",
      "[-1.1944539   3.12783693]\n",
      "Gradient Descent(3963/9): loss=16.2477577242356, w0=73.525010916311, w1=12.455421780982759\n",
      "[ 1.53327052 -2.39166259]\n",
      "Gradient Descent(3964/9): loss=15.937174583906128, w0=72.45172155126828, w1=14.129585595650347\n",
      "[ 0.91582274 -0.33580416]\n",
      "Gradient Descent(3965/9): loss=15.951706230998301, w0=71.810645634224, w1=14.364648507114781\n",
      "[ 0.1409939  -0.08809037]\n",
      "Gradient Descent(3966/9): loss=16.877498186461466, w0=71.71194990342339, w1=14.426311768286787\n",
      "[ 1.18682316 -0.13209026]\n",
      "Gradient Descent(3967/9): loss=17.085230878233116, w0=70.88117369119215, w1=14.518774949835409\n",
      "[-3.02414752 -0.44033218]\n",
      "Gradient Descent(3968/9): loss=18.8363905296157, w0=72.99807695388164, w1=14.827007472496046\n",
      "[-0.20851118 -0.74198933]\n",
      "Gradient Descent(3969/9): loss=16.33725197415409, w0=73.14403477746367, w1=15.346400003798955\n",
      "[ 2.19193688  2.00404104]\n",
      "Gradient Descent(3970/9): loss=17.139382198659487, w0=71.60967896415086, w1=13.94357127497047\n",
      "[-0.89823471  2.36183494]\n",
      "Gradient Descent(3971/9): loss=16.91180768599267, w0=72.2384432604299, w1=12.290286818885605\n",
      "[-4.63909505 -1.7171603 ]\n",
      "Gradient Descent(3972/9): loss=16.650272204015153, w0=75.48580979679674, w1=13.492299026845114\n",
      "[ 4.47461402  0.5831686 ]\n",
      "Gradient Descent(3973/9): loss=17.78815313223558, w0=72.35357998126554, w1=13.084081006336273\n",
      "[-0.80141159  1.80510066]\n",
      "Gradient Descent(3974/9): loss=15.906271540576714, w0=72.91456809443203, w1=11.820510544760763\n",
      "[-1.4356885  -4.16830093]\n",
      "Gradient Descent(3975/9): loss=16.83431801873141, w0=73.91955004503333, w1=14.7383211986525\n",
      "[ 2.3770042   1.17287341]\n",
      "Gradient Descent(3976/9): loss=16.37364110286367, w0=72.25564710797475, w1=13.917309811740525\n",
      "[-0.12181922  0.40779119]\n",
      "Gradient Descent(3977/9): loss=16.020640978790073, w0=72.34092056280664, w1=13.631855976986477\n",
      "[-3.69239434 -1.80021037]\n",
      "Gradient Descent(3978/9): loss=15.851567569167711, w0=74.92559659995973, w1=14.892003235750579\n",
      "[ 1.53522955  1.26387884]\n",
      "Gradient Descent(3979/9): loss=17.714351518429112, w0=73.85093591605182, w1=14.00728804812281\n",
      "[-0.71253481 -1.16626548]\n",
      "Gradient Descent(3980/9): loss=15.68018813278121, w0=74.34971028384068, w1=14.82367388302798\n",
      "[ 0.28972947 -0.34031859]\n",
      "Gradient Descent(3981/9): loss=16.846348503661837, w0=74.14689965454326, w1=15.061896896650717\n",
      "[-0.30044707  6.01759775]\n",
      "Gradient Descent(3982/9): loss=17.001327141970595, w0=74.3572126014075, w1=10.849578469103541\n",
      "[ 1.52819721 -3.00129044]\n",
      "Gradient Descent(3983/9): loss=19.409983657364048, w0=73.28747455748973, w1=12.95048177486227\n",
      "[ 2.97607842 -0.86371421]\n",
      "Gradient Descent(3984/9): loss=15.525951199409546, w0=71.20421966171772, w1=13.555081724418056\n",
      "[-2.2341117  -2.25252527]\n",
      "Gradient Descent(3985/9): loss=17.57215606943436, w0=72.76809785316289, w1=15.13184941491797\n",
      "[-0.28911087  3.77862955]\n",
      "Gradient Descent(3986/9): loss=16.888911686859178, w0=72.97047546394508, w1=12.486808728168082\n",
      "[-0.05602521  0.71045539]\n",
      "Gradient Descent(3987/9): loss=15.931125585862691, w0=73.00969310844545, w1=11.989489956908013\n",
      "[ 1.76466713 -1.34439657]\n",
      "Gradient Descent(3988/9): loss=16.536662417913913, w0=71.77442611649202, w1=12.930567552947103\n",
      "[-3.83062055 -1.10300767]\n",
      "Gradient Descent(3989/9): loss=16.6911017927635, w0=74.45586050253713, w1=13.7026729225376\n",
      "[ 6.00735299  2.02269262]\n",
      "Gradient Descent(3990/9): loss=16.085794097726364, w0=70.2507134103449, w1=12.28678808920675\n",
      "[-3.93427322 -3.12363878]\n",
      "Gradient Descent(3991/9): loss=20.727981382691283, w0=73.00470466385045, w1=14.473335232700565\n",
      "[ 0.4505406  -1.40263944]\n",
      "Gradient Descent(3992/9): loss=15.921354335269008, w0=72.68932624084081, w1=15.455182838346234\n",
      "[ 1.98649203  3.39286503]\n",
      "Gradient Descent(3993/9): loss=17.519897543368923, w0=71.29878182165663, w1=13.080177314402999\n",
      "[-3.56474753 -3.70475548]\n",
      "Gradient Descent(3994/9): loss=17.45599419494041, w0=73.79410509222623, w1=15.673506146932619\n",
      "[-1.03722767  5.6516878 ]\n",
      "Gradient Descent(3995/9): loss=17.91734485593249, w0=74.5201644614476, w1=11.71732468912325\n",
      "[ 3.06323417 -2.18771175]\n",
      "Gradient Descent(3996/9): loss=17.690728436765436, w0=72.37590054422378, w1=13.248722910943929\n",
      "[ 0.93261697  2.07779291]\n",
      "Gradient Descent(3997/9): loss=15.833947647504045, w0=71.72306866845632, w1=11.794267871753988\n",
      "[-3.48261209 -2.74373019]\n",
      "Gradient Descent(3998/9): loss=18.040039654616592, w0=74.16089712859709, w1=13.714879005672323\n",
      "[ 0.90808903  0.56504921]\n",
      "Gradient Descent(3999/9): loss=15.789362461790684, w0=73.52523480704932, w1=13.319344556228181\n",
      "[-1.5630986 -4.1007977]\n",
      "Gradient Descent(4000/9): loss=15.425499603964091, w0=74.619403826867, w1=16.189902945666056\n",
      "[ 0.96925707  0.49623987]\n",
      "Gradient Descent(4001/9): loss=19.936905204798194, w0=73.94092387756872, w1=15.842535039666451\n",
      "[ 2.56616279 -1.09272222]\n",
      "Gradient Descent(4002/9): loss=18.386658912843316, w0=72.14460992731321, w1=16.607440596139384\n",
      "[ 1.70709631  5.78339899]\n",
      "Gradient Descent(4003/9): loss=20.937688716487152, w0=70.9496425124493, w1=12.559061303113332\n",
      "[-1.41876273 -3.11198281]\n",
      "Gradient Descent(4004/9): loss=18.557510284952055, w0=71.94277642276185, w1=14.737449267495096\n",
      "[-3.19503296  1.59479657]\n",
      "Gradient Descent(4005/9): loss=17.089636027040093, w0=74.17929949232133, w1=13.621091667074651\n",
      "[-1.30711793  1.30536694]\n",
      "Gradient Descent(4006/9): loss=15.787828562552678, w0=75.0942820399426, w1=12.70733480735433\n",
      "[ 2.17538839 -1.97003687]\n",
      "Gradient Descent(4007/9): loss=17.304819601585677, w0=73.57151016998523, w1=14.086360618800251\n",
      "[-0.15312313  2.15948885]\n",
      "Gradient Descent(4008/9): loss=15.608426473763615, w0=73.67869636254105, w1=12.574718421438005\n",
      "[-3.02937347 -1.01663251]\n",
      "Gradient Descent(4009/9): loss=15.869420605335426, w0=75.7992577926745, w1=13.286361181428106\n",
      "[-0.39373936 -0.69435648]\n",
      "Gradient Descent(4010/9): loss=18.54293393420996, w0=76.07487534761412, w1=13.77241071611424\n",
      "[ 3.98347997 -2.58542203]\n",
      "Gradient Descent(4011/9): loss=19.295574765664888, w0=73.286439371346, w1=15.582206139158366\n",
      "[ 0.74237121  5.47375108]\n",
      "Gradient Descent(4012/9): loss=17.596155751746746, w0=72.76677952233476, w1=11.750580383120646\n",
      "[ 0.32701757 -1.54142902]\n",
      "Gradient Descent(4013/9): loss=17.019776292217976, w0=72.53786722020972, w1=12.829580697085351\n",
      "[ 0.86359693  0.20929107]\n",
      "Gradient Descent(4014/9): loss=15.883032923757744, w0=71.93334936722704, w1=12.68307694532502\n",
      "[-2.99838189  0.0863529 ]\n",
      "Gradient Descent(4015/9): loss=16.628780867915157, w0=74.03221668914398, w1=12.622629916924158\n",
      "[ 2.11719933 -2.83996916]\n",
      "Gradient Descent(4016/9): loss=16.025722612670478, w0=72.55017715864574, w1=14.610608331712161\n",
      "[-2.11973024  0.10732033]\n",
      "Gradient Descent(4017/9): loss=16.301928829528247, w0=74.03398832514893, w1=14.535484102497024\n",
      "[-0.63600769  2.00362352]\n",
      "Gradient Descent(4018/9): loss=16.217063857037424, w0=74.47919371067172, w1=13.132947635554473\n",
      "[ 3.4954157  -3.04282027]\n",
      "Gradient Descent(4019/9): loss=16.14844529345697, w0=72.03240271911106, w1=15.262921827237086\n",
      "[-0.54196681 -0.78288549]\n",
      "Gradient Descent(4020/9): loss=17.77152118781322, w0=72.41177948643275, w1=15.810941671017366\n",
      "[-0.93862343  0.04668097]\n",
      "Gradient Descent(4021/9): loss=18.492290453264438, w0=73.06881589007244, w1=15.778264991251406\n",
      "[ 0.03013113  0.29648833]\n",
      "Gradient Descent(4022/9): loss=18.05289617661676, w0=73.04772409638039, w1=15.570723162366928\n",
      "[-2.02091646  0.35502755]\n",
      "Gradient Descent(4023/9): loss=17.602357504225722, w0=74.46236561943508, w1=15.322203877694164\n",
      "[ 2.89084039  3.87786848]\n",
      "Gradient Descent(4024/9): loss=17.76590547048966, w0=72.43877734752719, w1=12.607695942997205\n",
      "[-3.19227783 -0.37495122]\n",
      "Gradient Descent(4025/9): loss=16.131730440108942, w0=74.67337183153776, w1=12.870161796926693\n",
      "[ 3.11876995 -0.77847963]\n",
      "Gradient Descent(4026/9): loss=16.523104774971294, w0=72.49023286622257, w1=13.415097538527046\n",
      "[-0.2436012   0.27793808]\n",
      "Gradient Descent(4027/9): loss=15.710933524819673, w0=72.66075370801222, w1=13.220540883700979\n",
      "[-2.06300609 -0.8797058 ]\n",
      "Gradient Descent(4028/9): loss=15.619923859650232, w0=74.10485796856865, w1=13.836334945326993\n",
      "[ 0.26826776  2.02945797]\n",
      "Gradient Descent(4029/9): loss=15.778286247121283, w0=73.9170705359378, w1=12.415714369208228\n",
      "[-0.0603851   1.69666335]\n",
      "Gradient Descent(4030/9): loss=16.146090858430927, w0=73.959340103656, w1=11.228050025212037\n",
      "[ 1.77873909 -4.33579708]\n",
      "Gradient Descent(4031/9): loss=18.142270297566558, w0=72.71422274137555, w1=14.263107981578333\n",
      "[-0.75190822  6.0656329 ]\n",
      "Gradient Descent(4032/9): loss=15.860767776482607, w0=73.24055849668731, w1=10.017164951225501\n",
      "[ 5.91368386 -5.10152619]\n",
      "Gradient Descent(4033/9): loss=21.381929239343282, w0=69.10097979181027, w1=13.58823328212399\n",
      "[-2.88666044 -0.46805347]\n",
      "Gradient Descent(4034/9): loss=24.182158445397242, w0=71.12164210004174, w1=13.915870712417059\n",
      "[-0.95653427  1.64785294]\n",
      "Gradient Descent(4035/9): loss=17.84040487676828, w0=71.7912160869617, w1=12.76237365724386\n",
      "[-0.89123789 -1.03253865]\n",
      "Gradient Descent(4036/9): loss=16.7722378635615, w0=72.41508261106064, w1=13.485150712142996\n",
      "[ 0.24001005 -1.04923814]\n",
      "Gradient Descent(4037/9): loss=15.772081993884377, w0=72.24707557566654, w1=14.219617412877806\n",
      "[ 0.4527815 -3.132944 ]\n",
      "Gradient Descent(4038/9): loss=16.207561277255362, w0=71.93012852801704, w1=16.412678213902453\n",
      "[-0.37350854  5.32865699]\n",
      "Gradient Descent(4039/9): loss=20.61699831895068, w0=72.1915845032839, w1=12.682618320658746\n",
      "[-0.44510336 -0.6318133 ]\n",
      "Gradient Descent(4040/9): loss=16.311141363033187, w0=72.50315685313457, w1=13.12488762854122\n",
      "[-2.63838048 -4.47451469]\n",
      "Gradient Descent(4041/9): loss=15.761492950878031, w0=74.35002318580445, w1=16.25704791465978\n",
      "[ 3.24460688  2.95341969]\n",
      "Gradient Descent(4042/9): loss=19.800358907253806, w0=72.07879837144488, w1=14.18965413508311\n",
      "[-3.23745551  1.85136425]\n",
      "Gradient Descent(4043/9): loss=16.376159196490175, w0=74.34501722997506, w1=12.893699158437686\n",
      "[ 1.57928249 -0.66543603]\n",
      "Gradient Descent(4044/9): loss=16.109994238002038, w0=73.23951948749475, w1=13.359504377277817\n",
      "[-0.55970926 -0.78474128]\n",
      "Gradient Descent(4045/9): loss=15.394592674196721, w0=73.63131596879325, w1=13.90882327286646\n",
      "[ 2.83377766 -1.51252845]\n",
      "Gradient Descent(4046/9): loss=15.534873268800078, w0=71.64767160740122, w1=14.967593190623617\n",
      "[-3.41350845  3.46414615]\n",
      "Gradient Descent(4047/9): loss=17.847852621354736, w0=74.03712752017366, w1=12.542690884570971\n",
      "[ 1.28444149  3.05863119]\n",
      "Gradient Descent(4048/9): loss=16.10106978284706, w0=73.13801847718754, w1=10.40164904871971\n",
      "[ 1.27269607 -1.62429367]\n",
      "Gradient Descent(4049/9): loss=20.13527792831621, w0=72.24713122667855, w1=11.538654615471138\n",
      "[-4.03023863 -3.36808121]\n",
      "Gradient Descent(4050/9): loss=17.817626061944363, w0=75.06829827103634, w1=13.896311463460867\n",
      "[ 2.17571763  0.19633836]\n",
      "Gradient Descent(4051/9): loss=17.046870815964247, w0=73.54529593291538, w1=13.758874612169137\n",
      "[-1.31771288 -1.5282909 ]\n",
      "Gradient Descent(4052/9): loss=15.456448055958846, w0=74.46769495210572, w1=14.828678239450891\n",
      "[-0.82686216  4.56171434]\n",
      "Gradient Descent(4053/9): loss=16.98461370870956, w0=75.0464984619162, w1=11.635478204599838\n",
      "[ 0.70807068 -2.74721879]\n",
      "Gradient Descent(4054/9): loss=18.622249940840895, w0=74.55084898466576, w1=13.558531357246599\n",
      "[ 3.76576009 -0.77115077]\n",
      "Gradient Descent(4055/9): loss=16.178926799826733, w0=71.91481692373992, w1=14.098336896054994\n",
      "[ 1.67156543  3.74781401]\n",
      "Gradient Descent(4056/9): loss=16.528201389330405, w0=70.74472112530445, w1=11.474867087990432\n",
      "[-2.10930513 -1.72874693]\n",
      "Gradient Descent(4057/9): loss=20.64480285666122, w0=72.22123471884834, w1=12.684989938356678\n",
      "[-2.34558299 -0.83551844]\n",
      "Gradient Descent(4058/9): loss=16.27700879598668, w0=73.86314281493493, w1=13.269852845478985\n",
      "[-1.5512804  0.8685673]\n",
      "Gradient Descent(4059/9): loss=15.569914559363344, w0=74.94903909562261, w1=12.661855734045488\n",
      "[ 1.27312159  0.22547902]\n",
      "Gradient Descent(4060/9): loss=17.09003895709532, w0=74.05785397989224, w1=12.504020422763636\n",
      "[ 1.37479842 -2.26182578]\n",
      "Gradient Descent(4061/9): loss=16.153671353532456, w0=73.09549508706408, w1=14.08729846900842\n",
      "[-0.18391069  0.46824803]\n",
      "Gradient Descent(4062/9): loss=15.59015488350346, w0=73.22423257192949, w1=13.759524848411116\n",
      "[-0.89138565  0.99931665]\n",
      "Gradient Descent(4063/9): loss=15.42746367052105, w0=73.848202526357, w1=13.060003193923466\n",
      "[-0.86156823 -1.07019993]\n",
      "Gradient Descent(4064/9): loss=15.627579242129757, w0=74.45130028963591, w1=13.80914314221945\n",
      "[ 4.41556738  1.70596602]\n",
      "Gradient Descent(4065/9): loss=16.109912414486335, w0=71.36040312260606, w1=12.61496692921314\n",
      "[-3.76778254 -3.22650472]\n",
      "Gradient Descent(4066/9): loss=17.629027892399, w0=73.99785090284436, w1=14.873520233968879\n",
      "[-0.17039452  7.09577862]\n",
      "Gradient Descent(4067/9): loss=16.60499590772583, w0=74.11712706536514, w1=9.906475201549044\n",
      "[ 3.51210918 -4.54629459]\n",
      "Gradient Descent(4068/9): loss=22.1087333201388, w0=71.65865064146153, w1=13.088881415448311\n",
      "[-5.03890463 -1.48248445]\n",
      "Gradient Descent(4069/9): loss=16.7993185232177, w0=75.18588388517315, w1=14.126620530740563\n",
      "[ 0.45443753 -0.1543629 ]\n",
      "Gradient Descent(4070/9): loss=17.384892794494863, w0=74.86777761164444, w1=14.23467455958418\n",
      "[ 4.19571582  2.75843991]\n",
      "Gradient Descent(4071/9): loss=16.909382513455082, w0=71.93077653537081, w1=12.303766621610281\n",
      "[-4.61858543 -2.94016173]\n",
      "Gradient Descent(4072/9): loss=17.006394928570167, w0=75.16378633358758, w1=14.361879832649661\n",
      "[ 2.85652605  2.41165695]\n",
      "Gradient Descent(4073/9): loss=17.52319383665209, w0=73.16421809623725, w1=12.673719967573462\n",
      "[ 0.41598646 -1.35827188]\n",
      "Gradient Descent(4074/9): loss=15.719111349193431, w0=72.87302757260537, w1=13.62451028212089\n",
      "[-2.684587   -1.49125506]\n",
      "Gradient Descent(4075/9): loss=15.4849471374884, w0=74.75223847330997, w1=14.6683888266403\n",
      "[ 1.5676263   1.32414763]\n",
      "Gradient Descent(4076/9): loss=17.1557071159575, w0=73.6549000625717, w1=13.741485484497158\n",
      "[-0.54848938 -0.93415818]\n",
      "Gradient Descent(4077/9): loss=15.485303013622872, w0=74.03884262706175, w1=14.395396210813598\n",
      "[-1.70096882  4.37249717]\n",
      "Gradient Descent(4078/9): loss=16.08257962622639, w0=75.22952080102404, w1=11.334648188708597\n",
      "[ 2.2009144  -1.54362996]\n",
      "Gradient Descent(4079/9): loss=19.559809534352812, w0=73.68888072037726, w1=12.415189158813892\n",
      "[ 2.6284328   0.18371981]\n",
      "Gradient Descent(4080/9): loss=16.030488966158302, w0=71.84897775716608, w1=12.28658528842819\n",
      "[-2.21653036 -3.88994823]\n",
      "Gradient Descent(4081/9): loss=17.14159599825095, w0=73.40054900626322, w1=15.009549047736561\n",
      "[ 2.2760444  -0.61553896]\n",
      "Gradient Descent(4082/9): loss=16.561772558688652, w0=71.80731792324383, w1=15.440426320992586\n",
      "[-3.14236252  5.87298686]\n",
      "Gradient Descent(4083/9): loss=18.413083183856564, w0=74.00697168694376, w1=11.329335520256333\n",
      "[ 1.77212655 -4.44579023]\n",
      "Gradient Descent(4084/9): loss=17.95216823306129, w0=72.7664831036078, w1=14.441388682602401\n",
      "[-2.44237846  1.63857014]\n",
      "Gradient Descent(4085/9): loss=15.98739436726532, w0=74.4761480280911, w1=13.294389586928473\n",
      "[ 3.49078278 -0.0076023 ]\n",
      "Gradient Descent(4086/9): loss=16.10188933609527, w0=72.03260008455474, w1=13.299711198705852\n",
      "[-2.13655633 -4.91875445]\n",
      "Gradient Descent(4087/9): loss=16.197554581207708, w0=73.52818951419454, w1=16.742839312319525\n",
      "[-0.7599763   2.55705334]\n",
      "Gradient Descent(4088/9): loss=20.73732701121794, w0=74.06017292149414, w1=14.952901977572418\n",
      "[-0.18610774  3.40752532]\n",
      "Gradient Descent(4089/9): loss=16.764601818750105, w0=74.19044833670195, w1=12.567634256302448\n",
      "[-2.3059042 -0.5918749]\n",
      "Gradient Descent(4090/9): loss=16.203710905160282, w0=75.80458127784486, w1=12.981946688161624\n",
      "[ 4.92382745  2.09064927]\n",
      "Gradient Descent(4091/9): loss=18.66147823761556, w0=72.35790206161184, w1=11.518492196966433\n",
      "[-2.91947018 -1.74912113]\n",
      "Gradient Descent(4092/9): loss=17.74714694434473, w0=74.40153119012986, w1=12.742876989146263\n",
      "[-0.36717683 -1.90995688]\n",
      "Gradient Descent(4093/9): loss=16.270750162652902, w0=74.65855496927647, w1=14.079846805669387\n",
      "[ 3.50689719  0.10354747]\n",
      "Gradient Descent(4094/9): loss=16.497080067810685, w0=72.20372693451762, w1=14.00736357943528\n",
      "[ 0.46835827 -0.04849142]\n",
      "Gradient Descent(4095/9): loss=16.119358376643245, w0=71.87587614383742, w1=14.041307572657253\n",
      "[-0.0966575   1.13308312]\n",
      "Gradient Descent(4096/9): loss=16.54900944623088, w0=71.9435363915829, w1=13.248149385868656\n",
      "[-2.02314179 -0.47000781]\n",
      "Gradient Descent(4097/9): loss=16.324469240241196, w0=73.35973564131586, w1=13.57715485544883\n",
      "[ 1.73647422 -0.9806105 ]\n",
      "Gradient Descent(4098/9): loss=15.392801099035006, w0=72.14420368718838, w1=14.263582203143311\n",
      "[-2.06681701 -2.41365492]\n",
      "Gradient Descent(4099/9): loss=16.354039877370084, w0=73.59097559244944, w1=15.953140644032233\n",
      "[ 0.5823331   2.40756973]\n",
      "Gradient Descent(4100/9): loss=18.48893183924289, w0=73.18334242242922, w1=14.267841830236764\n",
      "[-0.0202087   4.42296619]\n",
      "Gradient Descent(4101/9): loss=15.70257576237682, w0=73.19748851000841, w1=11.171765497425753\n",
      "[-0.42266037 -1.32129755]\n",
      "Gradient Descent(4102/9): loss=18.053847111332285, w0=73.49335076931446, w1=12.096673779907746\n",
      "[ 0.05104188 -1.01346288]\n",
      "Gradient Descent(4103/9): loss=16.362171746149254, w0=73.45762145671621, w1=12.806097795651167\n",
      "[ 1.43920725 -2.67116638]\n",
      "Gradient Descent(4104/9): loss=15.626164965714526, w0=72.45017638373963, w1=14.675914261263003\n",
      "[-1.86254156 -0.20758164]\n",
      "Gradient Descent(4105/9): loss=16.45729060767551, w0=73.7539554737784, w1=14.82122140859912\n",
      "[ 2.99777159 -0.28399277]\n",
      "Gradient Descent(4106/9): loss=16.39152642949743, w0=71.65551536195473, w1=15.020016349205529\n",
      "[-0.19532433  3.12263663]\n",
      "Gradient Descent(4107/9): loss=17.914344102149272, w0=71.79224239187178, w1=12.834170708139467\n",
      "[-1.2766016  -2.69920741]\n",
      "Gradient Descent(4108/9): loss=16.72177075527675, w0=72.6858635134273, w1=14.723615896001267\n",
      "[-1.44592645  1.85296705]\n",
      "Gradient Descent(4109/9): loss=16.344403341815113, w0=73.69801202787366, w1=13.426538961621397\n",
      "[ 0.25277209  2.37357749]\n",
      "Gradient Descent(4110/9): loss=15.468945952427172, w0=73.52107156805472, w1=11.765034721443456\n",
      "[ 0.65591136 -0.57932324]\n",
      "Gradient Descent(4111/9): loss=16.881746162149902, w0=73.06193361340846, w1=12.170560988860151\n",
      "[ 2.18592109 -0.79096601]\n",
      "Gradient Descent(4112/9): loss=16.269735929525144, w0=71.53178885373327, w1=12.724237198411227\n",
      "[-1.11481283 -1.0737085 ]\n",
      "Gradient Descent(4113/9): loss=17.22381590166611, w0=72.31215783603103, w1=13.47583315111104\n",
      "[-3.4829057  -2.56246295]\n",
      "Gradient Descent(4114/9): loss=15.867825832144744, w0=74.7501918259805, w1=15.269557219062461\n",
      "[ 0.21754974  1.97209832]\n",
      "Gradient Descent(4115/9): loss=18.048020944331704, w0=74.59790700649182, w1=13.889088392962973\n",
      "[ 2.08623801  0.0733063 ]\n",
      "Gradient Descent(4116/9): loss=16.31987065214554, w0=73.13754040269218, w1=13.837773983106345\n",
      "[-2.30866836 -0.52316374]\n",
      "Gradient Descent(4117/9): loss=15.462219507266965, w0=74.75360825706773, w1=14.20398859839173\n",
      "[ 3.72821347  1.26387146]\n",
      "Gradient Descent(4118/9): loss=16.713517830729327, w0=72.14385882758994, w1=13.319278577407474\n",
      "[-0.8063086  -2.22843224]\n",
      "Gradient Descent(4119/9): loss=16.060080032846702, w0=72.70827484851384, w1=14.879181144429735\n",
      "[-2.98052548  0.95236621]\n",
      "Gradient Descent(4120/9): loss=16.536635497436016, w0=74.7946426813906, w1=14.21252479525076\n",
      "[-0.06140322  1.10523518]\n",
      "Gradient Descent(4121/9): loss=16.780476125123, w0=74.83762493535441, w1=13.438860172266132\n",
      "[ 0.21453794 -0.23951852]\n",
      "Gradient Descent(4122/9): loss=16.57823169557532, w0=74.68744838030706, w1=13.606523137677799\n",
      "[ 1.92447123 -0.35845368]\n",
      "Gradient Descent(4123/9): loss=16.36488622935982, w0=73.34031851957756, w1=13.85744071167884\n",
      "[ 0.95861962  0.84967302]\n",
      "Gradient Descent(4124/9): loss=15.458303512751701, w0=72.66928478778779, w1=13.26266960061717\n",
      "[ 3.46512779 -1.34176573]\n",
      "Gradient Descent(4125/9): loss=15.604527489560589, w0=70.2436953333472, w1=14.201905611058654\n",
      "[-2.93217406  1.53618873]\n",
      "Gradient Descent(4126/9): loss=20.29861072601137, w0=72.2962171787091, w1=13.126573499147685\n",
      "[-1.6398853  -1.29175117]\n",
      "Gradient Descent(4127/9): loss=15.945948880146895, w0=73.44413688815294, w1=14.030799315902975\n",
      "[-0.31259447  0.60640652]\n",
      "Gradient Descent(4128/9): loss=15.549018499982292, w0=73.66295301615, w1=13.606314754053878\n",
      "[ 3.95535756  2.18236756]\n",
      "Gradient Descent(4129/9): loss=15.461993887089166, w0=70.89420272569755, w1=12.078657460587607\n",
      "[-2.37951701  1.27261236]\n",
      "Gradient Descent(4130/9): loss=19.2466916922581, w0=72.55986463253866, w1=11.187828810369918\n",
      "[-1.73782779 -4.51413661]\n",
      "Gradient Descent(4131/9): loss=18.281673254135523, w0=73.77634408765944, w1=14.347724434719815\n",
      "[-2.95939413  1.97473052]\n",
      "Gradient Descent(4132/9): loss=15.878975818982957, w0=75.84791997904934, w1=12.965413074153526\n",
      "[ 6.21888733  0.7066764 ]\n",
      "Gradient Descent(4133/9): loss=18.779592618224715, w0=71.494698848948, w1=12.470739597403306\n",
      "[ 1.22383191  0.434521  ]\n",
      "Gradient Descent(4134/9): loss=17.51350293975075, w0=70.6380165107428, w1=12.16657489446437\n",
      "[-1.36258392 -3.45340232]\n",
      "Gradient Descent(4135/9): loss=19.77496995852144, w0=71.59182525620184, w1=14.583956516782756\n",
      "[-2.10437302 -2.1459459 ]\n",
      "Gradient Descent(4136/9): loss=17.444132031124997, w0=73.06488637325798, w1=16.086118644827494\n",
      "[ 2.14292731  1.30699158]\n",
      "Gradient Descent(4137/9): loss=18.808793193812324, w0=71.56483725554074, w1=15.171224540835027\n",
      "[-3.7605384   1.79700448]\n",
      "Gradient Descent(4138/9): loss=18.311361501342077, w0=74.19721413604259, w1=13.91332140193614\n",
      "[-1.67910254  0.30759615]\n",
      "Gradient Descent(4139/9): loss=15.887864576554453, w0=75.37258591094844, w1=13.698004093914\n",
      "[ 1.62594495 -1.46562265]\n",
      "Gradient Descent(4140/9): loss=17.570135315971264, w0=74.23442444848156, w1=14.723939949606718\n",
      "[-2.16048949  3.60866953]\n",
      "Gradient Descent(4141/9): loss=16.6022113487152, w0=75.74676709243082, w1=12.197871280244364\n",
      "[ 0.83768462 -3.14176078]\n",
      "Gradient Descent(4142/9): loss=19.215670760395263, w0=75.16038785878686, w1=14.397103823375591\n",
      "[-1.14584502  0.04201797]\n",
      "Gradient Descent(4143/9): loss=17.548538745651516, w0=75.96247937277047, w1=14.367691246019868\n",
      "[ 2.96554212  4.08016833]\n",
      "Gradient Descent(4144/9): loss=19.34074027351526, w0=73.88659988848272, w1=11.511573413966325\n",
      "[-0.21058034 -1.93465897]\n",
      "Gradient Descent(4145/9): loss=17.498307010366013, w0=74.03400612995473, w1=12.865834689762282\n",
      "[ 4.12652824 -2.10641967]\n",
      "Gradient Descent(4146/9): loss=15.848173070019161, w0=71.14543636177577, w1=14.340328462038904\n",
      "[-3.14765539 -0.95170611]\n",
      "Gradient Descent(4147/9): loss=18.064213115187812, w0=73.3487951337295, w1=15.006522737798964\n",
      "[-1.48736639  5.13283502]\n",
      "Gradient Descent(4148/9): loss=16.55296824949978, w0=74.38995160920086, w1=11.413538223957126\n",
      "[ 0.33092539 -0.08029519]\n",
      "Gradient Descent(4149/9): loss=18.121066253811247, w0=74.15830383894611, w1=11.469744855900299\n",
      "[ 0.19506365 -1.03823065]\n",
      "Gradient Descent(4150/9): loss=17.77945068325359, w0=74.02175928137248, w1=12.196506311803518\n",
      "[-0.00987739  1.34004984]\n",
      "Gradient Descent(4151/9): loss=16.47407039866543, w0=74.02867345724772, w1=11.258471425322382\n",
      "[ 2.06563962 -3.23104607]\n",
      "Gradient Descent(4152/9): loss=18.12277353075893, w0=72.5827257210246, w1=13.520203672374656\n",
      "[ 0.62155287  1.71991649]\n",
      "Gradient Descent(4153/9): loss=15.639607714093337, w0=72.14763870928236, w1=12.316262128966041\n",
      "[-2.28993843 -3.97283109]\n",
      "Gradient Descent(4154/9): loss=16.719678869824286, w0=73.75059560946052, w1=15.097243891088423\n",
      "[ 1.12846198 -0.33712778]\n",
      "Gradient Descent(4155/9): loss=16.798367266392347, w0=72.96067222277742, w1=15.33323333637097\n",
      "[ 0.86807222  1.18489412]\n",
      "Gradient Descent(4156/9): loss=17.15918544247023, w0=72.35302166852165, w1=14.503807450380638\n",
      "[-4.47384667  5.92024314]\n",
      "Gradient Descent(4157/9): loss=16.352919887973158, w0=75.48471433714958, w1=10.359637250521033\n",
      "[ 1.80303616 -4.47264518]\n",
      "Gradient Descent(4158/9): loss=22.65310797484061, w0=74.22258902300854, w1=13.490488876725719\n",
      "[ 3.36471022 -1.09528481]\n",
      "Gradient Descent(4159/9): loss=15.817157152534405, w0=71.86729186983528, w1=14.257188245276481\n",
      "[-0.10103536 -0.71522518]\n",
      "Gradient Descent(4160/9): loss=16.705758953770676, w0=71.93801662075275, w1=14.757845870549815\n",
      "[ 4.02911207  2.52088137]\n",
      "Gradient Descent(4161/9): loss=17.121940109968843, w0=69.11763816992139, w1=12.99322891444332\n",
      "[-4.86182026  0.53017993]\n",
      "Gradient Descent(4162/9): loss=24.22489430019057, w0=72.52091235052114, w1=12.622102960398953\n",
      "[-2.58718044 -0.15490323]\n",
      "Gradient Descent(4163/9): loss=16.052406835003794, w0=74.33193866094119, w1=12.73053522063141\n",
      "[ 2.03108065 -1.63254948]\n",
      "Gradient Descent(4164/9): loss=16.205260410096262, w0=72.91018220657172, w1=13.873319857032756\n",
      "[-2.1910177  -0.23700996]\n",
      "Gradient Descent(4165/9): loss=15.53697938551138, w0=74.44389459796932, w1=14.039226832046774\n",
      "[ 1.03129907 -1.37872994]\n",
      "Gradient Descent(4166/9): loss=16.203634534706076, w0=73.72198525084464, w1=15.004337792095336\n",
      "[ 0.27737655  2.40837577]\n",
      "Gradient Descent(4167/9): loss=16.639748181055825, w0=73.52782166517149, w1=13.318474754200071\n",
      "[ 1.27484839  1.93088715]\n",
      "Gradient Descent(4168/9): loss=15.42624118987377, w0=72.63542779156373, w1=11.966853750795204\n",
      "[-1.84843176 -0.96525341]\n",
      "Gradient Descent(4169/9): loss=16.747065880658077, w0=73.92933002521883, w1=12.64253114063865\n",
      "[-2.0354652   1.24088217]\n",
      "Gradient Descent(4170/9): loss=15.938195806553098, w0=75.35415566831892, w1=11.773913618604356\n",
      "[ 4.94029338 -2.41975497]\n",
      "Gradient Descent(4171/9): loss=18.963044049519358, w0=71.89595030372382, w1=13.467742100053892\n",
      "[-2.90339425  0.08791631]\n",
      "Gradient Descent(4172/9): loss=16.363121948026272, w0=73.9283262806182, w1=13.406200685614385\n",
      "[ 0.17428216 -0.9555356 ]\n",
      "Gradient Descent(4173/9): loss=15.589824251775264, w0=73.80632876965343, w1=14.07507560609436\n",
      "[-0.35960308  0.95623518]\n",
      "Gradient Descent(4174/9): loss=15.694396869298306, w0=74.05805092800811, w1=13.405710980293117\n",
      "[-0.54308016  0.65709136]\n",
      "Gradient Descent(4175/9): loss=15.680572484178736, w0=74.43820704348407, w1=12.945747027692828\n",
      "[-1.66321732  0.61487282]\n",
      "Gradient Descent(4176/9): loss=16.18314152488564, w0=75.60245916939539, w1=12.51533605703022\n",
      "[ 0.59388509 -1.03667552]\n",
      "Gradient Descent(4177/9): loss=18.51557069439202, w0=75.18673960346571, w1=13.241008922001749\n",
      "[-1.4121047  -1.78128485]\n",
      "Gradient Descent(4178/9): loss=17.20575678839574, w0=76.17521289262535, w1=14.487908313986305\n",
      "[ 5.26377805 -0.38887786]\n",
      "Gradient Descent(4179/9): loss=20.045035931940163, w0=72.49056826011953, w1=14.760122817414166\n",
      "[ 0.45244113  0.79154136]\n",
      "Gradient Descent(4180/9): loss=16.5283018599216, w0=72.17385947015516, w1=14.206043863998325\n",
      "[ 1.10529156  0.25690172]\n",
      "Gradient Descent(4181/9): loss=16.27693657895188, w0=71.40015537876054, w1=14.026212661144916\n",
      "[-1.40317499 -3.95817812]\n",
      "Gradient Descent(4182/9): loss=17.328395129270696, w0=72.38237787207517, w1=16.796937344291948\n",
      "[-0.29806783  1.359759  ]\n",
      "Gradient Descent(4183/9): loss=21.303334768775315, w0=72.59102535509727, w1=15.845106043997697\n",
      "[-4.66329173  0.53726295]\n",
      "Gradient Descent(4184/9): loss=18.43046317978637, w0=75.85532956294958, w1=15.46902197661801\n",
      "[ 0.685008    1.28830688]\n",
      "Gradient Descent(4185/9): loss=20.644968441412807, w0=75.37582396584293, w1=14.567207161748339\n",
      "[ 2.09732305 -1.46300872]\n",
      "Gradient Descent(4186/9): loss=18.144368152501553, w0=73.9076978330847, w1=15.59131326454134\n",
      "[ 2.71662729  1.24761581]\n",
      "Gradient Descent(4187/9): loss=17.803677285859653, w0=72.00605872963709, w1=14.717982199601341\n",
      "[-0.04222704  0.4602157 ]\n",
      "Gradient Descent(4188/9): loss=16.981839778092013, w0=72.03561765706183, w1=14.39583120988401\n",
      "[-2.00869025 -0.07545628]\n",
      "Gradient Descent(4189/9): loss=16.597189586064417, w0=73.44170083124033, w1=14.448650603058894\n",
      "[-1.79326387 -0.41990542]\n",
      "Gradient Descent(4190/9): loss=15.86622774677095, w0=74.69698554267308, w1=14.742584393963769\n",
      "[-0.77828145 -0.53053316]\n",
      "Gradient Descent(4191/9): loss=17.167604310647167, w0=75.2417825583702, w1=15.11395760839748\n",
      "[ 0.39917018  0.41204711]\n",
      "Gradient Descent(4192/9): loss=18.618346885560303, w0=74.96236343260438, w1=14.825524628285104\n",
      "[ 6.12317244  2.24644121]\n",
      "Gradient Descent(4193/9): loss=17.683341502144653, w0=70.67614272304301, w1=13.253015784299125\n",
      "[-1.81219808 -1.53820439]\n",
      "Gradient Descent(4194/9): loss=18.837967731490053, w0=71.94468137644039, w1=14.329758856468246\n",
      "[-0.21495186 -0.63889244]\n",
      "Gradient Descent(4195/9): loss=16.65740246113637, w0=72.09514767797805, w1=14.77698356246741\n",
      "[-2.48147899  1.41372164]\n",
      "Gradient Descent(4196/9): loss=16.945873998017134, w0=73.8321829723026, w1=13.787378416689307\n",
      "[-1.10419849  0.48127131]\n",
      "Gradient Descent(4197/9): loss=15.578079482996124, w0=74.6051219134132, w1=13.450488501542088\n",
      "[ 1.64611327  0.16285544]\n",
      "Gradient Descent(4198/9): loss=16.245937491679523, w0=73.45284262375775, w1=13.336489695762758\n",
      "[-0.93145117 -0.04611973]\n",
      "Gradient Descent(4199/9): loss=15.40877212733836, w0=74.10485844334843, w1=13.368773507630465\n",
      "[ 0.56270385 -1.9395724 ]\n",
      "Gradient Descent(4200/9): loss=15.720850547499264, w0=73.71096574577582, w1=14.726474189143664\n",
      "[-0.04479548  0.13008466]\n",
      "Gradient Descent(4201/9): loss=16.25005804670816, w0=73.74232258372611, w1=14.635414923857908\n",
      "[ 7.4435582   6.97349769]\n",
      "Gradient Descent(4202/9): loss=16.154243531017226, w0=68.53183184241522, w1=9.753966541122038\n",
      "[-8.26985317 -0.84740491]\n",
      "Gradient Descent(4203/9): loss=33.66523044617087, w0=74.32072906251668, w1=10.347149980165204\n",
      "[-0.62780974 -4.75992849]\n",
      "Gradient Descent(4204/9): loss=20.819528005170838, w0=74.76019588019321, w1=13.679099922544967\n",
      "[ 0.0117531   1.30523775]\n",
      "Gradient Descent(4205/9): loss=16.480745096707974, w0=74.75196871260619, w1=12.765433497108516\n",
      "[-2.2428705   3.59797929]\n",
      "Gradient Descent(4206/9): loss=16.70393517438066, w0=76.32197806383799, w1=10.24684799577997\n",
      "[ 0.60091948 -2.62365773]\n",
      "Gradient Descent(4207/9): loss=25.196155866479106, w0=75.90133442877476, w1=12.083408406053382\n",
      "[ 0.84644666 -0.27419892]\n",
      "Gradient Descent(4208/9): loss=19.760020120815838, w0=75.30882176874186, w1=12.275347651997444\n",
      "[ 2.31169729  1.37318167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4209/9): loss=18.141045668880874, w0=73.69063366803661, w1=11.314120485677307\n",
      "[-0.97020768  0.59391823]\n",
      "Gradient Descent(4210/9): loss=17.809472187234356, w0=74.36977904696347, w1=10.898377724140948\n",
      "[-1.16283655 -0.63835561]\n",
      "Gradient Descent(4211/9): loss=19.296266504029624, w0=75.18376463362851, w1=11.345226650869302\n",
      "[ 1.41264857 -2.75866448]\n",
      "Gradient Descent(4212/9): loss=19.449655236095552, w0=74.19491063260585, w1=13.276291787473102\n",
      "[ 2.28972423  1.457791  ]\n",
      "Gradient Descent(4213/9): loss=15.812468104893034, w0=72.59210367230848, w1=12.255838089311315\n",
      "[ 1.35550834 -2.58519088]\n",
      "Gradient Descent(4214/9): loss=16.381096559852768, w0=71.64324783162806, w1=14.065471705543555\n",
      "[-4.07762101  3.59874764]\n",
      "Gradient Descent(4215/9): loss=16.919807438889855, w0=74.49758254147379, w1=11.546348355871363\n",
      "[ 1.85194727 -1.87000161]\n",
      "Gradient Descent(4216/9): loss=17.979235547057247, w0=73.20121945421977, w1=12.85534948472108\n",
      "[ 0.91581267 -1.32674094]\n",
      "Gradient Descent(4217/9): loss=15.585099296855287, w0=72.56015058689539, w1=13.784068143142008\n",
      "[-3.48814858 -1.16677442]\n",
      "Gradient Descent(4218/9): loss=15.70141431226154, w0=75.0018545954374, w1=14.60081023804429\n",
      "[ 2.08303289  1.12865453]\n",
      "Gradient Descent(4219/9): loss=17.472834882520274, w0=73.54373157352268, w1=13.810752069533834\n",
      "[-0.17629483 -0.0945082 ]\n",
      "Gradient Descent(4220/9): loss=15.471883899635067, w0=73.66713795390099, w1=13.8769078129538\n",
      "[ 0.25160451  0.51782503]\n",
      "Gradient Descent(4221/9): loss=15.534415026305103, w0=73.49101479656447, w1=13.514430290023967\n",
      "[ 0.90482282 -0.05443124]\n",
      "Gradient Descent(4222/9): loss=15.405913318372395, w0=72.85763881935483, w1=13.55253216126947\n",
      "[-1.23954654  0.72046812]\n",
      "Gradient Descent(4223/9): loss=15.483710732872568, w0=73.72532139560745, w1=13.048204474514117\n",
      "[ 0.83723427  2.2751969 ]\n",
      "Gradient Descent(4224/9): loss=15.57204014716308, w0=73.13925740480899, w1=11.455566643032503\n",
      "[-0.15964662 -6.42193332]\n",
      "Gradient Descent(4225/9): loss=17.446431531205498, w0=73.25101004112827, w1=15.950919967790954\n",
      "[-2.24518913  2.65545474]\n",
      "Gradient Descent(4226/9): loss=18.440241922115284, w0=74.8226424323199, w1=14.092101646369443\n",
      "[ 2.14983028 -0.69973045]\n",
      "Gradient Descent(4227/9): loss=16.741891218814864, w0=73.31776123325936, w1=14.581912959599212\n",
      "[ 1.95954428  0.62581595]\n",
      "Gradient Descent(4228/9): loss=15.993595021525872, w0=71.94608023398719, w1=14.14384179145481\n",
      "[-1.97063801 -1.43645756]\n",
      "Gradient Descent(4229/9): loss=16.514760485830944, w0=73.32552684313119, w1=15.149362084661558\n",
      "[ 1.18100942  1.15832971]\n",
      "Gradient Descent(4230/9): loss=16.780252278143305, w0=72.49882024671442, w1=14.33853128993671\n",
      "[-1.34635684  3.46033636]\n",
      "Gradient Descent(4231/9): loss=16.070766182348954, w0=73.4412700370921, w1=11.916295836489251\n",
      "[ 0.21569536 -1.25194146]\n",
      "Gradient Descent(4232/9): loss=16.618879320768993, w0=73.29028328413595, w1=12.79265485875803\n",
      "[ 2.74521433  2.5271035 ]\n",
      "Gradient Descent(4233/9): loss=15.621918545491852, w0=71.36863325105902, w1=11.023682409877182\n",
      "[-0.61803763 -2.12281311]\n",
      "Gradient Descent(4234/9): loss=20.255297998407347, w0=71.80125959212062, w1=12.509651583800261\n",
      "[-1.30460615  1.55799019]\n",
      "Gradient Descent(4235/9): loss=16.970417431424433, w0=72.71448389367714, w1=11.419058454008733\n",
      "[-0.2862351   0.91843067]\n",
      "Gradient Descent(4236/9): loss=17.67690954424375, w0=72.9148484659068, w1=10.776156986922487\n",
      "[ 0.83582628 -2.37951126]\n",
      "Gradient Descent(4237/9): loss=19.112342272137568, w0=72.32977007274643, w1=12.441814871547862\n",
      "[ 1.64836418 -0.15489702]\n",
      "Gradient Descent(4238/9): loss=16.389298016371086, w0=71.17591514350899, w1=12.550242785855348\n",
      "[-2.84448392 -0.22812747]\n",
      "Gradient Descent(4239/9): loss=18.060821309690038, w0=73.16705388601274, w1=12.709932017262615\n",
      "[ 0.76066113 -2.09134013]\n",
      "Gradient Descent(4240/9): loss=15.690216574027364, w0=72.63459109695222, w1=14.173870107196262\n",
      "[ 1.24314886  0.70280553]\n",
      "Gradient Descent(4241/9): loss=15.844173927016385, w0=71.76438689181975, w1=13.68190623679803\n",
      "[-5.01636786  2.23448139]\n",
      "Gradient Descent(4242/9): loss=16.576067862372323, w0=75.2758443908081, w1=12.117769263495715\n",
      "[ 5.00586273 -6.81378542]\n",
      "Gradient Descent(4243/9): loss=18.277340647438994, w0=71.77174048276713, w1=16.887419059503813\n",
      "[-1.78710629  8.59063768]\n",
      "Gradient Descent(4244/9): loss=22.35063837711747, w0=73.02271488830947, w1=10.873972686353106\n",
      "[ 2.82666631 -2.54258343]\n",
      "Gradient Descent(4245/9): loss=18.817604336926756, w0=71.04404847169407, w1=12.65378108527804\n",
      "[-0.86180672 -1.82573671]\n",
      "Gradient Descent(4246/9): loss=18.257934617469424, w0=71.64731317486884, w1=13.931796782685183\n",
      "[-2.59279676 -2.58618957]\n",
      "Gradient Descent(4247/9): loss=16.843738312511654, w0=73.46227090577474, w1=15.742129484875527\n",
      "[ 0.97418496  5.48680618]\n",
      "Gradient Descent(4248/9): loss=17.95932399932129, w0=72.78034143173971, w1=11.901365157931098\n",
      "[-1.92728548 -1.66704583]\n",
      "Gradient Descent(4249/9): loss=16.763360433455986, w0=74.12944126475519, w1=13.068297241122826\n",
      "[ 3.28141494 -0.41599452]\n",
      "Gradient Descent(4250/9): loss=15.819565318830989, w0=71.83245080564002, w1=13.359493404067537\n",
      "[-3.07021099 -2.1976393 ]\n",
      "Gradient Descent(4251/9): loss=16.46106320557592, w0=73.9815984963939, w1=14.897840914800625\n",
      "[ 0.07629506  0.70521436]\n",
      "Gradient Descent(4252/9): loss=16.627881541854354, w0=73.92819195679878, w1=14.404190864953685\n",
      "[ 2.44219792 -1.81681759]\n",
      "Gradient Descent(4253/9): loss=16.014367240277846, w0=72.21865341546713, w1=15.67596317898395\n",
      "[-2.46688513  1.02697569]\n",
      "Gradient Descent(4254/9): loss=18.375747800783788, w0=73.94547300760709, w1=14.957080194247881\n",
      "[ 1.07101907  0.97357208]\n",
      "Gradient Descent(4255/9): loss=16.689454973263402, w0=73.19575965628344, w1=14.27557973886917\n",
      "[ 3.20470527  1.07616673]\n",
      "Gradient Descent(4256/9): loss=15.707408174590723, w0=70.95246596794141, w1=13.522263030094454\n",
      "[-2.91237696  0.519808  ]\n",
      "Gradient Descent(4257/9): loss=18.12800132536231, w0=72.99112983861623, w1=13.158397428620972\n",
      "[ 0.07681434 -1.4941943 ]\n",
      "Gradient Descent(4258/9): loss=15.483351082623221, w0=72.93735980283259, w1=14.204333436228271\n",
      "[-4.2476204  -0.80981201]\n",
      "Gradient Descent(4259/9): loss=15.711993967522925, w0=75.91069408493725, w1=14.7712018403237\n",
      "[ 2.20098811 -0.82129396]\n",
      "Gradient Descent(4260/9): loss=19.643608377619838, w0=74.37000240648018, w1=15.34610761489311\n",
      "[-0.10817833  1.1950993 ]\n",
      "Gradient Descent(4261/9): loss=17.70657787095388, w0=74.4457272347783, w1=14.509538101533426\n",
      "[-0.81383999  0.98750446]\n",
      "Gradient Descent(4262/9): loss=16.579485967572765, w0=75.01541522919683, w1=13.81828498188901\n",
      "[ 2.62027903  3.6752771 ]\n",
      "Gradient Descent(4263/9): loss=16.924973019047762, w0=73.18121990722892, w1=11.245591011911703\n",
      "[ 0.75811144 -4.09577181]\n",
      "Gradient Descent(4264/9): loss=17.887888016450724, w0=72.65054189788853, w1=14.112631275719611\n",
      "[-0.63982979  0.36517471]\n",
      "Gradient Descent(4265/9): loss=15.79314997755618, w0=73.09842274909217, w1=13.857008978814207\n",
      "[ 0.24334038 -2.50728662]\n",
      "Gradient Descent(4266/9): loss=15.47617418878493, w0=72.92808448154557, w1=15.61210961272917\n",
      "[-0.43665588  3.61519745]\n",
      "Gradient Descent(4267/9): loss=17.726365276371027, w0=73.23374359642354, w1=13.081471400798364\n",
      "[ 2.15180699  1.73986598]\n",
      "Gradient Descent(4268/9): loss=15.466996549741225, w0=71.72747870462555, w1=11.863565213856825\n",
      "[ 0.67461475 -0.0827508 ]\n",
      "Gradient Descent(4269/9): loss=17.918726091125542, w0=71.25524837690578, w1=11.921490777155256\n",
      "[-1.78163715 -0.18055293]\n",
      "Gradient Descent(4270/9): loss=18.67801031134239, w0=72.50239438451595, w1=12.047877825189605\n",
      "[-2.59662451  0.21045261]\n",
      "Gradient Descent(4271/9): loss=16.724221028442408, w0=74.32003154317582, w1=11.900561001229013\n",
      "[ 0.95981633 -0.22234378]\n",
      "Gradient Descent(4272/9): loss=17.159197889340675, w0=73.64816011291751, w1=12.056201649518078\n",
      "[ 0.9654132 -1.421635 ]\n",
      "Gradient Descent(4273/9): loss=16.46182166658143, w0=72.9723708743505, w1=13.051346146236368\n",
      "[-0.3903775  -0.40078732]\n",
      "Gradient Descent(4274/9): loss=15.52933427137943, w0=73.24563512557835, w1=13.33189727186355\n",
      "[ 0.24241787  1.24064319]\n",
      "Gradient Descent(4275/9): loss=15.397978341276668, w0=73.07594261611268, w1=12.463447035871782\n",
      "[ 5.34505076 -2.66863814]\n",
      "Gradient Descent(4276/9): loss=15.926043055909721, w0=69.33440708504872, w1=14.331493734201096\n",
      "[-3.84778256  0.06722132]\n",
      "Gradient Descent(4277/9): loss=23.58753274886942, w0=72.0278548763775, w1=14.284438808039592\n",
      "[-3.13106031 -0.64236551]\n",
      "Gradient Descent(4278/9): loss=16.511143119995126, w0=74.21959708987873, w1=14.734094661722125\n",
      "[ 2.80531515  2.37276795]\n",
      "Gradient Descent(4279/9): loss=16.601062438263593, w0=72.25587648759844, w1=13.073157099835289\n",
      "[-0.64383499 -2.88139431]\n",
      "Gradient Descent(4280/9): loss=16.00730073419418, w0=72.70656098400491, w1=15.090133116739343\n",
      "[-1.86319937  2.90141036]\n",
      "Gradient Descent(4281/9): loss=16.855111737725842, w0=74.01080053976614, w1=13.059145863978305\n",
      "[-0.85048773 -2.03352875]\n",
      "Gradient Descent(4282/9): loss=15.731283408034718, w0=74.60614195058909, w1=14.48261598907668\n",
      "[ 4.40432674  6.435129  ]\n",
      "Gradient Descent(4283/9): loss=16.749756234829746, w0=71.52311323236283, w1=9.978025687028165\n",
      "[-4.63144895  0.11565793]\n",
      "Gradient Descent(4284/9): loss=23.08467475875005, w0=74.76512750043412, w1=9.897065135152317\n",
      "[ 1.65873813 -3.96173695]\n",
      "Gradient Descent(4285/9): loss=22.885791515499754, w0=73.60401081073871, w1=12.670281003152107\n",
      "[ 1.65364987 -5.41251019]\n",
      "Gradient Descent(4286/9): loss=15.76155502487208, w0=72.44645590240977, w1=16.459038134585654\n",
      "[-4.46537113  4.10693098]\n",
      "Gradient Descent(4287/9): loss=20.183178076034288, w0=75.57221569016153, w1=13.584186448171083\n",
      "[ 2.08190298 -0.15226281]\n",
      "Gradient Descent(4288/9): loss=17.98665634306326, w0=74.1148836063479, w1=13.690770413076207\n",
      "[ 0.25546244 -0.91095067]\n",
      "Gradient Descent(4289/9): loss=15.745149581706903, w0=73.93605989957216, w1=14.328435883390217\n",
      "[-1.65186125 -0.61712783]\n",
      "Gradient Descent(4290/9): loss=15.952224154444034, w0=75.09236277550447, w1=14.76042536346717\n",
      "[ 2.3812287   2.89367225]\n",
      "Gradient Descent(4291/9): loss=17.8231952791274, w0=73.42550268569155, w1=12.734854785764375\n",
      "[ 1.62326752 -0.55503042]\n",
      "Gradient Descent(4292/9): loss=15.671951065780181, w0=72.28921542401287, w1=13.123376082543645\n",
      "[ 0.53851398 -3.08258652]\n",
      "Gradient Descent(4293/9): loss=15.95409332089744, w0=71.91225563945542, w1=15.281186647965516\n",
      "[-1.43507435  2.69853868]\n",
      "Gradient Descent(4294/9): loss=17.96304350767791, w0=72.91680768096562, w1=13.392209570514545\n",
      "[ 0.17584795 -1.03561833]\n",
      "Gradient Descent(4295/9): loss=15.460823850079302, w0=72.79371411625942, w1=14.117142400695686\n",
      "[-1.02086628  0.27871166]\n",
      "Gradient Descent(4296/9): loss=15.71415031395093, w0=73.5083205146075, w1=13.922044235997339\n",
      "[ 0.20538357  0.03091935]\n",
      "Gradient Descent(4297/9): loss=15.506699941002621, w0=73.36455201814341, w1=13.900400692952829\n",
      "[-0.15008642  2.07665136]\n",
      "Gradient Descent(4298/9): loss=15.476871473606478, w0=73.46961251098695, w1=12.446744744050523\n",
      "[ 0.1014708  -0.70934779]\n",
      "Gradient Descent(4299/9): loss=15.934832571546398, w0=73.39858295226627, w1=12.943288197129256\n",
      "[ 4.25837083  1.53412533]\n",
      "Gradient Descent(4300/9): loss=15.53524030755544, w0=70.41772336988727, w1=11.869400467347777\n",
      "[ 0.22011546 -0.74117971]\n",
      "Gradient Descent(4301/9): loss=20.818699471379862, w0=70.26364254629588, w1=12.388226262472049\n",
      "[-0.54426123  1.45028487]\n",
      "Gradient Descent(4302/9): loss=20.57285569137727, w0=70.64462541072376, w1=11.37302685310715\n",
      "[-0.11775423 -4.54049479]\n",
      "Gradient Descent(4303/9): loss=21.114336153836554, w0=70.72705337158804, w1=14.551373207255875\n",
      "[-3.95343905  0.88565802]\n",
      "Gradient Descent(4304/9): loss=19.254523557403655, w0=73.49446070890914, w1=13.931412592741799\n",
      "[-0.1733967  -1.45622043]\n",
      "Gradient Descent(4305/9): loss=15.508012271549628, w0=73.61583839786759, w1=14.950766890342457\n",
      "[ 0.49638106  3.7505682 ]\n",
      "Gradient Descent(4306/9): loss=16.519703557067285, w0=73.26837165688535, w1=12.325369153106799\n",
      "[-1.75788034  0.24941687]\n",
      "Gradient Descent(4307/9): loss=16.05246848511317, w0=74.49888789595931, w1=12.150777341462659\n",
      "[ 2.49301523 -1.23720843]\n",
      "Gradient Descent(4308/9): loss=16.994893512908234, w0=72.753777234176, w1=13.016823245951308\n",
      "[ 1.51870097 -1.01075278]\n",
      "Gradient Descent(4309/9): loss=15.638899254654051, w0=71.69068655701331, w1=13.724350189444598\n",
      "[-1.56419909  0.71883051]\n",
      "Gradient Descent(4310/9): loss=16.70099363048141, w0=72.7856259230895, w1=13.221168829174882\n",
      "[ 0.11257885 -0.14275915]\n",
      "Gradient Descent(4311/9): loss=15.54849271885446, w0=72.7068207306482, w1=13.321100232275988\n",
      "[-2.50908164 -1.65336263]\n",
      "Gradient Descent(4312/9): loss=15.570810735727353, w0=74.46317787922813, w1=14.478454074813799\n",
      "[ 2.67856367 -1.46930865]\n",
      "Gradient Descent(4313/9): loss=16.568209953482576, w0=72.58818330842637, w1=15.506970132924993\n",
      "[-0.29017903  4.40960057]\n",
      "Gradient Descent(4314/9): loss=17.68980830762742, w0=72.79130863145312, w1=12.4202497344759\n",
      "[-0.3356928  -1.69904319]\n",
      "Gradient Descent(4315/9): loss=16.073428575897825, w0=73.0262935915768, w1=13.609579968052643\n",
      "[-0.53518757  0.23216767]\n",
      "Gradient Descent(4316/9): loss=15.430133139962388, w0=73.40092489178167, w1=13.447062595692218\n",
      "[ 2.54232696 -0.33177433]\n",
      "Gradient Descent(4317/9): loss=15.39214568403201, w0=71.6212960173364, w1=13.679304628121779\n",
      "[-3.55093463 -1.50103196]\n",
      "Gradient Descent(4318/9): loss=16.80464523307104, w0=74.1069502592308, w1=14.73002700143392\n",
      "[-1.86202173  1.28313165]\n",
      "Gradient Descent(4319/9): loss=16.498038599803863, w0=75.41036547067607, w1=13.831834844058916\n",
      "[-1.30822411 -0.14789851]\n",
      "Gradient Descent(4320/9): loss=17.68754944214215, w0=76.32612234515311, w1=13.935363798349881\n",
      "[ 3.33159871 -0.28657341]\n",
      "Gradient Descent(4321/9): loss=20.08681641148565, w0=73.994003249607, w1=14.135965182999561\n",
      "[ 4.17509813  0.64942524]\n",
      "Gradient Descent(4322/9): loss=15.846278580016921, w0=71.07143456119415, w1=13.681367514802664\n",
      "[-5.80927938  0.13293237]\n",
      "Gradient Descent(4323/9): loss=17.87594546694036, w0=75.1379301256831, w1=13.588314855917837\n",
      "[ 3.37425462 -0.1039964 ]\n",
      "Gradient Descent(4324/9): loss=17.091968091655854, w0=72.77595189382453, w1=13.661112337911053\n",
      "[-0.1357244   0.15411681]\n",
      "Gradient Descent(4325/9): loss=15.536487347755596, w0=72.87095897098479, w1=13.553230569063297\n",
      "[-1.6056357   0.27033877]\n",
      "Gradient Descent(4326/9): loss=15.478039189695558, w0=73.99490396070455, w1=13.363993432452377\n",
      "[-1.26813594  0.48274703]\n",
      "Gradient Descent(4327/9): loss=15.638271165744337, w0=74.882599120968, w1=13.026070514107854\n",
      "[-1.05654692 -1.66562329]\n",
      "Gradient Descent(4328/9): loss=16.7507308590189, w0=75.62218196773362, w1=14.192006818986734\n",
      "[ 1.39741015  1.11857254]\n",
      "Gradient Descent(4329/9): loss=18.34996674734078, w0=74.64399486322853, w1=13.409006042342652\n",
      "[ 3.08077007 -1.27804702]\n",
      "Gradient Descent(4330/9): loss=16.299735930980813, w0=72.4874558124509, w1=14.303638959487914\n",
      "[ 1.57490626  0.97722794]\n",
      "Gradient Descent(4331/9): loss=16.050509185243545, w0=71.38502142842846, w1=13.619579402778525\n",
      "[-2.00083013 -0.16940809]\n",
      "Gradient Descent(4332/9): loss=17.21761995326039, w0=72.78560252065692, w1=13.738165066200771\n",
      "[-0.4572983  -0.81871114]\n",
      "Gradient Descent(4333/9): loss=15.548481097729448, w0=73.1057113315503, w1=14.311262865538904\n",
      "[-2.54768564 -1.72565036]\n",
      "Gradient Descent(4334/9): loss=15.749337556358586, w0=74.88909127883495, w1=15.519218117630889\n",
      "[ 2.62214183  1.28813464]\n",
      "Gradient Descent(4335/9): loss=18.737962094304855, w0=73.05359199435915, w1=14.617523866774393\n",
      "[-1.3361346  -0.40579765]\n",
      "Gradient Descent(4336/9): loss=16.062074552291712, w0=73.98888621673656, w1=14.901582220616618\n",
      "[-0.01078036  0.30839322]\n",
      "Gradient Descent(4337/9): loss=16.63823234227879, w0=73.99643247091565, w1=14.685706966141893\n",
      "[ 4.41229203  1.00478047]\n",
      "Gradient Descent(4338/9): loss=16.35985975280883, w0=70.90782805193466, w1=13.98236063435339\n",
      "[-1.01369985  0.02377401]\n",
      "Gradient Descent(4339/9): loss=18.358937644511713, w0=71.61741794861669, w1=13.965718828575696\n",
      "[ 0.37381725  3.01948367]\n",
      "Gradient Descent(4340/9): loss=16.909321896814635, w0=71.35574587185808, w1=11.852080256614432\n",
      "[-3.73613409 -1.20377032]\n",
      "Gradient Descent(4341/9): loss=18.588744478799484, w0=73.97103973238191, w1=12.694719478414449\n",
      "[ 2.78245659 -0.67475866]\n",
      "Gradient Descent(4342/9): loss=15.923239050092816, w0=72.02332011781101, w1=13.16705053946648\n",
      "[-1.16564927 -0.73397857]\n",
      "Gradient Descent(4343/9): loss=16.241981173471242, w0=72.83927460476818, w1=13.680835539207594\n",
      "[ 1.24304154 -0.62727167]\n",
      "Gradient Descent(4344/9): loss=15.509465248307313, w0=71.96914552504015, w1=14.119925709340448\n",
      "[-2.62011803 -0.03948754]\n",
      "Gradient Descent(4345/9): loss=16.4683407442497, w0=73.80322814574494, w1=14.14756699020928\n",
      "[-3.00975577  1.84019583]\n",
      "Gradient Descent(4346/9): loss=15.7385990962682, w0=75.91005718461669, w1=12.859429910052484\n",
      "[ 2.021839    1.76321854]\n",
      "Gradient Descent(4347/9): loss=19.00034472078748, w0=74.49476988189626, w1=11.625176935427886\n",
      "[ 1.8757096  -4.08366497]\n",
      "Gradient Descent(4348/9): loss=17.826556643595048, w0=73.18177316348074, w1=14.48374241099718\n",
      "[ 2.03868509  1.76054602]\n",
      "Gradient Descent(4349/9): loss=15.89621464619325, w0=71.7546936014706, w1=13.251360198641137\n",
      "[-1.90069879  0.51824587]\n",
      "Gradient Descent(4350/9): loss=16.596572275411994, w0=73.0851827557637, w1=12.888588091062422\n",
      "[ 0.07010475 -0.79221406]\n",
      "Gradient Descent(4351/9): loss=15.582387900302349, w0=73.0361094308545, w1=13.443137932712613\n",
      "[ 0.11308547 -0.67451445]\n",
      "Gradient Descent(4352/9): loss=15.419790376885233, w0=72.95694960070976, w1=13.91529805053041\n",
      "[ 0.42744481  0.30148782]\n",
      "Gradient Descent(4353/9): loss=15.537530482713773, w0=72.65773823223785, w1=13.704256577954014\n",
      "[ 0.48886416  2.51720242]\n",
      "Gradient Descent(4354/9): loss=15.613462799420647, w0=72.31553332145677, w1=11.942214885489623\n",
      "[-0.23376461 -2.36273699]\n",
      "Gradient Descent(4355/9): loss=17.046459431398247, w0=72.47916855101768, w1=13.596130778084003\n",
      "[-1.87745451  2.43652209]\n",
      "Gradient Descent(4356/9): loss=15.724576077163391, w0=73.79338670861156, w1=11.890565318477151\n",
      "[-3.53162182 -2.19412398]\n",
      "Gradient Descent(4357/9): loss=16.77331464431119, w0=76.26552198484745, w1=13.426452105429277\n",
      "[ 1.9356075   0.18570188]\n",
      "Gradient Descent(4358/9): loss=19.80250942889872, w0=74.91059673563197, w1=13.296460787055022\n",
      "[ 0.40555923 -2.18518724]\n",
      "Gradient Descent(4359/9): loss=16.70949704907661, w0=74.62670527187606, w1=14.826091857583902\n",
      "[ 0.76301034 -0.16192946]\n",
      "Gradient Descent(4360/9): loss=17.180412265713397, w0=74.09259803299574, w1=14.939442482376748\n",
      "[ 1.5202187   3.69906232]\n",
      "Gradient Descent(4361/9): loss=16.770235475612186, w0=73.02844494493621, w1=12.350098855528858\n",
      "[ 2.97122349 -0.70540314]\n",
      "Gradient Descent(4362/9): loss=16.059140322221385, w0=70.94858850481003, w1=12.843881055834085\n",
      "[-1.4754987  -1.97642803]\n",
      "Gradient Descent(4363/9): loss=18.338323246955813, w0=71.98143759449954, w1=14.227380673737148\n",
      "[-2.31634348  1.08486065]\n",
      "Gradient Descent(4364/9): loss=16.526699426549722, w0=73.60287802880036, w1=13.46797821612161\n",
      "[ 1.92925521  2.55664636]\n",
      "Gradient Descent(4365/9): loss=15.43368362799125, w0=72.25239938239403, w1=11.678325761024691\n",
      "[-0.25116569 -0.73447898]\n",
      "Gradient Descent(4366/9): loss=17.550769527082576, w0=72.42821536748085, w1=12.192461043804872\n",
      "[-3.85830573  0.29835608]\n",
      "Gradient Descent(4367/9): loss=16.5891199294985, w0=75.12902937537204, w1=11.983611787005621\n",
      "[ 3.28238575 -3.98635928]\n",
      "Gradient Descent(4368/9): loss=18.1888559789868, w0=72.83135934881125, w1=14.774063280247258\n",
      "[ 1.21166806 -1.29156797]\n",
      "Gradient Descent(4369/9): loss=16.33054202825089, w0=71.98319170844626, w1=15.678160861148637\n",
      "[-0.66114527  4.7351238 ]\n",
      "Gradient Descent(4370/9): loss=18.66148256142881, w0=72.44599339436017, w1=12.363574197823546\n",
      "[ 5.81547293 -1.61589178]\n",
      "Gradient Descent(4371/9): loss=16.368261612977058, w0=68.37516234399895, w1=13.494698441503969\n",
      "[-5.54183687 -1.17391094]\n",
      "Gradient Descent(4372/9): loss=27.48309844613173, w0=72.25444815297097, w1=14.316436097307264\n",
      "[-1.33392889 -0.67421236]\n",
      "Gradient Descent(4373/9): loss=16.276194053887966, w0=73.18819837782175, w1=14.788384752368701\n",
      "[ 1.20276471 -1.47978899]\n",
      "Gradient Descent(4374/9): loss=16.247788228333132, w0=72.34626308016426, w1=15.82423704208697\n",
      "[ 1.37353705  0.84062569]\n",
      "Gradient Descent(4375/9): loss=18.583314401640354, w0=71.38478714289596, w1=15.235799056916035\n",
      "[ 0.53473431  5.62499146]\n",
      "Gradient Descent(4376/9): loss=18.750205936008797, w0=71.01047312491565, w1=11.298305032718849\n",
      "[-3.45182458 -2.13706136]\n",
      "Gradient Descent(4377/9): loss=20.37222638353809, w0=73.4267503333462, w1=12.794247986647418\n",
      "[ 2.34732321 -1.37314493]\n",
      "Gradient Descent(4378/9): loss=15.629640306589685, w0=71.78362408396603, w1=13.755449437170206\n",
      "[-2.26814675 -1.09932927]\n",
      "Gradient Descent(4379/9): loss=16.564403216783063, w0=73.37132681035847, w1=14.524979926516254\n",
      "[-1.94946176  0.3578417 ]\n",
      "Gradient Descent(4380/9): loss=15.935175685421552, w0=74.7359500432211, w1=14.2744907362976\n",
      "[-2.6017801  -1.58404591]\n",
      "Gradient Descent(4381/9): loss=16.741446578627137, w0=76.55719611143147, w1=15.383322875673917\n",
      "[ 4.85962967  0.94389895]\n",
      "Gradient Descent(4382/9): loss=22.522233180071215, w0=73.15545534144707, w1=14.722593610344092\n",
      "[-2.14885464  0.41366214]\n",
      "Gradient Descent(4383/9): loss=16.167851184912273, w0=74.65965359054198, w1=14.433030115838097\n",
      "[ 2.51861476 -1.03895415]\n",
      "Gradient Descent(4384/9): loss=16.77290655496614, w0=72.89662325787891, w1=15.160298017430792\n",
      "[-0.94052416  3.58453411]\n",
      "Gradient Descent(4385/9): loss=16.876994964866817, w0=73.55499016727377, w1=12.651124139224978\n",
      "[-0.28831207 -1.19963975]\n",
      "Gradient Descent(4386/9): loss=15.763245444200251, w0=73.75680861958226, w1=13.490871965292705\n",
      "[ 1.19312475  0.63198313]\n",
      "Gradient Descent(4387/9): loss=15.493082146707382, w0=72.92162129315942, w1=13.048483771887451\n",
      "[-1.24057947  1.37295633]\n",
      "Gradient Descent(4388/9): loss=15.548170857710357, w0=73.79002692315923, w1=12.087414337558862\n",
      "[-0.20660352  2.64682007]\n",
      "Gradient Descent(4389/9): loss=16.47819491123028, w0=73.9346493837012, w1=10.234640290316465\n",
      "[ 1.55102655 -5.80599566]\n",
      "Gradient Descent(4390/9): loss=20.856400269657804, w0=72.84893079897033, w1=14.298837252196464\n",
      "[ 2.5531001   0.98846396]\n",
      "Gradient Descent(4391/9): loss=15.820379187345647, w0=71.06176072712903, w1=13.606912479532982\n",
      "[-3.34039314 -6.29487678]\n",
      "Gradient Descent(4392/9): loss=17.885249773246993, w0=73.40003592602832, w1=18.01332622369174\n",
      "[ 0.03533462  5.6642208 ]\n",
      "Gradient Descent(4393/9): loss=25.66834494381218, w0=73.37530168973699, w1=14.04837166548661\n",
      "[-0.44808165  1.88858164]\n",
      "Gradient Descent(4394/9): loss=15.550885855823966, w0=73.68895884195236, w1=12.726364519020962\n",
      "[ 0.41116085  0.9023082 ]\n",
      "Gradient Descent(4395/9): loss=15.747681462494349, w0=73.40114624782964, w1=12.094748779712873\n",
      "[-0.90932458  0.14356038]\n",
      "Gradient Descent(4396/9): loss=16.35069855148296, w0=74.0376734571453, w1=11.99425651324382\n",
      "[ 1.98344782 -2.88772276]\n",
      "Gradient Descent(4397/9): loss=16.765760629990528, w0=72.64925998266969, w1=14.015662447141494\n",
      "[ 1.94747162  0.74951854]\n",
      "Gradient Descent(4398/9): loss=15.737303636243828, w0=71.28602984986763, w1=13.49099946723216\n",
      "[-2.80489834 -0.14283829]\n",
      "Gradient Descent(4399/9): loss=17.401767014886417, w0=73.2494586908231, w1=13.590986272831081\n",
      "[-0.98088666  1.87903204]\n",
      "Gradient Descent(4400/9): loss=15.393067295348528, w0=73.93607935325039, w1=12.275663841759169\n",
      "[ 0.12550582  1.52735189]\n",
      "Gradient Descent(4401/9): loss=16.316937408073738, w0=73.84822527935566, w1=11.20651751592011\n",
      "[ 0.28185067 -3.06265295]\n",
      "Gradient Descent(4402/9): loss=18.123221500455127, w0=73.6509298081619, w1=13.35037457957512\n",
      "[-2.48952399  2.44005582]\n",
      "Gradient Descent(4403/9): loss=15.457979296043652, w0=75.39359659821977, w1=11.64233550897257\n",
      "[ 1.45098196 -2.00834937]\n",
      "Gradient Descent(4404/9): loss=19.278181557742755, w0=74.37790922889072, w1=13.048180067701262\n",
      "[ 4.16271798  0.25324072]\n",
      "Gradient Descent(4405/9): loss=16.06651211475499, w0=71.46400663982035, w1=12.870911560730715\n",
      "[-2.88037065  1.14546944]\n",
      "Gradient Descent(4406/9): loss=17.245502237641276, w0=73.48026609158326, w1=12.069082953308058\n",
      "[ 1.07793328 -1.42855259]\n",
      "Gradient Descent(4407/9): loss=16.398187695964896, w0=72.7257127974337, w1=13.069069769277988\n",
      "[-0.329261    0.59815479]\n",
      "Gradient Descent(4408/9): loss=15.631632418417247, w0=72.95619549886963, w1=12.650361418921275\n",
      "[ 2.92787248  1.15859915]\n",
      "Gradient Descent(4409/9): loss=15.786829018249584, w0=70.9066847635108, w1=11.839342015057321\n",
      "[-2.27169414  1.41484108]\n",
      "Gradient Descent(4410/9): loss=19.580746242788788, w0=72.49687065804784, w1=10.848953259310944\n",
      "[ 2.77580314 -2.32949473]\n",
      "Gradient Descent(4411/9): loss=19.163980211568486, w0=70.55380846127014, w1=12.479599571487972\n",
      "[-2.19722352  1.74826435]\n",
      "Gradient Descent(4412/9): loss=19.640111847033364, w0=72.09186492440814, w1=11.25581452992272\n",
      "[-2.16912076 -1.04591536]\n",
      "Gradient Descent(4413/9): loss=18.58121942392938, w0=73.61024945760339, w1=11.987955278687322\n",
      "[-1.28804618 -0.75435792]\n",
      "Gradient Descent(4414/9): loss=16.54858910506909, w0=74.51188178426992, w1=12.516005822627339\n",
      "[ 1.50970611 -0.79664001]\n",
      "Gradient Descent(4415/9): loss=16.59196610166962, w0=73.455087507534, w1=13.073653826698001\n",
      "[-1.58434455 -1.58665191]\n",
      "Gradient Descent(4416/9): loss=15.481316825583095, w0=74.56412869178952, w1=14.184310167052894\n",
      "[ 6.10967402  3.59932109]\n",
      "Gradient Descent(4417/9): loss=16.440829368103568, w0=70.28735687974375, w1=11.664785400846526\n",
      "[-1.27912152 -0.36334479]\n",
      "Gradient Descent(4418/9): loss=21.552584855960195, w0=71.18274194561208, w1=11.919126757251032\n",
      "[-0.81411545 -3.88880992]\n",
      "Gradient Descent(4419/9): loss=18.832142313077036, w0=71.75262275766278, w1=14.64129370207702\n",
      "[ 1.8300697   1.02335271]\n",
      "Gradient Descent(4420/9): loss=17.248325069313626, w0=70.47157396835675, w1=13.924946807112702\n",
      "[-6.47363507  1.06026893]\n",
      "Gradient Descent(4421/9): loss=19.467828903691412, w0=75.00311851949193, w1=13.182758559584084\n",
      "[ 4.25431849  1.75033717]\n",
      "Gradient Descent(4422/9): loss=16.890655038411904, w0=72.02509557845832, w1=11.957522542776806\n",
      "[-0.18987957 -1.31864753]\n",
      "Gradient Descent(4423/9): loss=17.34937914947821, w0=72.15801127547572, w1=12.880575816583656\n",
      "[ 0.36848334  0.12713117]\n",
      "Gradient Descent(4424/9): loss=16.21051680202247, w0=71.90007293460212, w1=12.791583997882817\n",
      "[-4.15670887 -1.12704961]\n",
      "Gradient Descent(4425/9): loss=16.59405585329612, w0=74.80976914174549, w1=13.580518722969734\n",
      "[ 3.29908302 -0.60469558]\n",
      "Gradient Descent(4426/9): loss=16.539865098055465, w0=72.5004110269193, w1=14.003805630816258\n",
      "[ 1.38722552  0.84328179]\n",
      "Gradient Descent(4427/9): loss=15.838054541655822, w0=71.52935316320054, w1=13.413508380564773\n",
      "[-0.50662711  0.68824759]\n",
      "Gradient Descent(4428/9): loss=16.944930950857156, w0=71.88399213938453, w1=12.931735069113568\n",
      "[ 0.02996349 -3.88724979]\n",
      "Gradient Descent(4429/9): loss=16.529978574481063, w0=71.86301769439332, w1=15.652809925307437\n",
      "[-2.5988332   0.24807802]\n",
      "Gradient Descent(4430/9): loss=18.770807788957637, w0=73.6822009316205, w1=15.479155310040102\n",
      "[ 0.43756467 -1.32738775]\n",
      "Gradient Descent(4431/9): loss=17.460154037678386, w0=73.375905663873, w1=16.4083267350479\n",
      "[ 1.10377421  5.83730155]\n",
      "Gradient Descent(4432/9): loss=19.67763938848243, w0=72.60326371486393, w1=12.322215646726036\n",
      "[-0.40483538  0.16966716]\n",
      "Gradient Descent(4433/9): loss=16.294291711116507, w0=72.88664847902776, w1=12.203448637718115\n",
      "[-1.93735878  0.74797618]\n",
      "Gradient Descent(4434/9): loss=16.283248370241562, w0=74.24279962411711, w1=11.679865310728578\n",
      "[-0.33856791 -1.04131699]\n",
      "Gradient Descent(4435/9): loss=17.455797074961243, w0=74.47979716454897, w1=12.40878720566316\n",
      "[-1.04799685  1.442571  ]\n",
      "Gradient Descent(4436/9): loss=16.662478242683285, w0=75.21339495833035, w1=11.39898750269214\n",
      "[ 2.77999988 -1.70724965]\n",
      "Gradient Descent(4437/9): loss=19.392784205610276, w0=73.26739504310984, w1=12.594062255217256\n",
      "[ 0.23649485 -2.33924234]\n",
      "Gradient Descent(4438/9): loss=15.778427829071076, w0=73.10184864781658, w1=14.231531894497044\n",
      "[-0.23784542  0.90469137]\n",
      "Gradient Descent(4439/9): loss=15.686950205390685, w0=73.2683404416908, w1=13.598247932701645\n",
      "[ 3.10444432 -0.77100652]\n",
      "Gradient Descent(4440/9): loss=15.393240409055004, w0=71.09522941983141, w1=14.137952498167078\n",
      "[-1.91482986  1.52042125]\n",
      "Gradient Descent(4441/9): loss=18.019652394888574, w0=72.43561031981788, w1=13.073657625400635\n",
      "[-5.83357904  0.92559407]\n",
      "Gradient Descent(4442/9): loss=15.836677594999774, w0=76.51911564691974, w1=12.425741776650824\n",
      "[ 4.97254056 -2.24202116]\n",
      "Gradient Descent(4443/9): loss=21.142251966424457, w0=73.03833725656372, w1=13.995156587540986\n",
      "[-2.46518424  2.32460114]\n",
      "Gradient Descent(4444/9): loss=15.551390987106142, w0=74.7639662267011, w1=12.367935787219745\n",
      "[ 1.93073619  0.15301246]\n",
      "Gradient Descent(4445/9): loss=17.084426537225873, w0=73.41245089644829, w1=12.260827062290804\n",
      "[-1.90101812  0.6865222 ]\n",
      "Gradient Descent(4446/9): loss=16.13575319411537, w0=74.7431635809433, w1=11.78026152150592\n",
      "[ 1.81750638 -1.38971606]\n",
      "Gradient Descent(4447/9): loss=17.880105149415314, w0=73.4709091116918, w1=12.753062762421116\n",
      "[ 0.35112104 -0.85232953]\n",
      "Gradient Descent(4448/9): loss=15.66555996063085, w0=73.22512438041058, w1=13.349693434143518\n",
      "[ 0.55640397  0.00837859]\n",
      "Gradient Descent(4449/9): loss=15.396706895495253, w0=72.83564160235915, w1=13.343828417878772\n",
      "[ 0.5816058 -0.4510395]\n",
      "Gradient Descent(4450/9): loss=15.500130564278107, w0=72.42851754338315, w1=13.659556070453757\n",
      "[ 0.91384379 -3.10344412]\n",
      "Gradient Descent(4451/9): loss=15.776522174025972, w0=71.7888268908088, w1=15.831966956227706\n",
      "[ 0.96800872  3.01052826]\n",
      "Gradient Descent(4452/9): loss=19.285094182197394, w0=71.11122078359904, w1=13.724597173887613\n",
      "[-4.18158001 -2.4298935 ]\n",
      "Gradient Descent(4453/9): loss=17.79796444113623, w0=74.03832679260101, w1=15.425522622770071\n",
      "[ 1.04532288  0.42839659]\n",
      "Gradient Descent(4454/9): loss=17.556045758322174, w0=73.3066007753723, w1=15.1256450127223\n",
      "[ 1.78035273  0.37889657]\n",
      "Gradient Descent(4455/9): loss=16.740515269696992, w0=72.06035386203405, w1=14.860417413081064\n",
      "[ 0.67462525  2.56601784]\n",
      "Gradient Descent(4456/9): loss=17.099906165192724, w0=71.58811618383886, w1=13.064204922496437\n",
      "[-3.92276067 -2.17993007]\n",
      "Gradient Descent(4457/9): loss=16.927097860113925, w0=74.3340486546636, w1=14.590155971089926\n",
      "[-0.8111615   1.97582526]\n",
      "Gradient Descent(4458/9): loss=16.543362018944695, w0=74.90186170773175, w1=13.20707829210551\n",
      "[-1.65490381 -0.89453015]\n",
      "Gradient Descent(4459/9): loss=16.715787605227543, w0=76.0602943734407, w1=13.833249395906087\n",
      "[-0.18478191 -1.87829279]\n",
      "Gradient Descent(4460/9): loss=19.274790108640847, w0=76.1896417073668, w1=15.14805434867051\n",
      "[ 4.70569389  0.2095256 ]\n",
      "Gradient Descent(4461/9): loss=20.970166545022757, w0=72.89565598740072, w1=15.001386429446239\n",
      "[ 2.22008598  0.54822777]\n",
      "Gradient Descent(4462/9): loss=16.622941650767345, w0=71.34159580379686, w1=14.617626993391436\n",
      "[-2.93082061 -1.03464655]\n",
      "Gradient Descent(4463/9): loss=17.93910143224197, w0=73.39317023169417, w1=15.341879577970756\n",
      "[-2.49639608 -1.49709935]\n",
      "Gradient Descent(4464/9): loss=17.124646208568002, w0=75.14064748745507, w1=16.38984912236225\n",
      "[ 1.19349147  5.11686467]\n",
      "Gradient Descent(4465/9): loss=21.325533147547507, w0=74.30520345974354, w1=12.80804385033229\n",
      "[ 1.61264497 -3.87737671]\n",
      "Gradient Descent(4466/9): loss=16.12280230591838, w0=73.17635197804483, w1=15.522207544674178\n",
      "[ 2.03582924  1.54863169]\n",
      "Gradient Descent(4467/9): loss=17.47869236065201, w0=71.75127151112817, w1=14.438165362839815\n",
      "[ 1.42740738  0.66848923]\n",
      "Gradient Descent(4468/9): loss=17.035089144938077, w0=70.75208634727687, w1=13.970222901714633\n",
      "[-2.39827659  3.14900729]\n",
      "Gradient Descent(4469/9): loss=18.736652375891328, w0=72.43087995685542, w1=11.765917800129632\n",
      "[-1.84062165 -2.98733647]\n",
      "Gradient Descent(4470/9): loss=17.226854680000308, w0=73.7193151140608, w1=13.857053326776732\n",
      "[ 0.04406356  2.37149338]\n",
      "Gradient Descent(4471/9): loss=15.547560592986603, w0=73.68847062410886, w1=12.197007962467397\n",
      "[ 1.54360361 -0.14461487]\n",
      "Gradient Descent(4472/9): loss=16.28638755830542, w0=72.6079480961139, w1=12.298238372564903\n",
      "[-2.88769991  0.77418531]\n",
      "Gradient Descent(4473/9): loss=16.31910844877038, w0=74.62933803502573, w1=11.756308656721\n",
      "[ 0.86325858 -0.36453574]\n",
      "Gradient Descent(4474/9): loss=17.762616150794305, w0=74.02505702830072, w1=12.011483674601132\n",
      "[ 0.55795431  0.25139631]\n",
      "Gradient Descent(4475/9): loss=16.731014928509488, w0=73.63448901149508, w1=11.835506260259928\n",
      "[ 3.25255257 -1.77333958]\n",
      "Gradient Descent(4476/9): loss=16.79558778528047, w0=71.3577022147423, w1=13.076843964841473\n",
      "[-0.28676156 -1.43136144]\n",
      "Gradient Descent(4477/9): loss=17.34151290343671, w0=71.55843530506846, w1=14.078796972673576\n",
      "[-4.25416576 -1.75732029]\n",
      "Gradient Descent(4478/9): loss=17.071296048271474, w0=74.5363513359271, w1=15.308921178649499\n",
      "[ 0.91656386 -0.94874464]\n",
      "Gradient Descent(4479/9): loss=17.83070550754189, w0=73.8947566356733, w1=15.97304242862539\n",
      "[-0.60070993  6.40908013]\n",
      "Gradient Descent(4480/9): loss=18.674736225860165, w0=74.31525358827906, w1=11.486686334394538\n",
      "[-1.43471605 -0.95418098]\n",
      "Gradient Descent(4481/9): loss=17.8935234921131, w0=75.31955482177773, w1=12.154613018972942\n",
      "[ 2.49944814  2.33939336]\n",
      "Gradient Descent(4482/9): loss=18.315426260059777, w0=73.56994112030296, w1=10.517037667862013\n",
      "[ 0.64115547 -2.30786895]\n",
      "Gradient Descent(4483/9): loss=19.812702033520356, w0=73.12113229015543, w1=12.132545935214315\n",
      "[-1.60519007  0.8207803 ]\n",
      "Gradient Descent(4484/9): loss=16.308244800164893, w0=74.24476533930373, w1=11.557999727782047\n",
      "[ 0.96832159 -0.05536694]\n",
      "Gradient Descent(4485/9): loss=17.684429259297257, w0=73.56694022661594, w1=11.596756588879364\n",
      "[-0.21880961 -3.18272108]\n",
      "Gradient Descent(4486/9): loss=17.195918703486225, w0=73.72010695042499, w1=13.824661347038719\n",
      "[ 0.69362932 -1.12666285]\n",
      "Gradient Descent(4487/9): loss=15.536199549878702, w0=73.23456642465602, w1=14.61332534395483\n",
      "[ 0.787969    0.43503113]\n",
      "Gradient Descent(4488/9): loss=16.030188524803492, w0=72.68298812421756, w1=14.30880355477212\n",
      "[ 3.17244787  4.22631754]\n",
      "Gradient Descent(4489/9): loss=15.916204012856381, w0=70.46227461842682, w1=11.350381273306935\n",
      "[-4.73782801 -1.09507506]\n",
      "Gradient Descent(4490/9): loss=21.662026919630925, w0=73.77875422452648, w1=12.116933815198168\n",
      "[ 2.68963344 -1.81256395]\n",
      "Gradient Descent(4491/9): loss=16.432001794057953, w0=71.89601081523345, w1=13.385728582800116\n",
      "[-0.37340404 -1.36991185]\n",
      "Gradient Descent(4492/9): loss=16.36738219425611, w0=72.15739364485043, w1=14.34466687476934\n",
      "[-2.05222894  0.85669084]\n",
      "Gradient Descent(4493/9): loss=16.405809313699322, w0=73.59395390106148, w1=13.744983287084285\n",
      "[-1.45475098  1.44091036]\n",
      "Gradient Descent(4494/9): loss=15.466081751510728, w0=74.61227958597043, w1=12.736346032573971\n",
      "[ 3.90255005 -1.22253144]\n",
      "Gradient Descent(4495/9): loss=16.531218032416664, w0=71.8804945497899, w1=13.592118039527374\n",
      "[-2.80959517  2.25749758]\n",
      "Gradient Descent(4496/9): loss=16.39109396027446, w0=73.84721116553237, w1=12.01186973546083\n",
      "[ 0.23848633 -1.09575123]\n",
      "Gradient Descent(4497/9): loss=16.616233413291518, w0=73.68027073533878, w1=12.778895594019152\n",
      "[ 0.71919027 -0.81293724]\n",
      "Gradient Descent(4498/9): loss=15.70609266295851, w0=73.17683754549853, w1=13.347951661544046\n",
      "[ 2.19037674 -4.25950745]\n",
      "Gradient Descent(4499/9): loss=15.40142270452825, w0=71.64357382887938, w1=16.329606874936\n",
      "[-3.2188022   2.78048379]\n",
      "Gradient Descent(4500/9): loss=20.80866157468456, w0=73.89673536937885, w1=14.383268221184332\n",
      "[ 1.18577464  2.50280588]\n",
      "Gradient Descent(4501/9): loss=15.975786376094788, w0=73.06669311856913, w1=12.631304108431987\n",
      "[ 1.86461859 -0.48906215]\n",
      "Gradient Descent(4502/9): loss=15.771602695871598, w0=71.76146010872603, w1=12.973647615771759\n",
      "[ 3.1128099   0.04417396]\n",
      "Gradient Descent(4503/9): loss=16.688158396783738, w0=69.58249317946199, w1=12.94272584541376\n",
      "[-3.48449419 -1.1416823 ]\n",
      "Gradient Descent(4504/9): loss=22.417417120294584, w0=72.02163911323557, w1=13.74190345320942\n",
      "[-3.25186375 -1.08130334]\n",
      "Gradient Descent(4505/9): loss=16.229611808502334, w0=74.29794373558026, w1=14.498815789748553\n",
      "[ 0.35426416  1.0823118 ]\n",
      "Gradient Descent(4506/9): loss=16.409203513315582, w0=74.04995882402116, w1=13.741197529673899\n",
      "[ 0.59231354 -1.03904178]\n",
      "Gradient Descent(4507/9): loss=15.705870934246972, w0=73.63533934390327, w1=14.468526774599818\n",
      "[ 2.66197018  3.16558715]\n",
      "Gradient Descent(4508/9): loss=15.933047668579576, w0=71.77196021732561, w1=12.252615768627292\n",
      "[ 1.63768717 -1.49090013]\n",
      "Gradient Descent(4509/9): loss=17.296954820292182, w0=70.62557919738666, w1=13.296245858117082\n",
      "[-7.85565999 -0.22135879]\n",
      "Gradient Descent(4510/9): loss=18.962744522990544, w0=76.12454119193372, w1=13.45119701292092\n",
      "[ 2.71446994 -1.2578455 ]\n",
      "Gradient Descent(4511/9): loss=19.392496932390014, w0=74.22441223405743, w1=14.331688859633811\n",
      "[ 2.2391907  1.6340786]\n",
      "Gradient Descent(4512/9): loss=16.1817258187839, w0=72.65697874538083, w1=13.187833836617028\n",
      "[-0.15710677 -0.56578275]\n",
      "Gradient Descent(4513/9): loss=15.631332783066528, w0=72.76695348684777, w1=13.583881763486382\n",
      "[ 1.6653577   1.46031363]\n",
      "Gradient Descent(4514/9): loss=15.530161401365499, w0=71.60120309962808, w1=12.561662225167314\n",
      "[-2.67605934 -0.33086541]\n",
      "Gradient Descent(4515/9): loss=17.23994460410792, w0=73.47444463485611, w1=12.793268011400178\n",
      "[-0.38494706 -0.52227848]\n",
      "Gradient Descent(4516/9): loss=15.63778505263519, w0=73.74390757899053, w1=13.158862945841552\n",
      "[ 1.56445499 -0.40936793]\n",
      "Gradient Descent(4517/9): loss=15.538603575874923, w0=72.64878908893166, w1=13.445420499567858\n",
      "[-0.31043516  0.20990135]\n",
      "Gradient Descent(4518/9): loss=15.594574075076746, w0=72.86609370420265, w1=13.298489552896942\n",
      "[ 1.7733697   1.09159465]\n",
      "Gradient Descent(4519/9): loss=15.493827261569374, w0=71.62473491342712, w1=12.53437329748276\n",
      "[-1.15904599 -0.99267587]\n",
      "Gradient Descent(4520/9): loss=17.22581367978475, w0=72.43606710696497, w1=13.229246404964853\n",
      "[ 1.59861457  0.86831856]\n",
      "Gradient Descent(4521/9): loss=15.78521199548546, w0=71.31703690577004, w1=12.621423414423464\n",
      "[-2.73217202 -0.84252896]\n",
      "Gradient Descent(4522/9): loss=17.708255232297134, w0=73.22955732227878, w1=13.211193688107103\n",
      "[-2.23631361  1.09190392]\n",
      "Gradient Descent(4523/9): loss=15.424010433547505, w0=74.7949768517389, w1=12.4468609406783\n",
      "[ 1.65730349 -2.35847838]\n",
      "Gradient Descent(4524/9): loss=17.045861804283806, w0=73.63486441125856, w1=14.097795806858059\n",
      "[ 0.15862244 -0.58282594]\n",
      "Gradient Descent(4525/9): loss=15.635022259299536, w0=73.5238287063728, w1=14.505773964336047\n",
      "[ 0.81893374 -0.54809684]\n",
      "Gradient Descent(4526/9): loss=15.938717546165947, w0=72.95057509013168, w1=14.889441750384325\n",
      "[-1.99586761  1.06378615]\n",
      "Gradient Descent(4527/9): loss=16.43849979115269, w0=74.34768241734228, w1=14.144791444727275\n",
      "[-0.61141275 -1.2065693 ]\n",
      "Gradient Descent(4528/9): loss=16.162258419786916, w0=74.77567133940302, w1=14.989389952603846\n",
      "[ 3.84991133  2.72778135]\n",
      "Gradient Descent(4529/9): loss=17.623241521716466, w0=72.08073341101752, w1=13.079943005133556\n",
      "[ 2.63639713 -0.14254422]\n",
      "Gradient Descent(4530/9): loss=16.201708946125535, w0=70.23525541734345, w1=13.179723955894318\n",
      "[-4.68145141 -1.07066612]\n",
      "Gradient Descent(4531/9): loss=20.108605050993198, w0=73.51227140225718, w1=13.92919024290003\n",
      "[ 0.45856363  0.95454798]\n",
      "Gradient Descent(4532/9): loss=15.510741249005, w0=73.19127685971158, w1=13.261006656468641\n",
      "[-2.10202825 -3.54270403]\n",
      "Gradient Descent(4533/9): loss=15.41507199023701, w0=74.66269663739865, w1=15.74089947653314\n",
      "[ 0.45841574  0.29155355]\n",
      "Gradient Descent(4534/9): loss=18.879143288364236, w0=74.34180562155377, w1=15.536811991194941\n",
      "[ 2.18523245  2.20916749]\n",
      "Gradient Descent(4535/9): loss=18.05074720085497, w0=72.81214290411725, w1=13.990394749751218\n",
      "[-5.99737569 -1.02959183]\n",
      "Gradient Descent(4536/9): loss=15.63234163176386, w0=77.01030588520081, w1=14.711109028039095\n",
      "[ 6.93695008 -0.40587487]\n",
      "Gradient Descent(4537/9): loss=23.04981123678346, w0=72.15444083104933, w1=14.99522143523836\n",
      "[-2.26446046 -0.26662452]\n",
      "Gradient Descent(4538/9): loss=17.183480303343156, w0=73.73956315461291, w1=15.181858601216362\n",
      "[-1.3909392   0.37373885]\n",
      "Gradient Descent(4539/9): loss=16.933836672834776, w0=74.71322059294855, w1=14.920241408503447\n",
      "[-0.64518737  2.31704899]\n",
      "Gradient Descent(4540/9): loss=17.43065397558159, w0=75.16485175319819, w1=13.298307117497972\n",
      "[ 1.53282935 -1.84774926]\n",
      "Gradient Descent(4541/9): loss=17.152530880198885, w0=74.0918712113617, w1=14.591731596514675\n",
      "[ 3.53819834  0.58146911]\n",
      "Gradient Descent(4542/9): loss=16.322542646906026, w0=71.61513237402048, w1=14.184703216896239\n",
      "[-2.14371736 -1.23691938]\n",
      "Gradient Descent(4543/9): loss=17.04356117779886, w0=73.11573452365728, w1=15.050546783960279\n",
      "[ 1.67860214  0.76093666]\n",
      "Gradient Descent(4544/9): loss=16.635523533521145, w0=71.94071302731433, w1=14.517891119939437\n",
      "[-2.30148722  0.61114945]\n",
      "Gradient Descent(4545/9): loss=16.840382624499323, w0=73.55175408198795, w1=14.090086504607747\n",
      "[ 3.00973478  4.111282  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4546/9): loss=15.605404811969183, w0=71.44493973806144, w1=11.212189103068678\n",
      "[-2.60815566 -2.82168439]\n",
      "Gradient Descent(4547/9): loss=19.66608660560521, w0=73.27064870266052, w1=13.187368174137111\n",
      "[ 0.19924624  0.0570651 ]\n",
      "Gradient Descent(4548/9): loss=15.428891275489452, w0=73.13117633620324, w1=13.147422607600134\n",
      "[ 0.10789162  0.91414213]\n",
      "Gradient Descent(4549/9): loss=15.454339209407411, w0=73.05565220309157, w1=12.507523113822476\n",
      "[ 1.25233969 -2.73882229]\n",
      "Gradient Descent(4550/9): loss=15.886850155485556, w0=72.17901441803568, w1=14.424698713876207\n",
      "[-1.18234384  1.05931468]\n",
      "Gradient Descent(4551/9): loss=16.453896862979757, w0=73.00665510711966, w1=13.68317843768966\n",
      "[-0.06258381 -1.94221752]\n",
      "Gradient Descent(4552/9): loss=15.447848210434197, w0=73.05046377270249, w1=15.042730698931114\n",
      "[ 0.91738202  2.40999917]\n",
      "Gradient Descent(4553/9): loss=16.637036870269586, w0=72.40829635631928, w1=13.355731282005824\n",
      "[ 1.35319688  0.58119969]\n",
      "Gradient Descent(4554/9): loss=15.785739924213777, w0=71.46105854021766, w1=12.948891500420475\n",
      "[-4.80310555  0.52953991]\n",
      "Gradient Descent(4555/9): loss=17.206467536078687, w0=74.8232324276789, w1=12.578213561955227\n",
      "[ 1.42953543 -3.2045747 ]\n",
      "Gradient Descent(4556/9): loss=16.961633166754254, w0=73.82255762503708, w1=14.82141584854082\n",
      "[-0.37048498  0.2918662 ]\n",
      "Gradient Descent(4557/9): loss=16.42569970471393, w0=74.08189710870415, w1=14.61710950818494\n",
      "[ 3.80447883  0.17572738]\n",
      "Gradient Descent(4558/9): loss=16.343176304196515, w0=71.4187619277314, w1=14.494100345292072\n",
      "[ 0.64701649 -0.09191592]\n",
      "Gradient Descent(4559/9): loss=17.658491937376635, w0=70.96585038205208, w1=14.55844148709366\n",
      "[-4.28209389  1.56954209]\n",
      "Gradient Descent(4560/9): loss=18.677674786805014, w0=73.96331610800442, w1=13.459762027436987\n",
      "[-0.09416909 -0.59450616]\n",
      "Gradient Descent(4561/9): loss=15.610131112716463, w0=74.02923447056007, w1=13.875916342416945\n",
      "[ 1.5345105  -0.77962077]\n",
      "Gradient Descent(4562/9): loss=15.73471885009257, w0=72.95507712074703, w1=14.421650881121687\n",
      "[ 1.48010798  0.24801519]\n",
      "Gradient Descent(4563/9): loss=15.886919813792096, w0=71.9190015377683, w1=14.248040246281839\n",
      "[ 1.79399869  3.33433135]\n",
      "Gradient Descent(4564/9): loss=16.626254823258577, w0=70.66320245324435, w1=11.914008300761047\n",
      "[-1.44053846 -0.61249746]\n",
      "Gradient Descent(4565/9): loss=20.071945259178005, w0=71.67157937405244, w1=12.342756520933614\n",
      "[ 1.21585153 -0.80414549]\n",
      "Gradient Descent(4566/9): loss=17.348220045480762, w0=70.82048330488382, w1=12.905658360614908\n",
      "[-2.34409158  2.52672013]\n",
      "Gradient Descent(4567/9): loss=18.609606403438242, w0=72.46134741240299, w1=11.136954269514101\n",
      "[-0.32704655 -3.16045249]\n",
      "Gradient Descent(4568/9): loss=18.476736003488064, w0=72.69027999885711, w1=13.349271011786156\n",
      "[-2.29201562 -0.45726295]\n",
      "Gradient Descent(4569/9): loss=15.576587185315674, w0=74.29469093525425, w1=13.669355073963645\n",
      "[-0.11269841 -0.45859107]\n",
      "Gradient Descent(4570/9): loss=15.904639262866182, w0=74.37357982153092, w1=13.990368821816025\n",
      "[ 2.23950314  2.91202344]\n",
      "Gradient Descent(4571/9): loss=16.09910334505656, w0=72.80592762041826, w1=11.951952414259772\n",
      "[-3.99350125 -1.31394234]\n",
      "Gradient Descent(4572/9): loss=16.671982467577763, w0=75.60137849604506, w1=12.871712049203026\n",
      "[ 1.36564212 -0.01120456]\n",
      "Gradient Descent(4573/9): loss=18.232897839100016, w0=74.64542901169219, w1=12.879555238035397\n",
      "[-0.96857597 -1.22378745]\n",
      "Gradient Descent(4574/9): loss=16.479267797838425, w0=75.32343218982703, w1=13.736206455687253\n",
      "[ 3.41944042  0.55453828]\n",
      "Gradient Descent(4575/9): loss=17.478238261189738, w0=72.92982389833027, w1=13.348029658885851\n",
      "[-1.20123242 -1.59345135]\n",
      "Gradient Descent(4576/9): loss=15.46084176017677, w0=73.77068659185734, w1=14.463445604171245\n",
      "[ 2.02909783  0.99862406]\n",
      "Gradient Descent(4577/9): loss=15.983405579924794, w0=72.35031811238503, w1=13.764408763119276\n",
      "[-2.87005972 -1.52100706]\n",
      "Gradient Descent(4578/9): loss=15.87160801880233, w0=74.35935991921457, w1=14.82911370374743\n",
      "[-2.55772008  0.55476231]\n",
      "Gradient Descent(4579/9): loss=16.86390873849995, w0=76.14976397251849, w1=14.440780087660862\n",
      "[ 5.87625964  2.75714093]\n",
      "Gradient Descent(4580/9): loss=19.925630065322498, w0=72.0363822236753, w1=12.510781435405868\n",
      "[-2.26518134 -2.30164787]\n",
      "Gradient Descent(4581/9): loss=16.64600465697277, w0=73.62200916306061, w1=14.121934940990188\n",
      "[ 0.93532462 -0.00260686]\n",
      "Gradient Descent(4582/9): loss=15.645933335028486, w0=72.96728192584688, w1=14.123759741676913\n",
      "[-2.06319701  2.18935023]\n",
      "Gradient Descent(4583/9): loss=15.646633205164363, w0=74.41151983339991, w1=12.59121458118502\n",
      "[ 0.50754922 -1.06483444]\n",
      "Gradient Descent(4584/9): loss=16.40511454319391, w0=74.05623537642731, w1=13.336598690860088\n",
      "[-1.66956142 -1.53167903]\n",
      "Gradient Descent(4585/9): loss=15.686689481043892, w0=75.22492837088427, w1=14.408774009234403\n",
      "[ 2.55567592 -1.93428408]\n",
      "Gradient Descent(4586/9): loss=17.68185837133171, w0=73.43595522412032, w1=15.762772867005575\n",
      "[ 0.75427309  0.92365773]\n",
      "Gradient Descent(4587/9): loss=18.002157055027144, w0=72.90796406287016, w1=15.116212458476678\n",
      "[-1.13834151 -1.99215478]\n",
      "Gradient Descent(4588/9): loss=16.799435797696184, w0=73.70480312313533, w1=16.51072080723335\n",
      "[-1.01211219 -1.19089965]\n",
      "Gradient Descent(4589/9): loss=20.063805392946424, w0=74.41328165508767, w1=17.344350559450234\n",
      "[ 0.33471616  4.19013388]\n",
      "Gradient Descent(4590/9): loss=23.480084801711364, w0=74.17898034610569, w1=14.411256846948902\n",
      "[ 4.18097927  1.57702571]\n",
      "Gradient Descent(4591/9): loss=16.211439500698667, w0=71.25229485488234, w1=13.307338850666902\n",
      "[-1.44503891 -0.65637003]\n",
      "Gradient Descent(4592/9): loss=17.484864899254095, w0=72.26382209219294, w1=13.766797870947514\n",
      "[ 0.06508815  0.98081358]\n",
      "Gradient Descent(4593/9): loss=15.957649804799747, w0=72.21826038947462, w1=13.080228361579026\n",
      "[-2.53623584 -1.24727571]\n",
      "Gradient Descent(4594/9): loss=16.044205583727027, w0=73.99362547829786, w1=13.953321356286805\n",
      "[ 0.96241731  0.73953154]\n",
      "Gradient Descent(4595/9): loss=15.742833051293864, w0=73.31993335844678, w1=13.435649278301701\n",
      "[-3.75869537 -0.02461691]\n",
      "Gradient Descent(4596/9): loss=15.38719694504739, w0=75.95102011414014, w1=13.45288111300438\n",
      "[ 7.25171114 -1.49380369]\n",
      "Gradient Descent(4597/9): loss=18.91633301723896, w0=70.87482231565329, w1=14.49854369676163\n",
      "[-2.89294566  0.83472113]\n",
      "Gradient Descent(4598/9): loss=18.830918085307797, w0=72.89988427674228, w1=13.91423890689958\n",
      "[-1.91344722  1.47866887]\n",
      "Gradient Descent(4599/9): loss=15.557927360729495, w0=74.23929733250989, w1=12.879170699343415\n",
      "[ 1.67467623  1.60080218]\n",
      "Gradient Descent(4600/9): loss=16.01308031462444, w0=73.0670239731179, w1=11.758609176303818\n",
      "[ 1.57677563 -1.37701268]\n",
      "Gradient Descent(4601/9): loss=16.89272744013702, w0=71.96328103295973, w1=12.722518051503283\n",
      "[-4.63080662 -1.81536657]\n",
      "Gradient Descent(4602/9): loss=16.55786223040478, w0=75.20484566858453, w1=13.99327464862529\n",
      "[ 0.89015012  0.917779  ]\n",
      "Gradient Descent(4603/9): loss=17.34357557202229, w0=74.58174058275058, w1=13.350829351736216\n",
      "[ 0.71023085 -1.41874858]\n",
      "Gradient Descent(4604/9): loss=16.223431641731537, w0=74.08457898910818, w1=14.343953357744104\n",
      "[-0.48402911  2.70320734]\n",
      "Gradient Descent(4605/9): loss=16.071913290659978, w0=74.42339936484352, w1=12.451708217919988\n",
      "[ 1.44339288 -5.14251455]\n",
      "Gradient Descent(4606/9): loss=16.552143760454452, w0=73.4130243477362, w1=16.051468401517383\n",
      "[ 3.09079032  4.26201098]\n",
      "Gradient Descent(4607/9): loss=18.69994492888385, w0=71.24947112428914, w1=13.068060712186202\n",
      "[-5.11456619  1.85706868]\n",
      "Gradient Descent(4608/9): loss=17.560506135174073, w0=74.82966745621019, w1=11.768112635494687\n",
      "[-2.00344608 -3.60013134]\n",
      "Gradient Descent(4609/9): loss=18.029931855546057, w0=76.23207971473832, w1=14.288204574998723\n",
      "[ 3.88198657 -0.92588675]\n",
      "Gradient Descent(4610/9): loss=20.02910301121088, w0=73.51468911538221, w1=14.936325298669257\n",
      "[-3.73490461  0.64922582]\n",
      "Gradient Descent(4611/9): loss=16.471117445301065, w0=76.12912234028597, w1=14.481867225819686\n",
      "[ 2.36472248  2.49122511]\n",
      "Gradient Descent(4612/9): loss=19.907225460032013, w0=74.47381660737338, w1=12.738009646145223\n",
      "[ 1.58311694 -1.99903471]\n",
      "Gradient Descent(4613/9): loss=16.357025022089243, w0=73.36563475195568, w1=14.137333939959957\n",
      "[-2.503601    5.55332833]\n",
      "Gradient Descent(4614/9): loss=15.60469224997506, w0=75.11815544855459, w1=10.250004105789685\n",
      "[ 4.05057777 -1.63785826]\n",
      "Gradient Descent(4615/9): loss=22.26530964825161, w0=72.28275101131341, w1=11.396504888613805\n",
      "[-1.51069029 -1.6606821 ]\n",
      "Gradient Descent(4616/9): loss=18.066998095776196, w0=73.34023421510835, w1=12.558982358541105\n",
      "[-0.48063173 -0.22581071]\n",
      "Gradient Descent(4617/9): loss=15.810832216203943, w0=73.67667642415569, w1=12.717049852134044\n",
      "[ 0.94406211 -1.48124193]\n",
      "Gradient Descent(4618/9): loss=15.74996545027254, w0=73.01583294812276, w1=13.753919205680216\n",
      "[ 0.94751772  0.54197161]\n",
      "Gradient Descent(4619/9): loss=15.46214930634826, w0=72.35257054750303, w1=13.374539075847846\n",
      "[-1.6229293  -1.30275952]\n",
      "Gradient Descent(4620/9): loss=15.834489867106717, w0=73.48862105631639, w1=14.286470737730138\n",
      "[ 1.26707313  2.80948451]\n",
      "Gradient Descent(4621/9): loss=15.730271209205608, w0=72.60166986853588, w1=12.319831580349867\n",
      "[-4.0090516  -1.71409337]\n",
      "Gradient Descent(4622/9): loss=16.29815617552429, w0=75.40800598659418, w1=13.51969693727212\n",
      "[-0.52073222  2.14954722]\n",
      "Gradient Descent(4623/9): loss=17.621362795777227, w0=75.77251853934072, w1=12.015013885751706\n",
      "[ 4.01817285 -3.74952885]\n",
      "Gradient Descent(4624/9): loss=19.53027918609638, w0=72.95979754534346, w1=14.639684083310744\n",
      "[-0.33212633  1.00590432]\n",
      "Gradient Descent(4625/9): loss=16.11447455758763, w0=73.19228597916569, w1=13.935551060711001\n",
      "[ 0.54267091  0.51012786]\n",
      "Gradient Descent(4626/9): loss=15.49494723575892, w0=72.81241634198282, w1=13.578461557689568\n",
      "[-0.23357315  0.12881125]\n",
      "Gradient Descent(4627/9): loss=15.506687413811399, w0=72.97591754971648, w1=13.488293681797517\n",
      "[ 3.09598734 -0.32673883]\n",
      "Gradient Descent(4628/9): loss=15.436488103597311, w0=70.80872640927792, w1=13.71701086174523\n",
      "[-2.09085271  0.72857756]\n",
      "Gradient Descent(4629/9): loss=18.502141707803915, w0=72.27232330850882, w1=13.207006569295377\n",
      "[-3.68193746  1.14996896]\n",
      "Gradient Descent(4630/9): loss=15.944904058800175, w0=74.84967953243927, w1=12.402028298851057\n",
      "[ 2.28155383 -1.33201254]\n",
      "Gradient Descent(4631/9): loss=17.17678016406674, w0=73.25259184898846, w1=13.334437080104497\n",
      "[-2.3600175  -0.42933949]\n",
      "Gradient Descent(4632/9): loss=15.397294423976142, w0=74.90460409725648, w1=13.63497472477211\n",
      "[ 0.99796091 -0.20574908]\n",
      "Gradient Descent(4633/9): loss=16.695089463964216, w0=74.206031459025, w1=13.77899908270848\n",
      "[ 2.62566402 -1.67729796]\n",
      "Gradient Descent(4634/9): loss=15.846645948282243, w0=72.36806664312485, w1=14.953107656288974\n",
      "[-3.53830197  3.74533849]\n",
      "Gradient Descent(4635/9): loss=16.89993868078044, w0=74.8448780245691, w1=12.33137071471495\n",
      "[ 1.40016228 -1.60464081]\n",
      "Gradient Descent(4636/9): loss=17.24796451389897, w0=73.86476443058974, w1=13.45461927966375\n",
      "[-1.85405407  0.51220785]\n",
      "Gradient Descent(4637/9): loss=15.549133241130564, w0=75.1626022779792, w1=13.096073787829502\n",
      "[ 2.58019045 -1.77141243]\n",
      "Gradient Descent(4638/9): loss=17.205460161346895, w0=73.35646896010371, w1=14.336062488521074\n",
      "[-1.02796349 -1.18049022]\n",
      "Gradient Descent(4639/9): loss=15.754511636898986, w0=74.07604340500438, w1=15.162405639851361\n",
      "[ 0.10162831 -1.23785297]\n",
      "Gradient Descent(4640/9): loss=17.1074730241108, w0=74.00490358887218, w1=16.028902721822945\n",
      "[-0.91571653  1.92399207]\n",
      "Gradient Descent(4641/9): loss=18.88782083643439, w0=74.64590516268599, w1=14.682108272507099\n",
      "[ 0.44545398  2.48265263]\n",
      "Gradient Descent(4642/9): loss=17.022694977116792, w0=74.33408737830827, w1=12.944251433843482\n",
      "[-1.16209081 -1.40525229]\n",
      "Gradient Descent(4643/9): loss=16.070219115629154, w0=75.14755094876843, w1=13.927928038103119\n",
      "[ 4.38415725  3.21001457]\n",
      "Gradient Descent(4644/9): loss=17.204306618220784, w0=72.07864087221152, w1=11.680917838007936\n",
      "[-6.30384132 -1.15928968]\n",
      "Gradient Descent(4645/9): loss=17.742172982231434, w0=76.49132979650294, w1=12.492420615919423\n",
      "[ 4.52511887  0.32299405]\n",
      "Gradient Descent(4646/9): loss=20.98496873866805, w0=73.32374658697641, w1=12.266324783198703\n",
      "[ 1.24100791 -2.14587139]\n",
      "Gradient Descent(4647/9): loss=16.122487418519416, w0=72.45504105260927, w1=13.768434756901636\n",
      "[ 2.63708903  0.81999877]\n",
      "Gradient Descent(4648/9): loss=15.779428782128281, w0=70.60907873389549, w1=13.19443561520858\n",
      "[-2.13456163 -1.34977324]\n",
      "Gradient Descent(4649/9): loss=19.030770988206907, w0=72.10327187324336, w1=14.139276883976349\n",
      "[-2.23169069 -3.05838466]\n",
      "Gradient Descent(4650/9): loss=16.31222436469271, w0=73.66545535966384, w1=16.28014614716873\n",
      "[ 1.36369825  4.73943105]\n",
      "Gradient Descent(4651/9): loss=19.37612087487503, w0=72.71086658741645, w1=12.962544415378774\n",
      "[-0.02116771  0.55568011]\n",
      "Gradient Descent(4652/9): loss=15.689596057382136, w0=72.72568398325902, w1=12.573568336642065\n",
      "[ 1.06470319 -0.59089233]\n",
      "Gradient Descent(4653/9): loss=15.957883655345045, w0=71.98039175132284, w1=12.987192970427044\n",
      "[ 1.51630586 -1.53627844]\n",
      "Gradient Descent(4654/9): loss=16.36985644017579, w0=70.91897764685696, w1=14.062587881182722\n",
      "[-0.61176683 -0.13924202]\n",
      "Gradient Descent(4655/9): loss=18.375940106979854, w0=71.34721443012405, w1=14.160057296751926\n",
      "[-2.05565109  1.7630612 ]\n",
      "Gradient Descent(4656/9): loss=17.51215761969732, w0=72.78617019098702, w1=12.92591445836529\n",
      "[-2.34093965 -0.51325453]\n",
      "Gradient Descent(4657/9): loss=15.668139919132573, w0=74.42482794250037, w1=13.28519262842236\n",
      "[ 0.47998141 -1.44949812]\n",
      "Gradient Descent(4658/9): loss=16.04428096941332, w0=74.08884095578426, w1=14.299841311454049\n",
      "[ 0.69009548  2.25493329]\n",
      "Gradient Descent(4659/9): loss=16.03814162729439, w0=73.60577412263936, w1=12.72138801127709\n",
      "[-0.06057684  0.51168995]\n",
      "Gradient Descent(4660/9): loss=15.722041707169264, w0=73.64817790977759, w1=12.36320504935009\n",
      "[-0.77839359 -0.15516309]\n",
      "Gradient Descent(4661/9): loss=16.07193086398297, w0=74.19305342462039, w1=12.471819215406263\n",
      "[ 0.06605171 -0.68219479]\n",
      "Gradient Descent(4662/9): loss=16.298030897346976, w0=74.14681722494576, w1=12.949355569609084\n",
      "[ 1.13186171 -2.60886266]\n",
      "Gradient Descent(4663/9): loss=15.89024220172937, w0=73.35451402975968, w1=14.775559428382238\n",
      "[-0.14775666  6.33371558]\n",
      "Gradient Descent(4664/9): loss=16.227333280880128, w0=73.45794369463196, w1=10.341958524063724\n",
      "[ 3.70341603 -6.79783761]\n",
      "Gradient Descent(4665/9): loss=20.32208922940266, w0=70.86555247311254, w1=15.100444847897146\n",
      "[-4.11056947  2.35585807]\n",
      "Gradient Descent(4666/9): loss=19.647763930624834, w0=73.74295110080023, w1=13.451344197171274\n",
      "[ 1.07612966  1.27015599]\n",
      "Gradient Descent(4667/9): loss=15.48710381302528, w0=72.98966033921131, w1=12.56223500176372\n",
      "[ 0.1262131  -0.02134551]\n",
      "Gradient Descent(4668/9): loss=15.853057868821745, w0=72.90131116773348, w1=12.577176858531189\n",
      "[-1.89463962 -2.48004146]\n",
      "Gradient Descent(4669/9): loss=15.870244735848484, w0=74.22755890294539, w1=14.313205882046425\n",
      "[-1.23285757  0.02998309]\n",
      "Gradient Descent(4670/9): loss=16.169082463278443, w0=75.09055920113282, w1=14.292217720660762\n",
      "[ 0.72377033  0.96809918]\n",
      "Gradient Descent(4671/9): loss=17.329922900916557, w0=74.58391997077848, w1=13.614548295037821\n",
      "[ 0.82143701  1.7176082 ]\n",
      "Gradient Descent(4672/9): loss=16.227025602997557, w0=74.00891406705377, w1=12.412222553073804\n",
      "[ 1.05641412 -2.76406971]\n",
      "Gradient Descent(4673/9): loss=16.211262019294825, w0=73.26942418351406, w1=14.347071353050854\n",
      "[ 3.78662714  0.12813661]\n",
      "Gradient Descent(4674/9): loss=15.762343686757935, w0=70.6187851865019, w1=14.257375725341737\n",
      "[-1.3149649  -0.03397784]\n",
      "Gradient Descent(4675/9): loss=19.266446456508554, w0=71.53926061723584, w1=14.281160212132232\n",
      "[-3.11016532  1.46322294]\n",
      "Gradient Descent(4676/9): loss=17.24646542634909, w0=73.71637634158354, w1=13.25690415399993\n",
      "[-1.22430011  1.46708829]\n",
      "Gradient Descent(4677/9): loss=15.499943468340108, w0=74.57338642128772, w1=12.229942352360249\n",
      "[ 3.97072991 -2.15646765]\n",
      "Gradient Descent(4678/9): loss=16.98536509852345, w0=71.79387548334283, w1=13.739469710026373\n",
      "[-4.17057189  1.17991229]\n",
      "Gradient Descent(4679/9): loss=16.544694569022344, w0=74.7132758085107, w1=12.913531109590528\n",
      "[ 2.33824046  1.08569672]\n",
      "Gradient Descent(4680/9): loss=16.55345112932332, w0=73.07650748412468, w1=12.153543408524053\n",
      "[-0.16264436 -1.05352691]\n",
      "Gradient Descent(4681/9): loss=16.288884548521306, w0=73.1903585329136, w1=12.891012243283205\n",
      "[ 3.39191024 -1.43787919]\n",
      "Gradient Descent(4682/9): loss=15.564534522762147, w0=70.81602136443715, w1=13.89752767675223\n",
      "[-0.33664448  0.96324754]\n",
      "Gradient Descent(4683/9): loss=18.543168442032055, w0=71.05167249918136, w1=13.223254400117495\n",
      "[-3.96682247 -0.70331351]\n",
      "Gradient Descent(4684/9): loss=17.93261464733556, w0=73.8284482269107, w1=13.715573857665042\n",
      "[-0.62683134 -2.80320413]\n",
      "Gradient Descent(4685/9): loss=15.556562316685184, w0=74.26723016457898, w1=15.677816746294926\n",
      "[-4.10113688  1.07314972]\n",
      "Gradient Descent(4686/9): loss=18.275383540089194, w0=77.13802598187976, w1=14.926611944123602\n",
      "[ 2.69975997  4.78972477]\n",
      "Gradient Descent(4687/9): loss=23.82121466725566, w0=75.24819399948339, w1=11.57380460683807\n",
      "[ 1.3436537 -2.0971916]\n",
      "Gradient Descent(4688/9): loss=19.111719713401286, w0=74.30763640866057, w1=13.041838729962965\n",
      "[ 3.03433982 -0.30997441]\n",
      "Gradient Descent(4689/9): loss=15.995563008635, w0=72.18359853506158, w1=13.258820813801643\n",
      "[-3.4688619   1.61886204]\n",
      "Gradient Descent(4690/9): loss=16.026693523718674, w0=74.61180186323197, w1=12.125617384711287\n",
      "[ 4.84622103 -2.53146901]\n",
      "Gradient Descent(4691/9): loss=17.171078235604547, w0=71.21944714401137, w1=13.897645694273713\n",
      "[-3.1450796   1.08933383]\n",
      "Gradient Descent(4692/9): loss=17.62494494186924, w0=73.42100286240938, w1=13.13511201124406\n",
      "[-0.23032635 -1.67027226]\n",
      "Gradient Descent(4693/9): loss=15.45333736737984, w0=73.5822313087927, w1=14.304302591604177\n",
      "[ 3.27250709  0.01781083]\n",
      "Gradient Descent(4694/9): loss=15.767423460183997, w0=71.29147634385974, w1=14.29183501161362\n",
      "[ 2.61080248  3.04277126]\n",
      "Gradient Descent(4695/9): loss=17.72055371567409, w0=69.46391460567023, w1=12.161895130712818\n",
      "[-3.67208043 -2.71596674]\n",
      "Gradient Descent(4696/9): loss=23.588687420927595, w0=72.03437090451601, w1=14.063071846915083\n",
      "[-2.17635106  1.12719056]\n",
      "Gradient Descent(4697/9): loss=16.3492764542898, w0=73.5578166477728, w1=13.27403845812473\n",
      "[ 4.19782606  0.12475063]\n",
      "Gradient Descent(4698/9): loss=15.441858953215007, w0=70.61933840298151, w1=13.186713017181251\n",
      "[-3.04237676 -0.01323421]\n",
      "Gradient Descent(4699/9): loss=19.005510912597934, w0=72.74900213431293, w1=13.195976966114664\n",
      "[-1.7427944   0.96325236]\n",
      "Gradient Descent(4700/9): loss=15.574609608135448, w0=73.96895821379015, w1=12.521700311828209\n",
      "[ 2.74904139 -0.80919735]\n",
      "Gradient Descent(4701/9): loss=16.07261842643396, w0=72.0446292426048, w1=13.088138455514093\n",
      "[ 0.88125496  1.06774517]\n",
      "Gradient Descent(4702/9): loss=16.242919159000376, w0=71.42775076801442, w1=12.34071683574466\n",
      "[-1.07241322 -0.36532805]\n",
      "Gradient Descent(4703/9): loss=17.775840893852372, w0=72.17844001918945, w1=12.596446473082821\n",
      "[-1.24013933  0.57634591]\n",
      "Gradient Descent(4704/9): loss=16.398117275665275, w0=73.04653755288673, w1=12.193004337037692\n",
      "[-1.57083832 -1.60061164]\n",
      "Gradient Descent(4705/9): loss=16.24429626635376, w0=74.1461243744847, w1=13.313432486902434\n",
      "[-0.56459656  3.47306705]\n",
      "Gradient Descent(4706/9): loss=15.762836821141875, w0=74.54134196825447, w1=10.882285549822317\n",
      "[-0.51927544 -2.03094481]\n",
      "Gradient Descent(4707/9): loss=19.537229366696817, w0=74.90483477594394, w1=12.303946919642843\n",
      "[ 1.58137373  1.20155622]\n",
      "Gradient Descent(4708/9): loss=17.374620124826496, w0=73.79787316621194, w1=11.462857568893156\n",
      "[-0.20170363 -3.14979405]\n",
      "Gradient Descent(4709/9): loss=17.546723032179017, w0=73.93906570874759, w1=13.667713404459018\n",
      "[-1.84618497  2.07893948]\n",
      "Gradient Descent(4710/9): loss=15.611665252200375, w0=75.23139518540981, w1=12.21245576994665\n",
      "[ 0.75114733  1.97199403]\n",
      "Gradient Descent(4711/9): loss=18.06575876438886, w0=74.70559205637814, w1=10.832059948553365\n",
      "[ 0.79218868 -2.8903297 ]\n",
      "Gradient Descent(4712/9): loss=19.887325884359427, w0=74.1510599788679, w1=12.855290741859658\n",
      "[ 2.23229352  0.77637086]\n",
      "Gradient Descent(4713/9): loss=15.94818184985913, w0=72.58845451697182, w1=12.311831141873764\n",
      "[ 0.82506249 -0.20135628]\n",
      "Gradient Descent(4714/9): loss=16.31670341252391, w0=72.01091077573021, w1=12.452780534454764\n",
      "[-0.19963805  1.22477502]\n",
      "Gradient Descent(4715/9): loss=16.736241336498992, w0=72.15065740748216, w1=11.595438021878945\n",
      "[-4.25235625 -0.92404338]\n",
      "Gradient Descent(4716/9): loss=17.814659867439396, w0=75.12730678548087, w1=12.242268386795493\n",
      "[ 4.10458316 -2.60060927]\n",
      "Gradient Descent(4717/9): loss=17.832171636990974, w0=72.2540985733097, w1=14.062694877685702\n",
      "[-1.6902387  -0.53797536]\n",
      "Gradient Descent(4718/9): loss=16.096438514611677, w0=73.43726566178104, w1=14.439277629813985\n",
      "[ 1.12616661  1.17170951]\n",
      "Gradient Descent(4719/9): loss=15.856544252773741, w0=72.64894903622302, w1=13.619080970243191\n",
      "[ 0.33829246 -1.40844183]\n",
      "Gradient Descent(4720/9): loss=15.603594726498265, w0=72.41214431723782, w1=14.604990251814103\n",
      "[-1.15989459  0.89958812]\n",
      "Gradient Descent(4721/9): loss=16.40777889411371, w0=73.22407053162908, w1=13.9752785662688\n",
      "[-1.54614274 -0.04825764]\n",
      "Gradient Descent(4722/9): loss=15.511120378029027, w0=74.30637044728398, w1=14.009058915608007\n",
      "[-0.86715024  0.85962679]\n",
      "Gradient Descent(4723/9): loss=16.038517644173716, w0=74.91337561432282, w1=13.407320160054555\n",
      "[ 2.18852343 -1.17168536]\n",
      "Gradient Descent(4724/9): loss=16.69982319062686, w0=73.38140921447291, w1=14.227499913703555\n",
      "[ 1.98653237 -0.52759323]\n",
      "Gradient Descent(4725/9): loss=15.669307931654437, w0=71.99083655402305, w1=14.59681517193629\n",
      "[-2.29840844  3.03755366]\n",
      "Gradient Descent(4726/9): loss=16.85886297377863, w0=73.59972246019008, w1=12.470527610060781\n",
      "[ 0.4874342   0.26651328]\n",
      "Gradient Descent(4727/9): loss=15.941871834344616, w0=73.25851852128461, w1=12.283968314008044\n",
      "[ 0.74641405 -0.64788868]\n",
      "Gradient Descent(4728/9): loss=16.101416573486823, w0=72.73602868613953, w1=12.737490391720652\n",
      "[ 1.12472946 -0.92321512]\n",
      "Gradient Descent(4729/9): loss=15.816957125585732, w0=71.94871806183163, w1=13.383740973370768\n",
      "[-3.19225708 -0.50599682]\n",
      "Gradient Descent(4730/9): loss=16.295279950015733, w0=74.18329801973029, w1=13.737938745888313\n",
      "[ 4.27489538  1.24917041]\n",
      "Gradient Descent(4731/9): loss=15.814723133013059, w0=71.19087125036444, w1=12.863519457625662\n",
      "[ 2.38100636 -0.99933727]\n",
      "Gradient Descent(4732/9): loss=17.787145993704048, w0=69.52416679850376, w1=13.563055545077496\n",
      "[-4.82942878 -2.06113558]\n",
      "Gradient Descent(4733/9): loss=22.494888053369053, w0=72.90476694189992, w1=15.005850451886847\n",
      "[ 3.47847757 -0.5382675 ]\n",
      "Gradient Descent(4734/9): loss=16.626157322581456, w0=70.46983264268368, w1=15.382637699991568\n",
      "[-0.9201416  -0.13890105]\n",
      "Gradient Descent(4735/9): loss=21.184190505920743, w0=71.11393176457337, w1=15.47986843490386\n",
      "[ 0.10582596  1.7638876 ]\n",
      "Gradient Descent(4736/9): loss=19.762378598694042, w0=71.0398535927209, w1=14.245147113277385\n",
      "[-3.45492693  5.26493695]\n",
      "Gradient Descent(4737/9): loss=18.219245189284692, w0=73.45830244271046, w1=10.559691246772923\n",
      "[ 3.9968737  -2.34665398]\n",
      "Gradient Descent(4738/9): loss=19.662660203271752, w0=70.66049085486809, w1=12.202349031649689\n",
      "[-2.81433843 -1.39697618]\n",
      "Gradient Descent(4739/9): loss=19.669196304544016, w0=72.63052775804518, w1=13.18023235610443\n",
      "[-2.21927445  1.14303612]\n",
      "Gradient Descent(4740/9): loss=15.650777989179746, w0=74.18401987521813, w1=12.380107074549258\n",
      "[ 0.10130581 -4.54350562]\n",
      "Gradient Descent(4741/9): loss=16.38659095504345, w0=74.1131058109932, w1=15.56056100828134\n",
      "[ 0.85467505  5.34425843]\n",
      "Gradient Descent(4742/9): loss=17.886384317687817, w0=73.51483327677253, w1=11.8195801095926\n",
      "[-0.92232537  0.17278052]\n",
      "Gradient Descent(4743/9): loss=16.788308433380084, w0=74.16046103493899, w1=11.698633746564166\n",
      "[ 1.28969282 -3.6868972 ]\n",
      "Gradient Descent(4744/9): loss=17.34745346372232, w0=73.25767605935474, w1=14.279461784084313\n",
      "[ 1.93720859 -2.4480025 ]\n",
      "Gradient Descent(4745/9): loss=15.706344263701485, w0=71.90163004366654, w1=15.993063531056162\n",
      "[ 1.98233208  1.86797752]\n",
      "Gradient Descent(4746/9): loss=19.513593183646744, w0=70.51399758870483, w1=14.68547926782702\n",
      "[-0.77738276 -0.06711752]\n",
      "Gradient Descent(4747/9): loss=19.976814568525143, w0=71.05816551939071, w1=14.732461531929326\n",
      "[-2.49472208 -0.60977195]\n",
      "Gradient Descent(4748/9): loss=18.66988154377165, w0=72.80447097446383, w1=15.159301894872792\n",
      "[-1.12399716  1.8067815 ]\n",
      "Gradient Descent(4749/9): loss=16.916179399935277, w0=73.59126898818897, w1=13.894554846448726\n",
      "[-1.56734206  0.44340298]\n",
      "Gradient Descent(4750/9): loss=15.516142597068793, w0=74.68840842857531, w1=13.58417276119478\n",
      "[ 2.64412588 -2.83529497]\n",
      "Gradient Descent(4751/9): loss=16.363640045509612, w0=72.8375203115757, w1=15.568879243445434\n",
      "[ 2.17805817  0.75060706]\n",
      "Gradient Descent(4752/9): loss=17.672348097166406, w0=71.31287958961278, w1=15.04345430353252\n",
      "[-1.16837726  1.50140438]\n",
      "Gradient Descent(4753/9): loss=18.570796704594198, w0=72.13074367358458, w1=13.992471239618022\n",
      "[-0.24160161  0.11101956]\n",
      "Gradient Descent(4754/9): loss=16.19384057666167, w0=72.29986480312688, w1=13.914757547765808\n",
      "[-2.08318645 -1.18950832]\n",
      "Gradient Descent(4755/9): loss=15.974594851325174, w0=73.75809531932175, w1=14.747413374319077\n",
      "[-0.22960562  1.1241203 ]\n",
      "Gradient Descent(4756/9): loss=16.29714913882643, w0=73.9188192557387, w1=13.96052916584425\n",
      "[-0.60803532  0.21643502]\n",
      "Gradient Descent(4757/9): loss=15.69672852196389, w0=74.34444398316776, w1=13.809024652496456\n",
      "[ 1.61408576  0.97766634]\n",
      "Gradient Descent(4758/9): loss=15.991909353477043, w0=73.21458395211471, w1=13.124658211864313\n",
      "[-0.55336743 -0.84891436]\n",
      "Gradient Descent(4759/9): loss=15.452066882596899, w0=73.60194115423246, w1=13.718898265676435\n",
      "[ 1.04241073 -3.35443045]\n",
      "Gradient Descent(4760/9): loss=15.46193069866881, w0=72.8722536438836, w1=16.066999577190355\n",
      "[-0.50035632  1.80127953]\n",
      "Gradient Descent(4761/9): loss=18.821817349092154, w0=73.22250306980986, w1=14.806103906183363\n",
      "[ 0.05518636  0.64006663]\n",
      "Gradient Descent(4762/9): loss=16.268095368203014, w0=73.18387261785082, w1=14.358057264019648\n",
      "[ 0.09832973  1.25238263]\n",
      "Gradient Descent(4763/9): loss=15.77768812165918, w0=73.11504180433926, w1=13.481389426065105\n",
      "[ 1.15474863  0.68691122]\n",
      "Gradient Descent(4764/9): loss=15.401888337555324, w0=72.30671776019723, w1=13.00055157522091\n",
      "[-3.35963275  4.13608723]\n",
      "Gradient Descent(4765/9): loss=15.987971541216806, w0=74.65846068306634, w1=10.105290516147427\n",
      "[ 1.72958173 -4.28280685]\n",
      "Gradient Descent(4766/9): loss=22.01023241792839, w0=73.44775347190118, w1=13.103255312676115\n",
      "[ 0.20440852 -1.02495338]\n",
      "Gradient Descent(4767/9): loss=15.468579911849266, w0=73.30466750663408, w1=13.820722679290467\n",
      "[ 2.2957778   2.96476074]\n",
      "Gradient Descent(4768/9): loss=15.444089595122446, w0=71.6976230476044, w1=11.745390159565533\n",
      "[ 0.18095489 -0.15366854]\n",
      "Gradient Descent(4769/9): loss=18.16390992241466, w0=71.57095462241634, w1=11.85295813424756\n",
      "[-2.08593659 -1.88608337]\n",
      "Gradient Descent(4770/9): loss=18.193360942055797, w0=73.03111023367286, w1=13.173216493026827\n",
      "[-2.98457769  1.38670915]\n",
      "Gradient Descent(4771/9): loss=15.467392762862318, w0=75.12031461783567, w1=12.202520085358298\n",
      "[ 2.39778938 -2.14671647]\n",
      "Gradient Descent(4772/9): loss=17.86935301120448, w0=73.44186204939287, w1=13.705221616502081\n",
      "[-2.98985955  2.60880697]\n",
      "Gradient Descent(4773/9): loss=15.42225819309848, w0=75.53476373392692, w1=11.879056736289396\n",
      "[ 1.98433715 -1.05974792]\n",
      "Gradient Descent(4774/9): loss=19.177623035256154, w0=74.14572772792398, w1=12.620880281959248\n",
      "[-1.39984723 -0.26855726]\n",
      "Gradient Descent(4775/9): loss=16.117470699637146, w0=75.12562079207112, w1=12.808870360698775\n",
      "[ 3.75914593 -1.86243101]\n",
      "Gradient Descent(4776/9): loss=17.28846264172976, w0=72.49421864233184, w1=14.11257206850614\n",
      "[ 0.19644808 -0.01725015]\n",
      "Gradient Descent(4777/9): loss=15.90590625851348, w0=72.35670498474825, w1=14.124647174209226\n",
      "[-3.79087662  3.50182352]\n",
      "Gradient Descent(4778/9): loss=16.03304614656762, w0=75.010318615772, w1=11.673370711415387\n",
      "[ 1.57935337  0.60375564]\n",
      "Gradient Descent(4779/9): loss=18.490331747694373, w0=73.90477125779606, w1=11.250741766167115\n",
      "[-2.86265383 -1.21486671]\n",
      "Gradient Descent(4780/9): loss=18.05661139665269, w0=75.90862894215581, w1=12.101148463850643\n",
      "[ 3.26053769 -0.27938037]\n",
      "Gradient Descent(4781/9): loss=19.7544533712643, w0=73.62625255605299, w1=12.296714724607194\n",
      "[-0.01917256 -0.37792952]\n",
      "Gradient Descent(4782/9): loss=16.140851458757382, w0=73.63967334970387, w1=12.561265391501106\n",
      "[-0.73613286 -3.40376251]\n",
      "Gradient Descent(4783/9): loss=15.867432351858424, w0=74.15496635481063, w1=14.943899151536874\n",
      "[ 0.19398756  0.04351463]\n",
      "Gradient Descent(4784/9): loss=16.82850792794991, w0=74.01917506140026, w1=14.913438913647076\n",
      "[ 3.83895734 -0.71503151]\n",
      "Gradient Descent(4785/9): loss=16.6766696766404, w0=71.33190492096887, w1=15.413960971697572\n",
      "[-0.0857712  -0.17533696]\n",
      "Gradient Descent(4786/9): loss=19.181302083044283, w0=71.39194476118267, w1=15.536696842300975\n",
      "[-3.5404246  4.1376127]\n",
      "Gradient Descent(4787/9): loss=19.31023900728522, w0=73.87024197895283, w1=12.640367949511127\n",
      "[  8.77792159e-01  -4.18359009e-04]\n",
      "Gradient Descent(4788/9): loss=15.904209809337335, w0=73.25578746769813, w1=12.64066080081766\n",
      "[-1.05824605 -0.03501016]\n",
      "Gradient Descent(4789/9): loss=15.738618812589463, w0=73.99655970562955, w1=12.66516791529012\n",
      "[ 1.05494867 -3.45685088]\n",
      "Gradient Descent(4790/9): loss=15.964479127322175, w0=73.25809563587849, w1=15.0849635308943\n",
      "[ 0.06653218  2.27875928]\n",
      "Gradient Descent(4791/9): loss=16.67494517354041, w0=73.21152311264134, w1=13.489832036356406\n",
      "[-0.72155734 -0.21125232]\n",
      "Gradient Descent(4792/9): loss=15.389333860487753, w0=73.71661325180277, w1=13.637708658405312\n",
      "[ 2.59278936 -0.96955304]\n",
      "Gradient Descent(4793/9): loss=15.48770321842175, w0=71.90166070113266, w1=14.316395784828343\n",
      "[ 1.39359557  2.49794037]\n",
      "Gradient Descent(4794/9): loss=16.70510314787142, w0=70.92614379947685, w1=12.567837523496422\n",
      "[-1.31434202  0.99675879]\n",
      "Gradient Descent(4795/9): loss=18.60483260435519, w0=71.84618321635087, w1=11.870106371613732\n",
      "[-2.12035817 -3.51534983]\n",
      "Gradient Descent(4796/9): loss=17.72927750434539, w0=73.330433932876, w1=14.330851249349742\n",
      "[-1.1727086   1.52937381]\n",
      "Gradient Descent(4797/9): loss=15.74877307002937, w0=74.1513299523259, w1=13.26028958036192\n",
      "[-1.38494902 -2.52777197]\n",
      "Gradient Descent(4798/9): loss=15.777535259946596, w0=75.12079426610497, w1=15.02972996026473\n",
      "[ 1.90538247  3.76470354]\n",
      "Gradient Descent(4799/9): loss=18.255896167646107, w0=73.78702653371083, w1=12.394437485040275\n",
      "[ 4.20150089 -1.75415418]\n",
      "Gradient Descent(4800/9): loss=16.096374766867566, w0=70.84597591338417, w1=13.6223454103284\n",
      "[-2.02699472  0.7975228 ]\n",
      "Gradient Descent(4801/9): loss=18.39227997829876, w0=72.2648722188043, w1=13.064079447101406\n",
      "[-0.36092919 -0.17590933]\n",
      "Gradient Descent(4802/9): loss=16.00173498739541, w0=72.51752264901582, w1=13.18721598061777\n",
      "[-0.12349006 -2.71623862]\n",
      "Gradient Descent(4803/9): loss=15.730062934478081, w0=72.60396568881687, w1=15.08858301335664\n",
      "[ 0.28690179 -0.02377657]\n",
      "Gradient Descent(4804/9): loss=16.91813999492104, w0=72.40313443455348, w1=15.105226615757374\n",
      "[-0.31646738  1.54060348]\n",
      "Gradient Descent(4805/9): loss=17.103787290021206, w0=72.62466159846845, w1=14.026804179156354\n",
      "[ 1.89661939  1.90513744]\n",
      "Gradient Descent(4806/9): loss=15.759497301035418, w0=71.29702802447326, w1=12.693207969441358\n",
      "[-3.74190792 -0.30699634]\n",
      "Gradient Descent(4807/9): loss=17.688975284943957, w0=73.91636357052145, w1=12.908105407987724\n",
      "[-1.13836576  0.14667198]\n",
      "Gradient Descent(4808/9): loss=15.742971918534295, w0=74.71321960172723, w1=12.805435019063513\n",
      "[ 1.87107863  1.53422588]\n",
      "Gradient Descent(4809/9): loss=16.620415723789453, w0=73.40346456133774, w1=11.73147690538086\n",
      "[-1.28030403  0.78146661]\n",
      "Gradient Descent(4810/9): loss=16.92005138846322, w0=74.29967738488247, w1=11.184450275951036\n",
      "[ 0.59823933 -1.94736986]\n",
      "Gradient Descent(4811/9): loss=18.52577400317806, w0=73.88090985463471, w1=12.547609177193127\n",
      "[ 2.58756649 -1.70397706]\n",
      "Gradient Descent(4812/9): loss=15.992573479934892, w0=72.06961331209216, w1=13.74039311581114\n",
      "[-0.88421121 -0.18192149]\n",
      "Gradient Descent(4813/9): loss=16.16933096172705, w0=72.68856116022414, w1=13.867738155394393\n",
      "[-1.34141508 -1.08843191]\n",
      "Gradient Descent(4814/9): loss=15.644400723118912, w0=73.62755171494621, w1=14.629640493883057\n",
      "[ 0.10643227 -1.01791512]\n",
      "Gradient Descent(4815/9): loss=16.10270953179046, w0=73.55304912517126, w1=15.342181076008714\n",
      "[-1.17160674  4.2273642 ]\n",
      "Gradient Descent(4816/9): loss=17.15385602117447, w0=74.37317384182235, w1=12.383026134878998\n",
      "[ 0.3283703  -0.60664741]\n",
      "Gradient Descent(4817/9): loss=16.569640556020378, w0=74.14331463435623, w1=12.80767932157027\n",
      "[ 1.69052109 -1.2592097 ]\n",
      "Gradient Descent(4818/9): loss=15.972436043456243, w0=72.95994987357014, w1=13.68912611343599\n",
      "[-0.50194117  0.93394019]\n",
      "Gradient Descent(4819/9): loss=15.463583604508853, w0=73.31130869049828, w1=13.035367978653827\n",
      "[ 0.38180577 -1.36760064]\n",
      "Gradient Descent(4820/9): loss=15.484760015233961, w0=73.04404465483155, w1=13.992688424782209\n",
      "[-1.0578617   1.47259057]\n",
      "Gradient Descent(4821/9): loss=15.54867939622179, w0=73.78454784812148, w1=12.961875025084305\n",
      "[-0.98401516  0.78601611]\n",
      "Gradient Descent(4822/9): loss=15.640322520767425, w0=74.47335846277703, w1=12.41166374849781\n",
      "[ 0.68763851 -0.48317344]\n",
      "Gradient Descent(4823/9): loss=16.651787049568288, w0=73.99201150617593, w1=12.749885155269938\n",
      "[ 0.8917184  -0.94954454]\n",
      "Gradient Descent(4824/9): loss=15.895876275787362, w0=73.3678086227839, w1=13.414566335454507\n",
      "[-2.94120286  0.68459542]\n",
      "Gradient Descent(4825/9): loss=15.390739492329343, w0=75.42665062627637, w1=12.935349538714688\n",
      "[ 1.00801556 -1.7631781 ]\n",
      "Gradient Descent(4826/9): loss=17.808319042429055, w0=74.721039737515, w1=14.169574209030753\n",
      "[ 0.04695282 -0.44999106]\n",
      "Gradient Descent(4827/9): loss=16.642175017831995, w0=74.68817276312288, w1=14.484567954109679\n",
      "[ 1.60210162  0.54718929]\n",
      "Gradient Descent(4828/9): loss=16.862722768282197, w0=73.56670162582154, w1=14.101535450564615\n",
      "[-1.17814889  2.18380139]\n",
      "Gradient Descent(4829/9): loss=15.616424161736562, w0=74.39140585184167, w1=12.572874474977766\n",
      "[  3.89854353e-04  -1.24379223e+00]\n",
      "Gradient Descent(4830/9): loss=16.399300811904315, w0=74.39113295379455, w1=13.443529036217774\n",
      "[ 3.91731081  2.47432076]\n",
      "Gradient Descent(4831/9): loss=15.98847842425626, w0=71.64901538959609, w1=11.711504503748477\n",
      "[-3.98802403 -3.06593325]\n",
      "Gradient Descent(4832/9): loss=18.302026394818515, w0=74.44063221373021, w1=13.85765777537798\n",
      "[ 2.41435107  0.36729456]\n",
      "Gradient Descent(4833/9): loss=16.114781363712805, w0=72.75058646678568, w1=13.600551584217268\n",
      "[ 0.07117914 -1.34807579]\n",
      "Gradient Descent(4834/9): loss=15.540795670792972, w0=72.70076106954514, w1=14.544204640660496\n",
      "[-2.3879428   1.16957574]\n",
      "Gradient Descent(4835/9): loss=16.128379642754787, w0=74.37232103225648, w1=13.725501620904161\n",
      "[ 3.25264813  0.87663769]\n",
      "Gradient Descent(4836/9): loss=15.99756626490143, w0=72.09546733878454, w1=13.11185523486803\n",
      "[-2.02448628 -2.20656246]\n",
      "Gradient Descent(4837/9): loss=16.171694118687334, w0=73.51260773465437, w1=14.656448960060912\n",
      "[ 3.17558037  1.85798514]\n",
      "Gradient Descent(4838/9): loss=16.10215401835879, w0=71.28970147290526, w1=13.355859363210651\n",
      "[-1.58636747 -1.26310271]\n",
      "Gradient Descent(4839/9): loss=17.402007625357086, w0=72.40015870023295, w1=14.24003126072896\n",
      "[-0.57337448  1.1732857 ]\n",
      "Gradient Descent(4840/9): loss=16.07433664710344, w0=72.80152083828277, w1=13.418731267921778\n",
      "[-2.9657223   1.92540563]\n",
      "Gradient Descent(4841/9): loss=15.50897667326468, w0=74.87752645083253, w1=12.070947330387414\n",
      "[ 2.18637692 -2.58588135]\n",
      "Gradient Descent(4842/9): loss=17.63209895381555, w0=73.34706260515739, w1=13.88106427768533\n",
      "[ 0.59881625  0.69036243]\n",
      "Gradient Descent(4843/9): loss=15.467841481493624, w0=72.92789123350313, w1=13.397810580067818\n",
      "[ 0.27971031 -0.20131313]\n",
      "Gradient Descent(4844/9): loss=15.456231087530876, w0=72.73209401972522, w1=13.538729771078579\n",
      "[-1.27698076  0.79488057]\n",
      "Gradient Descent(4845/9): loss=15.545454732701522, w0=73.62598054926819, w1=12.982313374379116\n",
      "[-1.73359375  2.55988302]\n",
      "Gradient Descent(4846/9): loss=15.564722220949223, w0=74.83949617315929, w1=11.190395262203282\n",
      "[-0.16208961 -1.40097986]\n",
      "Gradient Descent(4847/9): loss=19.200774186750095, w0=74.95295890288692, w1=12.171081165376739\n",
      "[ 1.50678217 -0.10436268]\n",
      "Gradient Descent(4848/9): loss=17.61834748781069, w0=73.89821138393599, w1=12.244135039461211\n",
      "[-0.77024455 -0.71808789]\n",
      "Gradient Descent(4849/9): loss=16.3317964474958, w0=74.43738256979519, w1=12.746796562834303\n",
      "[-0.30056577  0.39477702]\n",
      "Gradient Descent(4850/9): loss=16.308221741588547, w0=74.64777861091166, w1=12.47045264726894\n",
      "[ 3.03900378 -2.17938193]\n",
      "Gradient Descent(4851/9): loss=16.811654386988298, w0=72.52047596584293, w1=13.996019999432535\n",
      "[ 2.91492467 -2.00078398]\n",
      "Gradient Descent(4852/9): loss=15.818284004885081, w0=70.48002869881084, w1=15.396568782098488\n",
      "[-2.43175903  4.93155979]\n",
      "Gradient Descent(4853/9): loss=21.182054757718667, w0=72.1822600220484, w1=11.944476930099828\n",
      "[-1.458566   -2.90714592]\n",
      "Gradient Descent(4854/9): loss=17.182258075517524, w0=73.20325622009912, w1=13.979479073459682\n",
      "[ 0.31985126  5.95209411]\n",
      "Gradient Descent(4855/9): loss=15.514881357306905, w0=72.97936033496822, w1=9.81301319804699\n",
      "[ 1.33760484 -4.5395607 ]\n",
      "Gradient Descent(4856/9): loss=22.15770403714113, w0=72.04303694917449, w1=12.99070568615328\n",
      "[-2.58317727 -1.38769642]\n",
      "Gradient Descent(4857/9): loss=16.28780837685558, w0=73.85126103615436, w1=13.962093183114243\n",
      "[ 3.42909016  2.7641522 ]\n",
      "Gradient Descent(4858/9): loss=15.65754686134774, w0=71.45089792613199, w1=12.027186644069594\n",
      "[-0.98026034  0.51156297]\n",
      "Gradient Descent(4859/9): loss=18.139172327780926, w0=72.1370801612924, w1=11.669092566351017\n",
      "[ 1.04786938 -2.04167143]\n",
      "Gradient Descent(4860/9): loss=17.694201545510357, w0=71.40357159685432, w1=13.098262568803428\n",
      "[-1.63618759 -0.16290703]\n",
      "Gradient Descent(4861/9): loss=17.245352196351977, w0=72.54890290944826, w1=13.212297490896809\n",
      "[-0.71331714  1.76490095]\n",
      "Gradient Descent(4862/9): loss=15.699169969203005, w0=73.04822490854094, w1=11.976866827127262\n",
      "[ 0.09163284 -2.2606504 ]\n",
      "Gradient Descent(4863/9): loss=16.54534386025709, w0=72.98408191937254, w1=13.559322108141744\n",
      "[-2.42917128  1.81793358]\n",
      "Gradient Descent(4864/9): loss=15.437057157293026, w0=74.68450181242166, w1=12.286768599171628\n",
      "[ 2.37151109  0.7610418 ]\n",
      "Gradient Descent(4865/9): loss=17.064301470966683, w0=73.02444405097299, w1=11.754039340193216\n",
      "[-4.08055791  2.17325503]\n",
      "Gradient Descent(4866/9): loss=16.911170866953714, w0=75.88083458783929, w1=10.232760820225053\n",
      "[ 4.45390568 -2.11486375]\n",
      "Gradient Descent(4867/9): loss=24.003293626253413, w0=72.76310061060342, w1=11.713165448211921\n",
      "[-3.75574346 -1.29999749]\n",
      "Gradient Descent(4868/9): loss=17.087117671913013, w0=75.39212102977795, w1=12.623163692336238\n",
      "[ 4.18994855 -1.69984932]\n",
      "Gradient Descent(4869/9): loss=17.953945322962927, w0=72.45915704133883, w1=13.81305821500155\n",
      "[ 1.48705511  2.72087261]\n",
      "Gradient Descent(4870/9): loss=15.789863843217105, w0=71.41821846248753, w1=11.908447387256839\n",
      "[ 2.25999172 -0.59869196]\n",
      "Gradient Descent(4871/9): loss=18.379456678208957, w0=69.8362242559127, w1=12.327531762336747\n",
      "[-4.09342473 -3.20534582]\n",
      "Gradient Descent(4872/9): loss=22.027484872058473, w0=72.70162156770783, w1=14.571273833702104\n",
      "[-1.35157704 -0.74079168]\n",
      "Gradient Descent(4873/9): loss=16.15705091470325, w0=73.64772549711441, w1=15.089828011688025\n",
      "[-1.0810148   1.51580459]\n",
      "Gradient Descent(4874/9): loss=16.744712410534106, w0=74.40443586001547, w1=14.02876479527242\n",
      "[-0.02738051 -0.44842791]\n",
      "Gradient Descent(4875/9): loss=16.153237630301156, w0=74.42360221693701, w1=14.34266433554319\n",
      "[ 2.40135144 -0.12354243]\n",
      "Gradient Descent(4876/9): loss=16.39631955405564, w0=72.74265620911171, w1=14.429144036439903\n",
      "[-3.20270078  1.16584603]\n",
      "Gradient Descent(4877/9): loss=15.988545039008528, w0=74.9845467553388, w1=13.613051817610879\n",
      "[ 1.99744775 -1.23973128]\n",
      "Gradient Descent(4878/9): loss=16.823883592431486, w0=73.58633333110203, w1=14.480863713867931\n",
      "[ 0.40755867 -2.36456986]\n",
      "Gradient Descent(4879/9): loss=15.92979200309266, w0=73.30104226503393, w1=16.13606261837146\n",
      "[ 0.05530676  1.35731906]\n",
      "Gradient Descent(4880/9): loss=18.914011366279375, w0=73.26232753189751, w1=15.185939277443095\n",
      "[-2.52432027  1.98478753]\n",
      "Gradient Descent(4881/9): loss=16.841991993058606, w0=75.02935172434347, w1=13.79658800791561\n",
      "[ 5.41796395  1.53313362]\n",
      "Gradient Descent(4882/9): loss=16.941951093602185, w0=71.2367769609083, w1=12.723394475636377\n",
      "[ 1.83732539 -1.57527204]\n",
      "Gradient Descent(4883/9): loss=17.787819156909567, w0=69.95064919094494, w1=13.826084902611653\n",
      "[-5.68222274  1.17588533]\n",
      "Gradient Descent(4884/9): loss=21.034611356914557, w0=73.92820511070728, w1=13.002965170766629\n",
      "[-0.5971609  -2.73913366]\n",
      "Gradient Descent(4885/9): loss=15.700689376730148, w0=74.34621774344683, w1=14.920358733807028\n",
      "[-0.06843195 -0.04585297]\n",
      "Gradient Descent(4886/9): loss=16.977281911601246, w0=74.39412011012357, w1=14.952455812651746\n",
      "[-0.84830495  1.98398266]\n",
      "Gradient Descent(4887/9): loss=17.07559233549767, w0=74.98793357675, w1=13.563667951698745\n",
      "[ 6.37857502 -2.71134494]\n",
      "Gradient Descent(4888/9): loss=16.82424974073768, w0=70.52293106606054, w1=15.46160940778432\n",
      "[-3.10770047  0.63019643]\n",
      "Gradient Descent(4889/9): loss=21.18904105803775, w0=72.69832139741857, w1=15.02047190831458\n",
      "[-0.75573805  3.01357602]\n",
      "Gradient Descent(4890/9): loss=16.75022778630212, w0=73.22733803387362, w1=12.910968695501753\n",
      "[ 0.4401538   0.07849292]\n",
      "Gradient Descent(4891/9): loss=15.549839301845129, w0=72.91923037667692, w1=12.856023649846614\n",
      "[ 2.75917958 -0.55173099]\n",
      "Gradient Descent(4892/9): loss=15.650578626268663, w0=70.9878046695164, w1=13.242235346315681\n",
      "[ 0.03381184  0.05741585]\n",
      "Gradient Descent(4893/9): loss=18.07317412848502, w0=70.96413638466478, w1=13.202044253199999\n",
      "[-1.55208615  1.11916243]\n",
      "Gradient Descent(4894/9): loss=18.138388190034505, w0=72.05059668838372, w1=12.418630554349063\n",
      "[-5.2966287  -1.84568526]\n",
      "Gradient Descent(4895/9): loss=16.72176416541094, w0=75.75823677876376, w1=13.710610237878369\n",
      "[ 1.71493974  1.82963839]\n",
      "Gradient Descent(4896/9): loss=18.448968425747847, w0=74.55777895956085, w1=12.429863362803559\n",
      "[ 2.26582846  0.96234294]\n",
      "Gradient Descent(4897/9): loss=16.735646610468304, w0=72.97169903569677, w1=11.7562233036552\n",
      "[-0.88785725 -2.7956746 ]\n",
      "Gradient Descent(4898/9): loss=16.923009081782872, w0=73.59319910989248, w1=13.713195521723964\n",
      "[ 2.20257078  0.09246703]\n",
      "Gradient Descent(4899/9): loss=15.457928438347793, w0=72.05139956493365, w1=13.648468597718681\n",
      "[-3.1911206  -3.59758103]\n",
      "Gradient Descent(4900/9): loss=16.17205819349637, w0=74.28518398275581, w1=16.16677531662308\n",
      "[ 0.1986399   0.40140535]\n",
      "Gradient Descent(4901/9): loss=19.48734149089879, w0=74.14613605297865, w1=15.885791574042099\n",
      "[ 2.54713035  0.62244134]\n",
      "Gradient Descent(4902/9): loss=18.643630674775604, w0=72.3631448088257, w1=15.450082637203334\n",
      "[-0.46932889  5.30773691]\n",
      "Gradient Descent(4903/9): loss=17.76024032748101, w0=72.69167502850047, w1=11.7346668034953\n",
      "[ 0.5069453  -0.25563396]\n",
      "Gradient Descent(4904/9): loss=17.08983070543513, w0=72.33681331939029, w1=11.913610576056112\n",
      "[-0.36477908 -2.62319856]\n",
      "Gradient Descent(4905/9): loss=17.070253900370123, w0=72.59215867713412, w1=13.749849570439217\n",
      "[-5.12772353 -4.02632377]\n",
      "Gradient Descent(4906/9): loss=15.668610786941235, w0=76.18156514886056, w1=16.568276207766978\n",
      "[ 2.30883761  0.32969148]\n",
      "Gradient Descent(4907/9): loss=24.32474242958913, w0=74.56537881892594, w1=16.33749216873681\n",
      "[ 3.46626002  4.75633728]\n",
      "Gradient Descent(4908/9): loss=20.277641590659076, w0=72.13899680393706, w1=13.008056072541528\n",
      "[ 0.48360784 -0.5254551 ]\n",
      "Gradient Descent(4909/9): loss=16.16404383762986, w0=71.80047131897449, w1=13.375874643011208\n",
      "[-1.69333859  0.09437939]\n",
      "Gradient Descent(4910/9): loss=16.506476483822595, w0=72.98580833433464, w1=13.309809066583203\n",
      "[-1.24393921  0.4944581 ]\n",
      "Gradient Descent(4911/9): loss=15.447788462260737, w0=73.856565779514, w1=12.963688395493929\n",
      "[-3.35906116  0.77039743]\n",
      "Gradient Descent(4912/9): loss=15.677312283626259, w0=76.20790858949691, w1=12.424410194411134\n",
      "[ 1.38796407 -0.30609381]\n",
      "Gradient Descent(4913/9): loss=20.18837819406321, w0=75.23633374207175, w1=12.638675864668624\n",
      "[ 1.67561942  0.25064635]\n",
      "Gradient Descent(4914/9): loss=17.626040808917534, w0=74.0634001455436, w1=12.463223418473964\n",
      "[ 1.5609156  -3.04808902]\n",
      "Gradient Descent(4915/9): loss=16.198561135792012, w0=72.97075922468366, w1=14.596885733594947\n",
      "[-0.59280072  0.12003439]\n",
      "Gradient Descent(4916/9): loss=16.06214304874379, w0=73.38571972905913, w1=14.512861663974153\n",
      "[ 0.1931277   2.33283679]\n",
      "Gradient Descent(4917/9): loss=15.923799944842612, w0=73.25053033898814, w1=12.879875912759136\n",
      "[ 0.37244247  0.69716943]\n",
      "Gradient Descent(4918/9): loss=15.566731213743871, w0=72.98982060650904, w1=12.39185731294556\n",
      "[-0.48235305  0.20189124]\n",
      "Gradient Descent(4919/9): loss=16.023841081509293, w0=73.32746774034648, w1=12.250533444143464\n",
      "[-0.20757412 -0.11884214]\n",
      "Gradient Descent(4920/9): loss=16.14189102287456, w0=73.47276962410248, w1=12.333722942582195\n",
      "[-1.50399965 -0.83506054]\n",
      "Gradient Descent(4921/9): loss=16.0585270631299, w0=74.5255693780977, w1=12.918265319444387\n",
      "[-1.33130436  1.08766709]\n",
      "Gradient Descent(4922/9): loss=16.30197693000073, w0=75.4574824299406, w1=12.156898354387444\n",
      "[ 1.66549219  0.93223562]\n",
      "Gradient Descent(4923/9): loss=18.601303277196006, w0=74.29163789885122, w1=11.50433341713291\n",
      "[ 0.23422561 -2.33018007]\n",
      "Gradient Descent(4924/9): loss=17.834667506232414, w0=74.12767997352401, w1=13.135459467641489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.22824927  0.33728082]\n",
      "Gradient Descent(4925/9): loss=15.792719099045414, w0=72.56790548427377, w1=12.89936289511897\n",
      "[-1.02853211 -3.43543441]\n",
      "Gradient Descent(4926/9): loss=15.817840655125138, w0=73.2878779586304, w1=15.304166981923004\n",
      "[-2.65749323  3.10336547]\n",
      "Gradient Descent(4927/9): loss=17.050223330974156, w0=75.14812321686586, w1=13.131811156014502\n",
      "[ 2.51291686  0.63864433]\n",
      "Gradient Descent(4928/9): loss=17.165436591195427, w0=73.38908141335365, w1=12.684760125484805\n",
      "[-3.68202042 -0.33566643]\n",
      "Gradient Descent(4929/9): loss=15.706390112797044, w0=75.96649570591845, w1=12.919726628255598\n",
      "[ 0.99814235 -1.18693406]\n",
      "Gradient Descent(4930/9): loss=19.114005021857917, w0=75.26779605906437, w1=13.750580471906437\n",
      "[ 3.25204517  0.13629739]\n",
      "Gradient Descent(4931/9): loss=17.37066201190938, w0=72.99136443834131, w1=13.655172301988744\n",
      "[ 1.43755374  0.60050375]\n",
      "Gradient Descent(4932/9): loss=15.447051490988544, w0=71.98507682264635, w1=13.234819680161774\n",
      "[-2.18039262 -2.05426894]\n",
      "Gradient Descent(4933/9): loss=16.27241195140917, w0=73.51135165914326, w1=14.672807937668171\n",
      "[ 0.69228678  1.32753159]\n",
      "Gradient Descent(4934/9): loss=16.12126413596582, w0=73.02675091556418, w1=13.743535824548479\n",
      "[-1.48268776  1.78531386]\n",
      "Gradient Descent(4935/9): loss=15.456379454010467, w0=74.06463234645432, w1=12.493816123149488\n",
      "[ 0.53024498 -1.86721391]\n",
      "Gradient Descent(4936/9): loss=16.1688808551222, w0=73.69346086296979, w1=13.80086585783166\n",
      "[-2.30543154  0.53383526]\n",
      "Gradient Descent(4937/9): loss=15.517273280001653, w0=75.30726294006269, w1=13.427181174361177\n",
      "[ 5.46355145  5.82628729]\n",
      "Gradient Descent(4938/9): loss=17.41403850172876, w0=71.48277692711169, w1=9.348780072586976\n",
      "[-3.34917108 -2.83964972]\n",
      "Gradient Descent(4939/9): loss=25.55831220153639, w0=73.82719668165038, w1=11.336534879578066\n",
      "[ 0.13321546 -2.53199675]\n",
      "Gradient Descent(4940/9): loss=17.824683827760108, w0=73.73394585852407, w1=13.108932605240426\n",
      "[-1.80718208  1.2578392 ]\n",
      "Gradient Descent(4941/9): loss=15.551437207012478, w0=74.9989733161776, w1=12.228445165335948\n",
      "[ 4.26854156 -2.47719663]\n",
      "Gradient Descent(4942/9): loss=17.62232275069199, w0=72.01099422760224, w1=13.962482803021222\n",
      "[-0.64668813 -1.32421921]\n",
      "Gradient Descent(4943/9): loss=16.32537332024991, w0=72.46367591850486, w1=14.889436251237807\n",
      "[-3.37830075  4.46595786]\n",
      "Gradient Descent(4944/9): loss=16.724202767545723, w0=74.82848644391207, w1=11.763265746702519\n",
      "[ 3.39885084 -0.21596807]\n",
      "Gradient Descent(4945/9): loss=18.036426498723333, w0=72.44929085562774, w1=11.914443393353979\n",
      "[-4.13415084 -4.56363854]\n",
      "Gradient Descent(4946/9): loss=16.96762234197989, w0=75.34319644404299, w1=15.108990368893208\n",
      "[ 3.83178061  2.18586352]\n",
      "Gradient Descent(4947/9): loss=18.812924030972635, w0=72.66095001577823, w1=13.578885904003206\n",
      "[ 0.81050626  1.4136759 ]\n",
      "Gradient Descent(4948/9): loss=15.591132325044903, w0=72.09359563382279, w1=12.589312774850168\n",
      "[ 0.08355618  1.2367647 ]\n",
      "Gradient Descent(4949/9): loss=16.50268534141413, w0=72.03510630938328, w1=11.723577481951985\n",
      "[-1.40888486 -2.74674227]\n",
      "Gradient Descent(4950/9): loss=17.72020132959011, w0=73.0213257116174, w1=13.646297067957642\n",
      "[-2.32278025  3.44464548]\n",
      "Gradient Descent(4951/9): loss=15.436917457593896, w0=74.64727188723064, w1=11.235045233870006\n",
      "[-2.72513257 -3.06885413]\n",
      "Gradient Descent(4952/9): loss=18.820931246503743, w0=76.55486468553325, w1=13.383243122661423\n",
      "[ 0.76720209  0.1061919 ]\n",
      "Gradient Descent(4953/9): loss=20.707414625241373, w0=76.0178232249758, w1=13.308908791192282\n",
      "[ 0.36030083 -1.27958201]\n",
      "Gradient Descent(4954/9): loss=19.11029374717453, w0=75.76561264366842, w1=14.204616196583256\n",
      "[ 1.66128615  1.08961016]\n",
      "Gradient Descent(4955/9): loss=18.703257914411736, w0=74.60271234189766, w1=13.441889087047446\n",
      "[ 1.58075509 -1.16885485]\n",
      "Gradient Descent(4956/9): loss=16.2430692484212, w0=73.49618377951795, w1=14.260087481461285\n",
      "[-0.26448055  1.47230052]\n",
      "Gradient Descent(4957/9): loss=15.710835388708755, w0=73.68132016580311, w1=13.22947711849842\n",
      "[-0.90704491 -0.08398466]\n",
      "Gradient Descent(4958/9): loss=15.492235394257246, w0=74.31625160135826, w1=13.288266383290113\n",
      "[ 0.54734402 -1.85467489]\n",
      "Gradient Descent(4959/9): loss=15.92679256893943, w0=73.93311078988535, w1=14.586538807152984\n",
      "[ 0.42532835 -1.34087331]\n",
      "Gradient Descent(4960/9): loss=16.202701331100123, w0=73.63538094513183, w1=15.525150120803016\n",
      "[ 0.04420808  4.22372714]\n",
      "Gradient Descent(4961/9): loss=17.536092636989835, w0=73.6044352902479, w1=12.568541125877491\n",
      "[ 1.06151478 -1.1195258 ]\n",
      "Gradient Descent(4962/9): loss=15.849213697160035, w0=72.86137494321346, w1=13.352209185845267\n",
      "[-2.15821263 -0.42095029]\n",
      "Gradient Descent(4963/9): loss=15.48756488717845, w0=74.37212378130634, w1=13.646874385822674\n",
      "[-0.38029171  2.32818968]\n",
      "Gradient Descent(4964/9): loss=15.981118966068916, w0=74.63832797562408, w1=12.017141607274649\n",
      "[-0.37299497  0.26395101]\n",
      "Gradient Descent(4965/9): loss=17.35915829268663, w0=74.89942445208962, w1=11.832375897906216\n",
      "[ 1.72488284 -0.50683143]\n",
      "Gradient Descent(4966/9): loss=18.031565760486437, w0=73.69200646665738, w1=12.187157897131774\n",
      "[-0.17250319 -2.74389743]\n",
      "Gradient Descent(4967/9): loss=16.30047210595602, w0=73.8127587029994, w1=14.107886097458104\n",
      "[ 0.8706953 -1.597321 ]\n",
      "Gradient Descent(4968/9): loss=15.717784705036689, w0=73.20327199401615, w1=15.22601080064816\n",
      "[-0.76996751 -0.56958776]\n",
      "Gradient Descent(4969/9): loss=16.914775571764515, w0=73.74224924881581, w1=15.624722235988973\n",
      "[ 0.37982287  2.10506764]\n",
      "Gradient Descent(4970/9): loss=17.786920052093826, w0=73.47637323940899, w1=14.151174884635363\n",
      "[-2.02341294 -0.06501066]\n",
      "Gradient Descent(4971/9): loss=15.627963006468757, w0=74.89276229929784, w1=14.196682344681207\n",
      "[ 2.60809734 -1.09199238]\n",
      "Gradient Descent(4972/9): loss=16.921055942494938, w0=73.06709416235026, w1=14.96107701356876\n",
      "[ 1.48353688  2.69664396]\n",
      "Gradient Descent(4973/9): loss=16.508833810608664, w0=72.02861834722943, w1=13.073426242943256\n",
      "[-0.957561   -4.70369118]\n",
      "Gradient Descent(4974/9): loss=16.26891877327392, w0=72.69891104637203, w1=16.366010069561437\n",
      "[-1.46710037  3.61039265]\n",
      "Gradient Descent(4975/9): loss=19.728263905219727, w0=73.72588130755729, w1=13.838735214565379\n",
      "[ 1.48891591  2.00616885]\n",
      "Gradient Descent(4976/9): loss=15.543630967740086, w0=72.68364017134026, w1=12.434417019268002\n",
      "[ 1.28894505 -1.68894267]\n",
      "Gradient Descent(4977/9): loss=16.118431078374016, w0=71.78137863853148, w1=13.616676891324616\n",
      "[-2.92209227 -1.47705033]\n",
      "Gradient Descent(4978/9): loss=16.539161213324487, w0=73.82684322768057, w1=14.650612122548218\n",
      "[ 1.60196088  1.61664915]\n",
      "Gradient Descent(4979/9): loss=16.213393424326863, w0=72.7054706118252, w1=13.518957718695164\n",
      "[ 0.84230563 -0.33097382]\n",
      "Gradient Descent(4980/9): loss=15.559795484337211, w0=72.11585667194596, w1=13.750639392416167\n",
      "[ 0.42154531  0.91289739]\n",
      "Gradient Descent(4981/9): loss=16.11650753802134, w0=71.82077495287962, w1=13.111611216020654\n",
      "[-1.79993349 -2.28934161]\n",
      "Gradient Descent(4982/9): loss=16.538718236853413, w0=73.08072839721613, w1=14.714150343539051\n",
      "[-3.61292255  1.44116584]\n",
      "Gradient Descent(4983/9): loss=16.17053210044485, w0=75.60977418027305, w1=13.705334254324667\n",
      "[-1.04915553 -2.31573558]\n",
      "Gradient Descent(4984/9): loss=18.09292612707197, w0=76.34418304840868, w1=15.326349158871441\n",
      "[ 5.77533397  0.07034083]\n",
      "Gradient Descent(4985/9): loss=21.742967689123176, w0=72.30144927248823, w1=15.277110578733339\n",
      "[-0.79178229  1.80970951]\n",
      "Gradient Descent(4986/9): loss=17.493708971913783, w0=72.85569687395686, w1=14.010313919808226\n",
      "[-3.70723844  0.28586225]\n",
      "Gradient Descent(4987/9): loss=15.622677468145868, w0=75.4507637799259, w1=13.810210342771418\n",
      "[ 1.15981574  1.67907135]\n",
      "Gradient Descent(4988/9): loss=17.76648552963005, w0=74.63889276304452, w1=12.634860400319782\n",
      "[ 1.67866259 -1.01011045]\n",
      "Gradient Descent(4989/9): loss=16.647248522962666, w0=73.46382895309524, w1=13.341937716445397\n",
      "[-0.2361071   0.27402477]\n",
      "Gradient Descent(4990/9): loss=15.40981299136166, w0=73.62910392141917, w1=13.150120375284656\n",
      "[-3.98314791  1.82644424]\n",
      "Gradient Descent(4991/9): loss=15.496376791256996, w0=76.41730745817091, w1=11.87160940610359\n",
      "[ 1.40271179 -0.98065379]\n",
      "Gradient Descent(4992/9): loss=21.556653898166136, w0=75.43540920617902, w1=12.55806705965696\n",
      "[ 2.3059785  -0.84983046]\n",
      "Gradient Descent(4993/9): loss=18.103586690370886, w0=73.8212242579582, w1=13.152948381616378\n",
      "[-3.03822411  1.56927789]\n",
      "Gradient Descent(4994/9): loss=15.578299076631508, w0=75.9479811351337, w1=12.054453858846735\n",
      "[ 6.76761969  0.56273154]\n",
      "Gradient Descent(4995/9): loss=19.92358381406904, w0=71.21064734986685, w1=11.660541782932654\n",
      "[-0.72101653 -3.2916203 ]\n",
      "Gradient Descent(4996/9): loss=19.210595437810433, w0=71.71535892290377, w1=13.96467599097658\n",
      "[-1.63814728  0.72109783]\n",
      "Gradient Descent(4997/9): loss=16.749413391656365, w0=72.86206202033273, w1=13.459907508869335\n",
      "[-1.94883922 -0.47866429]\n",
      "Gradient Descent(4998/9): loss=15.479335508306958, w0=74.22624947331158, w1=13.794972515045744\n",
      "[ 2.73807379  2.73739184]\n",
      "Gradient Descent(4999/9): loss=15.870199584651132, w0=72.30959781926373, w1=11.878798224058684\n",
      "[-1.06104531 -2.20478801]\n",
      "Gradient Descent(5000/9): loss=17.151798072672047, w0=73.05232953691794, w1=13.42214982816372\n",
      "[ 1.66716386 -0.21205921]\n",
      "Gradient Descent(5001/9): loss=15.41672805529929, w0=71.88531483813956, w1=13.570591276190155\n",
      "[-3.39664542  4.57780514]\n",
      "Gradient Descent(5002/9): loss=16.382104421906067, w0=74.26296663385862, w1=10.366127677725792\n",
      "[ 1.3664688  -5.40181366]\n",
      "Gradient Descent(5003/9): loss=20.702616638325527, w0=73.30643847584668, w1=14.147397242874582\n",
      "[ 0.2033402 -0.5914751]\n",
      "Gradient Descent(5004/9): loss=15.608867701227433, w0=73.16410033335191, w1=14.561429809922071\n",
      "[-0.5676168  -0.04639632]\n",
      "Gradient Descent(5005/9): loss=15.979370941284337, w0=73.56143209611069, w1=14.593907233784206\n",
      "[-1.21858393  0.05719991]\n",
      "Gradient Descent(5006/9): loss=16.04238371885791, w0=74.41444084769874, w1=14.553867297692737\n",
      "[ 2.12172299  0.2087432 ]\n",
      "Gradient Descent(5007/9): loss=16.59057344502954, w0=72.92923475792627, w1=14.4077470546616\n",
      "[-0.44725077 -0.62904699]\n",
      "Gradient Descent(5008/9): loss=15.883010389518196, w0=73.24231029403428, w1=14.84807994762776\n",
      "[ 0.96937137  1.88520709]\n",
      "Gradient Descent(5009/9): loss=16.323434577856926, w0=72.56375033507275, w1=13.528434987275773\n",
      "[ 0.61370183  0.85091991]\n",
      "Gradient Descent(5010/9): loss=15.65365014404853, w0=72.13415905441946, w1=12.932791052759594\n",
      "[ 1.89014987 -0.45844529]\n",
      "Gradient Descent(5011/9): loss=16.207974415411638, w0=70.81105414810159, w1=13.253702757030613\n",
      "[-1.11833531  2.42619102]\n",
      "Gradient Descent(5012/9): loss=18.493744446317056, w0=71.59388886726998, w1=11.555369044527092\n",
      "[-3.20818248 -1.87947665]\n",
      "Gradient Descent(5013/9): loss=18.682492940805517, w0=73.839616603377, w1=12.87100270252309\n",
      "[ 0.66603448  0.03519182]\n",
      "Gradient Descent(5014/9): loss=15.72004293695739, w0=73.37339246606558, w1=12.846368425645277\n",
      "[ 2.33261764 -0.94948438]\n",
      "Gradient Descent(5015/9): loss=15.589607963236261, w0=71.74056011521553, w1=13.511007491483447\n",
      "[ 4.3698173  -2.10820708]\n",
      "Gradient Descent(5016/9): loss=16.59284413493059, w0=68.68168800774541, w1=14.986752450647655\n",
      "[-4.02285306  0.59907052]\n",
      "Gradient Descent(5017/9): loss=27.15782388259155, w0=71.4976851532023, w1=14.567403086304983\n",
      "[-2.73777164 -2.61772965]\n",
      "Gradient Descent(5018/9): loss=17.590656753987737, w0=73.41412529884711, w1=16.399813843436675\n",
      "[ 2.41221757  2.44740101]\n",
      "Gradient Descent(5019/9): loss=19.65660840291212, w0=71.72557299928155, w1=14.686633135592903\n",
      "[-2.40844777  1.18966749]\n",
      "Gradient Descent(5020/9): loss=17.344075954931398, w0=73.41148643490368, w1=13.853865892419565\n",
      "[-0.39223142 -1.30206521]\n",
      "Gradient Descent(5021/9): loss=15.462793971612621, w0=73.68604842617445, w1=14.765311536505612\n",
      "[-1.24173845 -0.74219042]\n",
      "Gradient Descent(5022/9): loss=16.28915195996617, w0=74.55526533946033, w1=15.28484483182038\n",
      "[-0.16585109  5.3275468 ]\n",
      "Gradient Descent(5023/9): loss=17.810632861219517, w0=74.67136110181676, w1=11.555562070525456\n",
      "[ 3.34649024 -3.16488891]\n",
      "Gradient Descent(5024/9): loss=18.1857344180692, w0=72.3288179357486, w1=13.77098430872553\n",
      "[-0.5167363  -1.60284335]\n",
      "Gradient Descent(5025/9): loss=15.894020450493391, w0=72.69053334587743, w1=14.89297465099483\n",
      "[ 2.18503794 -2.89322898]\n",
      "Gradient Descent(5026/9): loss=16.56658184965636, w0=71.16100678921812, w1=16.91823493729926\n",
      "[ 0.01966474  5.66691931]\n",
      "Gradient Descent(5027/9): loss=23.572270020958797, w0=71.14724147133312, w1=12.95139141946213\n",
      "[ 0.46022713 -1.58225869]\n",
      "Gradient Descent(5028/9): loss=17.82956806715103, w0=70.82508248313138, w1=14.058972505749047\n",
      "[-3.86152289 -0.95426122]\n",
      "Gradient Descent(5029/9): loss=18.601243268841248, w0=73.52814850833197, w1=14.72695535641573\n",
      "[-3.77384224 -0.74064627]\n",
      "Gradient Descent(5030/9): loss=16.19112634946348, w0=76.16983807724046, w1=15.245407743919845\n",
      "[ 4.33917196  0.5433817 ]\n",
      "Gradient Descent(5031/9): loss=21.080174466430222, w0=73.13241770694061, w1=14.865040556948628\n",
      "[ 1.04313449  2.5599652 ]\n",
      "Gradient Descent(5032/9): loss=16.358496690253734, w0=72.40222356329176, w1=13.07306492016219\n",
      "[-2.11534234 -0.04658846]\n",
      "Gradient Descent(5033/9): loss=15.866132022377988, w0=73.88296320161885, w1=13.105676845398742\n",
      "[ 3.81546271 -1.05412326]\n",
      "Gradient Descent(5034/9): loss=15.62932394733173, w0=71.21213930652051, w1=13.843563129011548\n",
      "[-2.2063554   0.71897623]\n",
      "Gradient Descent(5035/9): loss=17.61899112841763, w0=72.7565880867395, w1=13.340279767010694\n",
      "[-0.87151216 -1.70198574]\n",
      "Gradient Descent(5036/9): loss=15.539972471580294, w0=73.36664660073762, w1=14.531669786952385\n",
      "[-0.74041273  0.21674269]\n",
      "Gradient Descent(5037/9): loss=15.941839437627385, w0=73.88493551019204, w1=14.379949901583231\n",
      "[ 0.25079031 -0.19659806]\n",
      "Gradient Descent(5038/9): loss=15.965750100329917, w0=73.70938229366237, w1=14.517568541645126\n",
      "[-0.58603461  0.36512827]\n",
      "Gradient Descent(5039/9): loss=16.01076414482144, w0=74.11960652219655, w1=14.261978751050538\n",
      "[ 0.29346191  0.22427328]\n",
      "Gradient Descent(5040/9): loss=16.032735626810855, w0=73.91418318844147, w1=14.104987457284414\n",
      "[ 0.6601249   1.06482765]\n",
      "Gradient Descent(5041/9): loss=15.773734265220279, w0=73.45209576158913, w1=13.359608105745846\n",
      "[-1.98180953 -1.93625451]\n",
      "Gradient Descent(5042/9): loss=15.405609862875522, w0=74.83936243084528, w1=14.71498626462085\n",
      "[ 1.29015486  2.39866403]\n",
      "Gradient Descent(5043/9): loss=17.34303164530819, w0=73.93625402709633, w1=13.035921446974724\n",
      "[-0.9401514   1.96941628]\n",
      "Gradient Descent(5044/9): loss=15.690658304515374, w0=74.59436000592312, w1=11.657330047638782\n",
      "[-0.46678913 -0.27519526]\n",
      "Gradient Descent(5045/9): loss=17.891996152578614, w0=74.92111239570573, w1=11.849966729669104\n",
      "[ 2.66003201 -4.174807  ]\n",
      "Gradient Descent(5046/9): loss=18.03779768934675, w0=73.05908998965774, w1=14.7723316322004\n",
      "[ 2.17575038 -0.00935435]\n",
      "Gradient Descent(5047/9): loss=16.24889310036412, w0=71.53606472330644, w1=14.778879678025184\n",
      "[ 1.51537874 -1.27377335]\n",
      "Gradient Descent(5048/9): loss=17.774836737831333, w0=70.47529960455788, w1=15.670521021351705\n",
      "[-6.50971255  0.85935651]\n",
      "Gradient Descent(5049/9): loss=21.758025109847264, w0=75.0320983862218, w1=15.068971467788812\n",
      "[ 1.59534285  2.84950755]\n",
      "Gradient Descent(5050/9): loss=18.15938857664757, w0=73.91535838987357, w1=13.074316182905555\n",
      "[ 1.36234763 -1.59672886]\n",
      "Gradient Descent(5051/9): loss=15.661152521452381, w0=72.96171504862095, w1=14.192026387073254\n",
      "[-2.7023982  -1.08832002]\n",
      "Gradient Descent(5052/9): loss=15.69476418196795, w0=74.85339378772527, w1=14.953850397929303\n",
      "[ 2.55402159 -2.0944666 ]\n",
      "Gradient Descent(5053/9): loss=17.688405360792757, w0=73.06557867461355, w1=16.419977015908962\n",
      "[-1.55290818  2.81499445]\n",
      "Gradient Descent(5054/9): loss=19.734536109340457, w0=74.15261440215833, w1=14.44948089877445\n",
      "[-1.21666376  2.20045082]\n",
      "Gradient Descent(5055/9): loss=16.224789624460264, w0=75.00427903518458, w1=12.909165322297651\n",
      "[-0.09949472 -1.28593443]\n",
      "Gradient Descent(5056/9): loss=17.011310463031716, w0=75.0739253378454, w1=13.809319423702858\n",
      "[ 0.99794013 -0.82477874]\n",
      "Gradient Descent(5057/9): loss=17.024414189957024, w0=74.37536724701052, w1=14.386664538261893\n",
      "[-0.32259869  0.92069036]\n",
      "Gradient Descent(5058/9): loss=16.38193083650909, w0=74.60118632670674, w1=13.742181287011363\n",
      "[ 1.89530494  0.47962366]\n",
      "Gradient Descent(5059/9): loss=16.27480282515833, w0=73.27447286785178, w1=13.406444725272179\n",
      "[-2.54849703  3.10379553]\n",
      "Gradient Descent(5060/9): loss=15.38876108188458, w0=75.05842078794402, w1=11.23378785753657\n",
      "[ 2.70346068 -1.50745747]\n",
      "Gradient Descent(5061/9): loss=19.464704455245297, w0=73.16599831081889, w1=12.289008084647723\n",
      "[ 1.9730233 -0.8583111]\n",
      "Gradient Descent(5062/9): loss=16.102958529186434, w0=71.7848819990899, w1=12.88982585744188\n",
      "[-2.38213101 -1.43670314]\n",
      "Gradient Descent(5063/9): loss=16.69847182136474, w0=73.45237370945763, w1=13.895518052478009\n",
      "[ 1.7824266   5.07072819]\n",
      "Gradient Descent(5064/9): loss=15.484888496378538, w0=72.20467509008641, w1=10.346008318790274\n",
      "[-4.16388774 -5.24920584]\n",
      "Gradient Descent(5065/9): loss=20.88916803044119, w0=75.11939651050517, w1=14.020452404990014\n",
      "[ 3.49893531  0.14408676]\n",
      "Gradient Descent(5066/9): loss=17.198266316816795, w0=72.6701417917605, w1=13.919591672387002\n",
      "[ 2.55020444 -1.08951369]\n",
      "Gradient Descent(5067/9): loss=15.677185615985136, w0=70.88499868581044, w1=14.682251253241073\n",
      "[-3.23067162  1.39110846]\n",
      "Gradient Descent(5068/9): loss=19.01039344542513, w0=73.14646882073728, w1=13.708475330662063\n",
      "[ 0.32499037 -0.42386685]\n",
      "Gradient Descent(5069/9): loss=15.422925320395507, w0=72.91897556472159, w1=14.005182128460838\n",
      "[-0.59514205  1.04383641]\n",
      "Gradient Descent(5070/9): loss=15.594239483661397, w0=73.33557500116895, w1=13.274496642207557\n",
      "[-1.53915692 -1.20193434]\n",
      "Gradient Descent(5071/9): loss=15.40781211579837, w0=74.41298484721683, w1=14.115850678636646\n",
      "[ 2.0673801  -0.50620275]\n",
      "Gradient Descent(5072/9): loss=16.214374626999607, w0=72.9658187785997, w1=14.470192604324668\n",
      "[-0.62344819 -1.25492966]\n",
      "Gradient Descent(5073/9): loss=15.930239214390307, w0=73.40223251221913, w1=15.348643366996233\n",
      "[ 0.78532345  1.68343109]\n",
      "Gradient Descent(5074/9): loss=17.138204866436592, w0=72.85250609712246, w1=14.170241604474446\n",
      "[-1.82830339  0.6623703 ]\n",
      "Gradient Descent(5075/9): loss=15.72172713637036, w0=74.13231847187622, w1=13.70658239196582\n",
      "[ 0.52477642 -2.46897645]\n",
      "Gradient Descent(5076/9): loss=15.763077177780984, w0=73.76497498119053, w1=15.434865905273757\n",
      "[ 1.13583991  0.16269347]\n",
      "Gradient Descent(5077/9): loss=17.40814586956516, w0=72.96988704466011, w1=15.320980475783552\n",
      "[-0.43232652  0.44968799]\n",
      "Gradient Descent(5078/9): loss=17.13352119467823, w0=73.27251560621711, w1=15.00619887963149\n",
      "[-3.13510927  1.44429889]\n",
      "Gradient Descent(5079/9): loss=16.55119741856042, w0=75.46709209712616, w1=13.995189655627772\n",
      "[ 2.90323024 -1.84777964]\n",
      "Gradient Descent(5080/9): loss=17.880080382274826, w0=73.43483093052659, w1=15.288635405719187\n",
      "[ 2.74191654  3.33824708]\n",
      "Gradient Descent(5081/9): loss=17.03191668890141, w0=71.5154893533861, w1=12.951862452692183\n",
      "[-1.00448153 -0.6527868 ]\n",
      "Gradient Descent(5082/9): loss=17.106612013749793, w0=72.21862642255448, w1=13.408813214576202\n",
      "[ 1.82012225 -0.48873943]\n",
      "Gradient Descent(5083/9): loss=15.966531510257623, w0=70.94454084460212, w1=13.750930817570508\n",
      "[ 0.38250207 -1.87813374]\n",
      "Gradient Descent(5084/9): loss=18.182463485969695, w0=70.67678939229008, w1=15.065624435995554\n",
      "[-6.98956658 -0.96256819]\n",
      "Gradient Descent(5085/9): loss=20.06813785497647, w0=75.56948600074725, w1=15.739422166491936\n",
      "[-0.48949551  1.30243977]\n",
      "Gradient Descent(5086/9): loss=20.528127660111732, w0=75.91213285476714, w1=14.827714325457025\n",
      "[ 3.15923304  0.0731001 ]\n",
      "Gradient Descent(5087/9): loss=19.72195645168042, w0=73.70066972619301, w1=14.776544253698537\n",
      "[-0.16231939  3.01335876]\n",
      "Gradient Descent(5088/9): loss=16.309496107363394, w0=73.81429329611292, w1=12.667193123238313\n",
      "[-0.29855315 -1.15071219]\n",
      "Gradient Descent(5089/9): loss=15.851374826626985, w0=74.02328050262449, w1=13.472691654215716\n",
      "[-0.93775638 -2.48905842]\n",
      "Gradient Descent(5090/9): loss=15.651894425650614, w0=74.6797099706658, w1=15.215032550944747\n",
      "[ 3.00277159 -1.15656638]\n",
      "Gradient Descent(5091/9): loss=17.851759968153313, w0=72.57776985920317, w1=16.02462902008621\n",
      "[-2.3330928   3.56171463]\n",
      "Gradient Descent(5092/9): loss=18.880625027272274, w0=74.21093481604048, w1=13.531428778260633\n",
      "[-0.60273994  1.00687261]\n",
      "Gradient Descent(5093/9): loss=15.807681409370852, w0=74.6328527732191, w1=12.826617954260586\n",
      "[ 4.55532128 -0.40511035]\n",
      "Gradient Descent(5094/9): loss=16.495521874126233, w0=71.44412788065334, w1=13.110195199916399\n",
      "[-3.09861851 -0.10762963]\n",
      "Gradient Descent(5095/9): loss=17.165028508216174, w0=73.6131608368316, w1=13.185535943291345\n",
      "[ 1.05436802  0.56403668]\n",
      "Gradient Descent(5096/9): loss=15.480114489761924, w0=72.8751032208028, w1=12.790710268404537\n",
      "[-2.34965291 -0.69212646]\n",
      "Gradient Descent(5097/9): loss=15.710954447394286, w0=74.51986025584925, w1=13.275198789392961\n",
      "[ 1.76480791  0.21161967]\n",
      "Gradient Descent(5098/9): loss=16.158263085443426, w0=73.2844947221286, w1=13.127065019422595\n",
      "[-0.3773075  -1.65048894]\n",
      "Gradient Descent(5099/9): loss=15.448112405486128, w0=73.54860997517282, w1=14.282407280220038\n",
      "[-2.4611375   2.43982746]\n",
      "Gradient Descent(5100/9): loss=15.740480357922253, w0=75.27140622489134, w1=12.57452805619935\n",
      "[ 1.08674052 -1.78919012]\n",
      "Gradient Descent(5101/9): loss=17.750789174315916, w0=74.51068786021722, w1=13.826961137540245\n",
      "[ 2.46690596 -1.9546079 ]\n",
      "Gradient Descent(5102/9): loss=16.186438276274693, w0=72.78385369088312, w1=15.19518666603801\n",
      "[-1.3332121   3.50143962]\n",
      "Gradient Descent(5103/9): loss=16.98739862858238, w0=73.71710216131191, w1=12.744178931539548\n",
      "[ 1.0454396   1.72995368]\n",
      "Gradient Descent(5104/9): loss=15.745933359750865, w0=72.98529444274588, w1=11.533211358045346\n",
      "[ 0.42437322  3.38295379]\n",
      "Gradient Descent(5105/9): loss=17.327946575298938, w0=72.68823318968423, w1=9.16514370394081\n",
      "[ 0.55653833 -3.18434157]\n",
      "Gradient Descent(5106/9): loss=24.87706900504496, w0=72.29865636019464, w1=11.394182805534058\n",
      "[ 3.44661456 -2.89764072]\n",
      "Gradient Descent(5107/9): loss=18.055881635480542, w0=69.88602617038843, w1=13.422531306556714\n",
      "[-4.54793982 -0.02971908]\n",
      "Gradient Descent(5108/9): loss=21.19439970947002, w0=73.06958404605658, w1=13.443334662361826\n",
      "[-3.4126888   4.98981366]\n",
      "Gradient Descent(5109/9): loss=15.411713299262093, w0=75.45846620518785, w1=9.950465103740333\n",
      "[ 6.54785178 -9.28535348]\n",
      "Gradient Descent(5110/9): loss=23.956307034941865, w0=70.87496995970936, w1=16.450212540985017\n",
      "[-4.66648048  7.08247357]\n",
      "Gradient Descent(5111/9): loss=22.723487800395915, w0=74.1415062938155, w1=11.4924810415526\n",
      "[ 1.30294224  0.36516413]\n",
      "Gradient Descent(5112/9): loss=17.71963174013612, w0=73.22944672706879, w1=11.236866149620386\n",
      "[ 0.91619727 -2.81909834]\n",
      "Gradient Descent(5113/9): loss=17.903146129270915, w0=72.58810863896751, w1=13.210234989653477\n",
      "[ 2.41492875  6.20942493]\n",
      "Gradient Descent(5114/9): loss=15.671283167393552, w0=70.89765851402146, w1=8.863637538301411\n",
      "[-3.27081992 -1.12410721]\n",
      "Gradient Descent(5115/9): loss=28.91100094690589, w0=73.18723245519148, w1=9.650512583547306\n",
      "[-1.20571249 -1.79656827]\n",
      "Gradient Descent(5116/9): loss=22.722964949680463, w0=74.03123120017146, w1=10.908110374140929\n",
      "[ 2.57167796 -3.58970136]\n",
      "Gradient Descent(5117/9): loss=18.964268875285114, w0=72.23105662701137, w1=13.420901329532647\n",
      "[-2.84760178 -0.83116911]\n",
      "Gradient Descent(5118/9): loss=15.952458644678567, w0=74.22437787038008, w1=14.002719706689717\n",
      "[-0.14158947  1.90819521]\n",
      "Gradient Descent(5119/9): loss=15.95553023335888, w0=74.32349049965647, w1=12.666983058101899\n",
      "[ 2.03227198 -0.03986926]\n",
      "Gradient Descent(5120/9): loss=16.246158034432092, w0=72.9009001146355, w1=12.694891539058748\n",
      "[-1.09759281 -1.32285126]\n",
      "Gradient Descent(5121/9): loss=15.771092890188935, w0=73.66921508439938, w1=13.620887418791435\n",
      "[ 1.94161626 -1.26719085]\n",
      "Gradient Descent(5122/9): loss=15.466275505664138, w0=72.31008370271034, w1=14.507921014078624\n",
      "[ 0.30737385 -0.88908301]\n",
      "Gradient Descent(5123/9): loss=16.398463209564174, w0=72.09492200675675, w1=15.130279123818967\n",
      "[-1.1660789  -0.35758493]\n",
      "Gradient Descent(5124/9): loss=17.46687356038967, w0=72.91117723776374, w1=15.380588575709174\n",
      "[-1.99938459  2.17111965]\n",
      "Gradient Descent(5125/9): loss=17.265799697324315, w0=74.31074645166065, w1=13.860804822975638\n",
      "[ 0.82118544  3.39988885]\n",
      "Gradient Descent(5126/9): loss=15.975469553526938, w0=73.73591664195978, w1=11.480882625556335\n",
      "[ 1.02075198 -1.16559299]\n",
      "Gradient Descent(5127/9): loss=17.481227803197903, w0=73.0213902562175, w1=12.296797721439404\n",
      "[-0.26492729 -0.26560707]\n",
      "Gradient Descent(5128/9): loss=16.122668254853814, w0=73.20683936048486, w1=12.482722670700596\n",
      "[-0.3414965  -0.35747018]\n",
      "Gradient Descent(5129/9): loss=15.886673857113161, w0=73.4458869115051, w1=12.732951799022539\n",
      "[-0.2544442 -0.8434207]\n",
      "Gradient Descent(5130/9): loss=15.676260259388414, w0=73.6239978526477, w1=13.323346289386341\n",
      "[ 1.60112311 -0.89554388]\n",
      "Gradient Descent(5131/9): loss=15.452588088130407, w0=72.50321167378952, w1=13.950227008201082\n",
      "[-1.89851768  1.20796024]\n",
      "Gradient Descent(5132/9): loss=15.809191262284386, w0=73.83217404744181, w1=13.104654840418668\n",
      "[ 0.02531932 -2.50445744]\n",
      "Gradient Descent(5133/9): loss=15.601079600606388, w0=73.81445052031614, w1=14.8577750509648\n",
      "[-0.11089962  1.95628771]\n",
      "Gradient Descent(5134/9): loss=16.47089112473981, w0=73.89208025316246, w1=13.488373650595019\n",
      "[ 1.85007667  2.17078203]\n",
      "Gradient Descent(5135/9): loss=15.564822023811232, w0=72.59702658452075, w1=11.968826228426511\n",
      "[-3.82845461 -0.81028532]\n",
      "Gradient Descent(5136/9): loss=16.770108044944962, w0=75.27694481485479, w1=12.536025951874542\n",
      "[ 2.03585748 -2.82886216]\n",
      "Gradient Descent(5137/9): loss=17.797349695978575, w0=73.85184457936361, w1=14.51622946242868\n",
      "[-3.56741247 -2.71161992]\n",
      "Gradient Descent(5138/9): loss=16.078710444022885, w0=76.34903330730768, w1=16.414363405122153\n",
      "[ 2.36822515  4.87979889]\n",
      "Gradient Descent(5139/9): loss=24.358828570669, w0=74.69127570298329, w1=12.998504185164235\n",
      "[ 1.48179738  0.4380523 ]\n",
      "Gradient Descent(5140/9): loss=16.477967241357995, w0=73.65401753903456, w1=12.691867577240634\n",
      "[-0.68875676 -2.20667739]\n",
      "Gradient Descent(5141/9): loss=15.761072026627936, w0=74.1361472696629, w1=14.236541749505568\n",
      "[ 1.51439746  1.94175721]\n",
      "Gradient Descent(5142/9): loss=16.0269548751415, w0=73.07606905077205, w1=12.877311700725155\n",
      "[-2.12935691 -0.79537366]\n",
      "Gradient Descent(5143/9): loss=15.59106114535252, w0=74.56661888553882, w1=13.434073260134905\n",
      "[ 2.8248631  -0.39746343]\n",
      "Gradient Descent(5144/9): loss=16.196808014520915, w0=72.58921471256133, w1=13.712297659539828\n",
      "[-0.85821893  3.21210636]\n",
      "Gradient Descent(5145/9): loss=15.661241994137194, w0=73.18996796417532, w1=11.463823208662102\n",
      "[-3.6589258  -3.88113867]\n",
      "Gradient Descent(5146/9): loss=17.423195776240888, w0=75.75121602691357, w1=14.180620280698072\n",
      "[ 0.07194764  0.57972968]\n",
      "Gradient Descent(5147/9): loss=18.65067073509711, w0=75.70085268059722, w1=13.774809502108372\n",
      "[ 2.40063115  3.57443234]\n",
      "Gradient Descent(5148/9): loss=18.32608665387367, w0=74.0204108722512, w1=11.272706864661146\n",
      "[ 1.10758958 -0.94299327]\n",
      "Gradient Descent(5149/9): loss=18.085217701781616, w0=73.24509816836402, w1=11.932802155677843\n",
      "[ 1.24619659 -2.25146814]\n",
      "Gradient Descent(5150/9): loss=16.58354545831933, w0=72.3727605543771, w1=13.508829850806183\n",
      "[-5.68464696  0.31128111]\n",
      "Gradient Descent(5151/9): loss=15.81058098717159, w0=76.35201342292811, w1=13.290933075975497\n",
      "[ 2.97672981  2.07427009]\n",
      "Gradient Descent(5152/9): loss=20.079668261079558, w0=74.26830255456676, w1=11.838944009489472\n",
      "[ 1.91170735 -1.79723424]\n",
      "Gradient Descent(5153/9): loss=17.206657112395238, w0=72.93010740699002, w1=13.097007974359299\n",
      "[-0.41665577 -3.72371611]\n",
      "Gradient Descent(5154/9): loss=15.525299750731763, w0=73.22176644582618, w1=15.703609251341518\n",
      "[-1.84975047 -0.09028125]\n",
      "Gradient Descent(5155/9): loss=17.86134960587169, w0=74.51659177253822, w1=15.766806125987477\n",
      "[ 5.33045333  3.00967998]\n",
      "Gradient Descent(5156/9): loss=18.748747328297185, w0=70.78527443953962, w1=13.660030140249063\n",
      "[-2.08233465  0.47371164]\n",
      "Gradient Descent(5157/9): loss=18.548801402827603, w0=72.24290869643497, w1=13.328431990400782\n",
      "[ 0.28188707 -0.18622216]\n",
      "Gradient Descent(5158/9): loss=15.949645239634735, w0=72.04558775001517, w1=13.458787501238929\n",
      "[-1.37645907 -1.68484531]\n",
      "Gradient Descent(5159/9): loss=16.165275997726194, w0=73.00910910024162, w1=14.638179218838141\n",
      "[ 0.37864092  1.91408451]\n",
      "Gradient Descent(5160/9): loss=16.09746970800421, w0=72.74406045453482, w1=13.298320061731683\n",
      "[-1.06505433 -0.69589764]\n",
      "Gradient Descent(5161/9): loss=15.553513326115612, w0=73.48959848222273, w1=13.785448411592146\n",
      "[ 1.5789708  -0.52650067]\n",
      "Gradient Descent(5162/9): loss=15.451769754959718, w0=72.38431892190233, w1=14.153998883409812\n",
      "[ 0.58243706  2.78867523]\n",
      "Gradient Descent(5163/9): loss=16.026907857848606, w0=71.97661297767444, w1=12.201926221947026\n",
      "[-1.11895194 -1.28958671]\n",
      "Gradient Descent(5164/9): loss=17.069908204872885, w0=72.75987933534762, w1=13.104636915524294\n",
      "[ 2.38049117  4.05381247]\n",
      "Gradient Descent(5165/9): loss=15.598829476439045, w0=71.093535516096, w1=10.266968183038031\n",
      "[-3.13543138  0.72626188]\n",
      "Gradient Descent(5166/9): loss=22.967601026957485, w0=73.2883374821636, w1=9.758584868460368\n",
      "[-0.768484  -0.5442721]\n",
      "Gradient Descent(5167/9): loss=22.309298645450724, w0=73.82627628170094, w1=10.13957533712387\n",
      "[ 2.35424332 -2.20574681]\n",
      "Gradient Descent(5168/9): loss=21.105846324598964, w0=72.17830595755656, w1=11.683598107223515\n",
      "[ 0.56521434 -3.77298587]\n",
      "Gradient Descent(5169/9): loss=17.62120078745888, w0=71.78265591689978, w1=14.324688214728944\n",
      "[-0.83487123  1.43009366]\n",
      "Gradient Descent(5170/9): loss=16.88484249314896, w0=72.36706578002125, w1=13.323622653008492\n",
      "[-1.64207229  0.8171763 ]\n",
      "Gradient Descent(5171/9): loss=15.827601107056624, w0=73.51651638392153, w1=12.751599244213935\n",
      "[-0.03394737 -0.70745641]\n",
      "Gradient Descent(5172/9): loss=15.675736407527857, w0=73.54027954518409, w1=13.246818729558258\n",
      "[-2.51585725  1.3035899 ]\n",
      "Gradient Descent(5173/9): loss=15.443353627359981, w0=75.30137962075976, w1=12.334305799266637\n",
      "[ 0.52478742 -1.25146409]\n",
      "Gradient Descent(5174/9): loss=18.05680909475501, w0=74.93402842755256, w1=13.210330662073378\n",
      "[ 3.69172231 -0.38136906]\n",
      "Gradient Descent(5175/9): loss=16.767145682015876, w0=72.34982281373935, w1=13.477289001857\n",
      "[-2.65405617  1.8554355 ]\n",
      "Gradient Descent(5176/9): loss=15.831552444079989, w0=74.20766213521546, w1=12.178484154114024\n",
      "[ 0.60317409 -2.21526619]\n",
      "Gradient Descent(5177/9): loss=16.649945903732075, w0=73.78544027287185, w1=13.729170483822088\n",
      "[-2.13877297  0.05287624]\n",
      "Gradient Descent(5178/9): loss=15.537797633141919, w0=75.28258134969815, w1=13.692157119121033\n",
      "[ 4.46444502  1.26061973]\n",
      "Gradient Descent(5179/9): loss=17.385837241121795, w0=72.1574698354155, w1=12.809723308910044\n",
      "[-1.75112479 -1.51832509]\n",
      "Gradient Descent(5180/9): loss=16.2560923469483, w0=73.38325718973917, w1=13.872550869155646\n",
      "[ 3.95869966  0.45037116]\n",
      "Gradient Descent(5181/9): loss=15.46703927438343, w0=70.61216742549587, w1=13.557291060128046\n",
      "[-3.26698785 -0.26776609]\n",
      "Gradient Descent(5182/9): loss=18.984800894951146, w0=72.8990589215647, w1=13.74472732479613\n",
      "[-0.56192752  0.42942456]\n",
      "Gradient Descent(5183/9): loss=15.498962740926093, w0=73.29240818659143, w1=13.44413013027167\n",
      "[-0.07635493 -0.43797164]\n",
      "Gradient Descent(5184/9): loss=15.386522064852604, w0=73.34585663820172, w1=13.75071027867837\n",
      "[-0.09292147  1.73422177]\n",
      "Gradient Descent(5185/9): loss=15.42395638768477, w0=73.41090166576426, w1=12.536755042589874\n",
      "[ 0.9097825  -2.92032076]\n",
      "Gradient Descent(5186/9): loss=15.837314311624418, w0=72.77405391809265, w1=14.58097957703582\n",
      "[ 4.1158949  -0.07807879]\n",
      "Gradient Descent(5187/9): loss=16.127413940292765, w0=69.89292748993121, w1=14.635634732705471\n",
      "[-4.0415792  -1.83841523]\n",
      "Gradient Descent(5188/9): loss=21.837347883927176, w0=72.72203293108721, w1=15.922525394252649\n",
      "[-2.71250768  1.63217389]\n",
      "Gradient Descent(5189/9): loss=18.5330840005774, w0=74.62078830589903, w1=14.78000367386256\n",
      "[ 1.37663289  2.89874022]\n",
      "Gradient Descent(5190/9): loss=17.111553615846763, w0=73.6571452794981, w1=12.750885519130046\n",
      "[ 0.76562323  1.6442851 ]\n",
      "Gradient Descent(5191/9): loss=15.717447780089698, w0=73.12120902060063, w1=11.599885949734382\n",
      "[-0.81481908 -1.30261937]\n",
      "Gradient Descent(5192/9): loss=17.167676563151954, w0=73.69158237417332, w1=12.51171950779499\n",
      "[ 3.22491864 -1.1711294 ]\n",
      "Gradient Descent(5193/9): loss=15.933459908134944, w0=71.43413932738245, w1=13.33151008838191\n",
      "[-0.33250316  0.34351874]\n",
      "Gradient Descent(5194/9): loss=17.126265635198756, w0=71.6668915415602, w1=13.091046972103015\n",
      "[-1.21253432  0.42461249]\n",
      "Gradient Descent(5195/9): loss=16.785032349620217, w0=72.51566556445759, w1=12.793818232087911\n",
      "[ 0.35104114 -0.90082419]\n",
      "Gradient Descent(5196/9): loss=15.923954838986058, w0=72.26993676457828, w1=13.42439516525545\n",
      "[ 1.23694276 -1.15914976]\n",
      "Gradient Descent(5197/9): loss=15.911690752331316, w0=71.40407683276412, w1=14.235799997791958\n",
      "[-1.21490136  0.1998331 ]\n",
      "Gradient Descent(5198/9): loss=17.457479452182913, w0=72.25450778519092, w1=14.095916826936422\n",
      "[-0.14908267  2.27616492]\n",
      "Gradient Descent(5199/9): loss=16.115932752318766, w0=72.35886565532047, w1=12.502601382756888\n",
      "[ 0.90178559 -1.40609623]\n",
      "Gradient Descent(5200/9): loss=16.30042605885771, w0=71.7276157419377, w1=13.48686874306313\n",
      "[-1.69830093  4.4537031 ]\n",
      "Gradient Descent(5201/9): loss=16.612571125521967, w0=72.91642639579565, w1=10.369276570663391\n",
      "[ 1.21825927 -0.54168769]\n",
      "Gradient Descent(5202/9): loss=20.29454496826255, w0=72.06364490711054, w1=10.748457951201878\n",
      "[-2.2152056  -6.05790689]\n",
      "Gradient Descent(5203/9): loss=19.872554261667393, w0=73.61428882390715, w1=14.98899277640307\n",
      "[-0.00687722 -1.85886205]\n",
      "Gradient Descent(5204/9): loss=16.576168893574557, w0=73.61910287721557, w1=16.290196210225165\n",
      "[-1.25469161  2.67978435]\n",
      "Gradient Descent(5205/9): loss=19.388168695030902, w0=74.49738700689362, w1=14.414347166570032\n",
      "[ 2.91864268  1.00733987]\n",
      "Gradient Descent(5206/9): loss=16.546822918443336, w0=72.45433712774589, w1=13.709209255340728\n",
      "[ 0.04627771  0.20038754]\n",
      "Gradient Descent(5207/9): loss=15.764673644731628, w0=72.42194272989616, w1=13.568937976265962\n",
      "[ 0.18628275 -0.03749694]\n",
      "Gradient Descent(5208/9): loss=15.770042393018572, w0=72.291544803195, w1=13.595185832626019\n",
      "[-1.53423853 -0.27740171]\n",
      "Gradient Descent(5209/9): loss=15.894934946057836, w0=73.36551177523516, w1=13.789367031850112\n",
      "[ 0.07479622 -1.68690983]\n",
      "Gradient Descent(5210/9): loss=15.436393401316396, w0=73.3131544192559, w1=14.970203913579262\n",
      "[ 1.68090948  1.43871311]\n",
      "Gradient Descent(5211/9): loss=16.49685523563915, w0=72.13651778610223, w1=13.963104733390654\n",
      "[-1.19610861  2.14808198]\n",
      "Gradient Descent(5212/9): loss=16.172514185517105, w0=72.97379381319834, w1=12.459447350294468\n",
      "[ 1.18697857 -1.86026445]\n",
      "Gradient Descent(5213/9): loss=15.95759931901921, w0=72.1429088144181, w1=13.761632466017982\n",
      "[-1.18781191 -0.14874265]\n",
      "Gradient Descent(5214/9): loss=16.08804299989187, w0=72.9743771510445, w1=13.865752319283168\n",
      "[-0.99093923  3.43658466]\n",
      "Gradient Descent(5215/9): loss=15.51145572088201, w0=73.66803461197806, w1=11.460143058326365\n",
      "[-1.69899169  0.04740068]\n",
      "Gradient Descent(5216/9): loss=17.495198224839196, w0=74.85732879486342, w1=11.426962583139865\n",
      "[ 0.36161095 -2.52751214]\n",
      "Gradient Descent(5217/9): loss=18.714899245784117, w0=74.60420112776872, w1=13.196221080382028\n",
      "[ 2.62719416 -0.96542569]\n",
      "Gradient Descent(5218/9): loss=16.284487236472653, w0=72.76516521505134, w1=13.872019065610075\n",
      "[-1.90718694 -2.64249991]\n",
      "Gradient Descent(5219/9): loss=15.602631984971767, w0=74.10019607055261, w1=15.721769005261475\n",
      "[-0.44227819  1.71970911]\n",
      "Gradient Descent(5220/9): loss=18.224335637705657, w0=74.40979080333543, w1=14.517972629877793\n",
      "[ 3.44317356 -2.1088607 ]\n",
      "Gradient Descent(5221/9): loss=16.547461575754106, w0=71.999569311515, w1=15.994175118782248\n",
      "[-1.76874192  5.24895709]\n",
      "Gradient Descent(5222/9): loss=19.38482360674268, w0=73.23768865741074, w1=12.319905158721017\n",
      "[-2.25813376  1.30859222]\n",
      "Gradient Descent(5223/9): loss=16.060045422399295, w0=74.81838228718884, w1=11.403890602490709\n",
      "[ 1.33906198 -0.64809789]\n",
      "Gradient Descent(5224/9): loss=18.70239558936645, w0=73.88103890427371, w1=11.857559124137156\n",
      "[ 0.36221909 -1.34276916]\n",
      "Gradient Descent(5225/9): loss=16.87393167918926, w0=73.62748553799014, w1=12.797497535271482\n",
      "[ 2.21922816  2.50933181]\n",
      "Gradient Descent(5226/9): loss=15.674228769763763, w0=72.07402582891224, w1=11.040965269650627\n",
      "[-2.41778847 -3.26168266]\n",
      "Gradient Descent(5227/9): loss=19.10370507373789, w0=73.76647775618052, w1=13.324143135085379\n",
      "[ 0.60119334  2.87741652]\n",
      "Gradient Descent(5228/9): loss=15.50964324272051, w0=73.34564241923266, w1=11.309951574179047\n",
      "[-0.34492829 -2.25374073]\n",
      "Gradient Descent(5229/9): loss=17.741156466154802, w0=73.58709221999477, w1=12.887570086764422\n",
      "[-0.53798894 -1.19441218]\n",
      "Gradient Descent(5230/9): loss=15.6041785374386, w0=73.96368448146251, w1=13.723658611749647\n",
      "[ 0.77466521 -0.71553922]\n",
      "Gradient Descent(5231/9): loss=15.639933626784885, w0=73.42141883137438, w1=14.224536063481295\n",
      "[ 4.41671095 -0.12305689]\n",
      "Gradient Descent(5232/9): loss=15.671396708346427, w0=70.32972116650646, w1=14.310675886480846\n",
      "[-3.02010321 -0.17002068]\n",
      "Gradient Descent(5233/9): loss=20.124381294569087, w0=72.4437934107331, w1=14.42969036247265\n",
      "[ 0.31972757 -1.05949873]\n",
      "Gradient Descent(5234/9): loss=16.198476211116574, w0=72.21998411056465, w1=15.171339472957492\n",
      "[-0.83298565  1.26722816]\n",
      "Gradient Descent(5235/9): loss=17.39336018406561, w0=72.80307406747754, w1=14.284279763662934\n",
      "[ 1.66339206  3.48597898]\n",
      "Gradient Descent(5236/9): loss=15.830018009478232, w0=71.63869962411422, w1=11.844094477532813\n",
      "[-1.01989629 -7.15353728]\n",
      "Gradient Descent(5237/9): loss=18.093391480507083, w0=72.352627025745, w1=16.851570575720018\n",
      "[-1.5683992   3.16453904]\n",
      "Gradient Descent(5238/9): loss=21.51361964569673, w0=73.4505064647323, w1=14.636393249930041\n",
      "[-1.07233718  1.08986491]\n",
      "Gradient Descent(5239/9): loss=16.06710246962379, w0=74.20114249141446, w1=13.87348781312186\n",
      "[-1.98946389  1.54950551]\n",
      "Gradient Descent(5240/9): loss=15.874941901152493, w0=75.59376721526462, w1=12.788833952715281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.70113212 -1.12332122]\n",
      "Gradient Descent(5241/9): loss=18.26918840971003, w0=72.30297473005938, w1=13.575158807487854\n",
      "[-1.13768973  1.40058817]\n",
      "Gradient Descent(5242/9): loss=15.881431121828513, w0=73.09935754143105, w1=12.594747085700595\n",
      "[ 0.95254434 -0.34374688]\n",
      "Gradient Descent(5243/9): loss=15.796397368228726, w0=72.43257650495804, w1=12.835369899403682\n",
      "[-1.44433848 -1.86695503]\n",
      "Gradient Descent(5244/9): loss=15.964434553139526, w0=73.44361343748028, w1=14.142238421853182\n",
      "[ 0.21373245 -1.06102778]\n",
      "Gradient Descent(5245/9): loss=15.616561973376875, w0=73.294000721809, w1=14.884957871346044\n",
      "[ 3.17464332  3.3150872 ]\n",
      "Gradient Descent(5246/9): loss=16.373245240128877, w0=71.07175039693853, w1=12.564396833228887\n",
      "[-2.44844585 -2.18859452]\n",
      "Gradient Descent(5247/9): loss=18.27381251564667, w0=72.78566248968036, w1=14.096412997956712\n",
      "[ 3.81826444  3.47229593]\n",
      "Gradient Descent(5248/9): loss=15.705211526996878, w0=70.11287738004927, w1=11.665805849297822\n",
      "[-2.431351   -4.17897148]\n",
      "Gradient Descent(5249/9): loss=22.09053886339184, w0=71.81482308197629, w1=14.591085888707731\n",
      "[-1.08294071  2.0853452 ]\n",
      "Gradient Descent(5250/9): loss=17.097330153407942, w0=72.5728815823593, w1=13.13134424847194\n",
      "[-0.22025382  0.78322256]\n",
      "Gradient Descent(5251/9): loss=15.706517708971672, w0=72.72705925326892, w1=12.583088459770998\n",
      "[-2.55081572 -0.56697116]\n",
      "Gradient Descent(5252/9): loss=15.948521833306414, w0=74.5126302582208, w1=12.979968269849886\n",
      "[ 3.9212078  -2.05234281]\n",
      "Gradient Descent(5253/9): loss=16.253384890886892, w0=71.76778479913553, w1=14.416608235112086\n",
      "[-4.27219919  2.94676803]\n",
      "Gradient Descent(5254/9): loss=16.98932212011753, w0=74.75832423455401, w1=12.353870616970479\n",
      "[ 0.55680286 -0.53745913]\n",
      "Gradient Descent(5255/9): loss=17.091884717629625, w0=74.36856223577311, w1=12.730092010739499\n",
      "[ 2.00182621 -3.76284354]\n",
      "Gradient Descent(5256/9): loss=16.244279074964364, w0=72.96728389135485, w1=15.364082490013175\n",
      "[-0.85459763  4.02183026]\n",
      "Gradient Descent(5257/9): loss=17.21465934866249, w0=73.56550223328047, w1=12.548801306535632\n",
      "[ 0.96853206 -0.38797411]\n",
      "Gradient Descent(5258/9): loss=15.856063544351217, w0=72.88752979327931, w1=12.820383180862676\n",
      "[-0.28000959 -6.54281562]\n",
      "Gradient Descent(5259/9): loss=15.685822715200006, w0=73.08353650362002, w1=17.40035411614724\n",
      "[-1.92725732  3.83517494]\n",
      "Gradient Descent(5260/9): loss=23.093734493833296, w0=74.43261662467366, w1=14.715731661187842\n",
      "[-0.78982936  2.11301998]\n",
      "Gradient Descent(5261/9): loss=16.798072354329104, w0=74.98549717624694, w1=13.236617672758113\n",
      "[ 1.13314395  1.06112966]\n",
      "Gradient Descent(5262/9): loss=16.8461486854278, w0=74.19229640857647, w1=12.493826913181403\n",
      "[-0.28768009  1.34375083]\n",
      "Gradient Descent(5263/9): loss=16.27541128698568, w0=74.39367247338699, w1=11.553201329847461\n",
      "[ 3.67362819 -1.94101516]\n",
      "Gradient Descent(5264/9): loss=17.8463359374886, w0=71.82213274212288, w1=12.911911941822494\n",
      "[-1.49474294 -0.61218021]\n",
      "Gradient Descent(5265/9): loss=16.630168381749137, w0=72.86845280181954, w1=13.340438090393329\n",
      "[-1.38515869  0.82813463]\n",
      "Gradient Descent(5266/9): loss=15.486098560556536, w0=73.83806388713896, w1=12.760743848907092\n",
      "[-1.1402803  0.3399838]\n",
      "Gradient Descent(5267/9): loss=15.792390978239796, w0=74.63626009916166, w1=12.522755188964739\n",
      "[-0.43055634 -1.27980994]\n",
      "Gradient Descent(5268/9): loss=16.74470723759321, w0=74.93764953582887, w1=13.418622150296482\n",
      "[ 0.38076866 -0.50042896]\n",
      "Gradient Descent(5269/9): loss=16.73867398283197, w0=74.67111147342048, w1=13.768922425696676\n",
      "[-1.46266685  2.03955367]\n",
      "Gradient Descent(5270/9): loss=16.3760344981428, w0=75.69497827118836, w1=12.341234854294845\n",
      "[ 2.52290967 -0.03215737]\n",
      "Gradient Descent(5271/9): loss=18.916489073352867, w0=73.92894150483865, w1=12.363745016079633\n",
      "[ 2.96680185 -1.67691373]\n",
      "Gradient Descent(5272/9): loss=16.210204393288993, w0=71.8521802093873, w1=13.5375846244564\n",
      "[ 1.94762747  1.80156042]\n",
      "Gradient Descent(5273/9): loss=16.42687216242097, w0=70.48884098319367, w1=12.276492332418478\n",
      "[-0.01813717 -0.68472053]\n",
      "Gradient Descent(5274/9): loss=20.043996937773215, w0=70.50153700536993, w1=12.75579670316563\n",
      "[-0.94397306 -1.25264489]\n",
      "Gradient Descent(5275/9): loss=19.546621847216215, w0=71.16231814475472, w1=13.632648124077637\n",
      "[-1.54969214 -1.41620801]\n",
      "Gradient Descent(5276/9): loss=17.6694500336636, w0=72.24710263948937, w1=14.623993730111772\n",
      "[-2.61355382 -0.61028744]\n",
      "Gradient Descent(5277/9): loss=16.588493098986962, w0=74.07659031390399, w1=15.05119493637196\n",
      "[-0.82037194  2.0145626 ]\n",
      "Gradient Descent(5278/9): loss=16.926951338052742, w0=74.65085067209286, w1=13.641001113808462\n",
      "[-1.05529366 -1.6903457 ]\n",
      "Gradient Descent(5279/9): loss=16.319522595504306, w0=75.3895562355893, w1=14.824243105604205\n",
      "[-0.0375917   1.69173528]\n",
      "Gradient Descent(5280/9): loss=18.48561065121708, w0=75.4158704230306, w1=13.640028408393755\n",
      "[ 2.73170537 -1.15490966]\n",
      "Gradient Descent(5281/9): loss=17.650071025027664, w0=73.5036766653518, w1=14.44846516738418\n",
      "[ 1.10557697 -4.98464409]\n",
      "Gradient Descent(5282/9): loss=15.877127306467766, w0=72.72977278688045, w1=17.937716033657626\n",
      "[-0.34844096  5.09690724]\n",
      "Gradient Descent(5283/9): loss=25.481918080219746, w0=72.97368145767382, w1=14.369880962631546\n",
      "[-1.07284822  2.35700155]\n",
      "Gradient Descent(5284/9): loss=15.833364875780855, w0=73.72467521184724, w1=12.719979876158604\n",
      "[-1.18453468  1.10854635]\n",
      "Gradient Descent(5285/9): loss=15.767258813154468, w0=74.55384948757374, w1=11.943997432833148\n",
      "[ 2.57273633 -1.38357145]\n",
      "Gradient Descent(5286/9): loss=17.358806787072304, w0=72.75293405331664, w1=12.912497447113932\n",
      "[-0.81272378 -1.30785711]\n",
      "Gradient Descent(5287/9): loss=15.693088270431705, w0=73.32184069769603, w1=13.827997426861243\n",
      "[-2.18308099  1.47459753]\n",
      "Gradient Descent(5288/9): loss=15.446928813392855, w0=74.8499973923982, w1=12.795779155395534\n",
      "[ 3.3868803  -2.85095553]\n",
      "Gradient Descent(5289/9): loss=16.830455544434937, w0=72.47918118527946, w1=14.791448025703183\n",
      "[-1.15438979  4.24130871]\n",
      "Gradient Descent(5290/9): loss=16.578114298103408, w0=73.28725403618239, w1=11.822531929773168\n",
      "[ 2.72128978 -0.30261201]\n",
      "Gradient Descent(5291/9): loss=16.759033713147954, w0=71.38235119309546, w1=12.034360340240038\n",
      "[-2.41064621 -2.06835239]\n",
      "Gradient Descent(5292/9): loss=18.25746068665613, w0=73.0698035422644, w1=13.482207011390145\n",
      "[-0.89959747 -1.51442521]\n",
      "Gradient Descent(5293/9): loss=15.411005522305816, w0=73.69952176793407, w1=14.5423046607036\n",
      "[ 0.12508801  1.69205908]\n",
      "Gradient Descent(5294/9): loss=16.032694572924125, w0=73.61196016214642, w1=13.357863306501839\n",
      "[-1.91603896  0.73396801]\n",
      "Gradient Descent(5295/9): loss=15.44388560950715, w0=74.9531874365814, w1=12.844085699415547\n",
      "[ 2.18414265 -0.76855964]\n",
      "Gradient Descent(5296/9): loss=16.96447943334108, w0=73.4242875807698, w1=13.382077448186923\n",
      "[ 0.55413421 -0.47873293]\n",
      "Gradient Descent(5297/9): loss=15.399151756203606, w0=73.03639363686565, w1=13.717190497414538\n",
      "[-1.39002284  4.37139337]\n",
      "Gradient Descent(5298/9): loss=15.447246213347558, w0=74.00940962807478, w1=10.657215137744782\n",
      "[-3.57745873 -2.97214502]\n",
      "Gradient Descent(5299/9): loss=19.6250946367628, w0=76.51363073714906, w1=12.737716654368674\n",
      "[-0.47403043  0.59387796]\n",
      "Gradient Descent(5300/9): loss=20.844428907317518, w0=76.84545203845958, w1=12.322002084188984\n",
      "[ 0.81072022 -1.31800818]\n",
      "Gradient Descent(5301/9): loss=22.362717296567933, w0=76.27794788704438, w1=13.244607809001165\n",
      "[ 3.64977498 -1.13298111]\n",
      "Gradient Descent(5302/9): loss=19.865730202403405, w0=73.72310539828506, w1=14.03769458457952\n",
      "[-2.43014723  2.74296253]\n",
      "Gradient Descent(5303/9): loss=15.633659102238441, w0=75.4242084621223, w1=12.117620811233419\n",
      "[ 0.07612412 -4.41165975]\n",
      "Gradient Descent(5304/9): loss=18.582594865448133, w0=75.37092157941044, w1=15.205782634930294\n",
      "[ 4.4310037   4.09365621]\n",
      "Gradient Descent(5305/9): loss=19.032510658455088, w0=72.2692189870242, w1=12.340223289600452\n",
      "[-0.82136798 -2.72131199]\n",
      "Gradient Descent(5306/9): loss=16.560113759616648, w0=72.84417657170883, w1=14.245141684555012\n",
      "[-1.18764777  0.09498566]\n",
      "Gradient Descent(5307/9): loss=15.779964312956162, w0=73.67553000997633, w1=14.17865172494804\n",
      "[-2.27039512 -2.37484716]\n",
      "Gradient Descent(5308/9): loss=15.702958270189276, w0=75.2648065968894, w1=15.841044740176683\n",
      "[-0.18821017  4.92514759]\n",
      "Gradient Descent(5309/9): loss=20.116026039569626, w0=75.39655371433088, w1=12.393441430624813\n",
      "[ 0.70025566 -0.71491374]\n",
      "Gradient Descent(5310/9): loss=18.18641027491921, w0=74.90637475147945, w1=12.893881046277281\n",
      "[ 0.80425671  0.65050421]\n",
      "Gradient Descent(5311/9): loss=16.85748901131168, w0=74.34339505356873, w1=12.438528098286428\n",
      "[ 3.01570528  0.68014751]\n",
      "Gradient Descent(5312/9): loss=16.478617123200937, w0=72.23240135765941, w1=11.962424840124642\n",
      "[-2.16435713 -2.29648719]\n",
      "Gradient Descent(5313/9): loss=17.100381730886394, w0=73.74745134554972, w1=13.56996587115025\n",
      "[ 0.77991591 -3.3337832 ]\n",
      "Gradient Descent(5314/9): loss=15.492805142881465, w0=73.20151020978791, w1=15.903614114472369\n",
      "[ 4.01905577  4.05979262]\n",
      "Gradient Descent(5315/9): loss=18.327807514410075, w0=70.38817116900172, w1=13.061759279333979\n",
      "[-5.78937497 -0.05689967]\n",
      "Gradient Descent(5316/9): loss=19.694924241031156, w0=74.44073364994671, w1=13.101589046097214\n",
      "[ 2.10933555  2.45356237]\n",
      "Gradient Descent(5317/9): loss=16.114964995255413, w0=72.96419876769399, w1=11.384095386870904\n",
      "[-3.00866669 -2.36958187]\n",
      "Gradient Descent(5318/9): loss=17.636051980666398, w0=75.07026545339482, w1=13.042802692994492\n",
      "[ 1.55023542 -2.29434281]\n",
      "Gradient Descent(5319/9): loss=17.059030958624064, w0=73.98510065851421, w1=14.648842656885208\n",
      "[ 1.9186847   2.65519538]\n",
      "Gradient Descent(5320/9): loss=16.30818457424257, w0=72.64202136891397, w1=12.790205891735631\n",
      "[-1.85019937  1.4433947 ]\n",
      "Gradient Descent(5321/9): loss=15.836084723201592, w0=73.93716092631257, w1=11.779829600704009\n",
      "[ 0.47178635  0.03007978]\n",
      "Gradient Descent(5322/9): loss=17.03756685078561, w0=73.60691048209719, w1=11.758773752022782\n",
      "[ 1.64623379 -1.01248829]\n",
      "Gradient Descent(5323/9): loss=16.915683738398087, w0=72.45454682986644, w1=12.467515553861439\n",
      "[ 1.59676267 -2.60380693]\n",
      "Gradient Descent(5324/9): loss=16.250434471797043, w0=71.33681296033731, w1=14.29018040614264\n",
      "[-1.59187997  1.61777677]\n",
      "Gradient Descent(5325/9): loss=17.629454935647104, w0=72.45112893862134, w1=13.157736668936957\n",
      "[-5.68951951  0.82246505]\n",
      "Gradient Descent(5326/9): loss=15.792872139720062, w0=76.43379259325704, w1=12.5820111304591\n",
      "[ 2.3957768   1.96751135]\n",
      "Gradient Descent(5327/9): loss=20.718215349496923, w0=74.75674883621745, w1=11.204753183263323\n",
      "[-1.06485594 -1.78575488]\n",
      "Gradient Descent(5328/9): loss=19.043538840635097, w0=75.50214799369478, w1=12.454781601339635\n",
      "[ 3.36643576 -1.23136312]\n",
      "Gradient Descent(5329/9): loss=18.349260490677953, w0=73.14564296462424, w1=13.316735788285\n",
      "[ 0.03488545 -1.19677395]\n",
      "Gradient Descent(5330/9): loss=15.410161898992985, w0=73.12122314652001, w1=14.154477556714253\n",
      "[-3.91026088  1.58269409]\n",
      "Gradient Descent(5331/9): loss=15.628454300938028, w0=75.85840576199845, w1=13.046591692543533\n",
      "[ 0.75759569 -0.21424921]\n",
      "Gradient Descent(5332/9): loss=18.76797313497583, w0=75.32808877561665, w1=13.1965661386509\n",
      "[ 3.25584667 -4.22797959]\n",
      "Gradient Descent(5333/9): loss=17.49489101262342, w0=73.0489961079437, w1=16.156151848520228\n",
      "[-1.58534242  1.1451744 ]\n",
      "Gradient Descent(5334/9): loss=18.997546182796366, w0=74.15873580055019, w1=15.354529765171263\n",
      "[-0.74661506  2.69088023]\n",
      "Gradient Descent(5335/9): loss=17.517309332595616, w0=74.68136634355774, w1=13.470913606795317\n",
      "[ 0.41734647 -1.58901215]\n",
      "Gradient Descent(5336/9): loss=16.34842747883254, w0=74.38922381690125, w1=14.583222112899707\n",
      "[ 2.80311942  1.28266994]\n",
      "Gradient Descent(5337/9): loss=16.594597706198414, w0=72.42704022284714, w1=13.685353156236916\n",
      "[ 0.01999551  2.89776691]\n",
      "Gradient Descent(5338/9): loss=15.782773931551876, w0=72.41304336911521, w1=11.656916316355773\n",
      "[-1.68084132 -1.23827702]\n",
      "Gradient Descent(5339/9): loss=17.43515429691081, w0=73.5896322903498, w1=12.523710233129224\n",
      "[ 4.14050571  0.44249348]\n",
      "Gradient Descent(5340/9): loss=15.886580261096672, w0=70.69127829521808, w1=12.213964797187433\n",
      "[-0.11372238  0.64194413]\n",
      "Gradient Descent(5341/9): loss=19.57382354262911, w0=70.77088395810243, w1=11.764603907365572\n",
      "[-3.95571372 -2.12515849]\n",
      "Gradient Descent(5342/9): loss=20.039546985335416, w0=73.53988355869275, w1=13.252214847506863\n",
      "[ 0.63803948 -1.73825891]\n",
      "Gradient Descent(5343/9): loss=15.442013988643994, w0=73.09325592553174, w1=14.468996085392988\n",
      "[ 2.39005589  1.06051825]\n",
      "Gradient Descent(5344/9): loss=15.895362376451365, w0=71.4202168052632, w1=13.726633311082228\n",
      "[ 2.0729203  -1.17762308]\n",
      "Gradient Descent(5345/9): loss=17.17175841069106, w0=69.96917259873902, w1=14.550969468609164\n",
      "[-3.44467328 -1.51457836]\n",
      "Gradient Descent(5346/9): loss=21.48666298246154, w0=72.38044389248499, w1=15.611174319882306\n",
      "[-0.2415568   2.52746805]\n",
      "Gradient Descent(5347/9): loss=18.074673880583408, w0=72.5495336492451, w1=13.841946681992336\n",
      "[ 0.48784449 -0.86969032]\n",
      "Gradient Descent(5348/9): loss=15.728551703617294, w0=72.20804250357226, w1=14.450729907827085\n",
      "[-0.944266    0.44731831]\n",
      "Gradient Descent(5349/9): loss=16.446892477774853, w0=72.86902870022803, w1=14.137607088873954\n",
      "[-0.96784754  3.35869819]\n",
      "Gradient Descent(5350/9): loss=15.692567715624609, w0=73.5465219773418, w1=11.786518356056714\n",
      "[ 0.4109459  -4.32195979]\n",
      "Gradient Descent(5351/9): loss=16.851244337039926, w0=73.25885984879936, w1=14.811890209038314\n",
      "[-4.88027796  3.0070467 ]\n",
      "Gradient Descent(5352/9): loss=16.273851356962048, w0=76.67505441742748, w1=12.706957517967373\n",
      "[ 3.50938077  1.28906575]\n",
      "Gradient Descent(5353/9): loss=21.40049115469155, w0=74.21848787826498, w1=11.804611490453595\n",
      "[ 3.83942463 -3.1180178 ]\n",
      "Gradient Descent(5354/9): loss=17.216280485700747, w0=71.5308906356913, w1=13.987223952991243\n",
      "[-4.09963859  1.06045162]\n",
      "Gradient Descent(5355/9): loss=17.06881163876145, w0=74.40063764820006, w1=13.244907816811008\n",
      "[ 2.01727927 -1.64573098]\n",
      "Gradient Descent(5356/9): loss=16.02586423384386, w0=72.98854215953523, w1=14.396919500157885\n",
      "[ 4.41766065 -0.71721473]\n",
      "Gradient Descent(5357/9): loss=15.853150693151244, w0=69.8961797047864, w1=14.898969810847479\n",
      "[-5.23637703 -1.01546405]\n",
      "Gradient Descent(5358/9): loss=22.16535997778825, w0=73.56164362760545, w1=15.609794647787899\n",
      "[-2.26255132  0.27966782]\n",
      "Gradient Descent(5359/9): loss=17.690350419850674, w0=75.1454295517939, w1=15.4140271746774\n",
      "[ 1.542211    3.65118009]\n",
      "Gradient Descent(5360/9): loss=18.97071472819435, w0=74.06588184997554, w1=12.858201111726213\n",
      "[ 1.77501956 -0.72640493]\n",
      "Gradient Descent(5361/9): loss=15.876987034663365, w0=72.82336815458777, w1=13.36668456454537\n",
      "[-2.41548057  2.62969323]\n",
      "Gradient Descent(5362/9): loss=15.502985980284638, w0=74.51420455695404, w1=11.525899301532824\n",
      "[ 3.00399906 -1.54951729]\n",
      "Gradient Descent(5363/9): loss=18.03912550589664, w0=72.4114052157853, w1=12.610561401547685\n",
      "[-1.71102071 -0.62829326]\n",
      "Gradient Descent(5364/9): loss=16.153017567363694, w0=73.60911971191152, w1=13.050366686786328\n",
      "[-0.12986058 -2.68599298]\n",
      "Gradient Descent(5365/9): loss=15.527731552712853, w0=73.70002212109475, w1=14.930561774255754\n",
      "[-1.96867665  3.09375972]\n",
      "Gradient Descent(5366/9): loss=16.520828424776386, w0=75.07809577362205, w1=12.764929967342699\n",
      "[ 4.01225279 -2.72954959]\n",
      "Gradient Descent(5367/9): loss=17.23298288034106, w0=72.26951882090606, w1=14.675614680125015\n",
      "[-3.33236691  0.14609258]\n",
      "Gradient Descent(5368/9): loss=16.625679897615473, w0=74.60217565465048, w1=14.573349876110713\n",
      "[-2.47517612 -1.75414425]\n",
      "Gradient Descent(5369/9): loss=16.839673104840017, w0=76.3347989373517, w1=15.80125084998789\n",
      "[ 1.78626842 -0.75966673]\n",
      "Gradient Descent(5370/9): loss=22.704124442644183, w0=75.08441104652341, w1=16.333017562985237\n",
      "[ 0.77786631  1.74439429]\n",
      "Gradient Descent(5371/9): loss=21.05948845464491, w0=74.53990463153477, w1=15.111941562627807\n",
      "[ 2.6211751   1.00524975]\n",
      "Gradient Descent(5372/9): loss=17.494210187805713, w0=72.70508206244887, w1=14.408266738382332\n",
      "[-2.05442672  0.22078873]\n",
      "Gradient Descent(5373/9): loss=15.990360653271724, w0=74.14318076765203, w1=14.253714626371108\n",
      "[ 1.04471275  0.7804475 ]\n",
      "Gradient Descent(5374/9): loss=16.046047790390585, w0=73.41188184298878, w1=13.707401375430692\n",
      "[ 1.91504788  1.95181433]\n",
      "Gradient Descent(5375/9): loss=15.41876625765976, w0=72.07134832440833, w1=12.34113134552938\n",
      "[-1.59043412 -3.72584191]\n",
      "Gradient Descent(5376/9): loss=16.781414516165587, w0=73.1846522091001, w1=14.949220683992461\n",
      "[-3.41775887  3.03463375]\n",
      "Gradient Descent(5377/9): loss=16.471585059605626, w0=75.57708342044565, w1=12.82497706213545\n",
      "[ 3.64591408  0.91977636]\n",
      "Gradient Descent(5378/9): loss=18.206640104161476, w0=73.02494356358483, w1=12.181133607210281\n",
      "[ 1.47576942 -0.90898248]\n",
      "Gradient Descent(5379/9): loss=16.265216055001464, w0=71.99190496757268, w1=12.817421342388267\n",
      "[ 0.26379808 -1.50270424]\n",
      "Gradient Descent(5380/9): loss=16.45282679360498, w0=71.8072463085, w1=13.869314307527652\n",
      "[-0.13580379 -2.37979221]\n",
      "Gradient Descent(5381/9): loss=16.566884987350434, w0=71.90230896219137, w1=15.535168852052337\n",
      "[-2.6006451   1.33569096]\n",
      "Gradient Descent(5382/9): loss=18.466631836481824, w0=73.72276053042668, w1=14.60018518078253\n",
      "[ 2.09120019  1.37807337]\n",
      "Gradient Descent(5383/9): loss=16.10556869754887, w0=72.25892039670188, w1=13.635533820673713\n",
      "[-0.50048715  1.34909042]\n",
      "Gradient Descent(5384/9): loss=15.933642182541458, w0=72.60926140303702, w1=12.691170527117945\n",
      "[-0.76620961 -2.39028895]\n",
      "Gradient Descent(5385/9): loss=15.931167107022087, w0=73.14560812734321, w1=14.364372791936898\n",
      "[ 0.79188653 -0.43507743]\n",
      "Gradient Descent(5386/9): loss=15.788198345130402, w0=72.59128755844971, w1=14.668926990795194\n",
      "[-1.43883244  1.92789183]\n",
      "Gradient Descent(5387/9): loss=16.33985107940553, w0=73.59847026399493, w1=13.319402708475957\n",
      "[ 0.33823349 -2.45696169]\n",
      "Gradient Descent(5388/9): loss=15.445112294946783, w0=73.36170681802554, w1=15.039275891250357\n",
      "[-0.08105979  1.2398804 ]\n",
      "Gradient Descent(5389/9): loss=16.604304346516937, w0=73.41844866792266, w1=14.171359611234998\n",
      "[-2.35060169  1.19368751]\n",
      "Gradient Descent(5390/9): loss=15.632829222283707, w0=75.06386984983824, w1=13.335778355605562\n",
      "[ 1.57006558 -0.84686997]\n",
      "Gradient Descent(5391/9): loss=16.96260407028082, w0=73.9648239447712, w1=13.928587337203885\n",
      "[-0.18458147  1.36568486]\n",
      "Gradient Descent(5392/9): loss=15.7116869160851, w0=74.09403097598526, w1=12.972607936136342\n",
      "[ 1.94316467  1.26973714]\n",
      "Gradient Descent(5393/9): loss=15.834552540249442, w0=72.73381570619495, w1=12.083791936348158\n",
      "[-1.82496023 -0.92160236]\n",
      "Gradient Descent(5394/9): loss=16.517044419451462, w0=74.01128786825619, w1=12.728913590351127\n",
      "[ 0.71859911 -1.6210097 ]\n",
      "Gradient Descent(5395/9): loss=15.925044214343506, w0=73.5082684903853, w1=13.863620383054812\n",
      "[ 3.52114232  0.63450184]\n",
      "Gradient Descent(5396/9): loss=15.482552733642443, w0=71.04346886886131, w1=13.41946909316915\n",
      "[-3.45969496 -1.11524534]\n",
      "Gradient Descent(5397/9): loss=17.91997215140981, w0=73.4652553405553, w1=14.200140834314395\n",
      "[-0.30958142  2.05811644]\n",
      "Gradient Descent(5398/9): loss=15.66007396453887, w0=73.6819623312001, w1=12.759459328988772\n",
      "[-2.17440787  1.44979251]\n",
      "Gradient Descent(5399/9): loss=15.720557785682969, w0=75.20404784247415, w1=11.74460456884709\n",
      "[ 2.74289583 -1.80124654]\n",
      "Gradient Descent(5400/9): loss=18.715477885425877, w0=73.28402076259492, w1=13.005477147644617\n",
      "[-3.14771242 -0.20837312]\n",
      "Gradient Descent(5401/9): loss=15.498386439982648, w0=75.48741945441134, w1=13.151338332351438\n",
      "[ 1.25800913  1.47700295]\n",
      "Gradient Descent(5402/9): loss=17.84551818110771, w0=74.60681306420683, w1=12.1174362649966\n",
      "[ 2.07218568 -2.3921744 ]\n",
      "Gradient Descent(5403/9): loss=17.17562752096724, w0=73.15628308610135, w1=13.791958343165811\n",
      "[-0.14099871  1.54221665]\n",
      "Gradient Descent(5404/9): loss=15.444108858015321, w0=73.25498218401576, w1=12.712406691116099\n",
      "[ 0.7016406   0.77380814]\n",
      "Gradient Descent(5405/9): loss=15.681025075836029, w0=72.76383376192284, w1=12.170740989861011\n",
      "[-0.55124015 -1.35546968]\n",
      "Gradient Descent(5406/9): loss=16.3830877620995, w0=73.14970186892403, w1=13.119569767368523\n",
      "[-0.60130046  0.99641217]\n",
      "Gradient Descent(5407/9): loss=15.46113896275721, w0=73.57061219441152, w1=12.422081249754603\n",
      "[-1.49988958 -0.91799584]\n",
      "Gradient Descent(5408/9): loss=15.98345846207887, w0=74.62053489910123, w1=13.064678339666063\n",
      "[-0.16162225 -1.48829749]\n",
      "Gradient Descent(5409/9): loss=16.3519654082078, w0=74.7336704708953, w1=14.106486583453842\n",
      "[ 3.67233349  2.97696567]\n",
      "Gradient Descent(5410/9): loss=16.618748612113023, w0=72.16303702795643, w1=12.022610614102954\n",
      "[-1.31361939 -2.20842709]\n",
      "Gradient Descent(5411/9): loss=17.0869111394219, w0=73.08257059960243, w1=13.568509575764235\n",
      "[ 0.13542887 -0.36913498]\n",
      "Gradient Descent(5412/9): loss=15.412165042604265, w0=72.98777038751427, w1=13.826904060688534\n",
      "[ 0.932746   -1.57128971]\n",
      "Gradient Descent(5413/9): loss=15.493023286865638, w0=72.33484818499953, w1=14.926806855982043\n",
      "[-1.99022272  5.15827559]\n",
      "Gradient Descent(5414/9): loss=16.89284029379274, w0=73.72800409086004, w1=11.316013945652468\n",
      "[ 1.32608596 -0.62687431]\n",
      "Gradient Descent(5415/9): loss=17.82089707509699, w0=72.79974391896721, w1=11.754825961620549\n",
      "[-0.74231633 -2.87030789]\n",
      "Gradient Descent(5416/9): loss=16.995610530761176, w0=73.31936534773406, w1=13.764041482316147\n",
      "[ 0.82630427  0.3898923 ]\n",
      "Gradient Descent(5417/9): loss=15.426633054324764, w0=72.74095236219901, w1=13.49111687031593\n",
      "[-0.80775303  1.78413243]\n",
      "Gradient Descent(5418/9): loss=15.538840610730947, w0=73.30637948599097, w1=12.242224167590207\n",
      "[-0.17119545 -1.81256575]\n",
      "Gradient Descent(5419/9): loss=16.151654069256676, w0=73.42621629948422, w1=13.511020189742437\n",
      "[-0.85817838  0.41670991]\n",
      "Gradient Descent(5420/9): loss=15.395128847142757, w0=74.0269411672611, w1=13.219323249570692\n",
      "[ 0.61850591  2.9144405 ]\n",
      "Gradient Descent(5421/9): loss=15.68844768101375, w0=73.5939870298214, w1=11.179214899278595\n",
      "[ 1.42528978 -0.51963142]\n",
      "Gradient Descent(5422/9): loss=18.07705183516351, w0=72.5962841840156, w1=11.54295689268617\n",
      "[ 0.72692365  0.21823697]\n",
      "Gradient Descent(5423/9): loss=17.504748146764257, w0=72.08743763243928, w1=11.39019101442221\n",
      "[-3.37453519 -1.18649095]\n",
      "Gradient Descent(5424/9): loss=18.296740019457303, w0=74.44961226846542, w1=12.2207346802173\n",
      "[ 1.81188097 -4.07128329]\n",
      "Gradient Descent(5425/9): loss=16.846210358214343, w0=73.1812955927477, w1=15.070632983773017\n",
      "[-1.23267899  1.91999707]\n",
      "Gradient Descent(5426/9): loss=16.657744319143323, w0=74.04417088283233, w1=13.726635034959402\n",
      "[-0.07629574  0.7405698 ]\n",
      "Gradient Descent(5427/9): loss=15.697809945533622, w0=74.09757789971559, w1=13.208236176662986\n",
      "[ 3.52766582  2.85914008]\n",
      "Gradient Descent(5428/9): loss=15.745668949128751, w0=71.62821182656981, w1=11.206838118756448\n",
      "[-1.15013049 -3.31600935]\n",
      "Gradient Descent(5429/9): loss=19.356161891965353, w0=72.43330317245923, w1=13.52804466398698\n",
      "[-0.96709879  2.18990023]\n",
      "Gradient Descent(5430/9): loss=15.757388255979942, w0=73.11027232335046, w1=11.995114504179163\n",
      "[-0.74384414 -5.14086885]\n",
      "Gradient Descent(5431/9): loss=16.504766979165254, w0=73.63096322033941, w1=15.593722699541946\n",
      "[ 0.68007574  4.09574887]\n",
      "Gradient Descent(5432/9): loss=17.67720595954131, w0=73.15491019994137, w1=12.726698492786674\n",
      "[ 0.01034192  0.12506108]\n",
      "Gradient Descent(5433/9): loss=15.679065007975394, w0=73.14767085485467, w1=12.63915573934948\n",
      "[ 3.59910119 -0.840046  ]\n",
      "Gradient Descent(5434/9): loss=15.749850347157702, w0=70.62830002355781, w1=13.227187938385969\n",
      "[-6.06137248 -1.73187846]\n",
      "Gradient Descent(5435/9): loss=18.97054244577913, w0=74.8712607560721, w1=14.4395028621364\n",
      "[ 0.87243164  0.0614932 ]\n",
      "Gradient Descent(5436/9): loss=17.090485473234196, w0=74.26055860673631, w1=14.396457623729576\n",
      "[ 1.43702453  1.70119674]\n",
      "Gradient Descent(5437/9): loss=16.273291902075243, w0=73.25464143238949, w1=13.205619907369913\n",
      "[ 3.27142291 -0.78223585]\n",
      "Gradient Descent(5438/9): loss=15.42422270725632, w0=70.96464539683764, w1=13.753184999368884\n",
      "[-4.69843085 -2.89266992]\n",
      "Gradient Descent(5439/9): loss=18.136046242487, w0=74.25354699084613, w1=15.778053941897905\n",
      "[ 2.57393861  3.54792462]\n",
      "Gradient Descent(5440/9): loss=18.48751476952746, w0=72.4517899626011, w1=13.29450670857384\n",
      "[ 0.73035358  0.12937745]\n",
      "Gradient Descent(5441/9): loss=15.75763163535755, w0=71.94054246000748, w1=13.203942490947627\n",
      "[-1.37714306  0.67159917]\n",
      "Gradient Descent(5442/9): loss=16.339730492332006, w0=72.90454260008832, w1=12.733823069782146\n",
      "[-1.16516655 -2.65871366]\n",
      "Gradient Descent(5443/9): loss=15.739871500751287, w0=73.72015918573945, w1=14.59492263110201\n",
      "[-0.9186113  -0.04856486]\n",
      "Gradient Descent(5444/9): loss=16.098573827942786, w0=74.36318709847235, w1=14.628918030659337\n",
      "[ 1.83591914  1.06891228]\n",
      "Gradient Descent(5445/9): loss=16.61788854254389, w0=73.07804370260688, w1=13.88067943188171\n",
      "[ 1.57540359 -0.51130123]\n",
      "Gradient Descent(5446/9): loss=15.489576855225101, w0=71.9752611867037, w1=14.238590294164364\n",
      "[-0.79350865 -2.29116298]\n",
      "Gradient Descent(5447/9): loss=16.543268844440313, w0=72.53071723903842, w1=15.842404379302044\n",
      "[ 0.89457155  0.822483  ]\n",
      "Gradient Descent(5448/9): loss=18.468285235874063, w0=71.90451715441415, w1=15.266666279558335\n",
      "[-0.17033898  0.53163202]\n",
      "Gradient Descent(5449/9): loss=17.94771280553356, w0=72.02375444382584, w1=14.894523867503192\n",
      "[-3.60580457 -0.02308293]\n",
      "Gradient Descent(5450/9): loss=17.193396376668428, w0=74.54781764425265, w1=14.910681920211138\n",
      "[-0.51979351  4.1139609 ]\n",
      "Gradient Descent(5451/9): loss=17.195851843345974, w0=74.91167310432624, w1=12.03090929207239\n",
      "[ 3.90112455 -1.02770228]\n",
      "Gradient Descent(5452/9): loss=17.743962456660707, w0=72.18088591911122, w1=12.75030089049778\n",
      "[ 0.52766317 -0.95292481]\n",
      "Gradient Descent(5453/9): loss=16.271333130471252, w0=71.81152170003529, w1=13.417348256601013\n",
      "[-1.04245518  1.67460639]\n",
      "Gradient Descent(5454/9): loss=16.48658784199088, w0=72.54124032357537, w1=12.245123785745966\n",
      "[-0.23560961 -0.45552751]\n",
      "Gradient Descent(5455/9): loss=16.431257289846567, w0=72.70616705182448, w1=12.563993039858182\n",
      "[ 2.33975315 -1.14181873]\n",
      "Gradient Descent(5456/9): loss=15.977886814928555, w0=71.06833984727825, w1=13.36326614886894\n",
      "[-0.88048876 -2.08485221]\n",
      "Gradient Descent(5457/9): loss=17.869275701547043, w0=71.6846819770544, w1=14.822662695689884\n",
      "[-0.75755461  6.05862921]\n",
      "Gradient Descent(5458/9): loss=17.58247229930036, w0=72.21497020632935, w1=10.581622246738295\n",
      "[ 0.9159746  -1.97356187]\n",
      "Gradient Descent(5459/9): loss=20.16741972725099, w0=71.57378798812061, w1=11.96311555622579\n",
      "[-1.99997578 -4.79340679]\n",
      "Gradient Descent(5460/9): loss=18.015351428199974, w0=72.9737710358617, w1=15.31850030894913\n",
      "[-1.82567448  0.87804674]\n",
      "Gradient Descent(5461/9): loss=17.127706612134038, w0=74.25174317511285, w1=14.703867593796689\n",
      "[-0.13753626  2.24125678]\n",
      "Gradient Descent(5462/9): loss=16.593876494977962, w0=74.34801855864731, w1=13.134987847950397\n",
      "[ 2.8274507  -0.44470622]\n",
      "Gradient Descent(5463/9): loss=16.000865164540865, w0=72.36880307154254, w1=13.446282204647417\n",
      "[-4.21732025 -0.61969836]\n",
      "Gradient Descent(5464/9): loss=15.814369176822431, w0=75.32092724472395, w1=13.880071056675245\n",
      "[ 1.18384574 -2.94581202]\n",
      "Gradient Descent(5465/9): loss=17.5204065086106, w0=74.492235224219, w1=15.942139473440463\n",
      "[ 1.93227243  5.91468069]\n",
      "Gradient Descent(5466/9): loss=19.135638617824092, w0=73.13964452185559, w1=11.80186299105072\n",
      "[-0.32039203  1.13833669]\n",
      "Gradient Descent(5467/9): loss=16.805378017547564, w0=73.36391894523074, w1=11.00502730990456\n",
      "[-0.78814946 -4.03528231]\n",
      "Gradient Descent(5468/9): loss=18.450370889010077, w0=73.91562356572496, w1=13.829724923981502\n",
      "[ 1.92771616 -1.41515044]\n",
      "Gradient Descent(5469/9): loss=15.640398657158379, w0=72.56622225698725, w1=14.820330231137254\n",
      "[-0.96229565  2.09734022]\n",
      "Gradient Descent(5470/9): loss=16.549289366026397, w0=73.23982921030765, w1=13.35219207694281\n",
      "[ 2.47613533 -0.91184939]\n",
      "Gradient Descent(5471/9): loss=15.395481604749747, w0=71.50653448276495, w1=13.99048665237408\n",
      "[ 0.07090092  4.92756966]\n",
      "Gradient Descent(5472/9): loss=17.113710091548676, w0=71.45690384167789, w1=10.541187888619657\n",
      "[-2.4755801  -3.63215522]\n",
      "Gradient Descent(5473/9): loss=21.390668984506974, w0=73.1898099085712, w1=13.083696539550093\n",
      "[ 1.35075492 -0.0363544 ]\n",
      "Gradient Descent(5474/9): loss=15.469721827559574, w0=72.24428146169916, w1=13.109144620639123\n",
      "[ 0.23217653 -1.40812272]\n",
      "Gradient Descent(5475/9): loss=16.005420753377372, w0=72.08175788795401, w1=14.094830521214165\n",
      "[ 1.70571663  1.23823946]\n",
      "Gradient Descent(5476/9): loss=16.309743918647982, w0=70.88775624780062, w1=13.228062896169007\n",
      "[-4.00138   -0.4073002]\n",
      "Gradient Descent(5477/9): loss=18.312368432617596, w0=73.6887222491203, w1=13.513173036054312\n",
      "[-3.80446402 -0.69007291]\n",
      "Gradient Descent(5478/9): loss=15.464381292262821, w0=76.35184706573519, w1=13.996224070796698\n",
      "[ 6.89370859  0.82797658]\n",
      "Gradient Descent(5479/9): loss=20.194732851180003, w0=71.52625105576705, w1=13.41664046168029\n",
      "[ 1.14657304  0.76850562]\n",
      "Gradient Descent(5480/9): loss=16.950207193001923, w0=70.72364992657948, w1=12.87868652658284\n",
      "[-0.79377881  1.77901254]\n",
      "Gradient Descent(5481/9): loss=18.869653211230762, w0=71.27929509087704, w1=11.63337774932164\n",
      "[ 0.39893634 -4.34561643]\n",
      "Gradient Descent(5482/9): loss=19.11972455030101, w0=71.0000396506609, w1=14.675309253119355\n",
      "[-1.63294554  1.0991876 ]\n",
      "Gradient Descent(5483/9): loss=18.731561865724863, w0=72.14310153077626, w1=13.90587793561124\n",
      "[-4.00373778  4.60942647]\n",
      "Gradient Descent(5484/9): loss=16.138890264404548, w0=74.94571797935428, w1=10.679279405272222\n",
      "[ 0.23478175 -4.63253153]\n",
      "Gradient Descent(5485/9): loss=20.671315421022033, w0=74.78137075757529, w1=13.922051475367464\n",
      "[ 3.33048811 -1.55427367]\n",
      "Gradient Descent(5486/9): loss=16.58997168222562, w0=72.45002907864063, w1=15.010043046454715\n",
      "[-0.52589573  0.35970941]\n",
      "Gradient Descent(5487/9): loss=16.912921392160627, w0=72.81815608953846, w1=14.758246462513343\n",
      "[ 0.47234077  0.15765739]\n",
      "Gradient Descent(5488/9): loss=16.316389100378373, w0=72.48751754974053, w1=14.647886286468765\n",
      "[ 0.69081514 -0.76621406]\n",
      "Gradient Descent(5489/9): loss=16.39334701286665, w0=72.00394695386989, w1=15.184236126101869\n",
      "[-2.13284396 -0.07790064]\n",
      "Gradient Descent(5490/9): loss=17.670606188146678, w0=73.4969377264311, w1=15.238766577359362\n",
      "[-1.95750507  2.59971459]\n",
      "Gradient Descent(5491/9): loss=16.953631298886265, w0=74.86719127629006, w1=13.418966365922953\n",
      "[ 3.52501548 -0.41723865]\n",
      "Gradient Descent(5492/9): loss=16.625321015829982, w0=72.39968044116199, w1=13.71103342134646\n",
      "[ 2.23031659  1.88691248]\n",
      "Gradient Descent(5493/9): loss=15.812476552853148, w0=70.83845882484403, w1=12.39019468293479\n",
      "[-3.04910971 -2.06845629]\n",
      "Gradient Descent(5494/9): loss=18.994062042292825, w0=72.97283562278233, w1=13.838114086072796\n",
      "[ 0.3034833  -0.37277782]\n",
      "Gradient Descent(5495/9): loss=15.501661972072512, w0=72.76039731107166, w1=14.099058557787457\n",
      "[-1.97591521 -0.61491158]\n",
      "Gradient Descent(5496/9): loss=15.720006976713325, w0=74.14353795991195, w1=14.529496666325144\n",
      "[-0.52918547  1.62940455]\n",
      "Gradient Descent(5497/9): loss=16.29783497289031, w0=74.51396778695477, w1=13.388913480852327\n",
      "[ 2.70624675 -0.52135637]\n",
      "Gradient Descent(5498/9): loss=16.13426595243017, w0=72.61959506146054, w1=13.75386293951091\n",
      "[ 0.22656155  0.11020143]\n",
      "Gradient Descent(5499/9): loss=15.650825529833778, w0=72.46100197357518, w1=13.676721936542364\n",
      "[-1.50187838 -0.00452342]\n",
      "Gradient Descent(5500/9): loss=15.75217212764376, w0=73.51231684192462, w1=13.679888331956596\n",
      "[-3.57138719  0.49962374]\n",
      "Gradient Descent(5501/9): loss=15.429771216722656, w0=76.01228787228803, w1=13.330151713743769\n",
      "[ 2.66778588 -0.95313111]\n",
      "Gradient Descent(5502/9): loss=19.091828575586554, w0=74.14483775693499, w1=13.997343492980828\n",
      "[ 0.60610531  1.08984096]\n",
      "Gradient Descent(5503/9): loss=15.881887635837026, w0=73.72056403962372, w1=13.234454819534676\n",
      "[ 1.24005641 -1.52639907]\n",
      "Gradient Descent(5504/9): loss=15.50697523188757, w0=72.85252455172446, w1=14.302934166078257\n",
      "[-3.79889326  1.96396624]\n",
      "Gradient Descent(5505/9): loss=15.82215073269946, w0=75.51174983514589, w1=12.928157801280161\n",
      "[ 4.08522286 -1.23484416]\n",
      "Gradient Descent(5506/9): loss=17.997374274317284, w0=72.65209383096543, w1=13.792548711981622\n",
      "[ 0.18463251  3.94337326]\n",
      "Gradient Descent(5507/9): loss=15.640792837564993, w0=72.52285107370199, w1=11.032187430186061\n",
      "[ 1.78916901 -3.23325143]\n",
      "Gradient Descent(5508/9): loss=18.678352381711615, w0=71.27043276895557, w1=13.295463430310534\n",
      "[-2.79802274 -0.87049331]\n",
      "Gradient Descent(5509/9): loss=17.450116055028122, w0=73.22904868711638, w1=13.904808749319809\n",
      "[ 0.11926521  1.04412026]\n",
      "Gradient Descent(5510/9): loss=15.478345580557018, w0=73.14556304103851, w1=13.173924564993076\n",
      "[-0.05791169  0.91220824]\n",
      "Gradient Descent(5511/9): loss=15.44364617021213, w0=73.18610122209809, w1=12.535378797714646\n",
      "[ 0.01673788 -2.57276965]\n",
      "Gradient Descent(5512/9): loss=15.837583538374023, w0=73.17438470957914, w1=14.336317549951161\n",
      "[-1.27360024 -0.85264375]\n",
      "Gradient Descent(5513/9): loss=15.759918612471255, w0=74.06590487774928, w1=14.933168176453625\n",
      "[ 0.29285519  1.58831835]\n",
      "Gradient Descent(5514/9): loss=16.740133445171427, w0=73.86090624426069, w1=13.821345332977863\n",
      "[-1.46584506 -0.02208002]\n",
      "Gradient Descent(5515/9): loss=15.604979952749844, w0=74.88699778306737, w1=13.836801349418966\n",
      "[ 3.27634408  2.66901692]\n",
      "Gradient Descent(5516/9): loss=16.7185893371779, w0=72.59355692440565, w1=11.968489504430893\n",
      "[-1.08766087 -6.53596572]\n",
      "Gradient Descent(5517/9): loss=16.77304086278233, w0=73.35491953042914, w1=16.543665506929706\n",
      "[-1.373355    2.29867678]\n",
      "Gradient Descent(5518/9): loss=20.081652431587514, w0=74.31626802989788, w1=14.934591759811456\n",
      "[ 0.06494921  1.71087832]\n",
      "Gradient Descent(5519/9): loss=16.966820493999002, w0=74.27080358314578, w1=13.736976935632013\n",
      "[ 2.23960393  1.00748798]\n",
      "Gradient Descent(5520/9): loss=15.896129192163118, w0=72.70308083366722, w1=13.031735348253811\n",
      "[-2.12532857 -0.53594778]\n",
      "Gradient Descent(5521/9): loss=15.66077624710987, w0=74.1908108342304, w1=13.406898795443835\n",
      "[-1.37385578  1.74137258]\n",
      "Gradient Descent(5522/9): loss=15.790743570476776, w0=75.15250988247277, w1=12.187937989880576\n",
      "[ 4.46372767 -1.44256767]\n",
      "Gradient Descent(5523/9): loss=17.947402931871686, w0=72.02790051442551, w1=13.197735358620687\n",
      "[ 1.53321372 -2.57989446]\n",
      "Gradient Descent(5524/9): loss=16.227048608261356, w0=70.95465090806069, w1=15.003661482638174\n",
      "[-2.36016137 -0.22397065]\n",
      "Gradient Descent(5525/9): loss=19.283192844460828, w0=72.60676386579993, w1=15.160440938048815\n",
      "[-2.67091341  1.36405388]\n",
      "Gradient Descent(5526/9): loss=17.034405171473416, w0=74.47640325378607, w1=14.205603224584765\n",
      "[-0.33828482  1.24641063]\n",
      "Gradient Descent(5527/9): loss=16.348477543327743, w0=74.71320262723629, w1=13.333115786998311\n",
      "[ 3.67776578 -1.84308781]\n",
      "Gradient Descent(5528/9): loss=16.403811903866718, w0=72.13876658278866, w1=14.623277252738598\n",
      "[-1.13613166 -0.50578297]\n",
      "Gradient Descent(5529/9): loss=16.70695013641485, w0=72.93405874141601, w1=14.977325331536807\n",
      "[ 0.68920809  2.69688036]\n",
      "Gradient Descent(5530/9): loss=16.572060845979408, w0=72.45161307592954, w1=13.08950908106566\n",
      "[ 0.20979188  1.45201439]\n",
      "Gradient Descent(5531/9): loss=15.816759361093515, w0=72.30475875761464, w1=12.073099010017861\n",
      "[-0.27006506 -0.27091401]\n",
      "Gradient Descent(5532/9): loss=16.864390494609527, w0=72.49380429941587, w1=12.262738815812389\n",
      "[ 1.1840856   1.91753738]\n",
      "Gradient Descent(5533/9): loss=16.446494432793784, w0=71.66494437633293, w1=10.92046264879649\n",
      "[ 1.44189431 -2.08494596]\n",
      "Gradient Descent(5534/9): loss=19.987551655526044, w0=70.65561836281023, w1=12.379924820879866\n",
      "[-3.64629987 -0.38739421]\n",
      "Gradient Descent(5535/9): loss=19.47097731346189, w0=73.20802827458833, w1=12.651100771183025\n",
      "[ 2.62096288  1.34751316]\n",
      "Gradient Descent(5536/9): loss=15.732875379740465, w0=71.37335425627042, w1=11.707841557204622\n",
      "[ 0.45441896  0.28675504]\n",
      "Gradient Descent(5537/9): loss=18.7999413057701, w0=71.05526098245119, w1=11.507113032572718\n",
      "[-4.08490297 -2.53293019]\n",
      "Gradient Descent(5538/9): loss=19.837263650495174, w0=73.91469306379706, w1=13.280164163250019\n",
      "[-2.64654608  0.22404866]\n",
      "Gradient Descent(5539/9): loss=15.598475980723443, w0=75.76727532119234, w1=13.123330100849701\n",
      "[ 4.05930354  0.49352374]\n",
      "Gradient Descent(5540/9): loss=18.508130373392422, w0=72.92576284384319, w1=12.777863482607492\n",
      "[-1.42618145 -3.64129841]\n",
      "Gradient Descent(5541/9): loss=15.699954427715037, w0=73.92408986198103, w1=15.326772370992042\n",
      "[-4.47388651  3.00762789]\n",
      "Gradient Descent(5542/9): loss=17.29025883823334, w0=77.05581041936387, w1=13.221432845306571\n",
      "[ 6.96813421  1.40120875]\n",
      "Gradient Descent(5543/9): loss=22.49514427400517, w0=72.17811647508643, w1=12.240586721839117\n",
      "[-0.05512094 -0.24709216]\n",
      "Gradient Descent(5544/9): loss=16.776115122386873, w0=72.21670113044355, w1=12.413551232457856\n",
      "[-1.83521467  0.07491916]\n",
      "Gradient Descent(5545/9): loss=16.534440126892505, w0=73.50135140054851, w1=12.361107822462749\n",
      "[-0.35161725  2.7472687 ]\n",
      "Gradient Descent(5546/9): loss=16.033039486081236, w0=73.74748347569428, w1=10.438019731019272\n",
      "[ 1.99310489 -4.28433813]\n",
      "Gradient Descent(5547/9): loss=20.114694126683045, w0=72.35231005267543, w1=13.437056421474919\n",
      "[-2.60179948  1.19917354]\n",
      "Gradient Descent(5548/9): loss=15.830114168228311, w0=74.17356968556861, w1=12.59763494597604\n",
      "[ 0.68950873 -4.12281753]\n",
      "Gradient Descent(5549/9): loss=16.161808240652427, w0=73.6909135754696, w1=15.48360721890387\n",
      "[ 4.18303506  0.49370911]\n",
      "Gradient Descent(5550/9): loss=17.472486175991083, w0=70.76278903181614, w1=15.138010842580018\n",
      "[-2.37479188  1.4222392 ]\n",
      "Gradient Descent(5551/9): loss=19.964181729780908, w0=72.42514334549627, w1=14.142443403122924\n",
      "[ 0.18344098  2.47974356]\n",
      "Gradient Descent(5552/9): loss=15.982882213980835, w0=72.29673466127562, w1=12.406622913003602\n",
      "[ 0.27870755  1.65964427]\n",
      "Gradient Descent(5553/9): loss=16.45883972628225, w0=72.10163937793563, w1=11.244871924792037\n",
      "[ 0.85526392 -2.82014217]\n",
      "Gradient Descent(5554/9): loss=18.593912849786545, w0=71.50295463301165, w1=13.218971443880744\n",
      "[-1.86015819 -0.91016081]\n",
      "Gradient Descent(5555/9): loss=17.02366285963038, w0=72.80506536738535, w1=13.85608401230179\n",
      "[ 0.71340575 -1.52610386]\n",
      "Gradient Descent(5556/9): loss=15.576206055588642, w0=72.30568134095613, w1=14.92435671333407\n",
      "[ 3.72865171  3.74403816]\n",
      "Gradient Descent(5557/9): loss=16.917696216481072, w0=69.69562514235633, w1=12.303530003473305\n",
      "[-4.44411412  1.05761815]\n",
      "Gradient Descent(5558/9): loss=22.551460570371773, w0=72.80650502931195, w1=11.563197296945907\n",
      "[-0.00870109 -1.28626766]\n",
      "Gradient Descent(5559/9): loss=17.341190658687125, w0=72.81259579308447, w1=12.463584657175936\n",
      "[ 1.52926225  0.90751569]\n",
      "Gradient Descent(5560/9): loss=16.017983158996234, w0=71.74211222157689, w1=11.82832367500772\n",
      "[-1.97379991 -0.43100908]\n",
      "Gradient Descent(5561/9): loss=17.953487084597377, w0=73.1237721560335, w1=12.130030028315224\n",
      "[-0.72268974 -2.58521725]\n",
      "Gradient Descent(5562/9): loss=16.311184653330933, w0=73.6296549730465, w1=13.939682104095603\n",
      "[ 0.6830626  -1.27209146]\n",
      "Gradient Descent(5563/9): loss=15.548032230966934, w0=73.15151115189175, w1=14.830146125347511\n",
      "[-2.2744069  -0.64562538]\n",
      "Gradient Descent(5564/9): loss=16.307863869986246, w0=74.74359598427738, w1=15.282083894494784\n",
      "[-1.43383665  3.68848238]\n",
      "Gradient Descent(5565/9): loss=18.0609366351433, w0=75.74728163923058, w1=12.700146229782963\n",
      "[ 3.26943777  1.56835298]\n",
      "Gradient Descent(5566/9): loss=18.699236357517123, w0=73.45867520302765, w1=11.60229914725071\n",
      "[-1.01362742 -2.4798608 ]\n",
      "Gradient Descent(5567/9): loss=17.16180000392474, w0=74.16821439976859, w1=13.338201705130423\n",
      "[ 4.95506738 -1.08444546]\n",
      "Gradient Descent(5568/9): loss=15.778094110467967, w0=70.69966723610378, w1=14.097313525356785\n",
      "[-2.54632228 -0.99912424]\n",
      "Gradient Descent(5569/9): loss=18.94168231770163, w0=72.48209283348335, w1=14.796700490245364\n",
      "[-2.5701148   1.13039414]\n",
      "Gradient Descent(5570/9): loss=16.58264993718592, w0=74.28117319522336, w1=14.00542459549461\n",
      "[ 1.38737471 -0.98160669]\n",
      "Gradient Descent(5571/9): loss=16.01140696583774, w0=73.31001089762276, w1=14.692549281716776\n",
      "[ 4.07524088  1.79674484]\n",
      "Gradient Descent(5572/9): loss=16.121503903499118, w0=70.45734228466092, w1=13.43482789699834\n",
      "[-5.66105408  2.99393813]\n",
      "Gradient Descent(5573/9): loss=19.40998742641281, w0=74.42008014170888, w1=11.339071207101261\n",
      "[ 1.42774768 -0.08056343]\n",
      "Gradient Descent(5574/9): loss=18.311176379793693, w0=73.42065676583489, w1=11.395465610717036\n",
      "[ 0.6090096  -0.41947038]\n",
      "Gradient Descent(5575/9): loss=17.565961131242197, w0=72.99435004234249, w1=11.689094877611076\n",
      "[ 0.87658714 -4.51698631]\n",
      "Gradient Descent(5576/9): loss=17.03391516676256, w0=72.38073904098403, w1=14.85098529517167\n",
      "[-2.23130566  0.75375443]\n",
      "Gradient Descent(5577/9): loss=16.743034057607122, w0=73.94265300172891, w1=14.3233571925737\n",
      "[ 0.31667505  0.01213268]\n",
      "Gradient Descent(5578/9): loss=15.952182062265832, w0=73.72098046339256, w1=14.314864316946181\n",
      "[-1.25141806 -1.12592412]\n",
      "Gradient Descent(5579/9): loss=15.825816666476243, w0=74.59697310647782, w1=15.103011197640427\n",
      "[ 0.54808874  1.2300729 ]\n",
      "Gradient Descent(5580/9): loss=17.55240839554551, w0=74.21331099082536, w1=14.241960167104091\n",
      "[ 2.47127153  3.33053841]\n",
      "Gradient Descent(5581/9): loss=16.099036727676612, w0=72.48342092291408, w1=11.910583276693906\n",
      "[-0.97542312 -2.15263834]\n",
      "Gradient Descent(5582/9): loss=16.94542702622039, w0=73.16621710848881, w1=13.417430117597501\n",
      "[-3.2990758   1.53947504]\n",
      "Gradient Descent(5583/9): loss=15.395981682286015, w0=75.47557016903053, w1=12.339797590760854\n",
      "[ 4.19450996 -1.44850403]\n",
      "Gradient Descent(5584/9): loss=18.415385156999655, w0=72.53941319393931, w1=13.353750410702586\n",
      "[-0.49004282  0.14816301]\n",
      "Gradient Descent(5585/9): loss=15.678462855410517, w0=72.88244316551717, w1=13.250036304949123\n",
      "[-0.36806179  1.01589161]\n",
      "Gradient Descent(5586/9): loss=15.496920847664372, w0=73.14008642010025, w1=12.538912181086602\n",
      "[-2.48080896  0.63527484]\n",
      "Gradient Descent(5587/9): loss=15.840273120846254, w0=74.87665269533902, w1=12.094219790211023\n",
      "[ 3.47076453 -1.82232259]\n",
      "Gradient Descent(5588/9): loss=17.598201026848624, w0=72.44711752296708, w1=13.369845603963864\n",
      "[-1.41402108  0.94947055]\n",
      "Gradient Descent(5589/9): loss=15.750462142053346, w0=73.43693228168874, w1=12.705216219112632\n",
      "[ 0.84426961  0.06190759]\n",
      "Gradient Descent(5590/9): loss=15.696036033066129, w0=72.84594355536973, w1=12.661880909236997\n",
      "[ 2.73723863 -1.27810153]\n",
      "Gradient Descent(5591/9): loss=15.820654415456124, w0=70.92987651736814, w1=13.556551979621377\n",
      "[ 0.03169287 -0.16277937]\n",
      "Gradient Descent(5592/9): loss=18.183195553591858, w0=70.90769150978943, w1=13.67049753583142\n",
      "[-0.09951922  2.30670887]\n",
      "Gradient Descent(5593/9): loss=18.251135327409873, w0=70.97735496450697, w1=12.055801325648424\n",
      "[-0.95978987 -2.48624638]\n",
      "Gradient Descent(5594/9): loss=19.082890712324513, w0=71.64920787075161, w1=13.796173789573874\n",
      "[-0.36932449 -1.49676777]\n",
      "Gradient Descent(5595/9): loss=16.78850405023931, w0=71.90773501348055, w1=14.843911226180339\n",
      "[ 1.734468    3.19517914]\n",
      "Gradient Descent(5596/9): loss=17.277164223489418, w0=70.6936074126863, w1=12.607285825903027\n",
      "[-3.01782903  1.44315677]\n",
      "Gradient Descent(5597/9): loss=19.147269944922417, w0=72.80608773318461, w1=11.597076089996966\n",
      "[-0.85684256 -3.67047454]\n",
      "Gradient Descent(5598/9): loss=17.27703880953861, w0=73.40587752471664, w1=14.166408265108247\n",
      "[-0.00662496  1.24907484]\n",
      "Gradient Descent(5599/9): loss=15.62793046990255, w0=73.41051499770977, w1=13.292055875109039\n",
      "[-0.29513382 -0.84236974]\n",
      "Gradient Descent(5600/9): loss=15.410292324374424, w0=73.61710867017801, w1=13.881714693919129\n",
      "[-1.48737111 -0.16917918]\n",
      "Gradient Descent(5601/9): loss=15.518915588131852, w0=74.65826844720226, w1=14.000140119095757\n",
      "[-0.09407929 -0.9786983 ]\n",
      "Gradient Descent(5602/9): loss=16.452030967146243, w0=74.72412395364674, w1=14.685228929802166\n",
      "[ 0.36954781 -0.29197989]\n",
      "Gradient Descent(5603/9): loss=17.13526168955928, w0=74.46544048440407, w1=14.889614855556689\n",
      "[-0.57700275  0.70479738]\n",
      "Gradient Descent(5604/9): loss=17.06602806377458, w0=74.86934240731554, w1=14.39625668689298\n",
      "[ 3.77424152 -0.11858086]\n",
      "Gradient Descent(5605/9): loss=17.04688927825504, w0=72.22737334628562, w1=14.47926329182242\n",
      "[-0.44717878  1.12419534]\n",
      "Gradient Descent(5606/9): loss=16.454201844142833, w0=72.54039848949718, w1=13.692326552280404\n",
      "[-1.48608269  0.55842426]\n",
      "Gradient Descent(5607/9): loss=15.69238909229175, w0=73.58065637448527, w1=13.301429570761032\n",
      "[-0.8764741   2.69858009]\n",
      "Gradient Descent(5608/9): loss=15.442888558820169, w0=74.1941882428168, w1=11.412423508286038\n",
      "[ 1.26580295 -0.92159088]\n",
      "Gradient Descent(5609/9): loss=17.927969274146346, w0=73.30812618028475, w1=12.05753712414948\n",
      "[-2.00528622 -0.90600652]\n",
      "Gradient Descent(5610/9): loss=16.397280055549093, w0=74.71182653346348, w1=12.691741688100395\n",
      "[ 2.82294699 -2.1359259 ]\n",
      "Gradient Descent(5611/9): loss=16.70156344782872, w0=72.7357636380793, w1=14.186889818462898\n",
      "[-0.33110815 -0.26716523]\n",
      "Gradient Descent(5612/9): loss=15.791708174343885, w0=72.96753933986282, w1=14.373905479263719\n",
      "[ 1.20139052  1.83079447]\n",
      "Gradient Descent(5613/9): loss=15.83894129015021, w0=72.12656597772502, w1=13.092349350725721\n",
      "[-0.80258446  0.53231673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5614/9): loss=16.142272992182733, w0=72.68837509889057, w1=12.719727637026992\n",
      "[ 0.50087637 -0.82934814]\n",
      "Gradient Descent(5615/9): loss=15.858019841392519, w0=72.33776164330904, w1=13.300271334198452\n",
      "[-0.47705436 -1.51855885]\n",
      "Gradient Descent(5616/9): loss=15.859108739022462, w0=72.67169969489922, w1=14.363262526867533\n",
      "[-0.88564559 -0.58619378]\n",
      "Gradient Descent(5617/9): loss=15.969798551051001, w0=73.29165160817017, w1=14.773598170570658\n",
      "[ 0.32515975  2.96458877]\n",
      "Gradient Descent(5618/9): loss=16.22296059454449, w0=73.06403978412153, w1=12.698386030071292\n",
      "[ 0.39901509 -1.59918238]\n",
      "Gradient Descent(5619/9): loss=15.717546261412796, w0=72.78472922207084, w1=13.817813696977773\n",
      "[ 1.57039198 -0.42641362]\n",
      "Gradient Descent(5620/9): loss=15.57268274412814, w0=71.68545483762543, w1=14.116303233016694\n",
      "[-2.84957461  2.99620798]\n",
      "Gradient Descent(5621/9): loss=16.88209510050092, w0=73.68015706542703, w1=12.018957643868575\n",
      "[-0.31581222 -0.93907838]\n",
      "Gradient Descent(5622/9): loss=16.52737891078972, w0=73.90122561794811, w1=12.67631250709524\n",
      "[ 0.26550475  1.77525996]\n",
      "Gradient Descent(5623/9): loss=15.893022431807234, w0=73.71537229094456, w1=11.433630537519786\n",
      "[ 2.00080869 -0.94181879]\n",
      "Gradient Descent(5624/9): loss=17.567923607386504, w0=72.31480621123238, w1=12.092903688684311\n",
      "[-3.45116581 -4.00559207]\n",
      "Gradient Descent(5625/9): loss=16.8268409842113, w0=74.73062227616485, w1=14.896818140952691\n",
      "[ 3.40327114  4.97528958]\n",
      "Gradient Descent(5626/9): loss=17.422035998508306, w0=72.3483324802102, w1=11.414115438209581\n",
      "[ 0.07598474 -2.43160967]\n",
      "Gradient Descent(5627/9): loss=17.96630311734037, w0=72.29514315913619, w1=13.11624221036503\n",
      "[-3.38689934  0.85125573]\n",
      "Gradient Descent(5628/9): loss=15.950722759504764, w0=74.66597269945731, w1=12.520363200347788\n",
      "[ 1.82399295 -0.88346144]\n",
      "Gradient Descent(5629/9): loss=16.787324903885015, w0=73.3891776345575, w1=13.13878621116711\n",
      "[-1.13166564 -0.36699142]\n",
      "Gradient Descent(5630/9): loss=15.44854003163109, w0=74.18134358498665, w1=13.39568020690821\n",
      "[ 2.24317526  1.07219522]\n",
      "Gradient Descent(5631/9): loss=15.783177109389433, w0=72.61112090593014, w1=12.645143556240797\n",
      "[-1.76214817 -1.83653463]\n",
      "Gradient Descent(5632/9): loss=15.967249143985883, w0=73.84462462527988, w1=13.930717794990176\n",
      "[-0.87416898  1.90860411]\n",
      "Gradient Descent(5633/9): loss=15.639227475790015, w0=74.45654291369999, w1=12.594694917106334\n",
      "[-0.20877585  1.30646136]\n",
      "Gradient Descent(5634/9): loss=16.453359564347803, w0=74.60268600822495, w1=11.680171968250816\n",
      "[  3.54268383e+00  -2.96025225e-03]\n",
      "Gradient Descent(5635/9): loss=17.861492426400954, w0=72.1228073276884, w1=11.682244144827115\n",
      "[-5.02356273  1.23836193]\n",
      "Gradient Descent(5636/9): loss=17.687088786215394, w0=75.63930124204994, w1=10.815390790937917\n",
      "[ 1.15946681 -3.57771697]\n",
      "Gradient Descent(5637/9): loss=21.68559466989098, w0=74.82767447277305, w1=13.319792669046386\n",
      "[ 1.70148522 -2.98931347]\n",
      "Gradient Descent(5638/9): loss=16.574873355238868, w0=73.63663482220537, w1=15.412312097347371\n",
      "[ 0.03938197  0.42350683]\n",
      "Gradient Descent(5639/9): loss=17.312084634833656, w0=73.60906744001569, w1=15.115857317844377\n",
      "[-1.61051928  0.49572551]\n",
      "Gradient Descent(5640/9): loss=16.77403123119414, w0=74.73643093478101, w1=14.768849464074968\n",
      "[ 3.32627388 -0.12552775]\n",
      "Gradient Descent(5641/9): loss=17.257241019134405, w0=72.40803921947852, w1=14.856718892333411\n",
      "[-0.44747386 -1.16286597]\n",
      "Gradient Descent(5642/9): loss=16.726355412890616, w0=72.72127091820077, w1=15.670725068394505\n",
      "[-0.68426168  1.29532861]\n",
      "Gradient Descent(5643/9): loss=17.95012068064901, w0=73.2002540907083, w1=14.763995041491231\n",
      "[-0.36970525  2.48003354]\n",
      "Gradient Descent(5644/9): loss=16.214965614324147, w0=73.4590477665565, w1=13.027971562411112\n",
      "[ 3.22421691 -5.6850399 ]\n",
      "Gradient Descent(5645/9): loss=15.501556035850953, w0=71.20209592638447, w1=17.007499491612226\n",
      "[-0.62364614 -0.74626588]\n",
      "Gradient Descent(5646/9): loss=23.796396792800987, w0=71.63864822241207, w1=17.529885609121177\n",
      "[-0.00373714  3.56584445]\n",
      "Gradient Descent(5647/9): loss=24.957804881928936, w0=71.64126421944985, w1=15.033794497560361\n",
      "[-3.44656908  3.01147296]\n",
      "Gradient Descent(5648/9): loss=17.959112270717988, w0=74.05386257610304, w1=12.92576342279857\n",
      "[ 1.19936828 -1.35321378]\n",
      "Gradient Descent(5649/9): loss=15.8280724608869, w0=73.21430477955484, w1=13.873013066351188\n",
      "[-1.20074049 -0.0463102 ]\n",
      "Gradient Descent(5650/9): loss=15.466400013207645, w0=74.05482312364686, w1=13.905430209096053\n",
      "[ 1.39251614 -1.68719677]\n",
      "Gradient Descent(5651/9): loss=15.765990938806395, w0=73.08006182856887, w1=15.086467950057955\n",
      "[-0.21490352 -2.49992615]\n",
      "Gradient Descent(5652/9): loss=16.699587598344067, w0=73.23049429237145, w1=16.836416257272685\n",
      "[ 1.52133828  3.73534444]\n",
      "Gradient Descent(5653/9): loss=21.021629681277222, w0=72.16555749626797, w1=14.221675152631668\n",
      "[-0.01216299  3.15615562]\n",
      "Gradient Descent(5654/9): loss=16.297745435031853, w0=72.17407158951457, w1=12.01236621590644\n",
      "[ 2.5133773  -0.62792223]\n",
      "Gradient Descent(5655/9): loss=17.089472805447105, w0=70.41470747672837, w1=12.451911776664145\n",
      "[-3.4315043  -2.00382417]\n",
      "Gradient Descent(5656/9): loss=20.05901310702639, w0=72.81676048837559, w1=13.854588696940029\n",
      "[-0.37932725  1.64337347]\n",
      "Gradient Descent(5657/9): loss=15.569995529808931, w0=73.08228956248992, w1=12.704227268608944\n",
      "[ 1.76716805 -1.78642128]\n",
      "Gradient Descent(5658/9): loss=15.708970635215945, w0=71.84527192654653, w1=13.954722163000108\n",
      "[-2.36404873 -2.23907918]\n",
      "Gradient Descent(5659/9): loss=16.547998510390027, w0=73.50010603668484, w1=15.522077586832058\n",
      "[ 2.87172155  4.59093796]\n",
      "Gradient Descent(5660/9): loss=17.492771503618535, w0=71.4899009490938, w1=12.308421011411589\n",
      "[-1.43347726 -0.97734752]\n",
      "Gradient Descent(5661/9): loss=17.69909564815663, w0=72.49333502929386, w1=12.99256427545798\n",
      "[-1.01325151 -1.26732406]\n",
      "Gradient Descent(5662/9): loss=15.825014284014259, w0=73.2026110852141, w1=13.87969111813164\n",
      "[ 0.10644222 -0.31627786]\n",
      "Gradient Descent(5663/9): loss=15.470048184085387, w0=73.12810153339038, w1=14.101085622535498\n",
      "[-0.06760282  2.43551216]\n",
      "Gradient Descent(5664/9): loss=15.592688401852618, w0=73.17542350946488, w1=12.396227109270967\n",
      "[-2.24922373 -0.2655524 ]\n",
      "Gradient Descent(5665/9): loss=15.979879040731621, w0=74.74988012306079, w1=12.582113786539495\n",
      "[ 1.6607009  -2.17872263]\n",
      "Gradient Descent(5666/9): loss=16.848636560666918, w0=73.5873894960615, w1=14.10721962791074\n",
      "[ 0.13963469 -1.13177898]\n",
      "Gradient Descent(5667/9): loss=15.62583209241813, w0=73.48964521075314, w1=14.89946491276344\n",
      "[-2.81238744  0.93841294]\n",
      "Gradient Descent(5668/9): loss=16.412890205104386, w0=75.45831641624977, w1=14.242575853506573\n",
      "[-0.70199966  2.32940824]\n",
      "Gradient Descent(5669/9): loss=18.019169756475655, w0=75.94971617617925, w1=12.61199008578938\n",
      "[ 2.23319099 -0.94629299]\n",
      "Gradient Descent(5670/9): loss=19.288980254002563, w0=74.38648248056026, w1=13.274395178728389\n",
      "[-1.8653318 -0.8880936]\n",
      "Gradient Descent(5671/9): loss=16.003809656229592, w0=75.69221473729954, w1=13.896060696303273\n",
      "[ 1.19265414 -2.86294692]\n",
      "Gradient Descent(5672/9): loss=18.34846482802208, w0=74.85735684188043, w1=15.900123538516347\n",
      "[-0.53319223  1.04865729]\n",
      "Gradient Descent(5673/9): loss=19.53724707298003, w0=75.23059140216601, w1=15.166063436026914\n",
      "[ 1.7576149   0.26059692]\n",
      "Gradient Descent(5674/9): loss=18.683121900746077, w0=74.00026097515399, w1=14.98364558907379\n",
      "[-1.69675074  2.94478784]\n",
      "Gradient Descent(5675/9): loss=16.76625270723086, w0=75.18798649106795, w1=12.92229409811687\n",
      "[ 1.43688878 -0.93680702]\n",
      "Gradient Descent(5676/9): loss=17.334985614144948, w0=74.18216434277569, w1=13.57805901475111\n",
      "[ 1.92760446  0.46432233]\n",
      "Gradient Descent(5677/9): loss=15.785211121584755, w0=72.83284122347821, w1=13.253033384816074\n",
      "[-2.062647   -1.84255929]\n",
      "Gradient Descent(5678/9): loss=15.517877306932688, w0=74.27669412034285, w1=14.542824890437348\n",
      "[ 0.61221586 -1.43322962]\n",
      "Gradient Descent(5679/9): loss=16.43391243348673, w0=73.84814301891242, w1=15.546085623955335\n",
      "[-3.41893441  0.30918414]\n",
      "Gradient Descent(5680/9): loss=17.67441741460417, w0=76.24139710564519, w1=15.329656723715821\n",
      "[ 7.0810756   3.65549285]\n",
      "Gradient Descent(5681/9): loss=21.440839547519868, w0=71.28464418346532, w1=12.770811730806221\n",
      "[-2.68294875 -1.01133034]\n",
      "Gradient Descent(5682/9): loss=17.655756649263957, w0=73.16270830766385, w1=13.478742966519441\n",
      "[-2.16846193  3.54718094]\n",
      "Gradient Descent(5683/9): loss=15.394496855568429, w0=74.68063165686122, w1=10.995716308296144\n",
      "[ 1.1099036  -3.33929328]\n",
      "Gradient Descent(5684/9): loss=19.432488080838883, w0=73.90369913584976, w1=13.333221606913769\n",
      "[ 1.55603091 -0.64096189]\n",
      "Gradient Descent(5685/9): loss=15.582531726603364, w0=72.81447750064926, w1=13.781894926947274\n",
      "[-0.58935134  0.69577419]\n",
      "Gradient Descent(5686/9): loss=15.546478513040602, w0=73.22702344054417, w1=13.294852997406398\n",
      "[-0.94486839  0.43994156]\n",
      "Gradient Descent(5687/9): loss=15.405212083430554, w0=73.88843131077428, w1=12.986893907340173\n",
      "[ 0.76836852 -0.89177206]\n",
      "Gradient Descent(5688/9): loss=15.684043578473501, w0=73.3505733466362, w1=13.611134347202452\n",
      "[ 0.86408287  0.93982459]\n",
      "Gradient Descent(5689/9): loss=15.396128415752898, w0=72.74571533650892, w1=12.953257136527743\n",
      "[-3.38589013 -1.76195305]\n",
      "Gradient Descent(5690/9): loss=15.67473073357048, w0=75.11583842470459, w1=14.186624272401083\n",
      "[ 0.3926311  -0.59865202]\n",
      "Gradient Descent(5691/9): loss=17.295439767234733, w0=74.84099665619851, w1=14.605680685339985\n",
      "[ 1.7471708  3.9707252]\n",
      "Gradient Descent(5692/9): loss=17.216510111897563, w0=73.61797709722626, w1=11.826173047121916\n",
      "[ 0.33163626  0.00695717]\n",
      "Gradient Descent(5693/9): loss=16.805489974780368, w0=73.38583171533993, w1=11.821303029748952\n",
      "[-0.82957236  0.0892764 ]\n",
      "Gradient Descent(5694/9): loss=16.765272444217253, w0=73.96653236421562, w1=11.758809552900676\n",
      "[-3.27039358 -2.67438492]\n",
      "Gradient Descent(5695/9): loss=17.092843583228603, w0=76.25580787189939, w1=13.63087899775168\n",
      "[ 1.39793846  1.18678241]\n",
      "Gradient Descent(5696/9): loss=19.783697486521415, w0=75.27725095003203, w1=12.80013130940173\n",
      "[-1.50752435 -0.49148565]\n",
      "Gradient Descent(5697/9): loss=17.583599979798954, w0=76.3325179925416, w1=13.144171267694288\n",
      "[ 4.36519838 -0.9914805 ]\n",
      "Gradient Descent(5698/9): loss=20.058714602852277, w0=73.27687912976454, w1=13.838207619274954\n",
      "[-1.90460868 -0.50758449]\n",
      "Gradient Descent(5699/9): loss=15.450292497156301, w0=74.61010520545003, w1=14.193516761134143\n",
      "[ 1.23739752  2.86530009]\n",
      "Gradient Descent(5700/9): loss=16.50681528922466, w0=73.7439269435287, w1=12.187806696597338\n",
      "[ 1.27664356  0.01825469]\n",
      "Gradient Descent(5701/9): loss=16.32165031092691, w0=72.85027644971863, w1=12.17502841275326\n",
      "[ 1.31555283 -0.4241326 ]\n",
      "Gradient Descent(5702/9): loss=16.335398755844263, w0=71.92938947159585, w1=12.471921232249322\n",
      "[-1.03084099 -1.50120586]\n",
      "Gradient Descent(5703/9): loss=16.824683936398294, w0=72.6509781652723, w1=13.522765330815751\n",
      "[-1.91957661  1.47093518]\n",
      "Gradient Descent(5704/9): loss=15.593503033409629, w0=73.99468179185439, w1=12.493110703447774\n",
      "[ 1.96671823 -0.62177009]\n",
      "Gradient Descent(5705/9): loss=16.118111498634192, w0=72.61797903334214, w1=12.928349763585702\n",
      "[-0.23358028 -0.01983683]\n",
      "Gradient Descent(5706/9): loss=15.766337715048019, w0=72.7814852316465, w1=12.942235543571584\n",
      "[-1.81672131  1.17885688]\n",
      "Gradient Descent(5707/9): loss=15.661624295092357, w0=74.05319014877458, w1=12.117035727804428\n",
      "[ 0.9545087  -1.53069281]\n",
      "Gradient Descent(5708/9): loss=16.602575832254544, w0=73.38503405713897, w1=13.188520695691464\n",
      "[-3.07858311 -0.72065273]\n",
      "Gradient Descent(5709/9): loss=15.432434886633215, w0=75.54004223519988, w1=13.692977605449926\n",
      "[ 1.57146037  0.21643872]\n",
      "Gradient Descent(5710/9): loss=17.93115693605392, w0=74.4400199781062, w1=13.541470504851919\n",
      "[ 0.01849157  2.47643024]\n",
      "Gradient Descent(5711/9): loss=16.044565183722806, w0=74.42707588087981, w1=11.807969333974881\n",
      "[-1.01983256 -1.44095278]\n",
      "Gradient Descent(5712/9): loss=17.42526922321462, w0=75.14095867048815, w1=12.816636281653167\n",
      "[ 3.30297158  0.76215351]\n",
      "Gradient Descent(5713/9): loss=17.311495088566364, w0=72.82887856750358, w1=12.283128827512058\n",
      "[-0.62620922 -1.14304562]\n",
      "Gradient Descent(5714/9): loss=16.209926731703753, w0=73.26722502231064, w1=13.083260763134675\n",
      "[-0.18853418 -1.89839781]\n",
      "Gradient Descent(5715/9): loss=15.46483119725254, w0=73.39919894534872, w1=14.412139230170613\n",
      "[-1.59672707  3.44310843]\n",
      "Gradient Descent(5716/9): loss=15.826139350405033, w0=74.51690789359604, w1=12.001963326682906\n",
      "[-0.89440551 -1.16991965]\n",
      "Gradient Descent(5717/9): loss=17.225606327772034, w0=75.1429917518509, w1=12.82090707931378\n",
      "[ 2.70750155 -1.46952337]\n",
      "Gradient Descent(5718/9): loss=17.312429586874945, w0=73.2477406665035, w1=13.849573439279643\n",
      "[ 0.16213077 -0.17956555]\n",
      "Gradient Descent(5719/9): loss=15.455352807955801, w0=73.13424912781147, w1=13.975269327068736\n",
      "[ 1.66341315 -1.571567  ]\n",
      "Gradient Descent(5720/9): loss=15.521423898865848, w0=71.96985992445539, w1=15.075366224314658\n",
      "[-2.30037753  1.47284302]\n",
      "Gradient Descent(5721/9): loss=17.53551356925914, w0=73.58012419433993, w1=14.0443761080975\n",
      "[ 1.33773987  2.01225898]\n",
      "Gradient Descent(5722/9): loss=15.586266248113551, w0=72.64370628883698, w1=12.63579482250141\n",
      "[ 0.79075981 -1.57028062]\n",
      "Gradient Descent(5723/9): loss=15.95337657405326, w0=72.09017442036276, w1=13.734991258822864\n",
      "[ 2.0820196   2.29593528]\n",
      "Gradient Descent(5724/9): loss=16.142975628053765, w0=70.63276070335526, w1=12.127836564260685\n",
      "[-3.11605866 -0.90476629]\n",
      "Gradient Descent(5725/9): loss=19.840561782740384, w0=72.81400176546387, w1=12.761172964884292\n",
      "[-3.24808551  0.79428771]\n",
      "Gradient Descent(5726/9): loss=15.759199070647542, w0=75.08766162350351, w1=12.205171565942367\n",
      "[ 2.71361583 -1.12081146]\n",
      "Gradient Descent(5727/9): loss=17.80686599695163, w0=73.18813054055651, w1=12.989739590066314\n",
      "[ 1.69339788 -1.4866562 ]\n",
      "Gradient Descent(5728/9): loss=15.51152047987854, w0=72.00275202208357, w1=14.03039893175538\n",
      "[ 1.00377592 -1.01887558]\n",
      "Gradient Descent(5729/9): loss=16.371075636344305, w0=71.30010888103304, w1=14.7436118385322\n",
      "[-3.63492811 -0.42597597]\n",
      "Gradient Descent(5730/9): loss=18.172254100847503, w0=73.84455855872875, w1=15.04179501444403\n",
      "[ 1.3371458   5.18147381]\n",
      "Gradient Descent(5731/9): loss=16.75753917009289, w0=72.90855649650874, w1=11.414763350269585\n",
      "[-2.92114043 -0.01733623]\n",
      "Gradient Descent(5732/9): loss=17.592148516523082, w0=74.95335480071024, w1=11.426898709501982\n",
      "[ 1.7795958  -1.39795864]\n",
      "Gradient Descent(5733/9): loss=18.86976857114655, w0=73.70763774126326, w1=12.405469754470431\n",
      "[ 2.86871647 -1.24914172]\n",
      "Gradient Descent(5734/9): loss=16.048466893566864, w0=71.69953621110591, w1=13.279868955400481\n",
      "[ 1.05643176 -0.94303405]\n",
      "Gradient Descent(5735/9): loss=16.676889602266638, w0=70.96003397715856, w1=13.93999278944691\n",
      "[-0.14206259  0.24902058]\n",
      "Gradient Descent(5736/9): loss=18.215333527673966, w0=71.0594777927742, w1=13.76567838617736\n",
      "[-0.69054478 -2.26598375]\n",
      "Gradient Descent(5737/9): loss=17.923146593755305, w0=71.54285913591053, w1=15.351867008320086\n",
      "[-2.00979584  0.76211585]\n",
      "Gradient Descent(5738/9): loss=18.671479822734486, w0=72.94971622419382, w1=14.818385916809184\n",
      "[ 1.22419341  2.4170683 ]\n",
      "Gradient Descent(5739/9): loss=16.34115002306741, w0=72.09278083962158, w1=13.126438103597456\n",
      "[-2.38437518 -0.87941672]\n",
      "Gradient Descent(5740/9): loss=16.16965929154572, w0=73.76184346277365, w1=13.742029806070109\n",
      "[ 1.13303991 -1.41729809]\n",
      "Gradient Descent(5741/9): loss=15.529768317091893, w0=72.96871552763153, w1=14.73413846926522\n",
      "[ 0.26698918  2.6603681 ]\n",
      "Gradient Descent(5742/9): loss=16.225559832084116, w0=72.78182310325703, w1=12.871880796930057\n",
      "[ 1.18290859 -1.35537386]\n",
      "Gradient Descent(5743/9): loss=15.701740160042892, w0=71.95378708888039, w1=13.820642498653234\n",
      "[-2.42362154 -0.14517702]\n",
      "Gradient Descent(5744/9): loss=16.341985315806454, w0=73.65032216991017, w1=13.922266409295323\n",
      "[ 3.13889365  0.36420407]\n",
      "Gradient Descent(5745/9): loss=15.54732541872225, w0=71.45309661520113, w1=13.667323560015648\n",
      "[-2.7673794  -0.13553517]\n",
      "Gradient Descent(5746/9): loss=17.097805888481506, w0=73.3902621934044, w1=13.762198175839309\n",
      "[ 1.2460302 -0.2819109]\n",
      "Gradient Descent(5747/9): loss=15.430427681951045, w0=72.51804105114188, w1=13.959535806414573\n",
      "[-1.21544544 -1.11046189]\n",
      "Gradient Descent(5748/9): loss=15.801998727746346, w0=73.36885285978893, w1=14.736859127963227\n",
      "[-2.86981596  2.27492798]\n",
      "Gradient Descent(5749/9): loss=16.17890408937397, w0=75.37772403118763, w1=13.144409545144583\n",
      "[ 1.82762459 -0.78200427]\n",
      "Gradient Descent(5750/9): loss=17.613217331002463, w0=74.09838682000539, w1=13.691812531564667\n",
      "[ 0.563984  -2.1601651]\n",
      "Gradient Descent(5751/9): loss=15.731962915932693, w0=73.70359802113221, w1=15.203928103269854\n",
      "[ 0.97180121  2.30753896]\n",
      "Gradient Descent(5752/9): loss=16.956264924484827, w0=73.02333717401949, w1=13.588650832937782\n",
      "[ 0.34668783  2.379088  ]\n",
      "Gradient Descent(5753/9): loss=15.428429730698303, w0=72.78065569079828, w1=11.923289233473747\n",
      "[-0.04837941 -2.06380263]\n",
      "Gradient Descent(5754/9): loss=16.728835613098266, w0=72.81452128063309, w1=13.367951077313556\n",
      "[-0.14560307 -4.58813132]\n",
      "Gradient Descent(5755/9): loss=15.507045695238125, w0=72.9164434285476, w1=16.57964299801803\n",
      "[-1.8381334   3.68924718]\n",
      "Gradient Descent(5756/9): loss=20.26191765337753, w0=74.20313680618942, w1=13.997169969336134\n",
      "[-0.02505536  1.88440892]\n",
      "Gradient Descent(5757/9): loss=15.933104798738643, w0=74.22067555884308, w1=12.678083724560512\n",
      "[-1.87175625 -2.09822782]\n",
      "Gradient Descent(5758/9): loss=16.13662824098423, w0=75.53090493046132, w1=14.14684319514335\n",
      "[-0.1773535   0.90428058]\n",
      "Gradient Descent(5759/9): loss=18.110465905279806, w0=75.65505237968907, w1=13.513846786443931\n",
      "[ 0.78452089  0.05436432]\n",
      "Gradient Descent(5760/9): loss=18.173938775778716, w0=75.10588775917797, w1=13.47579176254224\n",
      "[ 2.56597901  3.25035731]\n",
      "Gradient Descent(5761/9): loss=17.027505507067794, w0=73.30970245441996, w1=11.200541642206536\n",
      "[-1.04414907 -4.86065004]\n",
      "Gradient Descent(5762/9): loss=17.98332213150346, w0=74.04060680003616, w1=14.602996672103489\n",
      "[ 2.4505716   2.32742501]\n",
      "Gradient Descent(5763/9): loss=16.295540701234888, w0=72.32520667831903, w1=12.973799164450787\n",
      "[-0.65370834 -2.7772496 ]\n",
      "Gradient Descent(5764/9): loss=15.983066676751818, w0=72.78280251835258, w1=14.917873886053208\n",
      "[ 1.67041766 -0.43021282]\n",
      "Gradient Descent(5765/9): loss=16.550663611828654, w0=71.61351015945729, w1=15.21902285943101\n",
      "[-2.61055691  1.81235012]\n",
      "Gradient Descent(5766/9): loss=18.310380225571297, w0=73.4408999969075, w1=13.950377775720234\n",
      "[ 0.41304462  1.52358872]\n",
      "Gradient Descent(5767/9): loss=15.507452065790256, w0=73.15176876245887, w1=12.88386566984137\n",
      "[-1.16854418  2.53633198]\n",
      "Gradient Descent(5768/9): loss=15.57350832436885, w0=73.96974969068499, w1=11.108433282850008\n",
      "[ 2.57136916 -0.67410101]\n",
      "Gradient Descent(5769/9): loss=18.425741809839586, w0=72.16979128019801, w1=11.58030399051094\n",
      "[-1.32185686 -3.40790903]\n",
      "Gradient Descent(5770/9): loss=17.821599028274544, w0=73.09509108392297, w1=13.965840309998594\n",
      "[ 0.53582494 -0.46439639]\n",
      "Gradient Descent(5771/9): loss=15.523814891272641, w0=72.72001362619184, w1=14.29091778568339\n",
      "[ 0.2336196   1.37199197]\n",
      "Gradient Descent(5772/9): loss=15.879600341298714, w0=72.55647990954468, w1=13.330523405617853\n",
      "[-0.32597793  0.57472285]\n",
      "Gradient Descent(5773/9): loss=15.668926972011773, w0=72.78466445722559, w1=12.928217413612664\n",
      "[-1.01898678  0.86025654]\n",
      "Gradient Descent(5774/9): loss=15.667632871639269, w0=73.4979552018111, w1=12.326037837515432\n",
      "[ 1.51314399 -1.74201573]\n",
      "Gradient Descent(5775/9): loss=16.07218518054847, w0=72.43875440712742, w1=13.545448849457255\n",
      "[-2.43758857 -2.04281435]\n",
      "Gradient Descent(5776/9): loss=15.753704314673, w0=74.14506640509248, w1=14.975418895538372\n",
      "[ 1.35512875  0.24750179]\n",
      "Gradient Descent(5777/9): loss=16.866680174262186, w0=73.19647627942452, w1=14.802167643141258\n",
      "[-2.40085116 -2.3744041 ]\n",
      "Gradient Descent(5778/9): loss=16.26507959204823, w0=74.87707209229426, w1=16.464250511360643\n",
      "[-2.05255306  2.36782705]\n",
      "Gradient Descent(5779/9): loss=21.092803737518146, w0=76.31385923214064, w1=14.806771576805794\n",
      "[-0.09995826 -0.09986516]\n",
      "Gradient Descent(5780/9): loss=20.8264412884462, w0=76.38383001270265, w1=14.876677189800432\n",
      "[ 3.36932312  0.05664278]\n",
      "Gradient Descent(5781/9): loss=21.13540888889918, w0=74.02530382678783, w1=14.837027245967551\n",
      "[-1.3820279   2.62757896]\n",
      "Gradient Descent(5782/9): loss=16.57449930361826, w0=74.99272335975625, w1=12.997721975442715\n",
      "[-1.64712406 -1.72132137]\n",
      "Gradient Descent(5783/9): loss=16.945008296754793, w0=76.14571020029999, w1=14.202646934750476\n",
      "[ 1.45641381 -1.29122927]\n",
      "Gradient Descent(5784/9): loss=19.713552977983618, w0=75.1262205335793, w1=15.106507422674149\n",
      "[-0.58483453  1.39389737]\n",
      "Gradient Descent(5785/9): loss=18.38777778902917, w0=75.53560470734382, w1=14.130779263659543\n",
      "[ 3.65807946  2.0861073 ]\n",
      "Gradient Descent(5786/9): loss=18.110402552009923, w0=72.97494908558316, w1=12.670504156688878\n",
      "[-0.49292556 -2.48862282]\n",
      "Gradient Descent(5787/9): loss=15.764168748401449, w0=73.31999697856774, w1=14.412540133856629\n",
      "[ 3.20279185  1.48615388]\n",
      "Gradient Descent(5788/9): loss=15.821311578915454, w0=71.07804268679337, w1=13.37223241614801\n",
      "[-0.65625384  0.44132023]\n",
      "Gradient Descent(5789/9): loss=17.84672441606783, w0=71.53742037178624, w1=13.063308253352536\n",
      "[-2.69870932 -1.822886  ]\n",
      "Gradient Descent(5790/9): loss=17.01523307872815, w0=73.42651689248734, w1=14.339328451393545\n",
      "[ 3.27373145  0.64780429]\n",
      "Gradient Descent(5791/9): loss=15.764148419136694, w0=71.13490488083883, w1=13.885865451560992\n",
      "[ 1.44489531 -0.1794358 ]\n",
      "Gradient Descent(5792/9): loss=17.799045470225277, w0=70.1234781660157, w1=14.011470510188675\n",
      "[-3.93923561  0.06405526]\n",
      "Gradient Descent(5793/9): loss=20.553128252998317, w0=72.88094309074556, w1=13.966631830888435\n",
      "[-0.61028518 -0.01643207]\n",
      "Gradient Descent(5794/9): loss=15.589708908494801, w0=73.30814272006366, w1=13.978134280651549\n",
      "[ 2.21869223 -1.34376594]\n",
      "Gradient Descent(5795/9): loss=15.510201151355833, w0=71.75505815813301, w1=14.918770440818069\n",
      "[-1.21784443  2.54111991]\n",
      "Gradient Descent(5796/9): loss=17.605382806042115, w0=72.60754926239697, w1=13.139986504371107\n",
      "[-0.58533949  0.24918168]\n",
      "Gradient Descent(5797/9): loss=15.679148491703794, w0=73.01728690403637, w1=12.96555932887814\n",
      "[ 1.62322465  1.0802574 ]\n",
      "Gradient Descent(5798/9): loss=15.55632806583292, w0=71.88102964876843, w1=12.20937914687248\n",
      "[-4.97201475  1.54509504]\n",
      "Gradient Descent(5799/9): loss=17.190893601336665, w0=75.36143997251695, w1=11.127812617735001\n",
      "[ 3.46717221 -5.08597083]\n",
      "Gradient Descent(5800/9): loss=20.288919523016993, w0=72.93441942649116, w1=14.687992201501146\n",
      "[-5.21964107  2.91093311]\n",
      "Gradient Descent(5801/9): loss=16.180478916847225, w0=76.58816817574296, w1=12.650339024584937\n",
      "[ 1.9373329   0.79181368]\n",
      "Gradient Descent(5802/9): loss=21.155846922035636, w0=75.23203514754918, w1=12.096069447613376\n",
      "[ 1.41767071 -0.570543  ]\n",
      "Gradient Descent(5803/9): loss=18.22126310935775, w0=74.23966564992556, w1=12.495449546465494\n",
      "[ 0.48402548 -1.98801321]\n",
      "Gradient Descent(5804/9): loss=16.317490109388054, w0=73.90084781414602, w1=13.887058795259822\n",
      "[ 1.15391351  0.64135606]\n",
      "Gradient Descent(5805/9): loss=15.653032868103034, w0=73.09310835808571, w1=13.438109550487978\n",
      "[ 1.63788641 -2.28853302]\n",
      "Gradient Descent(5806/9): loss=15.406916328640996, w0=71.94658787179733, w1=15.04008266683821\n",
      "[-0.45867556  3.12423923]\n",
      "Gradient Descent(5807/9): loss=17.51092012839612, w0=72.26766076053458, w1=12.853115208186559\n",
      "[-0.65607406 -2.98703376]\n",
      "Gradient Descent(5808/9): loss=16.108805979122707, w0=72.7269126004302, w1=14.944038842887412\n",
      "[-0.22046268  0.64650712]\n",
      "Gradient Descent(5809/9): loss=16.61876361405758, w0=72.88123647326353, w1=14.491483861099944\n",
      "[-0.01412948 -0.51027748]\n",
      "Gradient Descent(5810/9): loss=15.9828832510343, w0=72.89112710833979, w1=14.848678098605106\n",
      "[-0.59045387 -0.26031296]\n",
      "Gradient Descent(5811/9): loss=16.404043226131016, w0=73.30444481486207, w1=15.030897173678442\n",
      "[-0.90721899  2.48340578]\n",
      "Gradient Descent(5812/9): loss=16.589030280395, w0=73.93949810477994, w1=13.292513131133438\n",
      "[-0.98028797 -1.1067476 ]\n",
      "Gradient Descent(5813/9): loss=15.611793910683772, w0=74.62569968264536, w1=14.067236448041738\n",
      "[ 3.54172191 -0.11842026]\n",
      "Gradient Descent(5814/9): loss=16.44529599697865, w0=72.1464943453509, w1=14.150130631657472\n",
      "[-2.20510934 -1.20887974]\n",
      "Gradient Descent(5815/9): loss=16.26891326178379, w0=73.69007088228065, w1=14.996346447814279\n",
      "[ 4.44183865  2.5019557 ]\n",
      "Gradient Descent(5816/9): loss=16.614444200890716, w0=70.58078382658267, w1=13.244977459961046\n",
      "[-3.93931532 -0.55965172]\n",
      "Gradient Descent(5817/9): loss=19.093997502818915, w0=73.3383045539971, w1=13.63673366675676\n",
      "[ 1.51204071  3.19185955]\n",
      "Gradient Descent(5818/9): loss=15.399200607898543, w0=72.27987605368415, w1=11.402431984392027\n",
      "[-0.59974887 -0.64259165]\n",
      "Gradient Descent(5819/9): loss=18.05757949680024, w0=72.69970026148188, w1=11.85224614230539\n",
      "[-1.81917825 -2.05168681]\n",
      "Gradient Descent(5820/9): loss=16.886760874254843, w0=73.97312503346362, w1=13.28842691157105\n",
      "[-0.27691075  1.16346476]\n",
      "Gradient Descent(5821/9): loss=15.634841323467288, w0=74.16696255618537, w1=12.474001577134908\n",
      "[ 0.69590115 -0.29120085]\n",
      "Gradient Descent(5822/9): loss=16.27271493816657, w0=73.67983175328729, w1=12.677842169077865\n",
      "[ 0.78928798 -1.69189656]\n",
      "Gradient Descent(5823/9): loss=15.781848998534354, w0=73.1273301649796, w1=13.862169760574496\n",
      "[-4.9322347   3.76325424]\n",
      "Gradient Descent(5824/9): loss=15.472901091874828, w0=76.57989445615455, w1=11.227891790498152\n",
      "[-0.15184153 -2.14122719]\n",
      "Gradient Descent(5825/9): loss=23.32004346069269, w0=76.68618352544381, w1=12.726750824367288\n",
      "[ 1.59917992  1.64227751]\n",
      "Gradient Descent(5826/9): loss=21.423082583726288, w0=75.5667575798318, w1=11.577156567504458\n",
      "[ 6.63219294 -2.86363668]\n",
      "Gradient Descent(5827/9): loss=19.778638064969353, w0=70.9242225235641, w1=13.581702244263777\n",
      "[-5.54608101  1.7070651 ]\n",
      "Gradient Descent(5828/9): loss=18.19882663872631, w0=74.80647923360512, w1=12.386756672631428\n",
      "[ 0.4039377  -2.60541818]\n",
      "Gradient Descent(5829/9): loss=17.12707870734613, w0=74.52372284077427, w1=14.210549400681842\n",
      "[ 2.55159805  0.47663977]\n",
      "Gradient Descent(5830/9): loss=16.40915425543651, w0=72.73760420613773, w1=13.876901559792321\n",
      "[-1.91563863  2.27530279]\n",
      "Gradient Descent(5831/9): loss=15.619512214315439, w0=74.07855124420567, w1=12.284189609516892\n",
      "[-3.93769602 -0.53149823]\n",
      "Gradient Descent(5832/9): loss=16.408346805721454, w0=76.83493845617534, w1=12.656238371693854\n",
      "[ 5.76134922 -3.70817789]\n",
      "Gradient Descent(5833/9): loss=21.994341399287112, w0=72.80199400178873, w1=15.25196289279284\n",
      "[-1.33736747  0.50583377]\n",
      "Gradient Descent(5834/9): loss=17.07732029016995, w0=73.73815123017732, w1=14.89787925316987\n",
      "[ 2.96332105  3.9453014 ]\n",
      "Gradient Descent(5835/9): loss=16.490156234460738, w0=71.66382649257542, w1=12.13616827390635\n",
      "[-0.20595687 -2.83223687]\n",
      "Gradient Descent(5836/9): loss=17.617049010313657, w0=71.80799630300379, w1=14.118734083888661\n",
      "[ 2.02439812  0.03031043]\n",
      "Gradient Descent(5837/9): loss=16.69404979433558, w0=70.39091762094974, w1=14.097516780819069\n",
      "[-1.249942    1.51960122]\n",
      "Gradient Descent(5838/9): loss=19.790446192196512, w0=71.26587702442062, w1=13.033795929151632\n",
      "[ 0.0235141   0.71514839]\n",
      "Gradient Descent(5839/9): loss=17.541791849674333, w0=71.24941715442179, w1=12.533192056452938\n",
      "[-0.82103052 -1.50511911]\n",
      "Gradient Descent(5840/9): loss=17.92383831842192, w0=71.82413851508477, w1=13.586775436535493\n",
      "[-2.09287314 -0.07295806]\n",
      "Gradient Descent(5841/9): loss=16.471750861338418, w0=73.28914971174815, w1=13.63784607519863\n",
      "[ 3.15049275  1.26218622]\n",
      "Gradient Descent(5842/9): loss=15.398402380289992, w0=71.08380478956185, w1=12.754315720919736\n",
      "[-0.6916016  -2.26273689]\n",
      "Gradient Descent(5843/9): loss=18.09129711181084, w0=71.56792591241381, w1=14.338231543823905\n",
      "[-1.9949545   0.21686491]\n",
      "Gradient Descent(5844/9): loss=17.24394664976166, w0=72.96439406264852, w1=14.186426107693666\n",
      "[-0.49470878  0.12749805]\n",
      "Gradient Descent(5845/9): loss=15.689904307864502, w0=73.31069020865725, w1=14.097177469554468\n",
      "[ 1.36536739  0.43552092]\n",
      "Gradient Descent(5846/9): loss=15.576659989660323, w0=72.35493303896547, w1=13.792312827781672\n",
      "[-0.63746809 -1.54004243]\n",
      "Gradient Descent(5847/9): loss=15.875597508065553, w0=72.80116069965347, w1=14.870342531540496\n",
      "[-1.4167365   2.35494286]\n",
      "Gradient Descent(5848/9): loss=16.4742207521437, w0=73.79287624657661, w1=13.221882529507887\n",
      "[-1.51096705 -1.37133676]\n",
      "Gradient Descent(5849/9): loss=15.543603667947638, w0=74.8505531837319, w1=14.181818258821082\n",
      "[-0.49153709  0.85740887]\n",
      "Gradient Descent(5850/9): loss=16.843914480565118, w0=75.19462914863752, w1=13.581632052010313\n",
      "[ 0.40574472  0.65236426]\n",
      "Gradient Descent(5851/9): loss=17.197425501435816, w0=74.91060784142631, w1=13.124977070574147\n",
      "[ 2.17392439 -1.4222103 ]\n",
      "Gradient Descent(5852/9): loss=16.75564300974341, w0=73.38886076955008, w1=14.120524277950087\n",
      "[ 1.33626079 -1.21827362]\n",
      "Gradient Descent(5853/9): loss=15.595714462650939, w0=72.45347821649291, w1=14.973315811299862\n",
      "[-3.61832271  4.63013948]\n",
      "Gradient Descent(5854/9): loss=16.85448627008008, w0=74.98630411250353, w1=11.73221817608592\n",
      "[ 0.14892717  1.40671661]\n",
      "Gradient Descent(5855/9): loss=18.344834565077274, w0=74.8820550956037, w1=10.747516551026925\n",
      "[-1.96207313 -0.19315764]\n",
      "Gradient Descent(5856/9): loss=20.379418404331773, w0=76.25550628663609, w1=10.882726901001503\n",
      "[ 1.65626414 -1.70102933]\n",
      "Gradient Descent(5857/9): loss=23.143545537889914, w0=75.09612138881826, w1=12.073447431513975\n",
      "[ 1.1415512  -0.01600482]\n",
      "Gradient Descent(5858/9): loss=17.998639813563265, w0=74.29703555188266, w1=12.084650807620838\n",
      "[-0.14486476 -2.33779881]\n",
      "Gradient Descent(5859/9): loss=16.862104737780598, w0=74.39844088249556, w1=13.721109977842413\n",
      "[-1.55812154  1.2714644 ]\n",
      "Gradient Descent(5860/9): loss=16.025005234246624, w0=75.48912596044798, w1=12.831084898684633\n",
      "[ 3.94477733  0.23254324]\n",
      "Gradient Descent(5861/9): loss=18.005706918617495, w0=72.7277818312336, w1=12.668304627337866\n",
      "[ 0.79093992 -0.35999275]\n",
      "Gradient Descent(5862/9): loss=15.875336530525308, w0=72.17412388585205, w1=12.920299550709515\n",
      "[-0.10712914 -0.23942203]\n",
      "Gradient Descent(5863/9): loss=16.169333166960406, w0=72.24911428091741, w1=13.087894970403838\n",
      "[-1.08040753  0.12039879]\n",
      "Gradient Descent(5864/9): loss=16.00845991873319, w0=73.00539955385125, w1=13.003615818866402\n",
      "[ 1.36251692 -0.48824937]\n",
      "Gradient Descent(5865/9): loss=15.540844464344339, w0=72.05163770664635, w1=13.345390380382629\n",
      "[ 1.94901637 -0.3632719 ]\n",
      "Gradient Descent(5866/9): loss=16.16654421137808, w0=70.68732624695836, w1=13.599680709201543\n",
      "[-4.49369678 -0.40646143]\n",
      "Gradient Descent(5867/9): loss=18.79025477761289, w0=73.83291399273355, w1=13.884203712345624\n",
      "[-3.15845403  0.18112694]\n",
      "Gradient Descent(5868/9): loss=15.612950648538938, w0=76.04383181346304, w1=13.757414851906882\n",
      "[ 4.32994226 -0.3202114 ]\n",
      "Gradient Descent(5869/9): loss=19.205449170311496, w0=73.01287223070379, w1=13.981562828568956\n",
      "[-0.43109526  0.7797757 ]\n",
      "Gradient Descent(5870/9): loss=15.551309264599942, w0=73.31463891617369, w1=13.435719837671723\n",
      "[ 0.04355069  0.83961289]\n",
      "Gradient Descent(5871/9): loss=15.387070138403022, w0=73.28415343249591, w1=12.847990814500518\n",
      "[ 0.22125597  0.82388151]\n",
      "Gradient Descent(5872/9): loss=15.585471684201837, w0=73.12927425203803, w1=12.271273760757271\n",
      "[ 0.00956254 -1.40867759]\n",
      "Gradient Descent(5873/9): loss=16.129604324320017, w0=73.12258047589016, w1=13.257348076950885\n",
      "[-1.51230486 -1.56387673]\n",
      "Gradient Descent(5874/9): loss=15.425289781995112, w0=74.18119387594099, w1=14.35206178460972\n",
      "[ 0.83350747  0.60458936]\n",
      "Gradient Descent(5875/9): loss=16.16001025177125, w0=73.59773864916208, w1=13.928849233230965\n",
      "[-0.13646352 -1.00525975]\n",
      "Gradient Descent(5876/9): loss=15.532902078111347, w0=73.69326311651065, w1=14.632531058026128\n",
      "[ 3.12565791  2.10144877]\n",
      "Gradient Descent(5877/9): loss=16.130119920467255, w0=71.50530257912132, w1=13.16151692082192\n",
      "[-2.89774738 -1.33519684]\n",
      "Gradient Descent(5878/9): loss=17.036091781585014, w0=73.53372574549839, w1=14.096154710564434\n",
      "[-1.80316863  3.15284485]\n",
      "Gradient Descent(5879/9): loss=15.604641326060376, w0=74.79594378394518, w1=11.889163318454216\n",
      "[ 1.13732498 -2.28020492]\n",
      "Gradient Descent(5880/9): loss=17.778845831445157, w0=73.99981629621229, w1=13.485306759193769\n",
      "[ 1.48853464  0.95335905]\n",
      "Gradient Descent(5881/9): loss=15.635046894287534, w0=72.95784204654409, w1=12.817955424124445\n",
      "[ 0.08122273  0.37448004]\n",
      "Gradient Descent(5882/9): loss=15.661323907808605, w0=72.90098613475378, w1=12.555819395725822\n",
      "[-3.36366912 -0.82259261]\n",
      "Gradient Descent(5883/9): loss=15.889876340754522, w0=75.2555545214197, w1=13.131634224507264\n",
      "[-0.83869914  1.14326398]\n",
      "Gradient Descent(5884/9): loss=17.370468159551592, w0=75.8426439226528, w1=12.331349439539029\n",
      "[ 1.73099224 -1.62189583]\n",
      "Gradient Descent(5885/9): loss=19.29324836762882, w0=74.63094935404742, w1=13.466676519029843\n",
      "[ 0.86452437  0.108285  ]\n",
      "Gradient Descent(5886/9): loss=16.27979390630267, w0=74.0257822923399, w1=13.39087701899302\n",
      "[-0.27461363 -1.61451481]\n",
      "Gradient Descent(5887/9): loss=15.657643476608209, w0=74.21801183176031, w1=14.521037387501337\n",
      "[ 4.15560555 -0.55398976]\n",
      "Gradient Descent(5888/9): loss=16.355037703827772, w0=71.30908794654887, w1=14.90883021946582\n",
      "[-2.95412321 -1.14265341]\n",
      "Gradient Descent(5889/9): loss=18.376859803831373, w0=73.37697419663345, w1=15.708687603685618\n",
      "[ 0.81748571  4.76979729]\n",
      "Gradient Descent(5890/9): loss=17.87350185367033, w0=72.80473420047821, w1=12.36982949851685\n",
      "[-2.12050188 -2.87198447]\n",
      "Gradient Descent(5891/9): loss=16.12146028779579, w0=74.28908551730235, w1=14.380218629868606\n",
      "[ 3.66192605  3.1168201 ]\n",
      "Gradient Descent(5892/9): loss=16.28651878332742, w0=71.72573728338799, w1=12.198444561464008\n",
      "[-0.47679434 -0.98171721]\n",
      "Gradient Descent(5893/9): loss=17.436313206702167, w0=72.05949332098386, w1=12.885646607013623\n",
      "[-2.47380223 -2.41448707]\n",
      "Gradient Descent(5894/9): loss=16.324252057200937, w0=73.79115487932901, w1=14.57578755598973\n",
      "[ 3.25349447 -0.94326421]\n",
      "Gradient Descent(5895/9): loss=16.11019847136387, w0=71.51370875040278, w1=15.236072501755894\n",
      "[-3.55154436  1.89982139]\n",
      "Gradient Descent(5896/9): loss=18.512867821664443, w0=73.99978979981138, w1=13.906197527440984\n",
      "[ 3.75556885 -2.85176645]\n",
      "Gradient Descent(5897/9): loss=15.725957309790564, w0=71.37089160694114, w1=15.90243404089346\n",
      "[ 1.377439    1.09888568]\n",
      "Gradient Descent(5898/9): loss=20.16970080904983, w0=70.40668430441929, w1=15.13321406279782\n",
      "[-4.55556712  0.72031481]\n",
      "Gradient Descent(5899/9): loss=20.920992446881833, w0=73.59558128911935, w1=14.628993694425436\n",
      "[-0.49063396  1.99286734]\n",
      "Gradient Descent(5900/9): loss=16.09181073819619, w0=73.93902506279771, w1=13.233986554401277\n",
      "[ 0.18540141 -1.22236451]\n",
      "Gradient Descent(5901/9): loss=15.624157452482148, w0=73.80924407642027, w1=14.089641713076523\n",
      "[ 1.46885854 -0.64931564]\n",
      "Gradient Descent(5902/9): loss=15.704673151101755, w0=72.78104310101291, w1=14.544162658696049\n",
      "[-2.00281898  0.34832159]\n",
      "Gradient Descent(5903/9): loss=16.083937391797154, w0=74.18301638748882, w1=14.30033754235278\n",
      "[ 1.90169805  2.94279038]\n",
      "Gradient Descent(5904/9): loss=16.117845065307613, w0=72.85182775515055, w1=12.24038427562952\n",
      "[-1.75665146 -3.27964997]\n",
      "Gradient Descent(5905/9): loss=16.25157867371533, w0=74.08148377693658, w1=14.53613925503497\n",
      "[ 2.01883298  0.04719202]\n",
      "Gradient Descent(5906/9): loss=16.25403345647335, w0=72.66830068791069, w1=14.503104841096293\n",
      "[-0.87675299 -0.67552752]\n",
      "Gradient Descent(5907/9): loss=16.10525489165562, w0=73.28202777985318, w1=14.975974101952582\n",
      "[ 0.539889    0.80214277]\n",
      "Gradient Descent(5908/9): loss=16.505358093103137, w0=72.90410548039516, w1=14.414474165613228\n",
      "[ 0.34997796  0.83141624]\n",
      "Gradient Descent(5909/9): loss=15.89875607564821, w0=72.65912090610526, w1=13.83248279797435\n",
      "[ 1.54477063 -0.58608518]\n",
      "Gradient Descent(5910/9): loss=15.649597549071151, w0=71.57778146388799, w1=14.242742426172317\n",
      "[-2.11197859  0.31317634]\n",
      "Gradient Descent(5911/9): loss=17.149564426008183, w0=73.05616647947048, w1=14.023518987060488\n",
      "[-1.10435609  3.1384304 ]\n",
      "Gradient Descent(5912/9): loss=15.562014496138971, w0=73.8292157421247, w1=11.82661770539887\n",
      "[-0.60877861  0.94862436]\n",
      "Gradient Descent(5913/9): loss=16.89551865538085, w0=74.25536077065799, w1=11.16258065381744\n",
      "[ 1.30140595 -4.12691441]\n",
      "Gradient Descent(5914/9): loss=18.532619967325317, w0=73.34437660646505, w1=14.051420740692201\n",
      "[-0.23562433 -1.5174331 ]\n",
      "Gradient Descent(5915/9): loss=15.550585895784941, w0=73.50931363536694, w1=15.113623908565103\n",
      "[ 1.31353608 -0.26496638]\n",
      "Gradient Descent(5916/9): loss=16.743917998410716, w0=72.58983838053321, w1=15.299100374497412\n",
      "[-2.04105786  2.00133682]\n",
      "Gradient Descent(5917/9): loss=17.2888409791266, w0=74.01857887982098, w1=13.898164603332928\n",
      "[-3.62115329  1.96464435]\n",
      "Gradient Descent(5918/9): loss=15.736002772635647, w0=76.55338618037402, w1=12.522913558413384\n",
      "[ 2.84050119 -4.82170233]\n",
      "Gradient Descent(5919/9): loss=21.15567327864648, w0=74.56503534648921, w1=15.898105189831268\n",
      "[ 4.16122823  5.15156784]\n",
      "Gradient Descent(5920/9): loss=19.118064194301635, w0=71.6521755855128, w1=12.292007698378981\n",
      "[-3.00821009 -0.63532328]\n",
      "Gradient Descent(5921/9): loss=17.43887478770938, w0=73.75792264703746, w1=12.736733994931551\n",
      "[ 0.09915113 -3.41742412]\n",
      "Gradient Descent(5922/9): loss=15.76954464927332, w0=73.68851685396551, w1=15.128930880583955\n",
      "[ 0.00936162  2.88507157]\n",
      "Gradient Descent(5923/9): loss=16.823701158031973, w0=73.6819637185957, w1=13.109380783528538\n",
      "[ 2.24341617 -3.91661985]\n",
      "Gradient Descent(5924/9): loss=15.529748821734584, w0=72.11157239756847, w1=15.851014677351554\n",
      "[-2.62692979  3.40269054]\n",
      "Gradient Descent(5925/9): loss=18.8964003248202, w0=73.9504232513908, w1=13.469131301566426\n",
      "[ 0.17728923 -0.24556402]\n",
      "Gradient Descent(5926/9): loss=15.601440794178439, w0=73.82632079219923, w1=13.64102611473747\n",
      "[ 0.28945745  1.26586075]\n",
      "Gradient Descent(5927/9): loss=15.540623156313185, w0=73.62370057662788, w1=12.754923591106989\n",
      "[ 0.08223582  0.77185187]\n",
      "Gradient Descent(5928/9): loss=15.702924257044456, w0=73.56613550351936, w1=12.21462727921691\n",
      "[ 2.47896433 -0.87304334]\n",
      "Gradient Descent(5929/9): loss=16.223158189682987, w0=71.83086047115174, w1=12.825757616407076\n",
      "[ 1.76249323  1.07267401]\n",
      "Gradient Descent(5930/9): loss=16.66999084288062, w0=70.59711520756167, w1=12.074885810083877\n",
      "[-2.62348291 -3.4156516 ]\n",
      "Gradient Descent(5931/9): loss=20.00904023539867, w0=72.43355324788251, w1=14.465841928497015\n",
      "[-1.31758162  0.05914651]\n",
      "Gradient Descent(5932/9): loss=16.242230754433876, w0=73.3558603837223, w1=14.424439373477462\n",
      "[ 1.95581043  1.94284342]\n",
      "Gradient Descent(5933/9): loss=15.83406054454092, w0=71.98679308212424, w1=13.064448976911832\n",
      "[-1.98437583  2.18241114]\n",
      "Gradient Descent(5934/9): loss=16.32640274536181, w0=73.37585616426391, w1=11.53676117918999\n",
      "[ 3.14947415 -4.83641906]\n",
      "Gradient Descent(5935/9): loss=17.2767742634993, w0=71.17122425690496, w1=14.922254520377878\n",
      "[-1.70423258  2.01068361]\n",
      "Gradient Descent(5936/9): loss=18.67927456162744, w0=72.36418705940702, w1=13.51477599278289\n",
      "[ 0.61007109  1.26498285]\n",
      "Gradient Descent(5937/9): loss=15.818706127208971, w0=71.93713729429271, w1=12.629287995036066\n",
      "[ 1.79739486  0.06074767]\n",
      "Gradient Descent(5938/9): loss=16.667931104540965, w0=70.67896089155113, w1=12.586764627101878\n",
      "[-3.74402399 -4.71688103]\n",
      "Gradient Descent(5939/9): loss=19.203576567489712, w0=73.29977768617152, w1=15.888581348298526\n",
      "[ 2.03940686  1.30094135]\n",
      "Gradient Descent(5940/9): loss=18.287229734101736, w0=71.87219288526897, w1=14.977922405684618\n",
      "[ 1.58358571  4.8879636 ]\n",
      "Gradient Descent(5941/9): loss=17.518861267805153, w0=70.7636828878534, w1=11.556347887635885\n",
      "[-2.9827166  -2.96726383]\n",
      "Gradient Descent(5942/9): loss=20.43660844748174, w0=72.85158450968027, w1=13.633432567478287\n",
      "[-1.61226403  0.05702081]\n",
      "Gradient Descent(5943/9): loss=15.495534036998041, w0=73.9801693290855, w1=13.593518002079687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3824343  -0.78986166]\n",
      "Gradient Descent(5944/9): loss=15.62783141927362, w0=73.71246531605632, w1=14.146421160671967\n",
      "[-2.55123503  2.56002841]\n",
      "Gradient Descent(5945/9): loss=15.695727384106867, w0=75.49832983989938, w1=12.354401275181377\n",
      "[ 2.40748195 -1.42685173]\n",
      "Gradient Descent(5946/9): loss=18.448757429687483, w0=73.81309247802629, w1=13.353197485248685\n",
      "[-2.75143449 -2.02400051]\n",
      "Gradient Descent(5947/9): loss=15.528659876617375, w0=75.73909662163848, w1=14.769997842688037\n",
      "[ 1.17196432  2.21981503]\n",
      "Gradient Descent(5948/9): loss=19.207745545494756, w0=74.91872160070429, w1=13.216127321062181\n",
      "[ 1.49776271 -2.71070306]\n",
      "Gradient Descent(5949/9): loss=16.740613292775215, w0=73.87028770202389, w1=15.113619459789593\n",
      "[ 1.58507288  3.57040378]\n",
      "Gradient Descent(5950/9): loss=16.886812661697068, w0=72.76073668913567, w1=12.614336813250176\n",
      "[ 0.3251736   0.24626781]\n",
      "Gradient Descent(5951/9): loss=15.902468641162573, w0=72.53311517167658, w1=12.441949345274775\n",
      "[ 2.11786364 -3.42268575]\n",
      "Gradient Descent(5952/9): loss=16.213777500629465, w0=71.05061062481968, w1=14.83782936931813\n",
      "[-3.43174603  0.8436506 ]\n",
      "Gradient Descent(5953/9): loss=18.82435164021441, w0=73.45283284810107, w1=14.247273949596563\n",
      "[-0.50805651  0.12404965]\n",
      "Gradient Descent(5954/9): loss=15.693089536670255, w0=73.80847240780109, w1=14.16043919569373\n",
      "[-0.92239882  0.01870399]\n",
      "Gradient Descent(5955/9): loss=15.749963390200055, w0=74.45415158231309, w1=14.147346404617377\n",
      "[-0.40190114  0.86934122]\n",
      "Gradient Descent(5956/9): loss=16.281821766924935, w0=74.73548237946267, w1=13.538807552855658\n",
      "[ 3.26634865 -3.43340806]\n",
      "Gradient Descent(5957/9): loss=16.426682146090748, w0=72.44903832217136, w1=15.94219319456248\n",
      "[-1.37748998  3.3105971 ]\n",
      "Gradient Descent(5958/9): loss=18.774707830773337, w0=73.41328130549212, w1=13.624775225495885\n",
      "[-2.04058431 -2.43725156]\n",
      "Gradient Descent(5959/9): loss=15.403532797076721, w0=74.84169031919762, w1=15.330851315093055\n",
      "[-1.02118114  3.02989556]\n",
      "Gradient Descent(5960/9): loss=18.297038827243327, w0=75.55651711942389, w1=13.209924421180379\n",
      "[ 3.07294971  2.23266391]\n",
      "Gradient Descent(5961/9): loss=17.981948987484017, w0=73.40545232528618, w1=11.647059683460448\n",
      "[ 0.04955003 -2.22270167]\n",
      "Gradient Descent(5962/9): loss=17.071415429166507, w0=73.37076730217181, w1=13.202950853294734\n",
      "[ 0.75641035 -1.34148284]\n",
      "Gradient Descent(5963/9): loss=15.427138955451534, w0=72.84128005868769, w1=14.14198883843034\n",
      "[ 1.32764291  0.85635353]\n",
      "Gradient Descent(5964/9): loss=15.70763525057735, w0=71.91193001988604, w1=13.542541369124685\n",
      "[ 1.5456965   0.07272854]\n",
      "Gradient Descent(5965/9): loss=16.342812525770714, w0=70.82994246840815, w1=13.491631389240453\n",
      "[-1.93374173  0.5256053 ]\n",
      "Gradient Descent(5966/9): loss=18.421556470803566, w0=72.1835616793, w1=13.123707678364758\n",
      "[-1.58887789 -2.0464457 ]\n",
      "Gradient Descent(5967/9): loss=16.065707585428985, w0=73.29577619985692, w1=14.55621967171338\n",
      "[-0.57121036  1.85937685]\n",
      "Gradient Descent(5968/9): loss=15.96532350321398, w0=73.69562345391975, w1=13.254655879182586\n",
      "[ 2.03162004 -0.92749309]\n",
      "Gradient Descent(5969/9): loss=15.491895123680099, w0=72.2734894260346, w1=13.90390104519335\n",
      "[ 1.00940872 -1.16690493]\n",
      "Gradient Descent(5970/9): loss=15.996497178495957, w0=71.56690332115798, w1=14.72073449941989\n",
      "[-0.44542598  0.29773791]\n",
      "Gradient Descent(5971/9): loss=17.647252513201813, w0=71.87870151038713, w1=14.51231796588914\n",
      "[-1.89184399  0.8327043 ]\n",
      "Gradient Descent(5972/9): loss=16.92044948014148, w0=73.2029923067162, w1=13.929424952594978\n",
      "[ 1.25858397  3.55243095]\n",
      "Gradient Descent(5973/9): loss=15.491142647826898, w0=72.3219835259022, w1=11.442723288848827\n",
      "[ 0.30749111  0.2960623 ]\n",
      "Gradient Descent(5974/9): loss=17.932882460337833, w0=72.10673974876286, w1=11.23547967801035\n",
      "[-2.21520611 -1.9377152 ]\n",
      "Gradient Descent(5975/9): loss=18.60887905390299, w0=73.65738402555625, w1=12.591880315017994\n",
      "[ 1.63271481  1.72460572]\n",
      "Gradient Descent(5976/9): loss=15.846063126701122, w0=72.51448365550618, w1=11.384656307714325\n",
      "[ 1.65217618  0.0759578 ]\n",
      "Gradient Descent(5977/9): loss=17.884280025119576, w0=71.3579603297643, w1=11.331485845259017\n",
      "[ 0.11427169 -2.26157712]\n",
      "Gradient Descent(5978/9): loss=19.56730040762743, w0=71.2779701476872, w1=12.914589829060635\n",
      "[-0.05412754 -1.1287584 ]\n",
      "Gradient Descent(5979/9): loss=17.577600588360728, w0=71.31585942563109, w1=13.70472071185406\n",
      "[-0.15069466  1.06581319]\n",
      "Gradient Descent(5980/9): loss=17.367568009381955, w0=71.4213456872635, w1=12.958651478041265\n",
      "[ 0.13777844 -0.08783644]\n",
      "Gradient Descent(5981/9): loss=17.274911156710118, w0=71.32490078164454, w1=13.020136984893007\n",
      "[-2.95140367  0.73151105]\n",
      "Gradient Descent(5982/9): loss=17.43001494930705, w0=73.39088334997025, w1=12.50807924923995\n",
      "[-1.63885576 -0.67173847]\n",
      "Gradient Descent(5983/9): loss=15.862624144143776, w0=74.53808238506778, w1=12.97829617929128\n",
      "[ 1.40144168  0.54939667]\n",
      "Gradient Descent(5984/9): loss=16.2855645288352, w0=73.5570732061909, w1=12.593718511082377\n",
      "[-0.06363886  0.45002657]\n",
      "Gradient Descent(5985/9): loss=15.81300476353505, w0=73.60162040871178, w1=12.278699911411108\n",
      "[-4.06091112 -2.60349914]\n",
      "Gradient Descent(5986/9): loss=16.154442564439044, w0=76.44425819069208, w1=14.101149310378712\n",
      "[ 1.83733494  0.13268513]\n",
      "Gradient Descent(5987/9): loss=20.541288814436527, w0=75.15812373001012, w1=14.008269718926837\n",
      "[ 1.659155   -2.09845533]\n",
      "Gradient Descent(5988/9): loss=17.263198311193104, w0=73.99671522728696, w1=15.477188450569594\n",
      "[-2.77169924  5.25289363]\n",
      "Gradient Descent(5989/9): loss=17.62780224391986, w0=75.93690469501352, w1=11.800162911879093\n",
      "[ 4.78525382  0.9011987 ]\n",
      "Gradient Descent(5990/9): loss=20.28900992662534, w0=72.58722702208922, w1=11.169323822658466\n",
      "[-3.62983142 -1.72054477]\n",
      "Gradient Descent(5991/9): loss=18.3045445362127, w0=75.12810901395481, w1=12.37370516041378\n",
      "[ 3.34125417 -1.19304277]\n",
      "Gradient Descent(5992/9): loss=17.679634911755034, w0=72.78923109155912, w1=13.208835101232218\n",
      "[ 2.38838927  0.46359905]\n",
      "Gradient Descent(5993/9): loss=15.549931591394916, w0=71.11735860505233, w1=12.884315769328893\n",
      "[-0.94865562  1.79876609]\n",
      "Gradient Descent(5994/9): loss=17.93185057426415, w0=71.78141754098705, w1=11.625179509698583\n",
      "[-2.34272505 -0.05889349]\n",
      "Gradient Descent(5995/9): loss=18.24936892677374, w0=73.42132507554575, w1=11.666404956097187\n",
      "[ 0.63740463 -3.79197912]\n",
      "Gradient Descent(5996/9): loss=17.038045646893032, w0=72.9751418330392, w1=14.320790343499375\n",
      "[ 1.29970202  0.86934072]\n",
      "Gradient Descent(5997/9): loss=15.790404291016323, w0=72.06535042073578, w1=13.712251837484896\n",
      "[-0.60902736 -1.86728841]\n",
      "Gradient Descent(5998/9): loss=16.167619220960226, w0=72.49166957375755, w1=15.01935372550342\n",
      "[-2.08036893  3.00027997]\n",
      "Gradient Descent(5999/9): loss=16.892939999952624, w0=73.94792782728737, w1=12.919157744138131\n",
      "[ 3.68876994 -4.33631977]\n",
      "Gradient Descent(6000/9): loss=15.756860459233, w0=71.36578886718205, w1=15.954581586453424\n",
      "[-1.83245226  0.98183949]\n",
      "Gradient Descent(6001/9): loss=20.307225220258676, w0=72.64850544901662, w1=15.267293942283883\n",
      "[-1.20754524  0.95623201]\n",
      "Gradient Descent(6002/9): loss=17.191892954941, w0=73.49378711529229, w1=14.597931534194483\n",
      "[-2.26575779 -0.34611162]\n",
      "Gradient Descent(6003/9): loss=16.031067877477955, w0=75.07981757138701, w1=14.840209667811738\n",
      "[ 3.21136862  4.86787695]\n",
      "Gradient Descent(6004/9): loss=17.906075721278718, w0=72.83185953599914, w1=11.432695803346931\n",
      "[-2.70120215 -3.1763576 ]\n",
      "Gradient Descent(6005/9): loss=17.58777727523112, w0=74.72270103900256, w1=13.656146125126854\n",
      "[ 4.28915813  0.76788017]\n",
      "Gradient Descent(6006/9): loss=16.422157060475907, w0=71.72029034603244, w1=13.118630007655335\n",
      "[ -9.27279378e-04   1.41769824e+00]\n",
      "Gradient Descent(6007/9): loss=16.68923642299113, w0=71.72093944159685, w1=12.126241239663667\n",
      "[ 0.53266069 -2.41201553]\n",
      "Gradient Descent(6008/9): loss=17.538967074948836, w0=71.34807695946125, w1=13.814652111661141\n",
      "[-3.95008597 -0.50738003]\n",
      "Gradient Descent(6009/9): loss=17.335136627325, w0=74.1131371369674, w1=14.169818134060423\n",
      "[-0.58553488  2.52626787]\n",
      "Gradient Descent(6010/9): loss=15.959567525368445, w0=74.52301155517783, w1=12.401430625622483\n",
      "[ 2.44022358 -2.83565266]\n",
      "Gradient Descent(6011/9): loss=16.72256426377097, w0=72.81485504877396, w1=14.38638748856254\n",
      "[ 1.21744764  1.79195489]\n",
      "Gradient Descent(6012/9): loss=15.91167026810268, w0=71.96264170217181, w1=13.132019062124007\n",
      "[-2.45449959  1.47422347]\n",
      "Gradient Descent(6013/9): loss=16.332486828091888, w0=73.68079141701573, w1=12.100062631727969\n",
      "[ 0.24422529 -0.68758625]\n",
      "Gradient Descent(6014/9): loss=16.41243863074518, w0=73.50983371130273, w1=12.581373008194129\n",
      "[ 2.15907997 -0.63557614]\n",
      "Gradient Descent(6015/9): loss=15.812703664780816, w0=71.99847773103512, w1=13.026276309188791\n",
      "[-3.37141063 -2.93929776]\n",
      "Gradient Descent(6016/9): loss=16.3277779586439, w0=74.35846516954783, w1=15.08378474103401\n",
      "[ 2.84164845  5.06327582]\n",
      "Gradient Descent(6017/9): loss=17.239037928014003, w0=72.36931125610839, w1=11.539491667490008\n",
      "[-1.22851813 -2.51138746]\n",
      "Gradient Descent(6018/9): loss=17.69556869795306, w0=73.22927395036393, w1=13.297462888540522\n",
      "[ 0.28562316 -1.90128954]\n",
      "Gradient Descent(6019/9): loss=15.404585002716715, w0=73.0293377375828, w1=14.628365565102484\n",
      "[ 0.25520165  2.52629922]\n",
      "Gradient Descent(6020/9): loss=16.080592292005527, w0=72.85069657914102, w1=12.859956108265745\n",
      "[ -2.31614779e+00  -1.69923268e-03]\n",
      "Gradient Descent(6021/9): loss=15.676161208867066, w0=74.47200003101797, w1=12.861145571139014\n",
      "[ 1.27123955  0.54883191]\n",
      "Gradient Descent(6022/9): loss=16.271134272459545, w0=73.58213234421159, w1=12.476963232745844\n",
      "[ 1.08220104 -0.60038446]\n",
      "Gradient Descent(6023/9): loss=15.930173450777634, w0=72.82459161444082, w1=12.897232357195124\n",
      "[-1.55457676 -1.09185056]\n",
      "Gradient Descent(6024/9): loss=15.665664895735402, w0=73.91279534416434, w1=13.661527745756441\n",
      "[-0.34729407 -3.21312922]\n",
      "Gradient Descent(6025/9): loss=15.59391837919985, w0=74.15590119611076, w1=15.910718197575676\n",
      "[ 2.01678755  4.7677723 ]\n",
      "Gradient Descent(6026/9): loss=18.712286443143338, w0=72.744149910157, w1=12.573277588070747\n",
      "[-0.49335166 -0.17208035]\n",
      "Gradient Descent(6027/9): loss=15.947824611225844, w0=73.08949606930739, w1=12.693733833476283\n",
      "[ 0.41278561 -0.68446888]\n",
      "Gradient Descent(6028/9): loss=15.715664030847506, w0=72.8005461399317, w1=13.172862049070623\n",
      "[ 0.20315377  1.66632505]\n",
      "Gradient Descent(6029/9): loss=15.554676319186262, w0=72.65833850419068, w1=12.006434515450831\n",
      "[ 1.83811138  0.052518  ]\n",
      "Gradient Descent(6030/9): loss=16.67314497433945, w0=71.37166054073899, w1=11.969671913997766\n",
      "[-1.70501177 -1.5227048 ]\n",
      "Gradient Descent(6031/9): loss=18.373543619274074, w0=72.5651687824252, w1=13.035565272786027\n",
      "[-1.35786867 -0.62741459]\n",
      "Gradient Descent(6032/9): loss=15.750061847272871, w0=73.51567685207704, w1=13.474755482951839\n",
      "[-1.16372281 -0.7334635 ]\n",
      "Gradient Descent(6033/9): loss=15.41048776125917, w0=74.33028281661309, w1=13.988179935316476\n",
      "[ 0.01126217  2.04261124]\n",
      "Gradient Descent(6034/9): loss=16.052179337197753, w0=74.3223992966602, w1=12.558352070217376\n",
      "[ 3.1659519  -2.61224405]\n",
      "Gradient Descent(6035/9): loss=16.339223102423087, w0=72.10623296497113, w1=14.38692290250284\n",
      "[-1.84857905 -1.00509847]\n",
      "Gradient Descent(6036/9): loss=16.502705909476916, w0=73.40023829813639, w1=15.09049182829505\n",
      "[ 0.92575749  1.38046839]\n",
      "Gradient Descent(6037/9): loss=16.688844573179924, w0=72.75220805424296, w1=14.124163952000954\n",
      "[ 0.56240916  2.43030662]\n",
      "Gradient Descent(6038/9): loss=15.740273748373118, w0=72.35852164569512, w1=12.4229493153695\n",
      "[-1.94541996  0.07232715]\n",
      "Gradient Descent(6039/9): loss=16.381748927709463, w0=73.72031561988624, w1=12.37232030992057\n",
      "[-1.27337145 -1.59140049]\n",
      "Gradient Descent(6040/9): loss=16.08995228680344, w0=74.61167563187358, w1=13.486300651300297\n",
      "[-0.63912768  3.91785056]\n",
      "Gradient Descent(6041/9): loss=16.25414688551037, w0=75.05906500859476, w1=10.743805257938984\n",
      "[ 1.24005602 -2.61395132]\n",
      "Gradient Descent(6042/9): loss=20.686346826225943, w0=74.19102579307528, w1=12.573571179350557\n",
      "[ 3.00116259 -0.89108452]\n",
      "Gradient Descent(6043/9): loss=16.19883146230091, w0=72.0902119781115, w1=13.197330345277905\n",
      "[ 1.34531982 -2.03883042]\n",
      "Gradient Descent(6044/9): loss=16.15021660205566, w0=71.1484881057141, w1=14.62451164090187\n",
      "[-1.20125373 -0.16107757]\n",
      "Gradient Descent(6045/9): loss=18.342613781650655, w0=71.98936571883043, w1=14.737265942391195\n",
      "[-1.96874858  0.84716526]\n",
      "Gradient Descent(6046/9): loss=17.027541828934954, w0=73.36748972426078, w1=14.144250261700147\n",
      "[-3.13671759 -0.77508914]\n",
      "Gradient Descent(6047/9): loss=15.609399235265938, w0=75.56319203898552, w1=14.68681266019092\n",
      "[ 3.40115246 -0.08888525]\n",
      "Gradient Descent(6048/9): loss=18.68922659581203, w0=73.18238531403429, w1=14.749032336966405\n",
      "[ 1.66592029 -0.71422359]\n",
      "Gradient Descent(6049/9): loss=16.197694592000214, w0=72.01624110817761, w1=15.248988849357488\n",
      "[-0.91496655  0.97960039]\n",
      "Gradient Descent(6050/9): loss=17.767291617403412, w0=72.6567176919322, w1=14.56326857975323\n",
      "[ 1.39531794  3.97555255]\n",
      "Gradient Descent(6051/9): loss=16.175949494709027, w0=71.67999513710957, w1=11.780381793279648\n",
      "[-2.88885567 -2.3074831 ]\n",
      "Gradient Descent(6052/9): loss=18.13213014653293, w0=73.70219410835674, w1=13.39561996177381\n",
      "[-0.44618996  1.19757291]\n",
      "Gradient Descent(6053/9): loss=15.472766697226662, w0=74.01452707731367, w1=12.557318921600185\n",
      "[ 1.89286398 -0.96646154]\n",
      "Gradient Descent(6054/9): loss=16.07092860280844, w0=72.68952229126761, w1=13.233842001642582\n",
      "[ 0.57820737  0.85936896]\n",
      "Gradient Descent(6055/9): loss=15.598763509056665, w0=72.28477713402386, w1=12.632283729969462\n",
      "[-2.04898878  0.50774217]\n",
      "Gradient Descent(6056/9): loss=16.254142256262433, w0=73.71906928218668, w1=12.276864212067787\n",
      "[ 3.87447485  0.02056895]\n",
      "Gradient Descent(6057/9): loss=16.199684897402165, w0=71.00693688710547, w1=12.262465949273597\n",
      "[-4.98683244  0.80478527]\n",
      "Gradient Descent(6058/9): loss=18.74188283043785, w0=74.4977195921849, w1=11.699116257330195\n",
      "[ 1.76494401  0.84648955]\n",
      "Gradient Descent(6059/9): loss=17.695713561716914, w0=73.2622587823196, w1=11.106573571476927\n",
      "[ 1.23009725 -1.45923647]\n",
      "Gradient Descent(6060/9): loss=18.202283181328795, w0=72.40119070569096, w1=12.128039099773957\n",
      "[-0.36112381  0.03615536]\n",
      "Gradient Descent(6061/9): loss=16.697882855193853, w0=72.65397737114111, w1=12.102730347972582\n",
      "[-4.45352843 -2.09621943]\n",
      "Gradient Descent(6062/9): loss=16.538692268161387, w0=75.77144727222046, w1=13.57008394943192\n",
      "[ 3.35456877 -0.66702791]\n",
      "Gradient Descent(6063/9): loss=18.45903710617061, w0=73.42324913123399, w1=14.03700348413108\n",
      "[-1.68952235  0.105536  ]\n",
      "Gradient Descent(6064/9): loss=15.549537278720662, w0=74.60591477785474, w1=13.963128281958184\n",
      "[-1.17983722 -0.38443707]\n",
      "Gradient Descent(6065/9): loss=16.363395831189344, w0=75.43180083107016, w1=14.232234233890912\n",
      "[ 4.14096506  0.84179448]\n",
      "Gradient Descent(6066/9): loss=17.95429534140897, w0=72.53312528828435, w1=13.642978100777864\n",
      "[-1.21778161 -0.12514567]\n",
      "Gradient Descent(6067/9): loss=15.688621527522423, w0=73.38557241294171, w1=13.730580072075245\n",
      "[-3.69449183  0.77078714]\n",
      "Gradient Descent(6068/9): loss=15.421555053401255, w0=75.97171669611538, w1=13.191029071606296\n",
      "[ 3.81332938 -5.13255742]\n",
      "Gradient Descent(6069/9): loss=19.01284912261095, w0=73.3023861289822, w1=16.783819263462043\n",
      "[-0.4930469  0.640282 ]\n",
      "Gradient Descent(6070/9): loss=20.84448465653223, w0=73.64751896099388, w1=16.335621864935227\n",
      "[-0.45164229  2.44261438]\n",
      "Gradient Descent(6071/9): loss=19.526512609524815, w0=73.9636685640427, w1=14.62579179857063\n",
      "[-0.3020189  0.0194655]\n",
      "Gradient Descent(6072/9): loss=16.26691705125674, w0=74.17508179337172, w1=14.612165945316136\n",
      "[ 1.62948331  1.401714  ]\n",
      "Gradient Descent(6073/9): loss=16.415334634227904, w0=73.03444347592036, w1=13.630966148509591\n",
      "[ 1.26124816 -1.49736627]\n",
      "Gradient Descent(6074/9): loss=15.430991264531809, w0=72.15156976301277, w1=14.679122535584787\n",
      "[ 1.85052343 -1.22771039]\n",
      "Gradient Descent(6075/9): loss=16.75766448261467, w0=70.85620336372276, w1=15.538519807828854\n",
      "[-1.04704202 -0.94830163]\n",
      "Gradient Descent(6076/9): loss=20.47646784801773, w0=71.58913277949443, w1=16.20233094839348\n",
      "[-2.99939093  3.04696804]\n",
      "Gradient Descent(6077/9): loss=20.54536680036058, w0=73.68870642738918, w1=14.069453319420527\n",
      "[-0.32674174  0.65628508]\n",
      "Gradient Descent(6078/9): loss=15.637712395437816, w0=73.91742564808754, w1=13.61005376636748\n",
      "[ 2.00744016 -4.80930347]\n",
      "Gradient Descent(6079/9): loss=15.588760698438797, w0=72.5122175360813, w1=16.976566192066823\n",
      "[-4.037525    4.67938768]\n",
      "Gradient Descent(6080/9): loss=21.80541190412473, w0=75.33848503312741, w1=13.700994817023389\n",
      "[ 1.56494793  1.10418114]\n",
      "Gradient Descent(6081/9): loss=17.500489809040182, w0=74.24302148071799, w1=12.928068017997704\n",
      "[ 2.87891882  1.89009937]\n",
      "Gradient Descent(6082/9): loss=15.988438560379803, w0=72.22777830670708, w1=11.604998457154949\n",
      "[-1.81792288 -0.04081026]\n",
      "Gradient Descent(6083/9): loss=17.711495307791246, w0=73.50032431973189, w1=11.633565639696158\n",
      "[ 1.29904202 -2.67750692]\n",
      "Gradient Descent(6084/9): loss=17.111317822075343, w0=72.5909949067674, w1=13.507820484530221\n",
      "[-2.26708293  0.07746451]\n",
      "Gradient Descent(6085/9): loss=15.63333615073391, w0=74.17795296073412, w1=13.453595330479278\n",
      "[ 5.32241339 -0.30311089]\n",
      "Gradient Descent(6086/9): loss=15.776984288310576, w0=70.4522635883894, w1=13.66577295196555\n",
      "[-3.65759799 -0.12986781]\n",
      "Gradient Descent(6087/9): loss=19.440708396939055, w0=73.0125821807062, w1=13.756680419087012\n",
      "[-1.17716392 -0.22072164]\n",
      "Gradient Descent(6088/9): loss=15.46381954848945, w0=73.83659692675272, w1=13.911185568957553\n",
      "[ 0.61230057  0.7526629 ]\n",
      "Gradient Descent(6089/9): loss=15.626220438418304, w0=73.40798653109867, w1=13.38432153724919\n",
      "[ 1.22279585  2.94472349]\n",
      "Gradient Descent(6090/9): loss=15.396942938902463, w0=72.5520294390165, w1=11.323015096582312\n",
      "[ 0.67759085 -3.32140709]\n",
      "Gradient Descent(6091/9): loss=17.986761861157905, w0=72.07771584190132, w1=13.648000061252617\n",
      "[ 1.13913209  0.10245209]\n",
      "Gradient Descent(6092/9): loss=16.13962694346503, w0=71.28032337898709, w1=13.57628359950461\n",
      "[-2.39320148  1.63498948]\n",
      "Gradient Descent(6093/9): loss=17.41784057124889, w0=72.95556441341293, w1=12.431790964577338\n",
      "[-0.62448362 -0.94667742]\n",
      "Gradient Descent(6094/9): loss=15.99220050181714, w0=73.39270294688187, w1=13.094465159599968\n",
      "[ 2.28894706  0.52302626]\n",
      "Gradient Descent(6095/9): loss=15.464974437952243, w0=71.79044000218735, w1=12.728346778890735\n",
      "[-1.86928749 -1.84424185]\n",
      "Gradient Descent(6096/9): loss=16.798392105449892, w0=73.09894124173977, w1=14.019316072610332\n",
      "[-0.59640914  1.21036726]\n",
      "Gradient Descent(6097/9): loss=15.550482660152799, w0=73.51642764062608, w1=13.172058992079595\n",
      "[ 5.15722541  0.40853814]\n",
      "Gradient Descent(6098/9): loss=15.457967568883213, w0=69.90636985531054, w1=12.886082297150892\n",
      "[-3.11063447 -3.23375797]\n",
      "Gradient Descent(6099/9): loss=21.299841012730777, w0=72.08381398231751, w1=15.149712873493645\n",
      "[-3.22506835 -0.96404175]\n",
      "Gradient Descent(6100/9): loss=17.512519310909404, w0=74.3413618275326, w1=15.824542098582125\n",
      "[-0.70071181  4.39985994]\n",
      "Gradient Descent(6101/9): loss=18.683566038408106, w0=74.83186009779796, w1=12.744640137368503\n",
      "[ 1.04205542  0.93433742]\n",
      "Gradient Descent(6102/9): loss=16.83868030328552, w0=74.10242130197038, w1=12.090603942578433\n",
      "[ 3.9153496  -1.62164666]\n",
      "Gradient Descent(6103/9): loss=16.677534629614303, w0=71.36167658160538, w1=13.225756605437017\n",
      "[-3.15941829  2.27598835]\n",
      "Gradient Descent(6104/9): loss=17.284920833032373, w0=73.57326938536133, w1=11.632564757145751\n",
      "[-2.82908204 -1.87390513]\n",
      "Gradient Descent(6105/9): loss=17.130882620976372, w0=75.55362681252852, w1=12.944298346418199\n",
      "[ 2.42281929  0.80552663]\n",
      "Gradient Descent(6106/9): loss=18.082354907074645, w0=73.85765330764687, w1=12.380429705172032\n",
      "[-1.04182426 -1.13091435]\n",
      "Gradient Descent(6107/9): loss=16.14899562129024, w0=74.58693028864069, w1=13.172069747548601\n",
      "[-0.74162867 -3.03767493]\n",
      "Gradient Descent(6108/9): loss=16.269145094921925, w0=75.1060703557658, w1=15.29844219991498\n",
      "[ 1.49529621 -0.69037466]\n",
      "Gradient Descent(6109/9): loss=18.681717675580586, w0=74.05936300997172, w1=15.781704460505193\n",
      "[-0.26661237  2.19879636]\n",
      "Gradient Descent(6110/9): loss=18.328421479861223, w0=74.24599166942633, w1=14.24254701101767\n",
      "[ 1.31266042  0.39941786]\n",
      "Gradient Descent(6111/9): loss=16.130064489738277, w0=73.32712937519625, w1=13.962954509530027\n",
      "[ 3.84486684 -0.77379842]\n",
      "Gradient Descent(6112/9): loss=15.503200684946536, w0=70.63572258754573, w1=14.50461340283748\n",
      "[-3.09204745  0.5557791 ]\n",
      "Gradient Descent(6113/9): loss=19.44411092955986, w0=72.80015580132863, w1=14.115568035325362\n",
      "[-1.44281044  1.2590855 ]\n",
      "Gradient Descent(6114/9): loss=15.709946571583586, w0=73.8101231073881, w1=13.234208188608715\n",
      "[-0.21614318  0.94156859]\n",
      "Gradient Descent(6115/9): loss=15.549255826872436, w0=73.96142332996497, w1=12.575110178797141\n",
      "[ 3.12028074 -2.98997573]\n",
      "Gradient Descent(6116/9): loss=16.017819501130433, w0=71.77722681239679, w1=14.668093188042732\n",
      "[-0.74153268 -2.02699213]\n",
      "Gradient Descent(6117/9): loss=17.24219442518592, w0=72.29629968738368, w1=16.08698767860297\n",
      "[-1.0349209   4.09854982]\n",
      "Gradient Descent(6118/9): loss=19.28245510822557, w0=73.02074432035494, w1=13.218002801532522\n",
      "[-0.40756235 -2.81149048]\n",
      "Gradient Descent(6119/9): loss=15.457446857854595, w0=73.30603796684959, w1=15.18604613805894\n",
      "[ 1.17434827  1.44968242]\n",
      "Gradient Descent(6120/9): loss=16.841748620246346, w0=72.4839941769256, w1=14.171268444195212\n",
      "[ 0.18205679  2.34392253]\n",
      "Gradient Descent(6121/9): loss=15.953004266764044, w0=72.35655442087486, w1=12.530522671803284\n",
      "[ 0.10697077 -0.44373758]\n",
      "Gradient Descent(6122/9): loss=16.275697463268525, w0=72.28167487969151, w1=12.841138975810123\n",
      "[ 1.76832794  0.63655728]\n",
      "Gradient Descent(6123/9): loss=16.10209801863066, w0=71.04384532157303, w1=12.395548878728748\n",
      "[-3.04657218 -4.07793244]\n",
      "Gradient Descent(6124/9): loss=18.505015711328202, w0=73.17644584621081, w1=15.250101585676473\n",
      "[-2.43540144  1.96653199]\n",
      "Gradient Descent(6125/9): loss=16.95992706486713, w0=74.88122685545346, w1=13.873529192145984\n",
      "[ 0.72679064  0.5535718 ]\n",
      "Gradient Descent(6126/9): loss=16.723202036669694, w0=74.3724734103746, w1=13.486028931466338\n",
      "[-0.87949341  1.07366818]\n",
      "Gradient Descent(6127/9): loss=15.967544388033234, w0=74.9881187970392, w1=12.734461207759788\n",
      "[ 2.35978284  0.86570637]\n",
      "Gradient Descent(6128/9): loss=17.09873895465511, w0=73.33627080579257, w1=12.128466748837218\n",
      "[-0.58179816 -2.20477678]\n",
      "Gradient Descent(6129/9): loss=16.29971703158824, w0=73.74352951639325, w1=13.671810493341974\n",
      "[ 1.70935797  0.84912849]\n",
      "Gradient Descent(6130/9): loss=15.505412159293028, w0=72.54697893769, w1=13.07742055312407\n",
      "[ 0.46869237 -1.15168702]\n",
      "Gradient Descent(6131/9): loss=15.745769218675612, w0=72.21889428134723, w1=13.88360146675671\n",
      "[-0.70063476 -1.65282896]\n",
      "Gradient Descent(6132/9): loss=16.045293344019534, w0=72.70933861378272, w1=15.040581740811097\n",
      "[ 0.31655829  0.94853267]\n",
      "Gradient Descent(6133/9): loss=16.774913232709395, w0=72.48774781401497, w1=14.376608870393543\n",
      "[-2.45275761  4.57611128]\n",
      "Gradient Descent(6134/9): loss=16.11305788752151, w0=74.20467814082922, w1=11.173330975296416\n",
      "[-0.22426419 -2.09177757]\n",
      "Gradient Descent(6135/9): loss=18.46032395974811, w0=74.36166307337852, w1=12.637575275704831\n",
      "[-2.9395185   0.03286796]\n",
      "Gradient Descent(6136/9): loss=16.310520863995013, w0=76.41932602540312, w1=12.614567704249886\n",
      "[ 3.67755305  0.73582071]\n",
      "Gradient Descent(6137/9): loss=20.64420072581573, w0=73.8450388917152, w1=12.099493210585567\n",
      "[-0.82344461 -0.64155705]\n",
      "Gradient Descent(6138/9): loss=16.49025533554258, w0=74.42145012074138, w1=12.54858314639199\n",
      "[ 3.09403106 -0.45731002]\n",
      "Gradient Descent(6139/9): loss=16.45504857402867, w0=72.2556283793088, w1=12.86870015864221\n",
      "[ 1.44864316 -1.54380058]\n",
      "Gradient Descent(6140/9): loss=16.111582693322497, w0=71.24157816734235, w1=13.949360562708435\n",
      "[ 0.37580991  0.5401016 ]\n",
      "Gradient Descent(6141/9): loss=17.602230158809075, w0=70.97851122685492, w1=13.571289443092812\n",
      "[-0.61213612 -2.00288371]\n",
      "Gradient Descent(6142/9): loss=18.07064457210855, w0=71.40700651162935, w1=14.97330804198659\n",
      "[-2.3166327   1.84292924]\n",
      "Gradient Descent(6143/9): loss=18.281526821549424, w0=73.02864940266475, w1=13.683257576792512\n",
      "[-2.670267    2.22223808]\n",
      "Gradient Descent(6144/9): loss=15.441787957212238, w0=74.89783630487547, w1=12.127690922122875\n",
      "[-0.29933224  0.1651399 ]\n",
      "Gradient Descent(6145/9): loss=17.586139499771406, w0=75.10736887148552, w1=12.012092990378974\n",
      "[-0.6420086  -3.49214288]\n",
      "Gradient Descent(6146/9): loss=18.107136059960855, w0=75.55677489137715, w1=14.45659300426658\n",
      "[ 0.12733449  4.16252346]\n",
      "Gradient Descent(6147/9): loss=18.42328729138861, w0=75.46764075041628, w1=11.542826582103723\n",
      "[ 0.94108822 -1.42699294]\n",
      "Gradient Descent(6148/9): loss=19.624177870762626, w0=74.80887899320912, w1=12.541721638140688\n",
      "[ 1.63711784 -0.76084727]\n",
      "Gradient Descent(6149/9): loss=16.973348578762852, w0=73.66289650668831, w1=13.074314727680962\n",
      "[ 0.94686466 -0.12035157]\n",
      "Gradient Descent(6150/9): loss=15.536132611890906, w0=73.0000912419851, w1=13.15856082547074\n",
      "[ 0.10391516 -1.16438282]\n",
      "Gradient Descent(6151/9): loss=15.480625304773875, w0=72.92735063202355, w1=13.973628796671338\n",
      "[ 0.96524051  1.53930405]\n",
      "Gradient Descent(6152/9): loss=15.575051839679901, w0=72.2516822744779, w1=12.896115962609398\n",
      "[ 0.93678903  1.60871657]\n",
      "Gradient Descent(6153/9): loss=16.099312115038686, w0=71.59592995304067, w1=11.770014361936767\n",
      "[-4.73219974 -2.64092164]\n",
      "Gradient Descent(6154/9): loss=18.289010118671907, w0=74.90846976986693, w1=13.618659508343718\n",
      "[ 1.89904567 -0.37699294]\n",
      "Gradient Descent(6155/9): loss=16.698923260618518, w0=73.57913780164978, w1=13.882554564870317\n",
      "[ 3.68159556  0.67523098]\n",
      "Gradient Descent(6156/9): loss=15.507702785787966, w0=71.00202090913864, w1=13.409892881402738\n",
      "[-5.004676   -1.64175598]\n",
      "Gradient Descent(6157/9): loss=18.014730563831534, w0=74.50529410862774, w1=14.559122069214\n",
      "[ 2.21850389  3.73937331]\n",
      "Gradient Descent(6158/9): loss=16.702161638288658, w0=72.95234138340915, w1=11.941560751278002\n",
      "[-1.66862888 -1.19948745]\n",
      "Gradient Descent(6159/9): loss=16.62718182941535, w0=74.12038159693067, w1=12.781201964439239\n",
      "[ 1.1819488  -0.58648882]\n",
      "Gradient Descent(6160/9): loss=15.97136403850281, w0=73.29301743880002, w1=13.191744139248703\n",
      "[-2.23180604 -1.80433418]\n",
      "Gradient Descent(6161/9): loss=15.427351147622586, w0=74.85528166979275, w1=14.454778067518324\n",
      "[ 3.61167537  2.44039835]\n",
      "Gradient Descent(6162/9): loss=17.080186368639954, w0=72.3271089110909, w1=12.746499225702507\n",
      "[-2.52289536 -0.28600699]\n",
      "Gradient Descent(6163/9): loss=16.12205245044383, w0=74.09313566148786, w1=12.946704116799932\n",
      "[-0.04041465  1.08467032]\n",
      "Gradient Descent(6164/9): loss=15.847308039130718, w0=74.12142591422315, w1=12.187434890604388\n",
      "[ 5.20000016 -2.17943957]\n",
      "Gradient Descent(6165/9): loss=16.563259856975098, w0=70.48142580375726, w1=13.713042587323804\n",
      "[-2.42366679 -0.68277425]\n",
      "Gradient Descent(6166/9): loss=19.368176781684454, w0=72.1779925552799, w1=14.190984562871735\n",
      "[-1.95359765 -0.29407203]\n",
      "Gradient Descent(6167/9): loss=16.26149115392668, w0=73.54551090755213, w1=14.39683498264946\n",
      "[ 0.36714849 -1.0759702 ]\n",
      "Gradient Descent(6168/9): loss=15.838093241214956, w0=73.28850696748931, w1=15.150014125286207\n",
      "[-2.10900964  4.98985318]\n",
      "Gradient Descent(6169/9): loss=16.780856398434118, w0=74.7648137148863, w1=11.657116900357984\n",
      "[ 1.15939235 -3.1939541 ]\n",
      "Gradient Descent(6170/9): loss=18.128576325622028, w0=73.95323906972186, w1=13.892884769152086\n",
      "[-1.22102878  0.35774611]\n",
      "Gradient Descent(6171/9): loss=15.68859305551359, w0=74.80795921655834, w1=13.642462491625126\n",
      "[ 0.59271794 -0.2878367 ]\n",
      "Gradient Descent(6172/9): loss=16.545286002671457, w0=74.39305666099992, w1=13.843948184381293\n",
      "[-1.672213   -0.13548425]\n",
      "Gradient Descent(6173/9): loss=16.05627020858888, w0=75.56360576153496, w1=13.938787160345836\n",
      "[ 3.59033059 -2.51194756]\n",
      "Gradient Descent(6174/9): loss=18.06699485446973, w0=73.05037435156007, w1=15.697150454823744\n",
      "[-1.84087044  0.80604891]\n",
      "Gradient Descent(6175/9): loss=17.87406128377663, w0=74.338983656469, w1=15.132916216634758\n",
      "[ 2.11635931  0.28163384]\n",
      "Gradient Descent(6176/9): loss=17.298506171364046, w0=72.85753213753792, w1=14.935772527997338\n",
      "[-0.37151035 -0.75396399]\n",
      "Gradient Descent(6177/9): loss=16.541161423003576, w0=73.1175893827425, w1=15.46354732111167\n",
      "[-0.25299015  0.96321413]\n",
      "Gradient Descent(6178/9): loss=17.369234892853633, w0=73.29468249009746, w1=14.789297426761594\n",
      "[-0.111401    0.77624693]\n",
      "Gradient Descent(6179/9): loss=16.243394583338343, w0=73.3726631884147, w1=14.245924577205658\n",
      "[ 0.11602424  1.00430387]\n",
      "Gradient Descent(6180/9): loss=15.682528479480196, w0=73.29144621970005, w1=13.542911865603223\n",
      "[-2.30492171 -1.01083545]\n",
      "Gradient Descent(6181/9): loss=15.387888017593635, w0=74.90489141453011, w1=14.250496677458704\n",
      "[-2.83191203  0.8155813 ]\n",
      "Gradient Descent(6182/9): loss=16.980553266933512, w0=76.88722983446534, w1=13.679589768612434\n",
      "[ 3.50118496 -0.54749977]\n",
      "Gradient Descent(6183/9): loss=21.86179393212801, w0=74.4364003639226, w1=14.062839606986774\n",
      "[ 0.53640701 -0.20990553]\n",
      "Gradient Descent(6184/9): loss=16.20853492180093, w0=74.0609154554459, w1=14.209773480310051\n",
      "[ 1.13678324  0.65020366]\n",
      "Gradient Descent(6185/9): loss=15.946521912510745, w0=73.26516719091272, w1=13.754630917282807\n",
      "[ 3.12086373  0.19788448]\n",
      "Gradient Descent(6186/9): loss=15.424091374366107, w0=71.0805625824914, w1=13.616111783221271\n",
      "[-1.32220804  2.59824253]\n",
      "Gradient Descent(6187/9): loss=17.844670220125042, w0=72.0061082138185, w1=11.797342010018856\n",
      "[ 0.4501259  -1.03820635]\n",
      "Gradient Descent(6188/9): loss=17.63030516888726, w0=71.69102008280655, w1=12.524086452574217\n",
      "[-0.86300058 -2.62185482]\n",
      "Gradient Descent(6189/9): loss=17.127145659408196, w0=72.29512048841454, w1=14.359384827528855\n",
      "[-2.87270333  0.98416442]\n",
      "Gradient Descent(6190/9): loss=16.27160185980312, w0=74.3060128203774, w1=13.670469733768602\n",
      "[ 0.50634948  1.61432826]\n",
      "Gradient Descent(6191/9): loss=15.916245954563696, w0=73.95156818581722, w1=12.540439948784407\n",
      "[ 0.47486584 -1.41095545]\n",
      "Gradient Descent(6192/9): loss=16.043253521975426, w0=73.61916210106678, w1=13.528108764055753\n",
      "[-1.22079223 -0.60922823]\n",
      "Gradient Descent(6193/9): loss=15.439949532149237, w0=74.47371666282478, w1=13.954568521729309\n",
      "[ 0.89618507  1.45741152]\n",
      "Gradient Descent(6194/9): loss=16.19458974111777, w0=73.84638711526401, w1=12.934380458725766\n",
      "[ 0.30750751  1.83783249]\n",
      "Gradient Descent(6195/9): loss=15.687190201625798, w0=73.63113185645689, w1=11.647897718951702\n",
      "[ 0.17449522 -0.79246953]\n",
      "Gradient Descent(6196/9): loss=17.120515688710842, w0=73.50898520177219, w1=12.202626391577963\n",
      "[ 2.65552718 -2.16653167]\n",
      "Gradient Descent(6197/9): loss=16.224488339892595, w0=71.65011617379706, w1=13.719198562625317\n",
      "[-3.01670022  4.42170509]\n",
      "Gradient Descent(6198/9): loss=16.7656134720844, w0=73.76180632997112, w1=10.624004998549578\n",
      "[ 0.37105698 -5.38147711]\n",
      "Gradient Descent(6199/9): loss=19.572878222228525, w0=73.50206644665465, w1=14.39103897419414\n",
      "[-1.76338758 -1.66393559]\n",
      "Gradient Descent(6200/9): loss=15.82280795425757, w0=74.73643775531923, w1=15.555793886576076\n",
      "[-1.31402768  0.47053986]\n",
      "Gradient Descent(6201/9): loss=18.581370814776584, w0=75.65625713395387, w1=15.226415984654297\n",
      "[ 2.01059633  0.99193046]\n",
      "Gradient Descent(6202/9): loss=19.70168815161925, w0=74.24883970187334, w1=14.532064661567583\n",
      "[-0.66943072  3.77326423]\n",
      "Gradient Descent(6203/9): loss=16.395544379887042, w0=74.71744120504393, w1=11.890779703281693\n",
      "[-0.88997005 -0.36201303]\n",
      "Gradient Descent(6204/9): loss=17.66144494234257, w0=75.34042023904216, w1=12.144188827494153\n",
      "[ 2.16989451 -0.67005106]\n",
      "Gradient Descent(6205/9): loss=18.37177703881056, w0=73.82149408313316, w1=12.613224570965018\n",
      "[-2.90404765 -2.51090408]\n",
      "Gradient Descent(6206/9): loss=15.900454628419958, w0=75.85432743806194, w1=14.370857423529714\n",
      "[ 4.6274901  -0.07280525]\n",
      "Gradient Descent(6207/9): loss=19.060795562368416, w0=72.61508436516253, w1=14.421821096277625\n",
      "[ 0.2171665   0.62673243]\n",
      "Gradient Descent(6208/9): loss=16.060082502331824, w0=72.46306781192392, w1=13.98310839380152\n",
      "[ 1.31819531 -2.10905791]\n",
      "Gradient Descent(6209/9): loss=15.857750957174652, w0=71.54033109752201, w1=15.459448928216332\n",
      "[-2.63900374  0.53276731]\n",
      "Gradient Descent(6210/9): loss=18.883106690455755, w0=73.38763371856449, w1=15.08651181113292\n",
      "[-0.94783653  3.23513356]\n",
      "Gradient Descent(6211/9): loss=16.681180929318444, w0=74.05111928947163, w1=12.821918320168681\n",
      "[-2.00566588 -2.37771319]\n",
      "Gradient Descent(6212/9): loss=15.888908283573103, w0=75.45508540316797, w1=14.486317554167348\n",
      "[-2.66238054  0.64032018]\n",
      "Gradient Descent(6213/9): loss=18.227828424854, w0=77.31875178188395, w1=14.038093428662858\n",
      "[ 5.38786085 -4.15181456]\n",
      "Gradient Descent(6214/9): loss=23.641409913974435, w0=73.54724918417226, w1=16.94436362356497\n",
      "[ 2.13389765  1.92230784]\n",
      "Gradient Descent(6215/9): loss=21.4198791286667, w0=72.05352082892296, w1=15.598748138012283\n",
      "[-2.64503325  5.38400749]\n",
      "Gradient Descent(6216/9): loss=18.40034155938892, w0=73.90504410087357, w1=11.829942893493255\n",
      "[-0.02350835 -0.91763105]\n",
      "Gradient Descent(6217/9): loss=16.93349274865455, w0=73.92149994847928, w1=12.47228462679954\n",
      "[ 3.14103946 -0.64368164]\n",
      "Gradient Descent(6218/9): loss=16.09027030257372, w0=71.72277232610705, w1=12.922861777273695\n",
      "[-1.93208663 -0.60662305]\n",
      "Gradient Descent(6219/9): loss=16.775184848522944, w0=73.07523296873447, w1=13.34749791441136\n",
      "[-0.91961992 -1.13517419]\n",
      "Gradient Descent(6220/9): loss=15.418540655213505, w0=73.71896691540181, w1=14.142119850222148\n",
      "[-0.60193854  1.41972854]\n",
      "Gradient Descent(6221/9): loss=15.695611249866962, w0=74.14032389035967, w1=13.14830987528613\n",
      "[ 3.4762246 -3.0796354]\n",
      "Gradient Descent(6222/9): loss=15.798999775338597, w0=71.70696666988485, w1=15.304054655823151\n",
      "[ 0.59993732  1.2991177 ]\n",
      "Gradient Descent(6223/9): loss=18.309213751419644, w0=71.28701054819325, w1=14.394672264364617\n",
      "[-3.01022777  2.40414799]\n",
      "Gradient Descent(6224/9): loss=17.81831040543635, w0=73.39416998467638, w1=12.711768673935978\n",
      "[-2.81035437 -2.18333143]\n",
      "Gradient Descent(6225/9): loss=15.685781507904364, w0=75.36141804383026, w1=14.240100671956842\n",
      "[ 1.56059337  2.32584863]\n",
      "Gradient Descent(6226/9): loss=17.8122529455633, w0=74.26900268288122, w1=12.612006627529633\n",
      "[ 1.77772209 -4.61130151]\n",
      "Gradient Descent(6227/9): loss=16.237735719990123, w0=73.02459722285593, w1=15.839917684402183\n",
      "[-1.0061015   2.27524041]\n",
      "Gradient Descent(6228/9): loss=18.20744019686689, w0=73.72886827599356, w1=14.24724940020616\n",
      "[-0.86039679 -0.35206904]\n",
      "Gradient Descent(6229/9): loss=15.775033495901438, w0=74.33114602635861, w1=14.493697729365737\n",
      "[ 1.9500397  -0.43779556]\n",
      "Gradient Descent(6230/9): loss=16.43788779567972, w0=72.96611823343814, w1=14.800154622346966\n",
      "[-1.00622984  0.62674201]\n",
      "Gradient Descent(6231/9): loss=16.311399309282844, w0=73.67047912081155, w1=14.361435216019142\n",
      "[-1.64144384 -0.33873899]\n",
      "Gradient Descent(6232/9): loss=15.845503031947338, w0=74.81948980742331, w1=14.598552506739154\n",
      "[ 1.20880178  3.33327562]\n",
      "Gradient Descent(6233/9): loss=17.17546798621786, w0=73.97332856373977, w1=12.265259569554761\n",
      "[-0.6520867  -0.38147415]\n",
      "Gradient Descent(6234/9): loss=16.35413238800624, w0=74.42978925105132, w1=12.532291471397476\n",
      "[ 2.72110504  0.5645905 ]\n",
      "Gradient Descent(6235/9): loss=16.479788313570015, w0=72.52501572493638, w1=12.137078124148044\n",
      "[-0.09637516 -1.43329955]\n",
      "Gradient Descent(6236/9): loss=16.58282974668795, w0=72.59247833795081, w1=13.14038780802667\n",
      "[ 1.24903337 -0.80210976]\n",
      "Gradient Descent(6237/9): loss=15.689470077052139, w0=71.71815498092498, w1=13.701864636908573\n",
      "[-0.86972601  1.00080613]\n",
      "Gradient Descent(6238/9): loss=16.65208452175782, w0=72.3269631892516, w1=13.00130034503106\n",
      "[-2.92355559 -1.9267417 ]\n",
      "Gradient Descent(6239/9): loss=15.967831605615997, w0=74.37345210333261, w1=14.350019532020658\n",
      "[ 3.01564133  1.23716595]\n",
      "Gradient Descent(6240/9): loss=16.347297710129233, w0=72.26250317109672, w1=13.484003364923565\n",
      "[-2.68263307 -0.22895161]\n",
      "Gradient Descent(6241/9): loss=15.917809477348683, w0=74.14034632214617, w1=13.64426949533513\n",
      "[ 1.07794107 -0.25497886]\n",
      "Gradient Descent(6242/9): loss=15.757644446662688, w0=73.38578757632695, w1=13.822754696186871\n",
      "[-0.36745966  0.53655649]\n",
      "Gradient Descent(6243/9): loss=15.448946507176803, w0=73.64300933998985, w1=13.44716515454238\n",
      "[-0.89186236 -0.42971375]\n",
      "Gradient Descent(6244/9): loss=15.447348516297335, w0=74.26731299232819, w1=13.747964782437032\n",
      "[ 1.28807826 -0.70350021]\n",
      "Gradient Descent(6245/9): loss=15.895612539708731, w0=73.36565821003582, w1=14.240414931903267\n",
      "[ 0.07511138 -0.44150417]\n",
      "Gradient Descent(6246/9): loss=15.677795054999297, w0=73.31308024075611, w1=14.549467852505419\n",
      "[-1.25968864 -2.21929275]\n",
      "Gradient Descent(6247/9): loss=15.958259714536316, w0=74.19486228826123, w1=16.10297277449518\n",
      "[ 1.0531487   2.91624813]\n",
      "Gradient Descent(6248/9): loss=19.232481972851772, w0=73.45765819625694, w1=14.061599082957425\n",
      "[-0.18260609 -1.63023357]\n",
      "Gradient Descent(6249/9): loss=15.568588675008987, w0=73.58548245643303, w1=15.202762582693886\n",
      "[-0.84187598  3.65409543]\n",
      "Gradient Descent(6250/9): loss=16.91284252384616, w0=74.17479564096865, w1=12.644895784046073\n",
      "[ 2.29696147 -0.59642109]\n",
      "Gradient Descent(6251/9): loss=16.122316472997504, w0=72.56692261185047, w1=13.062390546642177\n",
      "[ 0.09780125 -0.99937352]\n",
      "Gradient Descent(6252/9): loss=15.737230704791466, w0=72.49846174016187, w1=13.761952014039043\n",
      "[-1.95176836 -0.04728186]\n",
      "Gradient Descent(6253/9): loss=15.742095972986025, w0=73.86469959302693, w1=13.795049315866967\n",
      "[ 0.99744542 -0.51610044]\n",
      "Gradient Descent(6254/9): loss=15.598500072199519, w0=73.16648780153521, w1=14.156319626055799\n",
      "[ 1.33562923  1.13779924]\n",
      "Gradient Descent(6255/9): loss=15.622906252068473, w0=72.23154734307813, w1=13.35986015541504\n",
      "[-4.50806806 -0.81351389]\n",
      "Gradient Descent(6256/9): loss=15.95739011136037, w0=75.38719498339115, w1=13.92931987808945\n",
      "[-2.24406656  3.78296276]\n",
      "Gradient Descent(6257/9): loss=17.677857182365955, w0=76.95804157730507, w1=11.281245944984196\n",
      "[ 3.44762341 -2.23165023]\n",
      "Gradient Descent(6258/9): loss=24.51540145334801, w0=74.54470519326495, w1=12.843401105949544\n",
      "[ 0.98009775  1.01731202]\n",
      "Gradient Descent(6259/9): loss=16.3705632182053, w0=73.85863676809673, w1=12.131282692829101\n",
      "[ 2.73476141 -0.06897989]\n",
      "Gradient Descent(6260/9): loss=16.454470637064613, w0=71.94430377930156, w1=12.179568618407838\n",
      "[ 0.27088292 -1.93984812]\n",
      "Gradient Descent(6261/9): loss=17.141809514388385, w0=71.75468573270635, w1=13.537462302181304\n",
      "[-4.30717924 -1.52296989]\n",
      "Gradient Descent(6262/9): loss=16.57217953892619, w0=74.76971120179209, w1=14.603541223554258\n",
      "[ 2.6388546   4.39178078]\n",
      "Gradient Descent(6263/9): loss=17.106360322789623, w0=72.92251298215893, w1=11.529294677541635\n",
      "[ 1.99237837 -1.0694697 ]\n",
      "Gradient Descent(6264/9): loss=17.356924913161215, w0=71.52784812004543, w1=12.277923470899884\n",
      "[ 3.46670711 -2.02824362]\n",
      "Gradient Descent(6265/9): loss=17.667544704379463, w0=69.10115314339103, w1=13.697694004198375\n",
      "[-6.3867672   1.93742812]\n",
      "Gradient Descent(6266/9): loss=24.199301202388494, w0=73.57189018269052, w1=12.341494320713668\n",
      "[ 2.36155547 -0.6185987 ]\n",
      "Gradient Descent(6267/9): loss=16.072291261370662, w0=71.918801355463, w1=12.774513409330215\n",
      "[-2.39492983 -0.74341659]\n",
      "Gradient Descent(6268/9): loss=16.580019098135292, w0=73.59525223357056, w1=13.294905024270685\n",
      "[-0.24385061 -2.2819844 ]\n",
      "Gradient Descent(6269/9): loss=15.448364712555101, w0=73.76594766268866, w1=14.89229410110669\n",
      "[ 2.07407273  2.4050535 ]\n",
      "Gradient Descent(6270/9): loss=16.494985462679878, w0=72.31409675135266, w1=13.20875665311065\n",
      "[-0.5910718   1.90999568]\n",
      "Gradient Descent(6271/9): loss=15.90262514770219, w0=72.72784701328989, w1=11.87175968002189\n",
      "[-1.72809068 -1.19590124]\n",
      "Gradient Descent(6272/9): loss=16.838864346413757, w0=73.93751048621435, w1=12.708890547081442\n",
      "[ 1.21664953 -0.33802755]\n",
      "Gradient Descent(6273/9): loss=15.890074128707088, w0=73.08585581752824, w1=12.945509835402973\n",
      "[ 0.15915186 -2.98015217]\n",
      "Gradient Descent(6274/9): loss=15.550219846113864, w0=72.97444951340945, w1=15.031616355650662\n",
      "[-0.07821822  1.37617627]\n",
      "Gradient Descent(6275/9): loss=16.64112209382857, w0=73.02920226602, w1=14.068292965221039\n",
      "[-0.06076558 -1.20470838]\n",
      "Gradient Descent(6276/9): loss=15.594139658449992, w0=73.0717381712599, w1=14.911588830645206\n",
      "[-2.30751515  0.44178703]\n",
      "Gradient Descent(6277/9): loss=16.435705702392575, w0=74.68699877665648, w1=14.602337911504767\n",
      "[ 2.02779879  2.75682007]\n",
      "Gradient Descent(6278/9): loss=16.986363298987584, w0=73.26753962360073, w1=12.672563865111144\n",
      "[ 3.65615508 -2.37883252]\n",
      "Gradient Descent(6279/9): loss=15.711980290705151, w0=70.70823106541992, w1=14.337746629958726\n",
      "[-0.45462019  2.43771871]\n",
      "Gradient Descent(6280/9): loss=19.0968980187262, w0=71.02646519670023, w1=12.631343532271703\n",
      "[-1.99696438 -1.90760583]\n",
      "Gradient Descent(6281/9): loss=18.316432948566945, w0=72.42434026250254, w1=13.96666761200428\n",
      "[ 1.68121215  0.76726547]\n",
      "Gradient Descent(6282/9): loss=15.882536741965556, w0=71.24749175784397, w1=13.429581780269311\n",
      "[-0.86866597  0.2966434 ]\n",
      "Gradient Descent(6283/9): loss=17.481082782414223, w0=71.85555793345506, w1=13.221931397202763\n",
      "[ 3.48129992 -2.60282703]\n",
      "Gradient Descent(6284/9): loss=16.453558997542366, w0=69.41864798670117, w1=15.043910316667217\n",
      "[-2.13050604  6.70735867]\n",
      "Gradient Descent(6285/9): loss=24.118119722585426, w0=70.9100022150887, w1=10.348759245290697\n",
      "[-6.33859142 -2.91121531]\n",
      "Gradient Descent(6286/9): loss=23.128858582334917, w0=75.34701620934483, w1=12.38660996336199\n",
      "[ 2.91580547  0.98671791]\n",
      "Gradient Descent(6287/9): loss=18.09092228746848, w0=73.30595237685844, w1=11.695907429581162\n",
      "[ 1.27989922 -0.49084018]\n",
      "Gradient Descent(6288/9): loss=16.976940382446866, w0=72.41002291955868, w1=12.039495557575801\n",
      "[ 0.88410865 -3.72249706]\n",
      "Gradient Descent(6289/9): loss=16.813638989885654, w0=71.79114686298449, w1=14.645243496134857\n",
      "[ 0.33202594  2.33590761]\n",
      "Gradient Descent(6290/9): loss=17.194285755456857, w0=71.55872870459872, w1=13.010108165684457\n",
      "[-0.4913394 -1.8754105]\n",
      "Gradient Descent(6291/9): loss=17.00159984355963, w0=71.9026662880966, w1=14.32289551559893\n",
      "[-3.42470356  1.69488354]\n",
      "Gradient Descent(6292/9): loss=16.70916295342356, w0=74.29995878053288, w1=13.136477035294941\n",
      "[ 0.32988502 -1.17601537]\n",
      "Gradient Descent(6293/9): loss=15.950848138405563, w0=74.06903926749774, w1=13.959687792546617\n",
      "[ 4.487857   -1.04237831]\n",
      "Gradient Descent(6294/9): loss=15.801479428315474, w0=70.92753936930934, w1=14.689352607535785\n",
      "[-2.16239498 -0.43445273]\n",
      "Gradient Descent(6295/9): loss=18.917385924747748, w0=72.44121585567571, w1=14.993469517559332\n",
      "[-2.64731344  0.40076933]\n",
      "Gradient Descent(6296/9): loss=16.89517200742456, w0=74.29433526341887, w1=14.712930988579982\n",
      "[ 1.38296791  1.17338682]\n",
      "Gradient Descent(6297/9): loss=16.646715215995997, w0=73.32625772446964, w1=13.891560213535513\n",
      "[-1.37192763 -2.31413757]\n",
      "Gradient Descent(6298/9): loss=15.471219964646645, w0=74.28660706459908, w1=15.511456513339843\n",
      "[ 2.56582719  1.1649855 ]\n",
      "Gradient Descent(6299/9): loss=17.94259168543541, w0=72.49052803078142, w1=14.695966665406589\n",
      "[-0.60908293 -0.26313682]\n",
      "Gradient Descent(6300/9): loss=16.44824598191337, w0=72.91688608007732, w1=14.880162437560697\n",
      "[-3.06442874  0.6870123 ]\n",
      "Gradient Descent(6301/9): loss=16.437596016930566, w0=75.06198620116709, w1=14.399253828737992\n",
      "[ 2.29423977  1.80540044]\n",
      "Gradient Descent(6302/9): loss=17.37169156224047, w0=73.45601836007738, w1=13.13547352357979\n",
      "[-3.5319637 -3.8931427]\n",
      "Gradient Descent(6303/9): loss=15.458275697527442, w0=75.92839295032852, w1=15.860673412867158\n",
      "[ 4.49697747  5.92918713]\n",
      "Gradient Descent(6304/9): loss=21.69059404643489, w0=72.78050872266134, w1=11.710242424742216\n",
      "[ 2.2198426  -0.90354604]\n",
      "Gradient Descent(6305/9): loss=17.08319652516551, w0=71.2266189031687, w1=12.342724654914528\n",
      "[-2.24006755  0.07572802]\n",
      "Gradient Descent(6306/9): loss=18.169129526284998, w0=72.79466618592815, w1=12.289715041685858\n",
      "[-2.57127404  1.70613033]\n",
      "Gradient Descent(6307/9): loss=16.218562951856896, w0=74.59455801637014, w1=11.095423809755527\n",
      "[ 0.98285916  0.18209396]\n",
      "Gradient Descent(6308/9): loss=19.074131013839896, w0=73.90655660293598, w1=10.967958036507424\n",
      "[ 1.55013848 -4.03797248]\n",
      "Gradient Descent(6309/9): loss=18.728003525042848, w0=72.82145966841196, w1=13.794538770472647\n",
      "[-2.33444122 -1.57798007]\n",
      "Gradient Descent(6310/9): loss=15.54705600796584, w0=74.45556852023782, w1=14.89912481833618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56092799 -2.63347077]\n",
      "Gradient Descent(6311/9): loss=17.06796494237382, w0=74.06291893061709, w1=16.7425543552639\n",
      "[ 1.29993097  3.26203663]\n",
      "Gradient Descent(6312/9): loss=21.0046347052112, w0=73.15296724949646, w1=14.459128716560429\n",
      "[ 0.31897341 -2.14992601]\n",
      "Gradient Descent(6313/9): loss=15.875450116274447, w0=72.9296858632496, w1=15.964076923499855\n",
      "[-2.157395    3.17942815]\n",
      "Gradient Descent(6314/9): loss=18.538255307140393, w0=74.4398623603394, w1=13.738477215411198\n",
      "[-1.04319616  0.67372388]\n",
      "Gradient Descent(6315/9): loss=16.075957126937844, w0=75.17009967408691, w1=13.26687049972371\n",
      "[-0.08842073  1.47894669]\n",
      "Gradient Descent(6316/9): loss=17.168560041954525, w0=75.23199418296235, w1=12.231607813819583\n",
      "[ 4.98360874 -2.67373757]\n",
      "Gradient Descent(6317/9): loss=18.042832330627913, w0=71.74346806173591, w1=14.103224114324295\n",
      "[ 0.6660182   0.38906268]\n",
      "Gradient Descent(6318/9): loss=16.782224986566444, w0=71.2772553184346, w1=13.830880240501713\n",
      "[-0.68604376  2.74639944]\n",
      "Gradient Descent(6319/9): loss=17.481019539157163, w0=71.75748594747257, w1=11.908400631791352\n",
      "[-2.60104979  0.34684492]\n",
      "Gradient Descent(6320/9): loss=17.80071613525111, w0=73.57822080312484, w1=11.665609188210722\n",
      "[-0.66617119 -2.02654778]\n",
      "Gradient Descent(6321/9): loss=17.07178606794584, w0=74.04454063865188, w1=13.084192634795475\n",
      "[ 2.16935924  0.13464256]\n",
      "Gradient Descent(6322/9): loss=15.74581999376759, w0=72.52598916948865, w1=12.989942843974033\n",
      "[ 1.34918029 -0.0767996 ]\n",
      "Gradient Descent(6323/9): loss=15.800685412676138, w0=71.5815629644724, w1=13.0437025631586\n",
      "[-0.58858003 -0.6685416 ]\n",
      "Gradient Descent(6324/9): loss=16.94702690987755, w0=71.99356898770341, w1=13.511681680037402\n",
      "[-2.18522057 -2.42208275]\n",
      "Gradient Descent(6325/9): loss=16.231857866175776, w0=73.523223387647, w1=15.207139602530486\n",
      "[-1.4897453   4.51644872]\n",
      "Gradient Descent(6326/9): loss=16.90417974111521, w0=74.56604509944782, w1=12.045625500312\n",
      "[ 0.16828921 -2.21787698]\n",
      "Gradient Descent(6327/9): loss=17.223339124331414, w0=74.44824265570827, w1=13.598139383289526\n",
      "[-2.70009308 -0.6593305 ]\n",
      "Gradient Descent(6328/9): loss=16.0591284255386, w0=76.33830781463453, w1=14.05967072985904\n",
      "[ 3.92768048  0.85738455]\n",
      "Gradient Descent(6329/9): loss=20.188206168488627, w0=73.58893147868835, w1=13.459501546141096\n",
      "[ 1.86411855 -1.13733236]\n",
      "Gradient Descent(6330/9): loss=15.429607404480349, w0=72.28404849456096, w1=14.255634200902291\n",
      "[-1.4350854   1.10507607]\n",
      "Gradient Descent(6331/9): loss=16.1968374128582, w0=73.28860827156215, w1=13.48208095150259\n",
      "[ 2.07766452  0.10868293]\n",
      "Gradient Descent(6332/9): loss=15.385904791630779, w0=71.8342431089067, w1=13.40600290043921\n",
      "[-0.84542789 -1.78176289]\n",
      "Gradient Descent(6333/9): loss=16.453935652195753, w0=72.42604263033482, w1=14.653236922321401\n",
      "[-2.93817321  1.53325374]\n",
      "Gradient Descent(6334/9): loss=16.451075031985997, w0=74.48276388020807, w1=13.579959304436782\n",
      "[-1.08118683 -2.34002839]\n",
      "Gradient Descent(6335/9): loss=16.097585091812025, w0=75.23959466194883, w1=15.217979175756868\n",
      "[ 1.42342415  1.94576662]\n",
      "Gradient Descent(6336/9): loss=18.789494549490698, w0=74.24319775677661, w1=13.855942542443655\n",
      "[ 4.22145998  0.51826803]\n",
      "Gradient Descent(6337/9): loss=15.90722464491055, w0=71.28817577347463, w1=13.493154923226369\n",
      "[-2.40002198 -1.12012343]\n",
      "Gradient Descent(6338/9): loss=17.397487185907252, w0=72.96819116199998, w1=14.277241325125491\n",
      "[ 0.4548985   0.30862494]\n",
      "Gradient Descent(6339/9): loss=15.756964324228354, w0=72.649762213415, w1=14.06120386941403\n",
      "[ 1.57185401 -0.35197378]\n",
      "Gradient Descent(6340/9): loss=15.762424929666903, w0=71.5494644086502, w1=14.30758551778038\n",
      "[-1.79725942  3.59169261]\n",
      "Gradient Descent(6341/9): loss=17.250140937116008, w0=72.80754600214823, w1=11.793400688951474\n",
      "[-2.57516507 -0.53497   ]\n",
      "Gradient Descent(6342/9): loss=16.925992327908613, w0=74.61016154988486, w1=12.167879690534473\n",
      "[ 0.03045253  1.68075299]\n",
      "Gradient Descent(6343/9): loss=17.11258371711064, w0=74.58884477981734, w1=10.991352597712048\n",
      "[ 2.2720509  -6.34264287]\n",
      "Gradient Descent(6344/9): loss=19.320267708834884, w0=72.99840914669188, w1=15.431202608344964\n",
      "[-1.72214368  0.00715588]\n",
      "Gradient Descent(6345/9): loss=17.333708741039015, w0=74.20390972017269, w1=15.426193492829116\n",
      "[ 2.07967426  1.56274949]\n",
      "Gradient Descent(6346/9): loss=17.694320946611338, w0=72.7481377364089, w1=14.332268852504232\n",
      "[ 3.01091858  1.63001533]\n",
      "Gradient Descent(6347/9): loss=15.89825432369338, w0=70.6404947311409, w1=13.191258120693563\n",
      "[-1.86397068  0.39008565]\n",
      "Gradient Descent(6348/9): loss=18.947828955695726, w0=71.94527420790624, w1=12.918198164008203\n",
      "[-0.92083326  1.82453856]\n",
      "Gradient Descent(6349/9): loss=16.452962443485823, w0=72.58985748944126, w1=11.64102117091845\n",
      "[ 0.37224618 -0.9243811 ]\n",
      "Gradient Descent(6350/9): loss=17.324134070110517, w0=72.32928516035594, w1=12.288087944285925\n",
      "[-3.24567732  2.52765823]\n",
      "Gradient Descent(6351/9): loss=16.561134450481124, w0=74.60125928415347, w1=10.518727182956946\n",
      "[ 1.34217394 -1.36355924]\n",
      "Gradient Descent(6352/9): loss=20.62417008472191, w0=73.66173752794361, w1=11.47321865266445\n",
      "[ 0.64740097 -1.29847923]\n",
      "Gradient Descent(6353/9): loss=17.466540648606937, w0=73.20855685022822, w1=12.38215411217798\n",
      "[ 0.43018254 -1.2114797 ]\n",
      "Gradient Descent(6354/9): loss=15.991848609392813, w0=72.90742907068638, w1=13.23018990444454\n",
      "[ 0.58133835 -0.14685573]\n",
      "Gradient Descent(6355/9): loss=15.491707008472423, w0=72.50049222509284, w1=13.332988917741549\n",
      "[-0.74546718  1.26429204]\n",
      "Gradient Descent(6356/9): loss=15.711417169611071, w0=73.0223192497481, w1=12.447984488555843\n",
      "[-2.79671088 -1.82535392]\n",
      "Gradient Descent(6357/9): loss=15.955003174099014, w0=74.98001686652437, w1=13.725732229608683\n",
      "[ 5.05543059  0.25166627]\n",
      "Gradient Descent(6358/9): loss=16.837608684412103, w0=71.44121545046012, w1=13.549565838586952\n",
      "[ 2.71343051  0.01014747]\n",
      "Gradient Descent(6359/9): loss=17.10458840108079, w0=69.54181409584592, w1=13.542462608489398\n",
      "[-2.60642235 -1.96315732]\n",
      "Gradient Descent(6360/9): loss=22.427013531073218, w0=71.36630974272528, w1=14.91667273218877\n",
      "[-2.87267957  1.41851262]\n",
      "Gradient Descent(6361/9): loss=18.27615982794942, w0=73.37718544050264, w1=13.923713900170418\n",
      "[-2.59304579 -0.09688021]\n",
      "Gradient Descent(6362/9): loss=15.487922919457889, w0=75.19231749218518, w1=13.991530048608901\n",
      "[ 2.16586568  1.98483072]\n",
      "Gradient Descent(6363/9): loss=17.31881922201319, w0=73.67621151447278, w1=12.602148546662995\n",
      "[ 4.05971273  1.76457594]\n",
      "Gradient Descent(6364/9): loss=15.844019693509495, w0=70.83441260315354, w1=11.36694539160887\n",
      "[-1.21962281 -1.31794579]\n",
      "Gradient Descent(6365/9): loss=20.64237340039186, w0=71.6881485699484, w1=12.289507442681561\n",
      "[-1.4787613  -0.35748142]\n",
      "Gradient Descent(6366/9): loss=17.383435988396535, w0=72.7232814806619, w1=12.539744437946707\n",
      "[-2.61180563 -2.28080831]\n",
      "Gradient Descent(6367/9): loss=15.990473088917831, w0=74.55154542118973, w1=14.136310258083764\n",
      "[ 2.5480791 -1.8306136]\n",
      "Gradient Descent(6368/9): loss=16.392256551590705, w0=72.76789005317485, w1=15.417739781472124\n",
      "[-2.07217592  1.38649931]\n",
      "Gradient Descent(6369/9): loss=17.402217672335233, w0=74.21841319763604, w1=14.447190263788528\n",
      "[ 2.77003697 -1.30234255]\n",
      "Gradient Descent(6370/9): loss=16.28123652874571, w0=72.27938731934671, w1=15.358830049329194\n",
      "[ 0.35712322  1.09692181]\n",
      "Gradient Descent(6371/9): loss=17.666069684351026, w0=72.0294010673399, w1=14.590984783607011\n",
      "[-0.9550354   0.71559671]\n",
      "Gradient Descent(6372/9): loss=16.802857582460675, w0=72.69792584878795, w1=14.090067087131244\n",
      "[-1.72805551  0.4768961 ]\n",
      "Gradient Descent(6373/9): loss=15.749759976909688, w0=73.90756470395732, w1=13.756239817484378\n",
      "[-1.83093738 -2.74226681]\n",
      "Gradient Descent(6374/9): loss=15.612400248232449, w0=75.1892208672991, w1=15.67582658615862\n",
      "[ 0.32664852  6.812639  ]\n",
      "Gradient Descent(6375/9): loss=19.593425445515678, w0=74.96056690182314, w1=10.906979284803814\n",
      "[ 3.82037467 -4.5037396 ]\n",
      "Gradient Descent(6376/9): loss=20.08421841073836, w0=72.28630463217145, w1=14.0595970049039\n",
      "[-1.13706519 -0.24785352]\n",
      "Gradient Descent(6377/9): loss=16.06166730813816, w0=73.08225026494802, w1=14.233094471906616\n",
      "[-0.74341203 -0.54415966]\n",
      "Gradient Descent(6378/9): loss=15.692082577759999, w0=73.60263868595737, w1=14.614006231443444\n",
      "[ 0.50390892  1.24139385]\n",
      "Gradient Descent(6379/9): loss=16.076852072611207, w0=73.24990244012476, w1=13.745030533124368\n",
      "[-0.56949992  1.20088116]\n",
      "Gradient Descent(6380/9): loss=15.422053576346947, w0=73.64855238247561, w1=12.904413718917782\n",
      "[ 0.90799703 -1.33181839]\n",
      "Gradient Descent(6381/9): loss=15.614253528526861, w0=73.01295445961443, w1=13.836686592774427\n",
      "[-2.18566037 -1.89938278]\n",
      "Gradient Descent(6382/9): loss=15.489074523459337, w0=74.54291671961718, w1=15.166254540117144\n",
      "[ 2.31818367  0.61328987]\n",
      "Gradient Descent(6383/9): loss=17.588093907200783, w0=72.92018814971243, w1=14.736951628482727\n",
      "[-4.43187934  0.14046481]\n",
      "Gradient Descent(6384/9): loss=16.246051559869883, w0=76.02250368468043, w1=14.638626261161484\n",
      "[ 3.74152962 -1.89385672]\n",
      "Gradient Descent(6385/9): loss=19.78000749631878, w0=73.40343294835723, w1=15.964325964773046\n",
      "[ 0.49350609 -0.68407982]\n",
      "Gradient Descent(6386/9): loss=18.478536388696757, w0=73.0579786876082, w1=16.443181840463083\n",
      "[-1.70519722  5.26404283]\n",
      "Gradient Descent(6387/9): loss=19.80479795124763, w0=74.25161674051006, w1=12.758351861458998\n",
      "[ 4.3103367  -2.90580725]\n",
      "Gradient Descent(6388/9): loss=16.10465801333539, w0=71.2343810510645, w1=14.792416935484056\n",
      "[-2.61977997  2.89731376]\n",
      "Gradient Descent(6389/9): loss=18.368338886146113, w0=73.06822702822218, w1=12.764297301127675\n",
      "[-3.59886499  0.4345572 ]\n",
      "Gradient Descent(6390/9): loss=15.667266386326368, w0=75.58743251778262, w1=12.460107263370226\n",
      "[-0.64900654 -0.67383035]\n",
      "Gradient Descent(6391/9): loss=18.535780464586797, w0=76.04173709854685, w1=12.931788506875352\n",
      "[ 1.11328811  0.87714298]\n",
      "Gradient Descent(6392/9): loss=19.31124208644551, w0=75.26243541809842, w1=12.3177884242424\n",
      "[ 3.80462573 -1.75395575]\n",
      "Gradient Descent(6393/9): loss=17.99844410667686, w0=72.59919740871847, w1=13.54555745152339\n",
      "[-0.63133283  2.83329674]\n",
      "Gradient Descent(6394/9): loss=15.629376782258777, w0=73.04113038931195, w1=11.562249732582554\n",
      "[-4.22595669  1.38316947]\n",
      "Gradient Descent(6395/9): loss=17.256171276138712, w0=75.99930007222804, w1=10.594031103578788\n",
      "[ 2.1048941  -1.00085088]\n",
      "Gradient Descent(6396/9): loss=23.209001493205076, w0=74.52587419986507, w1=11.294626719416916\n",
      "[ 0.31059584 -0.79760585]\n",
      "Gradient Descent(6397/9): loss=18.532040769810795, w0=74.30845711125005, w1=11.852950813335484\n",
      "[ 2.49786221 -0.7821664 ]\n",
      "Gradient Descent(6398/9): loss=17.223705299515654, w0=72.55995356733249, w1=12.400467296123766\n",
      "[-0.32895516 -1.5180017 ]\n",
      "Gradient Descent(6399/9): loss=16.237627735332918, w0=72.7902221812599, w1=13.463068485192032\n",
      "[-2.87204396  2.13667957]\n",
      "Gradient Descent(6400/9): loss=15.512883134121608, w0=74.80065295118412, w1=11.967392788222885\n",
      "[ 2.3368808  -3.96566489]\n",
      "Gradient Descent(6401/9): loss=17.664562302283016, w0=73.16483639154816, w1=14.743358211742791\n",
      "[ 0.70172217  2.11386704]\n",
      "Gradient Descent(6402/9): loss=16.192619740809626, w0=72.67363087255401, w1=13.2636512848756\n",
      "[-3.46414275 -0.70958525]\n",
      "Gradient Descent(6403/9): loss=15.60160962182351, w0=75.09853079446937, w1=13.760360956614521\n",
      "[ 1.26655475  0.59997641]\n",
      "Gradient Descent(6404/9): loss=17.053576111913735, w0=74.21194246929579, w1=13.340377467464025\n",
      "[ 3.58279612 -0.70573242]\n",
      "Gradient Descent(6405/9): loss=15.816975774507428, w0=71.70398518759852, w1=13.834390159431193\n",
      "[ 1.23746902 -1.18491384]\n",
      "Gradient Descent(6406/9): loss=16.71273554999894, w0=70.8377568762926, w1=14.663829844539887\n",
      "[-2.49861236  1.19796198]\n",
      "Gradient Descent(6407/9): loss=19.103328451259078, w0=72.58678552714458, w1=13.825256458811484\n",
      "[-0.50101071  2.76638885]\n",
      "Gradient Descent(6408/9): loss=15.695609202138957, w0=72.93749302412456, w1=11.888784264504695\n",
      "[ 1.06502415 -4.80748699]\n",
      "Gradient Descent(6409/9): loss=16.7149348988219, w0=72.19197611798667, w1=15.25402515712977\n",
      "[-2.14771286  2.04825774]\n",
      "Gradient Descent(6410/9): loss=17.56712305256748, w0=73.69537512260851, w1=13.820244735968616\n",
      "[ 1.82880438  2.66959539]\n",
      "Gradient Descent(6411/9): loss=15.524451296815547, w0=72.41521205753702, w1=11.951527961704196\n",
      "[-2.05703409 -4.3956279 ]\n",
      "Gradient Descent(6412/9): loss=16.93962734436535, w0=73.85513591747402, w1=15.028467492952668\n",
      "[-1.30658856  0.74921388]\n",
      "Gradient Descent(6413/9): loss=16.742689513015154, w0=74.76974790860093, w1=14.504017780220988\n",
      "[ 3.74102162  3.29296475]\n",
      "Gradient Descent(6414/9): loss=16.99951964210675, w0=72.151032774754, w1=12.198942454352363\n",
      "[-0.00295658 -1.64499814]\n",
      "Gradient Descent(6415/9): loss=16.859171633477146, w0=72.15310238422113, w1=13.350441151789372\n",
      "[-0.06180124 -1.26438752]\n",
      "Gradient Descent(6416/9): loss=16.044978101434012, w0=72.19636325565503, w1=14.23551241408733\n",
      "[ 1.02484098 -0.34282381]\n",
      "Gradient Descent(6417/9): loss=16.273822273986507, w0=71.47897456895429, w1=14.475489083807927\n",
      "[-1.04174224  1.86022638]\n",
      "Gradient Descent(6418/9): loss=17.52869052854641, w0=72.20819413387218, w1=13.173330618545545\n",
      "[-3.27848284 -1.10585983]\n",
      "Gradient Descent(6419/9): loss=16.022225279481905, w0=74.50313212400198, w1=13.947432500012388\n",
      "[ 2.06833685  0.4642347 ]\n",
      "Gradient Descent(6420/9): loss=16.226363457890944, w0=73.05529633131673, w1=13.622468210350094\n",
      "[ 0.67240322 -0.31059696]\n",
      "Gradient Descent(6421/9): loss=15.424548579908489, w0=72.58461407503258, w1=13.839886084169816\n",
      "[ 0.57547836  1.4456081 ]\n",
      "Gradient Descent(6422/9): loss=15.70230926531552, w0=72.1817792196707, w1=12.82796041226336\n",
      "[-2.29718491 -0.68993901]\n",
      "Gradient Descent(6423/9): loss=16.216709002653474, w0=73.78980865535809, w1=13.310917719694007\n",
      "[-0.66289349  0.29552209]\n",
      "Gradient Descent(6424/9): loss=15.523085483222347, w0=74.25383409753528, w1=13.104052253405388\n",
      "[ 0.02024655 -0.93609922]\n",
      "Gradient Descent(6425/9): loss=15.917163770319625, w0=74.23966151208447, w1=13.759321710013182\n",
      "[ 0.12824951  1.6224044 ]\n",
      "Gradient Descent(6426/9): loss=15.872190152537083, w0=74.1498868540719, w1=12.62363863093764\n",
      "[-0.91792329 -0.58920998]\n",
      "Gradient Descent(6427/9): loss=16.118656961722113, w0=74.79243315777146, w1=13.03608562016681\n",
      "[ 2.01550392 -1.50591931]\n",
      "Gradient Descent(6428/9): loss=16.607058086072186, w0=73.3815804156568, w1=14.090229139759657\n",
      "[ 0.58259473  1.61757503]\n",
      "Gradient Descent(6429/9): loss=15.576095190964574, w0=72.973764101819, w1=12.957926620920823\n",
      "[-1.27471569  0.47528801]\n",
      "Gradient Descent(6430/9): loss=15.573268627268652, w0=73.86606508632558, w1=12.625225014522067\n",
      "[ 5.21422207  3.3096891 ]\n",
      "Gradient Descent(6431/9): loss=15.914636099108169, w0=70.2161096373208, w1=10.308442644053052\n",
      "[-4.56855103 -1.15774899]\n",
      "Gradient Descent(6432/9): loss=25.15082838869085, w0=73.41409535916829, w1=11.118866933903142\n",
      "[-5.34560496 -4.20850358]\n",
      "Gradient Descent(6433/9): loss=18.179904426702088, w0=77.15601883068436, w1=14.064819438155702\n",
      "[  3.98951609e+00  -2.81000210e-03]\n",
      "Gradient Descent(6434/9): loss=23.01495892806737, w0=74.36335756798329, w1=14.066786439622964\n",
      "[ 1.27371949  0.1233937 ]\n",
      "Gradient Descent(6435/9): loss=16.130062027070352, w0=73.47175392310639, w1=13.980410850298975\n",
      "[ 0.87825375  0.63125014]\n",
      "Gradient Descent(6436/9): loss=15.527049416439827, w0=72.85697630032993, w1=13.53853575287458\n",
      "[-2.70856679  0.05539832]\n",
      "Gradient Descent(6437/9): loss=15.483078733342868, w0=74.7529730498314, w1=13.499756928416918\n",
      "[ 3.39024235  1.79502514]\n",
      "Gradient Descent(6438/9): loss=16.450503739623258, w0=72.37980340823655, w1=12.243239333811172\n",
      "[-0.8590691  -2.26359455]\n",
      "Gradient Descent(6439/9): loss=16.5681271356258, w0=72.98115178023863, w1=13.827755518635826\n",
      "[-1.72084541  0.15231536]\n",
      "Gradient Descent(6440/9): loss=15.49536746870981, w0=74.18574356399357, w1=13.721134763203185\n",
      "[-1.54492727 -0.36786594]\n",
      "Gradient Descent(6441/9): loss=15.81270308823408, w0=75.26719265214709, w1=13.978640920552177\n",
      "[ 1.43807498  0.82054016]\n",
      "Gradient Descent(6442/9): loss=17.45725121484095, w0=74.26054016785564, w1=13.40426281148396\n",
      "[ 0.57808795 -0.39122817]\n",
      "Gradient Descent(6443/9): loss=15.85590953085231, w0=73.85587860551955, w1=13.67812252948859\n",
      "[ 1.49499661 -0.40017384]\n",
      "Gradient Descent(6444/9): loss=15.56346876368956, w0=72.80938097709311, w1=13.95824421920727\n",
      "[ 1.32579415  1.25413902]\n",
      "Gradient Descent(6445/9): loss=15.617774205542815, w0=71.88132507283507, w1=13.080346902816554\n",
      "[-5.81677494 -0.78232438]\n",
      "Gradient Descent(6446/9): loss=16.463349325264797, w0=75.95306752786941, w1=13.627973967095222\n",
      "[ 2.16228596  1.46393675]\n",
      "Gradient Descent(6447/9): loss=18.932406073376562, w0=74.43946735539964, w1=12.603218244120313\n",
      "[ 0.76252589  0.24223467]\n",
      "Gradient Descent(6448/9): loss=16.426145980369977, w0=73.90569923011563, w1=12.433653972503565\n",
      "[ 1.83169947  0.67606185]\n",
      "Gradient Descent(6449/9): loss=16.120142710654218, w0=72.62350960254928, w1=11.96041067498346\n",
      "[ 0.56134498 -1.67006157]\n",
      "Gradient Descent(6450/9): loss=16.764753180546595, w0=72.23056811969101, w1=13.12945377199653\n",
      "[-2.67583004  0.31788654]\n",
      "Gradient Descent(6451/9): loss=16.0125891739527, w0=74.1036491496194, w1=12.906933194948765\n",
      "[ 0.41537617 -0.52058123]\n",
      "Gradient Descent(6452/9): loss=15.877754924450713, w0=73.81288583274791, w1=13.27134005723069\n",
      "[-1.90764607 -2.7735131 ]\n",
      "Gradient Descent(6453/9): loss=15.54225912149342, w0=75.14823808214155, w1=15.21279922834669\n",
      "[ 0.28698117  0.54568613]\n",
      "Gradient Descent(6454/9): loss=18.60692684782543, w0=74.94735126052164, w1=14.830818940324084\n",
      "[ 2.07169768 -0.14346802]\n",
      "Gradient Descent(6455/9): loss=17.665546419502515, w0=73.49716288177028, w1=14.931246553679172\n",
      "[ 1.02570904  4.43253063]\n",
      "Gradient Descent(6456/9): loss=16.460016945273672, w0=72.77916655165258, w1=11.828475113112415\n",
      "[ 1.93262589 -2.87376303]\n",
      "Gradient Descent(6457/9): loss=16.881666802293893, w0=71.42632842950687, w1=13.840109232734642\n",
      "[-0.17892648  3.00812468]\n",
      "Gradient Descent(6458/9): loss=17.194783670947324, w0=71.55157696857988, w1=11.73442195636378\n",
      "[-3.68533747 -3.39420549]\n",
      "Gradient Descent(6459/9): loss=18.426790404144562, w0=74.13131319624108, w1=14.110365799368392\n",
      "[ 1.50442322 -0.06013413]\n",
      "Gradient Descent(6460/9): loss=15.93536170783906, w0=73.07821694357534, w1=14.152459687820096\n",
      "[ 3.24048264  0.49188403]\n",
      "Gradient Descent(6461/9): loss=15.635446638062943, w0=70.80987909449725, w1=13.808140865963317\n",
      "[-1.90292906  0.2812244 ]\n",
      "Gradient Descent(6462/9): loss=18.52505506938417, w0=72.14192943675172, w1=13.611283785525634\n",
      "[ 2.82907883  0.14767128]\n",
      "Gradient Descent(6463/9): loss=16.05808681428524, w0=70.1615742530842, w1=13.507913890969476\n",
      "[ 0.21788578  1.39224972]\n",
      "Gradient Descent(6464/9): loss=20.29208674028755, w0=70.00905420581991, w1=12.533339087854078\n",
      "[-3.6464485  -0.13808867]\n",
      "Gradient Descent(6465/9): loss=21.22887734444918, w0=72.5615681536341, w1=12.630001159701527\n",
      "[-2.35788521 -1.52452277]\n",
      "Gradient Descent(6466/9): loss=16.01506357418998, w0=74.2120878014978, w1=13.697167097808101\n",
      "[ 4.31453305 -0.48795546]\n",
      "Gradient Descent(6467/9): loss=15.831045351607413, w0=71.1919146652547, w1=14.038735919684804\n",
      "[-2.13409626  1.11805265]\n",
      "Gradient Descent(6468/9): loss=17.751358919136734, w0=72.68578204891296, w1=13.25609906297355\n",
      "[-1.03550013 -0.43204928]\n",
      "Gradient Descent(6469/9): loss=15.5958064402358, w0=73.41063214275268, w1=13.558533560726923\n",
      "[-3.85869148  3.48696144]\n",
      "Gradient Descent(6470/9): loss=15.39580488222567, w0=76.11171617828319, w1=11.117660549274342\n",
      "[ 1.27051948  0.31541662]\n",
      "Gradient Descent(6471/9): loss=22.145514433884934, w0=75.22235253895151, w1=10.896868915236125\n",
      "[ 1.12420338 -0.67473009]\n",
      "Gradient Descent(6472/9): loss=20.580850360314937, w0=74.43541016958557, w1=11.369179977093408\n",
      "[ 2.00194619 -3.21622878]\n",
      "Gradient Descent(6473/9): loss=18.264559114993766, w0=73.03404783633228, w1=13.62054012398952\n",
      "[ 1.35070014 -1.33461896]\n",
      "Gradient Descent(6474/9): loss=15.42957137884209, w0=72.08855774010108, w1=14.554773395189798\n",
      "[-1.22092316  3.2608629 ]\n",
      "Gradient Descent(6475/9): loss=16.690217404961633, w0=72.94320394967414, w1=12.272169367805223\n",
      "[-4.30577926  3.65821192]\n",
      "Gradient Descent(6476/9): loss=16.176469574531772, w0=75.95724943514014, w1=9.711421020760799\n",
      "[ 2.83115119 -7.38366619]\n",
      "Gradient Descent(6477/9): loss=26.032554467880857, w0=73.97544360321874, w1=14.87998735712625\n",
      "[ 6.16884791  5.01160113]\n",
      "Gradient Descent(6478/9): loss=16.598508644004756, w0=69.65725006523562, w1=11.371866563325868\n",
      "[-5.76730027 -3.52646111]\n",
      "Gradient Descent(6479/9): loss=24.2200863663805, w0=73.69436025304654, w1=13.840389342368308\n",
      "[ 0.81954921  0.20915161]\n",
      "Gradient Descent(6480/9): loss=15.531107180996216, w0=73.12067580746219, w1=13.693983218800014\n",
      "[ 1.1549385  -0.87548141]\n",
      "Gradient Descent(6481/9): loss=15.42385097520602, w0=72.31221885514675, w1=14.306820207483716\n",
      "[ 0.71919872  0.84388185]\n",
      "Gradient Descent(6482/9): loss=16.209812036863994, w0=71.80877974961564, w1=13.716102910251156\n",
      "[-1.30129696  0.33495828]\n",
      "Gradient Descent(6483/9): loss=16.51665185229159, w0=72.71968761903526, w1=13.481632113244407\n",
      "[-0.11613361  0.09578808]\n",
      "Gradient Descent(6484/9): loss=15.550762274761556, w0=72.80098114902721, w1=13.41458045480313\n",
      "[ 0.91542038  1.30378774]\n",
      "Gradient Descent(6485/9): loss=15.509504298567492, w0=72.16018688396034, w1=12.501929034981353\n",
      "[-0.2992624  -1.11427273]\n",
      "Gradient Descent(6486/9): loss=16.506595716552166, w0=72.36967056446268, w1=13.28191994573661\n",
      "[ 1.82859496  0.73758773]\n",
      "Gradient Descent(6487/9): loss=15.832569163223859, w0=71.08965409289236, w1=12.765608534968372\n",
      "[-0.63720577  0.07828001]\n",
      "Gradient Descent(6488/9): loss=18.070258566634518, w0=71.53569813270795, w1=12.710812530888942\n",
      "[ 0.08211552 -1.93558862]\n",
      "Gradient Descent(6489/9): loss=17.22716698755108, w0=71.47821726668045, w1=14.065724563670832\n",
      "[-2.13323498  1.03695274]\n",
      "Gradient Descent(6490/9): loss=17.2059848194324, w0=72.97148175592802, w1=13.339857645772973\n",
      "[-0.49455879 -0.11591875]\n",
      "Gradient Descent(6491/9): loss=15.447651406040134, w0=73.31767290611816, w1=13.421000774024325\n",
      "[-1.00029422  2.89559758]\n",
      "Gradient Descent(6492/9): loss=15.387893451116735, w0=74.01787886132058, w1=11.394082470052936\n",
      "[ 3.3427029  -1.53092549]\n",
      "Gradient Descent(6493/9): loss=17.822870811151613, w0=71.6779868331054, w1=12.46573031476139\n",
      "[-0.78918062  0.31666007]\n",
      "Gradient Descent(6494/9): loss=17.205590974105288, w0=72.2304132660805, w1=12.244068269187785\n",
      "[-1.77123358 -0.34966599]\n",
      "Gradient Descent(6495/9): loss=16.714821536869156, w0=73.470276769465, w1=12.488834461718664\n",
      "[ 1.6963794  -2.98680997]\n",
      "Gradient Descent(6496/9): loss=15.89235794977088, w0=72.2828111908023, w1=14.579601441366346\n",
      "[-2.98942918  2.41268192]\n",
      "Gradient Descent(6497/9): loss=16.50193831837101, w0=74.37541161738854, w1=12.890724096356319\n",
      "[ 3.59518226 -1.15357671]\n",
      "Gradient Descent(6498/9): loss=16.144151394334937, w0=71.85878403303319, w1=13.69822779297451\n",
      "[ 0.31867552  0.35339222]\n",
      "Gradient Descent(6499/9): loss=16.439572844803212, w0=71.63571116616674, w1=13.450853239058425\n",
      "[-0.21112872  0.35866714]\n",
      "Gradient Descent(6500/9): loss=16.761135883636122, w0=71.78350126866668, w1=13.199786244162716\n",
      "[-7.376543   -0.72908359]\n",
      "Gradient Descent(6501/9): loss=16.565752600985128, w0=76.94708137086899, w1=13.710144757391998\n",
      "[ 8.04667366 -0.57688499]\n",
      "Gradient Descent(6502/9): loss=22.085224083226755, w0=71.31440980813906, w1=14.113964251680882\n",
      "[-3.07105345  2.1026843 ]\n",
      "Gradient Descent(6503/9): loss=17.546259815348147, w0=73.46414722042921, w1=12.642085240382448\n",
      "[-3.3241574   0.65759761]\n",
      "Gradient Descent(6504/9): loss=15.751185839878392, w0=75.791057398003, w1=12.181766912444072\n",
      "[ 3.77941013 -2.54018685]\n",
      "Gradient Descent(6505/9): loss=19.34606175129955, w0=73.14547030754646, w1=13.959897709999941\n",
      "[-2.5852603  -0.01460009]\n",
      "Gradient Descent(6506/9): loss=15.51219577080672, w0=74.9551525177843, w1=13.970117775668909\n",
      "[ 3.60295172 -0.42492336]\n",
      "Gradient Descent(6507/9): loss=16.885979981024803, w0=72.43308631644142, w1=14.267564130962114\n",
      "[-1.88866008 -0.6444751 ]\n",
      "Gradient Descent(6508/9): loss=16.066762055109326, w0=73.75514837389115, w1=14.718696703310831\n",
      "[ 0.23731545  0.6640315 ]\n",
      "Gradient Descent(6509/9): loss=16.25979376041925, w0=73.58902756231198, w1=14.253874656532282\n",
      "[ 0.65221579  1.04852525]\n",
      "Gradient Descent(6510/9): loss=15.729095087294258, w0=73.13247650648333, w1=13.519906982415273\n",
      "[ 0.90559235 -1.22014628]\n",
      "Gradient Descent(6511/9): loss=15.399727993679093, w0=72.49856186027411, w1=14.374009381072774\n",
      "[ 1.64067877 -0.22213434]\n",
      "Gradient Descent(6512/9): loss=16.10207026032352, w0=71.35008672398358, w1=14.529503419088792\n",
      "[-3.0396924   1.52527384]\n",
      "Gradient Descent(6513/9): loss=17.82616621821301, w0=73.477871407465, w1=13.461811731488854\n",
      "[ 0.20558507  2.66564456]\n",
      "Gradient Descent(6514/9): loss=15.402966778288414, w0=73.33396185608353, w1=11.595860542859153\n",
      "[ 2.21844494 -1.05417017]\n",
      "Gradient Descent(6515/9): loss=17.1611384395234, w0=71.78105039908743, w1=12.333779662065398\n",
      "[ 3.09595082 -0.07687034]\n",
      "Gradient Descent(6516/9): loss=17.1868590724684, w0=69.61388482232687, w1=12.387588902052443\n",
      "[-5.30236533 -0.57377626]\n",
      "Gradient Descent(6517/9): loss=22.75359159670176, w0=73.32554055118165, w1=12.789232284712844\n",
      "[-0.66953829  1.63261972]\n",
      "Gradient Descent(6518/9): loss=15.624769154114974, w0=73.7942173509117, w1=11.64639848136236\n",
      "[-0.47471982 -2.59003048]\n",
      "Gradient Descent(6519/9): loss=17.19155561312927, w0=74.1265212274514, w1=13.459419820432464\n",
      "[ 0.30714703  0.75485889]\n",
      "Gradient Descent(6520/9): loss=15.73270449895573, w0=73.91151830947699, w1=12.931018598229953\n",
      "[ 3.09179905 -0.32090943]\n",
      "Gradient Descent(6521/9): loss=15.727132931517751, w0=71.74725897777522, w1=13.155655198231798\n",
      "[-1.9041547   0.98794272]\n",
      "Gradient Descent(6522/9): loss=16.63447767059154, w0=73.08016726625893, w1=12.46409529487844\n",
      "[ 1.70028225 -0.16374388]\n",
      "Gradient Descent(6523/9): loss=15.924472500020975, w0=71.88996969087461, w1=12.57871601398024\n",
      "[-0.26293991  1.09311514]\n",
      "Gradient Descent(6524/9): loss=16.777326190269587, w0=72.07402763034177, w1=11.813535415007916\n",
      "[-2.08286014 -2.08555565]\n",
      "Gradient Descent(6525/9): loss=17.518031938916028, w0=73.53202973014943, w1=13.27342437148053\n",
      "[ 0.35130358 -1.31248647]\n",
      "Gradient Descent(6526/9): loss=15.435512896479644, w0=73.28611722093548, w1=14.192164898201227\n",
      "[ 1.25317386  3.43541659]\n",
      "Gradient Descent(6527/9): loss=15.639712582302504, w0=72.4088955190574, w1=11.787373282821\n",
      "[-3.28971106 -0.32685693]\n",
      "Gradient Descent(6528/9): loss=17.209529709657808, w0=74.71169326453551, w1=12.016173135692835\n",
      "[ 2.15198754 -3.08368223]\n",
      "Gradient Descent(6529/9): loss=17.461899185408264, w0=73.20530198814616, w1=14.174750696997748\n",
      "[ 3.91992829  5.96151151]\n",
      "Gradient Descent(6530/9): loss=15.631353715094487, w0=70.4613521857528, w1=10.001692642464425\n",
      "[-3.39072645 -3.21442509]\n",
      "Gradient Descent(6531/9): loss=25.445924589681205, w0=72.83486070307274, w1=12.251790202974604\n",
      "[-0.17417929 -1.27010019]\n",
      "Gradient Descent(6532/9): loss=16.24515301090175, w0=72.95678620744151, w1=13.140860337215338\n",
      "[-3.85018192 -5.21788041]\n",
      "Gradient Descent(6533/9): loss=15.500128512933978, w0=75.65191355434031, w1=16.793376622661825\n",
      "[-0.43154612  1.135564  ]\n",
      "Gradient Descent(6534/9): loss=23.65613512336803, w0=75.9539958367966, w1=15.99848182332892\n",
      "[ 2.32087696  4.56464805]\n",
      "Gradient Descent(6535/9): loss=22.095983887653315, w0=74.32938196313088, w1=12.803228187207488\n",
      "[ 1.29782606 -4.49040262]\n",
      "Gradient Descent(6536/9): loss=16.150792003021362, w0=73.42090371799577, w1=15.946510017804343\n",
      "[ 2.09062667  4.55364023]\n",
      "Gradient Descent(6537/9): loss=18.436495204206345, w0=71.95746505032564, w1=12.758961857590904\n",
      "[-0.59430105  0.70099147]\n",
      "Gradient Descent(6538/9): loss=16.538687158219222, w0=72.37347578397434, w1=12.268267829159369\n",
      "[-3.9843344  -1.41795982]\n",
      "Gradient Descent(6539/9): loss=16.543297505562002, w0=75.16250986579739, w1=13.260839700763684\n",
      "[ 2.70816175 -1.3631511 ]\n",
      "Gradient Descent(6540/9): loss=17.155650807891934, w0=73.26679664116142, w1=14.215045467590809\n",
      "[ 0.77986838 -0.41457352]\n",
      "Gradient Descent(6541/9): loss=15.656613095850215, w0=72.72088877554984, w1=14.50524692922192\n",
      "[-2.02524375  0.74752812]\n",
      "Gradient Descent(6542/9): loss=16.075931907628355, w0=74.13855940220948, w1=13.981977244319696\n",
      "[ 0.60865678  1.64591897]\n",
      "Gradient Descent(6543/9): loss=15.868729007002843, w0=73.71249965435106, w1=12.829833965563338\n",
      "[-2.76100718 -0.49885369]\n",
      "Gradient Descent(6544/9): loss=15.684662506820786, w0=75.64520468226596, w1=13.179031550690974\n",
      "[ 2.31913285 -0.76574489]\n",
      "Gradient Descent(6545/9): loss=18.195357486932533, w0=74.02181168501892, w1=13.715052971702223\n",
      "[ 0.83906185 -0.46617976]\n",
      "Gradient Descent(6546/9): loss=15.678492148185745, w0=73.43446838876525, w1=14.04137880436513\n",
      "[-1.47216877 -1.1096268 ]\n",
      "Gradient Descent(6547/9): loss=15.553499067475055, w0=74.46498652971586, w1=14.818117567451086\n",
      "[ 2.31160805  7.02582339]\n",
      "Gradient Descent(6548/9): loss=16.96724808204376, w0=72.84686089796728, w1=9.900041193070905\n",
      "[ 3.47742055 -0.65525039]\n",
      "Gradient Descent(6549/9): loss=21.892842784353796, w0=70.41266651407881, w1=10.358716465120164\n",
      "[-2.19143021 -5.40268012]\n",
      "Gradient Descent(6550/9): loss=24.40701238443937, w0=71.94666766356416, w1=14.140592549636658\n",
      "[-4.95884903 -0.51465674]\n",
      "Gradient Descent(6551/9): loss=16.51181625815654, w0=75.41786198263226, w1=14.5008522680503\n",
      "[ 0.37627921  1.89514069]\n",
      "Gradient Descent(6552/9): loss=18.162811668602245, w0=75.15446653780901, w1=13.174253788145327\n",
      "[ 2.14874105 -4.5049176 ]\n",
      "Gradient Descent(6553/9): loss=17.16335334596388, w0=73.65034780096407, w1=16.32769610847223\n",
      "[ 1.42568156  0.74761385]\n",
      "Gradient Descent(6554/9): loss=19.50491304608888, w0=72.65237070713181, w1=15.80436641034648\n",
      "[ 0.68671864  0.48829531]\n",
      "Gradient Descent(6555/9): loss=18.29368995344297, w0=72.171667662608, w1=15.462559695687188\n",
      "[ 0.85308434  0.56116138]\n",
      "Gradient Descent(6556/9): loss=17.981456899718598, w0=71.57450862760415, w1=15.069746727917627\n",
      "[-2.53648273  2.323906  ]\n",
      "Gradient Descent(6557/9): loss=18.128183571380376, w0=73.3500465363841, w1=13.443012528635498\n",
      "[ 2.31160356  1.18176192]\n",
      "Gradient Descent(6558/9): loss=15.388136292066589, w0=71.73192404435848, w1=12.615779185534265\n",
      "[ 0.05628717 -0.2656115 ]\n",
      "Gradient Descent(6559/9): loss=16.978997008588596, w0=71.69252302484884, w1=12.801707236565758\n",
      "[-2.12165614  2.11439803]\n",
      "Gradient Descent(6560/9): loss=16.897972735552745, w0=73.17768232222564, w1=11.321628612409986\n",
      "[-2.4938213  -2.64048786]\n",
      "Gradient Descent(6561/9): loss=17.72130659305738, w0=74.92335723547984, w1=13.16997011515912\n",
      "[ 4.50235476  1.93469935]\n",
      "Gradient Descent(6562/9): loss=16.761387611057568, w0=71.77170890585398, w1=11.8156805682523\n",
      "[-2.47116864 -2.88340656]\n",
      "Gradient Descent(6563/9): loss=17.92895525078643, w0=73.50152695701412, w1=13.834065160769434\n",
      "[-2.71470165  1.17974558]\n",
      "Gradient Descent(6564/9): loss=15.470220704614764, w0=75.40181811354161, w1=13.008243256814097\n",
      "[ 1.40625115 -2.18172106]\n",
      "Gradient Descent(6565/9): loss=17.718642470118272, w0=74.41744230780667, w1=14.535448001284797\n",
      "[-1.04962622 -1.32884585]\n",
      "Gradient Descent(6566/9): loss=16.57432560046207, w0=75.15218066267155, w1=15.465640094495589\n",
      "[ 2.2223281  -1.60273413]\n",
      "Gradient Descent(6567/9): loss=19.08440482801091, w0=73.59655099576915, w1=16.58755398354083\n",
      "[ 0.64790106  5.62143064]\n",
      "Gradient Descent(6568/9): loss=20.261019568184796, w0=73.14302025709523, w1=12.65255253379404\n",
      "[-2.2202883   1.31311657]\n",
      "Gradient Descent(6569/9): loss=15.739370288225395, w0=74.69722206814158, w1=11.733370932784883\n",
      "[ 0.42505427 -1.79499085]\n",
      "Gradient Descent(6570/9): loss=17.89536772765863, w0=74.3996840818141, w1=12.989864524894896\n",
      "[ 3.51616116 -4.63924695]\n",
      "Gradient Descent(6571/9): loss=16.117218244802295, w0=71.93837127231407, w1=16.237337389998537\n",
      "[-4.40359871  2.14288358]\n",
      "Gradient Descent(6572/9): loss=20.10689445559356, w0=75.0208903716854, w1=14.737318884615991\n",
      "[ 2.84901597 -1.45243148]\n",
      "Gradient Descent(6573/9): loss=17.6678847346663, w0=73.02657919153232, w1=15.754020918549429\n",
      "[-0.7420406   1.00560574]\n",
      "Gradient Descent(6574/9): loss=18.007863497209257, w0=73.54600761030612, w1=15.050096901388233\n",
      "[ 1.19219448  0.79624354]\n",
      "Gradient Descent(6575/9): loss=16.650715131914342, w0=72.71147147383444, w1=14.4927264232674\n",
      "[ 0.05384696  0.0106373 ]\n",
      "Gradient Descent(6576/9): loss=16.068610847994645, w0=72.67377860470592, w1=14.485280314077773\n",
      "[ 2.24851192  4.58147612]\n",
      "Gradient Descent(6577/9): loss=16.083760165225854, w0=71.0998202576823, w1=11.278247031315928\n",
      "[-0.39205771 -1.35793964]\n",
      "Gradient Descent(6578/9): loss=20.216154063054017, w0=71.37426065518713, w1=12.22880478256051\n",
      "[-3.05179914  1.35963379]\n",
      "Gradient Descent(6579/9): loss=18.010822689707165, w0=73.51052005479892, w1=11.277061127440867\n",
      "[-2.61028657 -1.679238  ]\n",
      "Gradient Descent(6580/9): loss=17.83518161836662, w0=75.33772065686264, w1=12.45252773012045\n",
      "[ 2.73082399 -4.5044534 ]\n",
      "Gradient Descent(6581/9): loss=18.001998548381586, w0=73.42614386188896, w1=15.605645111657793\n",
      "[-0.32158198  0.70983044]\n",
      "Gradient Descent(6582/9): loss=17.654424051795704, w0=73.65125124974018, w1=15.108763804478588\n",
      "[-0.04291906  1.25348918]\n",
      "Gradient Descent(6583/9): loss=16.776634146654967, w0=73.68129458846255, w1=14.23132137861097\n",
      "[ 0.54435618  2.93605021]\n",
      "Gradient Descent(6584/9): loss=15.743374631226226, w0=73.30024525929073, w1=12.176086230204831\n",
      "[-0.47347487 -5.04819581]\n",
      "Gradient Descent(6585/9): loss=16.23562850152017, w0=73.6316776658408, w1=15.709823297432472\n",
      "[-0.74787625  0.73237609]\n",
      "Gradient Descent(6586/9): loss=17.929624542416217, w0=74.15519103911124, w1=15.197160035104408\n",
      "[ 1.18714082  0.92981778]\n",
      "Gradient Descent(6587/9): loss=17.23159317545307, w0=73.32419246427928, w1=14.546287589162109\n",
      "[-2.00697867  1.44827306]\n",
      "Gradient Descent(6588/9): loss=15.955137299019162, w0=74.72907753461641, w1=13.532496445044982\n",
      "[ 5.2128362  -1.87941913]\n",
      "Gradient Descent(6589/9): loss=16.41711664593697, w0=71.08009219581903, w1=14.848089832670205\n",
      "[-3.59083501 -0.26687343]\n",
      "Gradient Descent(6590/9): loss=18.77263742567223, w0=73.59367670435817, w1=15.03490123443966\n",
      "[ 0.61535249 -0.49329844]\n",
      "Gradient Descent(6591/9): loss=16.640120410559106, w0=73.16292996419409, w1=15.380210140501777\n",
      "[-4.48052611  0.01934233]\n",
      "Gradient Descent(6592/9): loss=17.200413090157024, w0=76.29929824215924, w1=15.36667051189525\n",
      "[ 3.02903854  0.72071024]\n",
      "Gradient Descent(6593/9): loss=21.682336432970878, w0=74.17897126203331, w1=14.862173341275247\n",
      "[-0.48967782 -1.0767137 ]\n",
      "Gradient Descent(6594/9): loss=16.73314304378389, w0=74.52174573738893, w1=15.615872934480588\n",
      "[ 3.68771849  4.01102093]\n",
      "Gradient Descent(6595/9): loss=18.421254271086426, w0=71.94034279663114, w1=12.808158283306646\n",
      "[-1.16359704 -2.41911208]\n",
      "Gradient Descent(6596/9): loss=16.52746869089621, w0=72.7548607219966, w1=14.50153673853999\n",
      "[-2.40912422  1.72070305]\n",
      "Gradient Descent(6597/9): loss=16.053243854349244, w0=74.44124767667394, w1=13.297044600706233\n",
      "[ 2.0067997   2.16618504]\n",
      "Gradient Descent(6598/9): loss=16.060749739432506, w0=73.03648788717415, w1=11.780715071484916\n",
      "[ 0.88690253 -2.65319169]\n",
      "Gradient Descent(6599/9): loss=16.86232005119156, w0=72.41565611866906, w1=13.63794925122971\n",
      "[-0.73892813 -3.71048207]\n",
      "Gradient Descent(6600/9): loss=15.78408279484031, w0=72.9329058080421, w1=16.23528670165741\n",
      "[-2.694874    4.46129275]\n",
      "Gradient Descent(6601/9): loss=19.247648984579744, w0=74.81931760557204, w1=13.112381777237184\n",
      "[-0.22863799  0.42877853]\n",
      "Gradient Descent(6602/9): loss=16.616769648429607, w0=74.97936420039595, w1=12.812236803727956\n",
      "[ 2.77045216 -0.46196978]\n",
      "Gradient Descent(6603/9): loss=17.029007429882693, w0=73.0400476901195, w1=13.135615647604814\n",
      "[-2.30777993  1.12631838]\n",
      "Gradient Descent(6604/9): loss=15.477315251516579, w0=74.65549363991586, w1=12.347192780887054\n",
      "[ 2.6206721   0.40131669]\n",
      "Gradient Descent(6605/9): loss=16.95412691473827, w0=72.82102317310563, w1=12.066271096474503\n",
      "[ 2.05837165 -5.77312986]\n",
      "Gradient Descent(6606/9): loss=16.49661272877487, w0=71.38016301657906, w1=16.107461995060973\n",
      "[-4.33200034  5.15749814]\n",
      "Gradient Descent(6607/9): loss=20.6696584713995, w0=74.41256325404416, w1=12.497213296458183\n",
      "[-0.27141751 -1.77923631]\n",
      "Gradient Descent(6608/9): loss=16.49421927270614, w0=74.60255551139988, w1=13.742678715935494\n",
      "[-0.13947419  0.86190462]\n",
      "Gradient Descent(6609/9): loss=16.276724332111275, w0=74.70018744532887, w1=13.139345482449745\n",
      "[ 0.80922578  2.1426781 ]\n",
      "Gradient Descent(6610/9): loss=16.432603948422386, w0=74.13372940249191, w1=11.639470809982603\n",
      "[ 1.67079369  0.5583236 ]\n",
      "Gradient Descent(6611/9): loss=17.431770722904734, w0=72.96417381607492, w1=11.248644286783323\n",
      "[-1.64717373 -3.93496958]\n",
      "Gradient Descent(6612/9): loss=17.92908734289359, w0=74.1171954295359, w1=14.003122990285801\n",
      "[-0.96502562  1.15161269]\n",
      "Gradient Descent(6613/9): loss=15.86175674168418, w0=74.79271336219585, w1=13.19699410957947\n",
      "[ 3.16653326 -1.80623272]\n",
      "Gradient Descent(6614/9): loss=16.54904046513181, w0=72.57614008018355, w1=14.4613570110054\n",
      "[ 1.09187475 -0.87918256]\n",
      "Gradient Descent(6615/9): loss=16.125306349359324, w0=71.81182775691188, w1=15.076784799917434\n",
      "[-0.50019013 -0.00402109]\n",
      "Gradient Descent(6616/9): loss=17.759509614055936, w0=72.16196084480949, w1=15.07959956254796\n",
      "[ 2.31407262  0.44640206]\n",
      "Gradient Descent(6617/9): loss=17.306375310106873, w0=70.54211001198942, w1=14.767118120476187\n",
      "[-1.26445571  0.11051829]\n",
      "Gradient Descent(6618/9): loss=20.00082918281416, w0=71.42722900988672, w1=14.689755314489755\n",
      "[-1.66252529  0.44825747]\n",
      "Gradient Descent(6619/9): loss=17.860261117543363, w0=72.5909967106583, w1=14.375975087405239\n",
      "[ 0.38047993  1.61767056]\n",
      "Gradient Descent(6620/9): loss=16.0345832225653, w0=72.32466075711379, w1=13.243605696233415\n",
      "[ 0.33462637 -1.61837007]\n",
      "Gradient Descent(6621/9): loss=15.883494745393449, w0=72.09042230116981, w1=14.376464742957625\n",
      "[-2.70701806  2.4393845 ]\n",
      "Gradient Descent(6622/9): loss=16.51217598482866, w0=73.98533494022722, w1=12.668895593733396\n",
      "[ 4.26825953 -0.04529982]\n",
      "Gradient Descent(6623/9): loss=15.953625769362564, w0=70.99755326766456, w1=12.700605466231488\n",
      "[-1.87727637  1.33372867]\n",
      "Gradient Descent(6624/9): loss=18.326046385470928, w0=72.31164672580356, w1=11.766995399694007\n",
      "[-1.91029783 -2.29075201]\n",
      "Gradient Descent(6625/9): loss=17.33502004954104, w0=73.64885520353022, w1=13.370521809038769\n",
      "[-3.64768181 -0.45304579]\n",
      "Gradient Descent(6626/9): loss=15.454837953964018, w0=76.20223247008857, w1=13.687653862402534\n",
      "[-1.18237586  0.45795291]\n",
      "Gradient Descent(6627/9): loss=19.636642576537646, w0=77.02989557393987, w1=13.367086824831405\n",
      "[ 5.54500167 -1.49455017]\n",
      "Gradient Descent(6628/9): loss=22.370979397584673, w0=73.14839440591132, w1=14.413271940854345\n",
      "[ 0.72323295  2.07427882]\n",
      "Gradient Descent(6629/9): loss=15.832243684952113, w0=72.6421313436603, w1=12.961276767272363\n",
      "[-1.17310576 -2.65227157]\n",
      "Gradient Descent(6630/9): loss=15.732691170827835, w0=73.46330537842182, w1=14.817866864400582\n",
      "[ 0.52411878  4.3927337 ]\n",
      "Gradient Descent(6631/9): loss=16.295561871392465, w0=73.0964222305663, w1=11.7429532736492\n",
      "[ 0.34704687  0.54954719]\n",
      "Gradient Descent(6632/9): loss=16.913557140957302, w0=72.85348942319519, w1=11.358270243518716\n",
      "[-2.61390123 -3.10546864]\n",
      "Gradient Descent(6633/9): loss=17.73313678298728, w0=74.68322028616917, w1=13.53209829324685\n",
      "[ 0.93246379 -4.29998723]\n",
      "Gradient Descent(6634/9): loss=16.35233486895367, w0=74.0304956328268, w1=16.542089357615126\n",
      "[ 0.46981151  4.14914174]\n",
      "Gradient Descent(6635/9): loss=20.346234433682984, w0=73.70162757518213, w1=13.637690140386008\n",
      "[-0.46919476 -0.73441806]\n",
      "Gradient Descent(6636/9): loss=15.481478263689642, w0=74.03006390404288, w1=14.151782781168944\n",
      "[ 1.10494851  1.23624364]\n",
      "Gradient Descent(6637/9): loss=15.882679593830801, w0=73.25659994546321, w1=13.286412232008347\n",
      "[ 0.40183869 -2.0081956 ]\n",
      "Gradient Descent(6638/9): loss=15.405266821021586, w0=72.97531286247778, w1=14.692149149030284\n",
      "[-2.27908652 -2.20236981]\n",
      "Gradient Descent(6639/9): loss=16.171645153534012, w0=74.57067342370276, w1=16.23380801529027\n",
      "[ 2.23456421  2.62966173]\n",
      "Gradient Descent(6640/9): loss=19.99345619782246, w0=73.00647847873078, w1=14.393044803516103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.07658232  1.62192599]\n",
      "Gradient Descent(6641/9): loss=15.844287766093965, w0=74.46008610001202, w1=13.257696612154989\n",
      "[ 4.26917482 -3.3905035 ]\n",
      "Gradient Descent(6642/9): loss=16.09050273324717, w0=71.47166372922787, w1=15.631049060162354\n",
      "[-2.34637954  1.6348902 ]\n",
      "Gradient Descent(6643/9): loss=19.36032511277035, w0=73.11412940925044, w1=14.486625921956282\n",
      "[ 1.0754625   2.90614515]\n",
      "Gradient Descent(6644/9): loss=15.908987942170375, w0=72.36130565741735, w1=12.452324319974231\n",
      "[-1.72165014 -1.37833566]\n",
      "Gradient Descent(6645/9): loss=16.348537661455705, w0=73.56646075399682, w1=13.417159282676895\n",
      "[-0.4128737  -0.85420518]\n",
      "Gradient Descent(6646/9): loss=15.424983002902819, w0=73.85547234605944, w1=14.015102905448765\n",
      "[ 0.4519958  0.6804123]\n",
      "Gradient Descent(6647/9): loss=15.68687874115651, w0=73.53907528952917, w1=13.538814293142726\n",
      "[ 0.91916164 -2.03769712]\n",
      "Gradient Descent(6648/9): loss=15.417684450815402, w0=72.89566214310946, w1=14.965202278474301\n",
      "[ 0.99449639  2.33425022]\n",
      "Gradient Descent(6649/9): loss=16.56853336402197, w0=72.19951466808027, w1=13.33122712243876\n",
      "[-0.76980294  0.8598693 ]\n",
      "Gradient Descent(6650/9): loss=15.995775519234746, w0=72.73837672328133, w1=12.72931861234581\n",
      "[-1.01311074 -1.56193548]\n",
      "Gradient Descent(6651/9): loss=15.821748591771705, w0=73.44755423906113, w1=13.822673449744673\n",
      "[-1.88306158  0.20464975]\n",
      "Gradient Descent(6652/9): loss=15.45650042976655, w0=74.7656973480112, w1=13.67941862799318\n",
      "[ 0.71481855  1.51559628]\n",
      "Gradient Descent(6653/9): loss=16.48889048499988, w0=74.26532436379641, w1=12.61850123301366\n",
      "[ 2.60125646 -1.40792759]\n",
      "Gradient Descent(6654/9): loss=16.228541510182986, w0=72.4444448420079, w1=13.604050547810496\n",
      "[ 0.09169262 -0.66848466]\n",
      "Gradient Descent(6655/9): loss=15.754423574742878, w0=72.38026001084593, w1=14.071989811059627\n",
      "[-0.17333398  0.75503963]\n",
      "Gradient Descent(6656/9): loss=15.978673231067843, w0=72.5015937941787, w1=13.543462071405695\n",
      "[-0.25319331 -1.13196154]\n",
      "Gradient Descent(6657/9): loss=15.701811871439027, w0=72.67882910935182, w1=14.335835150117841\n",
      "[ 0.89213315  1.81122174]\n",
      "Gradient Descent(6658/9): loss=15.941530553866999, w0=72.05433590112082, w1=13.067979934488921\n",
      "[-2.4390021  -1.39433292]\n",
      "Gradient Descent(6659/9): loss=16.238936545690255, w0=73.76163737137584, w1=14.044012981162098\n",
      "[ 2.14337163  0.32362515]\n",
      "Gradient Descent(6660/9): loss=15.654484255360993, w0=72.26127723110686, w1=13.817475374791226\n",
      "[ 0.65348063 -1.12144026]\n",
      "Gradient Descent(6661/9): loss=15.976107382116403, w0=71.80384079173226, w1=14.602483554436278\n",
      "[-0.47886148 -2.03118115]\n",
      "Gradient Descent(6662/9): loss=17.126366368915026, w0=72.13904382620017, w1=16.02431036205014\n",
      "[-0.21361241  1.72896519]\n",
      "Gradient Descent(6663/9): loss=19.290248974622063, w0=72.28857251365683, w1=14.814034729229437\n",
      "[-2.75424722 -0.31504978]\n",
      "Gradient Descent(6664/9): loss=16.781459658244557, w0=74.21654556641616, w1=15.034569573409154\n",
      "[ 2.69397805  1.05782194]\n",
      "Gradient Descent(6665/9): loss=17.020295349988317, w0=72.33076093150834, w1=14.294094213959719\n",
      "[ 0.45311002  2.10598372]\n",
      "Gradient Descent(6666/9): loss=16.181336333745755, w0=72.01358391853474, w1=12.819905609042578\n",
      "[-1.59361726 -2.31132238]\n",
      "Gradient Descent(6667/9): loss=16.423193196732605, w0=73.129115998424, w1=14.43783127334703\n",
      "[-1.31973993  0.25138828]\n",
      "Gradient Descent(6668/9): loss=15.858464232462307, w0=74.05293394659223, w1=14.261859480164853\n",
      "[ 4.42532264  1.19666929]\n",
      "Gradient Descent(6669/9): loss=15.979814434905025, w0=70.95520810076412, w1=13.424190977231865\n",
      "[-2.06454375 -1.34941946]\n",
      "Gradient Descent(6670/9): loss=18.122220541128115, w0=72.40038872317653, w1=14.368784595770538\n",
      "[ 0.29818822  1.17206869]\n",
      "Gradient Descent(6671/9): loss=16.180313382644236, w0=72.19165697060956, w1=13.548336509323011\n",
      "[-3.49639607 -0.40097485]\n",
      "Gradient Descent(6672/9): loss=15.99573660044753, w0=74.63913422183818, w1=13.829018906245162\n",
      "[ 0.44771834  0.15597259]\n",
      "Gradient Descent(6673/9): loss=16.351693332319574, w0=74.32573138221379, w1=13.719838094394742\n",
      "[ 0.8284575   0.57793196]\n",
      "Gradient Descent(6674/9): loss=15.947033333421956, w0=73.74581112913063, w1=13.315285719159208\n",
      "[ 1.19808627 -1.53040728]\n",
      "Gradient Descent(6675/9): loss=15.501507832830603, w0=72.90715073711358, w1=14.38657081668283\n",
      "[-0.99560764  2.65688581]\n",
      "Gradient Descent(6676/9): loss=15.871879936765135, w0=73.60407608763555, w1=12.526750752589093\n",
      "[-1.32305828 -2.35357308]\n",
      "Gradient Descent(6677/9): loss=15.888053631276264, w0=74.53021688135829, w1=14.174251907315378\n",
      "[ 1.37169377 -1.05643674]\n",
      "Gradient Descent(6678/9): loss=16.39129292237278, w0=73.57003124179246, w1=14.913757626320077\n",
      "[-1.58138361  2.50042719]\n",
      "Gradient Descent(6679/9): loss=16.452248830339563, w0=74.67699976641578, w1=13.163458592058344\n",
      "[ 3.25219513  0.04697526]\n",
      "Gradient Descent(6680/9): loss=16.392348166478804, w0=72.40046317534487, w1=13.13057590869329\n",
      "[-0.67608167  0.26522003]\n",
      "Gradient Descent(6681/9): loss=15.845970363384298, w0=72.87372034610718, w1=12.94492188889844\n",
      "[-0.0682033  -1.51084465]\n",
      "Gradient Descent(6682/9): loss=15.617173048775081, w0=72.92146265694142, w1=14.002513145070262\n",
      "[-1.0586212   0.48049211]\n",
      "Gradient Descent(6683/9): loss=15.591911141960026, w0=73.66249749588238, w1=13.666168669818612\n",
      "[-0.588599   -0.93022128]\n",
      "Gradient Descent(6684/9): loss=15.471194779889355, w0=74.0745167966227, w1=14.317323563007914\n",
      "[ 0.48074959  3.6031926 ]\n",
      "Gradient Descent(6685/9): loss=16.041348186333835, w0=73.73799208589514, w1=11.795088746109458\n",
      "[-0.17075509 -4.52540425]\n",
      "Gradient Descent(6686/9): loss=16.903465475055146, w0=73.85752064911766, w1=14.962871722794834\n",
      "[ 1.64107779 -0.30190237]\n",
      "Gradient Descent(6687/9): loss=16.644590322788826, w0=72.70876619603568, w1=15.17420337959009\n",
      "[-1.45755    -1.83358751]\n",
      "Gradient Descent(6688/9): loss=16.992741308185295, w0=73.72905119485726, w1=16.4577146341795\n",
      "[ 5.60317759  4.17356816]\n",
      "Gradient Descent(6689/9): loss=19.914805125213512, w0=69.80682687999025, w1=13.53621692160042\n",
      "[-4.31946088  1.3864041 ]\n",
      "Gradient Descent(6690/9): loss=21.46740044267189, w0=72.8304494964772, w1=12.565734050384414\n",
      "[ 1.51696505 -1.40543645]\n",
      "Gradient Descent(6691/9): loss=15.91096949432819, w0=71.76857396068648, w1=13.549539568031538\n",
      "[-1.16554434 -2.3464009 ]\n",
      "Gradient Descent(6692/9): loss=16.55166910681381, w0=72.58445499881434, w1=15.19202019659199\n",
      "[-3.10662625  1.80291183]\n",
      "Gradient Descent(6693/9): loss=17.10355851843149, w0=74.7590933722789, w1=13.929981912863491\n",
      "[ 0.98131721  1.61452319]\n",
      "Gradient Descent(6694/9): loss=16.56062274217041, w0=74.07217132631222, w1=12.799815681642382\n",
      "[-0.74673414 -0.87735931]\n",
      "Gradient Descent(6695/9): loss=15.919853671749419, w0=74.59488522403814, w1=13.413967200069408\n",
      "[-2.2991401 -0.5058608]\n",
      "Gradient Descent(6696/9): loss=16.234301739197793, w0=76.20428329191628, w1=13.768069758819657\n",
      "[ 2.76113215 -0.14375216]\n",
      "Gradient Descent(6697/9): loss=19.662564260548223, w0=74.27149078415388, w1=13.868696270672642\n",
      "[ 0.94442598 -1.11261467]\n",
      "Gradient Descent(6698/9): loss=15.939362442859045, w0=73.61039259538748, w1=14.647526540103986\n",
      "[ 0.71591089  4.96698528]\n",
      "Gradient Descent(6699/9): loss=16.117859579088325, w0=73.10925497428134, w1=11.170636841342844\n",
      "[ 2.76286073 -0.07578142]\n",
      "Gradient Descent(6700/9): loss=18.068853872998332, w0=71.17525245992312, w1=11.223683834638289\n",
      "[-3.8357319  -2.54302264]\n",
      "Gradient Descent(6701/9): loss=20.175100706114698, w0=73.86026479170987, w1=13.003799683722804\n",
      "[ 0.00625148  0.28334699]\n",
      "Gradient Descent(6702/9): loss=15.659506419906908, w0=73.85588875617667, w1=12.805456787359116\n",
      "[-1.22597081 -1.5608665 ]\n",
      "Gradient Descent(6703/9): loss=15.771101524350646, w0=74.71406832265275, w1=13.898063338255351\n",
      "[ 0.13008643 -0.04416567]\n",
      "Gradient Descent(6704/9): loss=16.481804393843653, w0=74.62300781877457, w1=13.928979303815572\n",
      "[-1.5173148  -0.83102151]\n",
      "Gradient Descent(6705/9): loss=16.370042782577848, w0=75.68512817691735, w1=14.510694357482421\n",
      "[ 5.28171118  4.49147622]\n",
      "Gradient Descent(6706/9): loss=18.776283216313356, w0=71.98793035397712, w1=11.366661003536422\n",
      "[-2.45749267 -1.14844775]\n",
      "Gradient Descent(6707/9): loss=18.47118813730153, w0=73.70817522577163, w1=12.17057442699565\n",
      "[-1.13473532  1.48839515]\n",
      "Gradient Descent(6708/9): loss=16.328611897474882, w0=74.50248995020912, w1=11.128697819120474\n",
      "[ 0.56753229 -3.66124006]\n",
      "Gradient Descent(6709/9): loss=18.879840973435314, w0=74.1052173455615, w1=13.691565859459487\n",
      "[ 1.47870606 -1.47076555]\n",
      "Gradient Descent(6710/9): loss=15.737428872716272, w0=73.07012310022274, w1=14.721101741715643\n",
      "[-0.15184859  0.47001416]\n",
      "Gradient Descent(6711/9): loss=16.181454548498863, w0=73.1764171117575, w1=14.392091827904764\n",
      "[-0.49482657  0.81944543]\n",
      "Gradient Descent(6712/9): loss=15.809009646765835, w0=73.52279571018889, w1=13.818480025897266\n",
      "[  2.93561850e-04   9.23369701e-01]\n",
      "Gradient Descent(6713/9): loss=15.469461196280268, w0=73.52259021689396, w1=13.172121235357315\n",
      "[-1.52840823  0.01530466]\n",
      "Gradient Descent(6714/9): loss=15.459338618102187, w0=74.59247597715407, w1=13.161407969919159\n",
      "[ 0.9774044  -0.89220507]\n",
      "Gradient Descent(6715/9): loss=16.279667948128736, w0=73.90829289895098, w1=13.785951521879511\n",
      "[-1.67804336 -0.76534052]\n",
      "Gradient Descent(6716/9): loss=15.6215048574447, w0=75.08292325061129, w1=14.321689886467906\n",
      "[ 0.53655364  0.20481487]\n",
      "Gradient Descent(6717/9): loss=17.340613616807, w0=74.7073357025555, w1=14.178319476516933\n",
      "[ 4.38741873 -3.26581822]\n",
      "Gradient Descent(6718/9): loss=16.628782912375886, w0=71.63614259366001, w1=16.464392232842226\n",
      "[-0.61014583  1.77183588]\n",
      "Gradient Descent(6719/9): loss=21.21416090021827, w0=72.06324467118705, w1=15.224107117023731\n",
      "[-0.19962786  9.73457217]\n",
      "Gradient Descent(6720/9): loss=17.664627618602744, w0=72.20298417504333, w1=8.40990659663678\n",
      "[-0.57224697 -2.33494525]\n",
      "Gradient Descent(6721/9): loss=28.832426159382, w0=72.60355705731938, w1=10.044368275028022\n",
      "[ 1.3798582  -6.09271992]\n",
      "Gradient Descent(6722/9): loss=21.524984496013122, w0=71.63765631948358, w1=14.309272216666677\n",
      "[ 0.76794883  0.43347376]\n",
      "Gradient Descent(6723/9): loss=17.101580590232928, w0=71.10009213867973, w1=14.00584058259946\n",
      "[ 0.47348902  3.08059306]\n",
      "Gradient Descent(6724/9): loss=17.930738017512073, w0=70.76864982463442, w1=11.849425442808554\n",
      "[-6.77953202 -0.54336717]\n",
      "Gradient Descent(6725/9): loss=19.903305492419832, w0=75.51432223691165, w1=12.229782460395679\n",
      "[ 1.96314728 -2.59797694]\n",
      "Gradient Descent(6726/9): loss=18.632138940887195, w0=74.14011914318898, w1=14.048366320931448\n",
      "[ 0.07539852  3.15943233]\n",
      "Gradient Descent(6727/9): loss=15.905596290617288, w0=74.0873401811797, w1=11.836763692642837\n",
      "[ 1.70996135 -4.29962015]\n",
      "Gradient Descent(6728/9): loss=17.050284357260853, w0=72.89036723876853, w1=14.84649779505923\n",
      "[-4.67604163 -1.08513755]\n",
      "Gradient Descent(6729/9): loss=16.401367202586343, w0=76.16359638256537, w1=15.606094082666155\n",
      "[ 4.97559806  2.87218649]\n",
      "Gradient Descent(6730/9): loss=21.764152849553167, w0=72.6806777395718, w1=13.595563542848938\n",
      "[-0.60029291 -0.33301597]\n",
      "Gradient Descent(6731/9): loss=15.58063287119064, w0=73.100882778145, w1=13.828674721065722\n",
      "[ 0.88227887  5.73641693]\n",
      "Gradient Descent(6732/9): loss=15.465407278374903, w0=72.4832875720793, w1=9.813182867277195\n",
      "[ 0.44271638 -3.89460624]\n",
      "Gradient Descent(6733/9): loss=22.436171493853728, w0=72.17338610519514, w1=12.539407235017737\n",
      "[-2.62976683 -1.250885  ]\n",
      "Gradient Descent(6734/9): loss=16.455775151507954, w0=74.01422288417947, w1=13.415026734147736\n",
      "[ 0.37866443  0.29889391]\n",
      "Gradient Descent(6735/9): loss=15.647396669134562, w0=73.74915778542372, w1=13.20580099530152\n",
      "[ 1.18269705 -0.77318862]\n",
      "Gradient Descent(6736/9): loss=15.527021416432067, w0=72.92126984923777, w1=13.747033029651305\n",
      "[ 0.88665833  0.06672972]\n",
      "Gradient Descent(6737/9): loss=15.491052832513049, w0=72.30060902110203, w1=13.700322226446648\n",
      "[-7.27150801  0.4366986 ]\n",
      "Gradient Descent(6738/9): loss=15.903557547987573, w0=77.3906646276532, w1=13.394633206497463\n",
      "[ 7.65581698  0.41427294]\n",
      "Gradient Descent(6739/9): loss=23.78115717638074, w0=72.03159274497415, w1=13.104642147173005\n",
      "[-1.07163795  1.5095697 ]\n",
      "Gradient Descent(6740/9): loss=16.252964305935112, w0=72.78173930917065, w1=12.047943356819871\n",
      "[-0.12830981 -0.16912813]\n",
      "Gradient Descent(6741/9): loss=16.542034770900948, w0=72.87155617592434, w1=12.166333046541183\n",
      "[-0.14037952 -2.39523733]\n",
      "Gradient Descent(6742/9): loss=16.33756702339196, w0=72.96982184081412, w1=13.842999176900111\n",
      "[-3.54472331  2.38171807]\n",
      "Gradient Descent(6743/9): loss=15.504396954528026, w0=75.4511281544068, w1=12.175796529980719\n",
      "[ 1.23511103  0.98663219]\n",
      "Gradient Descent(6744/9): loss=18.562755404260216, w0=74.58655043113191, w1=11.485153994867135\n",
      "[ 0.33333519 -1.62368211]\n",
      "Gradient Descent(6745/9): loss=18.210463682124228, w0=74.3532157973451, w1=12.621731472542633\n",
      "[-2.6073785  -0.31129447]\n",
      "Gradient Descent(6746/9): loss=16.315005207106527, w0=76.1783807505549, w1=12.839637598314045\n",
      "[ 1.0048647   1.90848392]\n",
      "Gradient Descent(6747/9): loss=19.750786902855683, w0=75.47497546236193, w1=11.503698851634864\n",
      "[ 0.11409497 -0.17815024]\n",
      "Gradient Descent(6748/9): loss=19.716699807878467, w0=75.39510898576881, w1=11.62840401947346\n",
      "[-1.01449283 -2.60877819]\n",
      "Gradient Descent(6749/9): loss=19.307052663667548, w0=76.10525396541634, w1=13.454548752331636\n",
      "[ 4.38581886 -0.02186647]\n",
      "Gradient Descent(6750/9): loss=19.3379981782593, w0=73.03518076281793, w1=13.469855279829288\n",
      "[-1.17361243 -1.25680805]\n",
      "Gradient Descent(6751/9): loss=15.419409965037275, w0=73.85670946077322, w1=14.34962091491613\n",
      "[ 0.2317905   1.37591119]\n",
      "Gradient Descent(6752/9): loss=15.92262311237093, w0=73.69445610910682, w1=13.386483084731262\n",
      "[ 0.6786011  -0.04775123]\n",
      "Gradient Descent(6753/9): loss=15.470447510139941, w0=73.21943534020203, w1=13.41990894586954\n",
      "[ 2.70502837  3.09288345]\n",
      "Gradient Descent(6754/9): loss=15.390450228885573, w0=71.3259154794805, w1=11.254890529415846\n",
      "[-1.61187387 -0.95399544]\n",
      "Gradient Descent(6755/9): loss=19.797328961135246, w0=72.45422718888116, w1=11.92268733987155\n",
      "[-1.58782729 -0.04967262]\n",
      "Gradient Descent(6756/9): loss=16.950595131919894, w0=73.56570629207924, w1=11.957458176111471\n",
      "[ 0.99506187 -4.09008   ]\n",
      "Gradient Descent(6757/9): loss=16.581450233303215, w0=72.86916298532408, w1=14.820514179078138\n",
      "[-0.19194374  3.3899979 ]\n",
      "Gradient Descent(6758/9): loss=16.374972638474, w0=73.00352360064275, w1=12.447515650087091\n",
      "[ 3.95389545 -3.07749611]\n",
      "Gradient Descent(6759/9): loss=15.960768585996341, w0=70.23579678453393, w1=14.601762924963177\n",
      "[-3.72066444 -0.9375689 ]\n",
      "Gradient Descent(6760/9): loss=20.69145144302762, w0=72.8402618947055, w1=15.258061156468269\n",
      "[-2.81568852 -1.7908188 ]\n",
      "Gradient Descent(6761/9): loss=17.070053702945735, w0=74.81124385548627, w1=16.511634318153224\n",
      "[-1.70887924  4.98042049]\n",
      "Gradient Descent(6762/9): loss=21.13329582500819, w0=76.00745932284902, w1=13.025339973618433\n",
      "[ 2.45146534 -1.49806318]\n",
      "Gradient Descent(6763/9): loss=19.170757431190186, w0=74.2914335818335, w1=14.073984196524137\n",
      "[ 0.84106108  2.53380607]\n",
      "Gradient Descent(6764/9): loss=16.059982007954446, w0=73.70269082839648, w1=12.300319950574494\n",
      "[-1.83569466 -3.21336435]\n",
      "Gradient Descent(6765/9): loss=16.16491716164995, w0=74.98767709181367, w1=14.549674994377165\n",
      "[ 2.31263433  0.05701401]\n",
      "Gradient Descent(6766/9): loss=17.392700960032265, w0=73.36883305735789, w1=14.509765186797855\n",
      "[ 2.56084284  1.01484255]\n",
      "Gradient Descent(6767/9): loss=15.919198037683385, w0=71.57624307072832, w1=13.799375402160083\n",
      "[-0.41988751  0.7616734 ]\n",
      "Gradient Descent(6768/9): loss=16.912190530767678, w0=71.87016432565814, w1=13.266204019051388\n",
      "[ 0.13594314 -1.87841858]\n",
      "Gradient Descent(6769/9): loss=16.422223751288456, w0=71.77500412966063, w1=14.581097027873362\n",
      "[ 0.0036939 -1.2856617]\n",
      "Gradient Descent(6770/9): loss=17.14596763116663, w0=71.77241840186493, w1=15.481060215612477\n",
      "[-2.42258956  5.44516214]\n",
      "Gradient Descent(6771/9): loss=18.546070941104606, w0=73.46823109233225, w1=11.669446719047631\n",
      "[-2.9583417  -0.95467353]\n",
      "Gradient Descent(6772/9): loss=17.03961067945374, w0=75.53907028429052, w1=12.33771818661011\n",
      "[ 2.91586001 -3.39178447]\n",
      "Gradient Descent(6773/9): loss=18.558308704994555, w0=73.4979682784895, w1=14.711967317761431\n",
      "[-0.92195127 -1.24000806]\n",
      "Gradient Descent(6774/9): loss=16.16593135834074, w0=74.1433341691023, w1=15.579972960068117\n",
      "[-0.42081409  1.402124  ]\n",
      "Gradient Descent(6775/9): loss=17.952185520153467, w0=74.43790403408768, w1=14.598486157060227\n",
      "[ 1.79730383 -0.45148118]\n",
      "Gradient Descent(6776/9): loss=16.66606263417729, w0=73.1797913559852, w1=14.914522982658985\n",
      "[ 0.2383983   0.06791159]\n",
      "Gradient Descent(6777/9): loss=16.421741424873733, w0=73.01291254661984, w1=14.866984871397307\n",
      "[-1.52958829  0.79329315]\n",
      "Gradient Descent(6778/9): loss=16.387633432274537, w0=74.08362434650635, w1=14.311679667136845\n",
      "[ 2.07927422  0.72962073]\n",
      "Gradient Descent(6779/9): loss=16.043787502889582, w0=72.62813239375191, w1=13.800945153466934\n",
      "[-2.64974    -1.70950874]\n",
      "Gradient Descent(6780/9): loss=15.65912099983535, w0=74.4829503908923, w1=14.997601271105607\n",
      "[ 2.20918871  2.80885223]\n",
      "Gradient Descent(6781/9): loss=17.244775382903875, w0=72.93651829736365, w1=13.031404709001684\n",
      "[-1.28945711  0.23854631]\n",
      "Gradient Descent(6782/9): loss=15.55024648150087, w0=73.8391382776153, w1=12.864422294042676\n",
      "[-0.8038709  -1.55052167]\n",
      "Gradient Descent(6783/9): loss=15.723809241142867, w0=74.40184790736201, w1=13.949787463547953\n",
      "[ 1.57823876  2.29697016]\n",
      "Gradient Descent(6784/9): loss=16.110123040836303, w0=73.29708077633174, w1=12.34190835286293\n",
      "[ 2.40170384 -0.27287728]\n",
      "Gradient Descent(6785/9): loss=16.03319192240814, w0=71.61588809180347, w1=12.532922451826217\n",
      "[-3.45341507 -0.19519742]\n",
      "Gradient Descent(6786/9): loss=17.24199240699938, w0=74.03327863969622, w1=12.669560643267598\n",
      "[-0.44243962 -3.00405224]\n",
      "Gradient Descent(6787/9): loss=15.987384950419097, w0=74.34298637282305, w1=14.772397210673056\n",
      "[-1.58761647  1.2890231 ]\n",
      "Gradient Descent(6788/9): loss=16.771672860426833, w0=75.45431789985275, w1=13.870081041858775\n",
      "[ 0.14410953  1.08413259]\n",
      "Gradient Descent(6789/9): loss=17.795736910946356, w0=75.35344123052111, w1=13.111188227026213\n",
      "[-1.41484836  1.75595592]\n",
      "Gradient Descent(6790/9): loss=17.574602640864182, w0=76.34383507973824, w1=11.882019083663536\n",
      "[ 3.5415601   0.87575199]\n",
      "Gradient Descent(6791/9): loss=21.313184781822837, w0=73.86474300644618, w1=11.268992692574306\n",
      "[ 4.30194695 -2.20799034]\n",
      "Gradient Descent(6792/9): loss=17.992447068078977, w0=70.85338014180817, w1=12.814585931655749\n",
      "[-1.60480952  1.21502272]\n",
      "Gradient Descent(6793/9): loss=18.585206787478597, w0=71.97674680398679, w1=11.964070030807353\n",
      "[-1.87644977 -3.08860627]\n",
      "Gradient Descent(6794/9): loss=17.401949068775362, w0=73.29026164328302, w1=14.12609442010947\n",
      "[-2.2537725  -1.31485097]\n",
      "Gradient Descent(6795/9): loss=15.594799403286862, w0=74.86790239189862, w1=15.046490101652758\n",
      "[ 1.20212153  5.13209912]\n",
      "Gradient Descent(6796/9): loss=17.851991130934636, w0=74.02641731914149, w1=11.454020714876506\n",
      "[ 2.11410387 -0.89484766]\n",
      "Gradient Descent(6797/9): loss=17.70587603603571, w0=72.54654461342061, w1=12.080414080000612\n",
      "[-2.24460096 -0.44037587]\n",
      "Gradient Descent(6798/9): loss=16.644192292524565, w0=74.11776528756567, w1=12.388677188936969\n",
      "[-1.15984764 -0.97968983]\n",
      "Gradient Descent(6799/9): loss=16.320425702392527, w0=74.92965863547242, w1=13.074460073367211\n",
      "[-0.60714704 -0.88908204]\n",
      "Gradient Descent(6800/9): loss=16.80581977399922, w0=75.35466156358079, w1=13.696817498553983\n",
      "[ 5.52764061 -1.8607205 ]\n",
      "Gradient Descent(6801/9): loss=17.532778943257494, w0=71.48531313442187, w1=14.99932185111275\n",
      "[-2.63805319  1.81683315]\n",
      "Gradient Descent(6802/9): loss=18.17602727574668, w0=73.33195036633472, w1=13.727538646436322\n",
      "[ 5.13878093  1.22154491]\n",
      "Gradient Descent(6803/9): loss=15.41731986261254, w0=69.734803716464, w1=12.872457208232808\n",
      "[-5.13633672 -1.42145287]\n",
      "Gradient Descent(6804/9): loss=21.903928809633523, w0=73.3302394222941, w1=13.867474220060302\n",
      "[-1.63645776  1.17227656]\n",
      "Gradient Descent(6805/9): loss=15.46172694731481, w0=74.4757598533036, w1=13.046880626152927\n",
      "[ 1.7472229   0.17663707]\n",
      "Gradient Descent(6806/9): loss=16.177929909462215, w0=73.25270382185633, w1=12.923234678317357\n",
      "[-2.29445486  1.20403473]\n",
      "Gradient Descent(6807/9): loss=15.541571084856091, w0=74.85882222271562, w1=12.080410364355926\n",
      "[ 2.38922186 -2.96502062]\n",
      "Gradient Descent(6808/9): loss=17.589367361501754, w0=73.18636692120698, w1=14.15592479730853\n",
      "[-1.18429032  1.0494708 ]\n",
      "Gradient Descent(6809/9): loss=15.620303496019758, w0=74.01537014708596, w1=13.421295236678692\n",
      "[-2.09264788 -2.72032529]\n",
      "Gradient Descent(6810/9): loss=15.647837866306716, w0=75.48022365975676, w1=15.325522938583005\n",
      "[ 1.24394399  2.40558783]\n",
      "Gradient Descent(6811/9): loss=19.479353545543287, w0=74.60946286550715, w1=13.64161146069788\n",
      "[-0.01011983 -0.36047708]\n",
      "Gradient Descent(6812/9): loss=16.264317397732327, w0=74.61654674913932, w1=13.893945419981756\n",
      "[ 2.00812299  0.01253017]\n",
      "Gradient Descent(6813/9): loss=16.34635046249093, w0=73.2108606536605, w1=13.885174302336695\n",
      "[ 0.15449648  1.31977256]\n",
      "Gradient Descent(6814/9): loss=15.471537125568645, w0=73.10271311935286, w1=12.961333512221483\n",
      "[ 0.22785674  1.3738678 ]\n",
      "Gradient Descent(6815/9): loss=15.53852664103593, w0=72.94321340006661, w1=11.99962605496886\n",
      "[-0.33519906 -2.40269317]\n",
      "Gradient Descent(6816/9): loss=16.54271397676196, w0=73.17785274229605, w1=13.681511275834259\n",
      "[-3.20920237  3.35583119]\n",
      "Gradient Descent(6817/9): loss=15.412985291448956, w0=75.42429440268336, w1=11.332429446323715\n",
      "[ 1.60426482 -2.80937315]\n",
      "Gradient Descent(6818/9): loss=19.960543268107763, w0=74.30130903168083, w1=13.298990648141231\n",
      "[-1.412617   -0.70349246]\n",
      "Gradient Descent(6819/9): loss=15.909632364628752, w0=75.29014093388199, w1=13.791435370526717\n",
      "[ 0.19753329  1.18930931]\n",
      "Gradient Descent(6820/9): loss=17.426918474891558, w0=75.15186762830865, w1=12.958918854320107\n",
      "[ 1.85013284  0.28007357]\n",
      "Gradient Descent(6821/9): loss=17.247481820626675, w0=73.85677463925879, w1=12.76286735270565\n",
      "[-0.17657818 -1.93563728]\n",
      "Gradient Descent(6822/9): loss=15.801222850401725, w0=73.98037936489615, w1=14.117813447302419\n",
      "[ 0.86403511  0.63906871]\n",
      "Gradient Descent(6823/9): loss=15.825086175252034, w0=73.37555479049443, w1=13.67046534933094\n",
      "[-0.68374932 -0.54333034]\n",
      "Gradient Descent(6824/9): loss=15.407413162064465, w0=73.85417931144214, w1=14.050796588257278\n",
      "[ 0.50318499 -0.10011519]\n",
      "Gradient Descent(6825/9): loss=15.705900550219184, w0=73.50194981651327, w1=14.1208772226406\n",
      "[-0.7563283  -0.95397199]\n",
      "Gradient Descent(6826/9): loss=15.61307179707523, w0=74.03137962606183, w1=14.788657614044714\n",
      "[-0.9055543   0.40871076]\n",
      "Gradient Descent(6827/9): loss=16.514478483281824, w0=74.66526763371043, w1=14.502560082819468\n",
      "[ 0.13707255  0.47242661]\n",
      "Gradient Descent(6828/9): loss=16.849290944826894, w0=74.56931684617557, w1=14.171861457242999\n",
      "[ 1.91987874 -0.91415873]\n",
      "Gradient Descent(6829/9): loss=16.4387390074736, w0=73.22540172999621, w1=14.811772570070117\n",
      "[ 0.67540871  1.09942413]\n",
      "Gradient Descent(6830/9): loss=16.275427484410443, w0=72.7526156331401, w1=14.042175678999808\n",
      "[-1.02638024  1.81907856]\n",
      "Gradient Descent(6831/9): loss=15.69057661180204, w0=73.47108180326543, w1=12.768820689528724\n",
      "[ 1.51548231 -0.9000323 ]\n",
      "Gradient Descent(6832/9): loss=15.654264203284786, w0=72.41024418857398, w1=13.398843296121212\n",
      "[ 0.13144977 -0.48066301]\n",
      "Gradient Descent(6833/9): loss=15.779601016703658, w0=72.31822934654123, w1=13.735307406440532\n",
      "[-1.45296789 -1.23136293]\n",
      "Gradient Descent(6834/9): loss=15.894540342605765, w0=73.3353068705037, w1=14.597261458192667\n",
      "[ 1.49950866 -0.99924887]\n",
      "Gradient Descent(6835/9): loss=16.011202132127263, w0=72.28565080782398, w1=15.296735664353003\n",
      "[-3.41796276  1.05354059]\n",
      "Gradient Descent(6836/9): loss=17.544979977462134, w0=74.67822473765638, w1=14.559257249655222\n",
      "[ 3.09452156  1.54340341]\n",
      "Gradient Descent(6837/9): loss=16.92674340409296, w0=72.5120596476575, w1=13.478874859914123\n",
      "[-3.7016401   3.64313439]\n",
      "Gradient Descent(6838/9): loss=15.69154259024665, w0=75.10320771523321, w1=10.928680786694558\n",
      "[ 0.12555569 -3.40258037]\n",
      "Gradient Descent(6839/9): loss=20.276526499994045, w0=75.01531873042879, w1=13.310487046010461\n",
      "[ 6.13834065 -0.06762469]\n",
      "Gradient Descent(6840/9): loss=16.88180983310847, w0=70.71848027346182, w1=13.35782432987547\n",
      "[-1.00500507 -2.17050983]\n",
      "Gradient Descent(6841/9): loss=18.709766272732256, w0=71.42198382305793, w1=14.877181207885828\n",
      "[-0.51035911  3.07774511]\n",
      "Gradient Descent(6842/9): loss=18.114423627527607, w0=71.77923519939525, w1=12.722759633849359\n",
      "[-1.24321291 -1.61498551]\n",
      "Gradient Descent(6843/9): loss=16.819514695557846, w0=72.64948423636466, w1=13.853249489154953\n",
      "[-0.79292087  0.57111694]\n",
      "Gradient Descent(6844/9): loss=15.663302851203195, w0=73.20452884834566, w1=13.453467630415943\n",
      "[-3.01446373 -0.40592558]\n",
      "Gradient Descent(6845/9): loss=15.390227831682479, w0=75.31465346050885, w1=13.73761553879034\n",
      "[ 3.25809108  0.02327097]\n",
      "Gradient Descent(6846/9): loss=17.460822687795662, w0=73.03398970691778, w1=13.721325862976904\n",
      "[-0.56612682  0.00884115]\n",
      "Gradient Descent(6847/9): loss=15.448858792162119, w0=73.43027848212294, w1=13.715137056370866\n",
      "[ 0.78592611 -0.13069673]\n",
      "Gradient Descent(6848/9): loss=15.422896789827204, w0=72.88013020634374, w1=13.806624766991243\n",
      "[ 1.44155617  2.86151403]\n",
      "Gradient Descent(6849/9): loss=15.524935530356702, w0=71.87104088644814, w1=11.803564946785086\n",
      "[ 1.1470996  -2.46650989]\n",
      "Gradient Descent(6850/9): loss=17.802918404582343, w0=71.06807116574329, w1=13.530121869127088\n",
      "[-1.35456233 -0.5700657 ]\n",
      "Gradient Descent(6851/9): loss=17.864364397220942, w0=72.01626479393816, w1=13.929167855996724\n",
      "[-3.23124553  0.26472083]\n",
      "Gradient Descent(6852/9): loss=16.303096927356574, w0=74.27813666565503, w1=13.743863275133197\n",
      "[-1.69154714  1.98616766]\n",
      "Gradient Descent(6853/9): loss=15.905114953977089, w0=75.46221966462322, w1=12.35354591099556\n",
      "[ 1.10169163 -0.64573141]\n",
      "Gradient Descent(6854/9): loss=18.370770765351764, w0=74.6910355223564, w1=12.805557896387738\n",
      "[ 1.29714001 -0.68798979]\n",
      "Gradient Descent(6855/9): loss=16.589093134022143, w0=73.78303751197856, w1=13.28715075133197\n",
      "[-0.12879807  4.93630069]\n",
      "Gradient Descent(6856/9): loss=15.52404486083517, w0=73.87319615957013, w1=9.831740265872595\n",
      "[ 0.38710377 -1.78117049]\n",
      "Gradient Descent(6857/9): loss=22.207517616906863, w0=73.60222352085763, w1=11.078559606489042\n",
      "[-2.03614272 -6.00867201]\n",
      "Gradient Descent(6858/9): loss=18.316180234968723, w0=75.0275234272733, w1=15.284630011415073\n",
      "[ 5.33310058  2.2434009 ]\n",
      "Gradient Descent(6859/9): loss=18.517438548347638, w0=71.29435301921286, w1=13.714249378063517\n",
      "[-0.83699303 -2.04002764]\n",
      "Gradient Descent(6860/9): loss=17.412529716335285, w0=71.88024813993114, w1=15.142268728946064\n",
      "[-0.74465773  0.53483972]\n",
      "Gradient Descent(6861/9): loss=17.767171478414493, w0=72.4015085528646, w1=14.767880928424859\n",
      "[-0.5239153   1.55828558]\n",
      "Gradient Descent(6862/9): loss=16.61377778476249, w0=72.76824926457918, w1=13.67708101972555\n",
      "[-3.56958916  1.68313344]\n",
      "Gradient Descent(6863/9): loss=15.543530961438888, w0=75.2669616735257, w1=12.498887608780462\n",
      "[-0.35398463 -0.55663297]\n",
      "Gradient Descent(6864/9): loss=17.813339311182528, w0=75.51475091568498, w1=12.888530689925481\n",
      "[ 3.50667275 -0.77296926]\n",
      "Gradient Descent(6865/9): loss=18.026676328373615, w0=73.0600799908154, w1=13.429609173684517\n",
      "[-1.21493326 -0.32610968]\n",
      "Gradient Descent(6866/9): loss=15.414484080348103, w0=73.91053327574701, w1=13.657885953150874\n",
      "[ 1.53360194 -0.74698473]\n",
      "Gradient Descent(6867/9): loss=15.591865501507579, w0=72.8370119174821, w1=14.180775267570535\n",
      "[ 0.71401837  2.82372787]\n",
      "Gradient Descent(6868/9): loss=15.736015829158125, w0=72.33719906145447, w1=12.204165756308333\n",
      "[ 3.37887548 -1.51135602]\n",
      "Gradient Descent(6869/9): loss=16.657056926159783, w0=69.97198622870438, w1=13.262114971001365\n",
      "[-2.68199617  0.37674246]\n",
      "Gradient Descent(6870/9): loss=20.92719083829635, w0=71.84938354699098, w1=12.998395248698596\n",
      "[-0.58926661  0.29505658]\n",
      "Gradient Descent(6871/9): loss=16.545066659890555, w0=72.26187017221366, w1=12.791855639318715\n",
      "[-0.52093114 -4.41961595]\n",
      "Gradient Descent(6872/9): loss=16.155026844295556, w0=72.62652196681002, w1=15.885586802685346\n",
      "[ 0.50402585  0.5282474 ]\n",
      "Gradient Descent(6873/9): loss=18.50271500895443, w0=72.27370387208964, w1=15.515813621327137\n",
      "[-0.05807229  1.76725282]\n",
      "Gradient Descent(6874/9): loss=17.979164405739297, w0=72.31435447834595, w1=14.278736649401385\n",
      "[ 0.08898657 -0.1248722 ]\n",
      "Gradient Descent(6875/9): loss=16.18488398323993, w0=72.25206387895463, w1=14.366147188447618\n",
      "[-0.95330359  4.40779236]\n",
      "Gradient Descent(6876/9): loss=16.32150532928638, w0=72.91937639468992, w1=11.280692535732161\n",
      "[-2.85560227 -2.82450759]\n",
      "Gradient Descent(6877/9): loss=17.873874333510322, w0=74.91829798697948, w1=13.257847848190746\n",
      "[ 1.4621069  -1.93724789]\n",
      "Gradient Descent(6878/9): loss=16.729798486385153, w0=73.89482315608765, w1=14.613921373732493\n",
      "[-0.62315628  1.68897837]\n",
      "Gradient Descent(6879/9): loss=16.209643925620895, w0=74.33103255421548, w1=13.431636517419436\n",
      "[ 1.94975097 -0.51344797]\n",
      "Gradient Descent(6880/9): loss=15.924842664403739, w0=72.96620687479907, w1=13.79105009547174\n",
      "[-0.00995519 -0.12577915]\n",
      "Gradient Descent(6881/9): loss=15.488052040579449, w0=72.97317550434686, w1=13.879095503776053\n",
      "[ 0.76057496  0.6157774 ]\n",
      "Gradient Descent(6882/9): loss=15.517080444558381, w0=72.44077303580374, w1=13.448051320812349\n",
      "[-2.64650322  1.30857759]\n",
      "Gradient Descent(6883/9): loss=15.750320661255467, w0=74.2933252907472, w1=12.532047008192315\n",
      "[ 0.98662053 -3.88085225]\n",
      "Gradient Descent(6884/9): loss=16.334326216076555, w0=73.602690920028, w1=15.24864358446594\n",
      "[-0.42166352  0.45160757]\n",
      "Gradient Descent(6885/9): loss=16.99811569696183, w0=73.89785538078611, w1=14.932518284913817\n",
      "[ 2.3052912  1.1265924]\n",
      "Gradient Descent(6886/9): loss=16.62357805055969, w0=72.2841515401743, w1=14.143903606626537\n",
      "[-2.69658572  1.62947286]\n",
      "Gradient Descent(6887/9): loss=16.11628101796404, w0=74.1717615432787, w1=13.00327260301179\n",
      "[-0.05711111 -3.19292194]\n",
      "Gradient Descent(6888/9): loss=15.884686455600525, w0=74.21173932122181, w1=15.238317957672946\n",
      "[ 0.75847742  2.52244531]\n",
      "Gradient Descent(6889/9): loss=17.35342887667176, w0=73.68080512769494, w1=13.472606240727993\n",
      "[-0.92793932 -0.61657697]\n",
      "Gradient Descent(6890/9): loss=15.460752394260885, w0=74.33036265448993, w1=13.904210119308146\n",
      "[ 0.63244329  0.30064273]\n",
      "Gradient Descent(6891/9): loss=16.013091623783392, w0=73.88765235397261, w1=13.693760210378935\n",
      "[ 3.31354701  1.91712329]\n",
      "Gradient Descent(6892/9): loss=15.585053959268388, w0=71.56816944523561, w1=12.351773910533757\n",
      "[-4.46126727  0.19536908]\n",
      "Gradient Descent(6893/9): loss=17.51112147007553, w0=74.69105653159774, w1=12.215015555571522\n",
      "[ 0.41664636  0.03565283]\n",
      "Gradient Descent(6894/9): loss=17.1616094139838, w0=74.39940407953216, w1=12.190058576401665\n",
      "[ 1.1505647 -1.6646394]\n",
      "Gradient Descent(6895/9): loss=16.828536718070193, w0=73.59400879033093, w1=13.355306156308531\n",
      "[-0.9979586   1.00367682]\n",
      "Gradient Descent(6896/9): loss=15.43865237015079, w0=74.29257981124437, w1=12.65273238399538\n",
      "[-1.26218393 -1.15485875]\n",
      "Gradient Descent(6897/9): loss=16.226494581077475, w0=75.17610856382144, w1=13.461133509387345\n",
      "[ 1.37811581 -0.37372027]\n",
      "Gradient Descent(6898/9): loss=17.157373583620284, w0=74.21142749724561, w1=13.722737695806002\n",
      "[ 2.73215615 -0.60805755]\n",
      "Gradient Descent(6899/9): loss=15.836326674333407, w0=72.2989181919859, w1=14.148377978480703\n",
      "[-2.63626117  2.57088461]\n",
      "Gradient Descent(6900/9): loss=16.10446096443184, w0=74.14430100768672, w1=12.348758749249608\n",
      "[-0.28761982  1.99647023]\n",
      "Gradient Descent(6901/9): loss=16.38698821504013, w0=74.34563488102151, w1=10.951229585931177\n",
      "[-0.55081793 -1.45148246]\n",
      "Gradient Descent(6902/9): loss=19.135550617658538, w0=74.73120742924058, w1=11.967267308514035\n",
      "[ 3.24827778 -1.69523551]\n",
      "Gradient Descent(6903/9): loss=17.5625276986563, w0=72.45741298367112, w1=13.153932162953218\n",
      "[-0.54268816 -0.47481946]\n",
      "Gradient Descent(6904/9): loss=15.788827930614035, w0=72.83729469905764, w1=13.486305784938551\n",
      "[ 0.54007376  0.34011118]\n",
      "Gradient Descent(6905/9): loss=15.49016385190542, w0=72.45924306977693, w1=13.248227956707904\n",
      "[-2.21540643 -2.99941579]\n",
      "Gradient Descent(6906/9): loss=15.761024860708266, w0=74.01002756968917, w1=15.347819008843272\n",
      "[ 2.34034744  1.33371333]\n",
      "Gradient Descent(6907/9): loss=17.38720254643047, w0=72.371784361119, w1=14.414219679223905\n",
      "[ 2.52340277  0.65995007]\n",
      "Gradient Descent(6908/9): loss=16.247708678054902, w0=70.60540242116573, w1=13.952254633612798\n",
      "[-2.6728259  -0.14053818]\n",
      "Gradient Descent(6909/9): loss=19.11160470211691, w0=72.47638055322658, w1=14.050631358304235\n",
      "[-2.69551338 -0.44640228]\n",
      "Gradient Descent(6910/9): loss=15.883049087646356, w0=74.36323991892488, w1=14.363112952448082\n",
      "[-0.69597748  4.52417636]\n",
      "Gradient Descent(6911/9): loss=16.34780650956864, w0=74.85042415246144, w1=11.196189503256804\n",
      "[ 3.22519608 -2.54859008]\n",
      "Gradient Descent(6912/9): loss=19.204475830734737, w0=72.5927868948936, w1=12.980202559104908\n",
      "[-4.27872087 -0.47584343]\n",
      "Gradient Descent(6913/9): loss=15.756438146164594, w0=75.58789150175765, w1=13.313292961273918\n",
      "[ 0.61139322  1.22484415]\n",
      "Gradient Descent(6914/9): loss=18.030883622113098, w0=75.15991624538928, w1=12.455902055083124\n",
      "[ 0.07144168  0.98401354]\n",
      "Gradient Descent(6915/9): loss=17.650948973815648, w0=75.10990706934678, w1=11.767092574560216\n",
      "[ 1.98410564 -0.10361827]\n",
      "Gradient Descent(6916/9): loss=18.501322144219255, w0=73.72103312431514, w1=11.839625367045283\n",
      "[ 2.96728944 -1.67354124]\n",
      "Gradient Descent(6917/9): loss=16.822042619405316, w0=71.64393051310637, w1=13.011104238187867\n",
      "[-0.02820522  0.98016553]\n",
      "Gradient Descent(6918/9): loss=16.856920646768298, w0=71.66367416929532, w1=12.324988369821773\n",
      "[-1.8249894  -1.00818559]\n",
      "Gradient Descent(6919/9): loss=17.381435700358256, w0=72.94116675159053, w1=13.03071828087993\n",
      "[-0.8430188  -1.64874982]\n",
      "Gradient Descent(6920/9): loss=15.548903877424312, w0=73.53127990993919, w1=14.184843153464264\n",
      "[-1.32434844  1.34734706]\n",
      "Gradient Descent(6921/9): loss=15.662661922103752, w0=74.45832382081149, w1=13.241700209120527\n",
      "[ 3.20047341  0.99180468]\n",
      "Gradient Descent(6922/9): loss=16.09212857636411, w0=72.21799243086599, w1=12.54743693272725\n",
      "[-1.31978961 -1.4306986 ]\n",
      "Gradient Descent(6923/9): loss=16.399268896021624, w0=73.14184515748826, w1=13.548925951746982\n",
      "[ 2.44974042 -2.17560936]\n",
      "Gradient Descent(6924/9): loss=15.39984680761472, w0=71.42702686559333, w1=15.07185250298885\n",
      "[-0.91451983  3.28067926]\n",
      "Gradient Descent(6925/9): loss=18.395991592260422, w0=72.06719074346483, w1=12.775377017805923\n",
      "[-3.2514619  -1.81650568]\n",
      "Gradient Descent(6926/9): loss=16.386366849241337, w0=74.34321407325524, w1=14.046930996819006\n",
      "[ 1.56451165 -2.71411626]\n",
      "Gradient Descent(6927/9): loss=16.097263242560807, w0=73.24805591760786, w1=15.94681237858248\n",
      "[ 0.63101942  1.29482162]\n",
      "Gradient Descent(6928/9): loss=18.430230783522315, w0=72.80634232405855, w1=15.040437246225714\n",
      "[ 0.2132481  -0.02276335]\n",
      "Gradient Descent(6929/9): loss=16.722685808256298, w0=72.6570686574032, w1=15.056371591575562\n",
      "[ 2.41232853  0.12530807]\n",
      "Gradient Descent(6930/9): loss=16.831606008182504, w0=70.96843868344062, w1=14.968655943892651\n",
      "[-4.59797555  2.32075845]\n",
      "Gradient Descent(6931/9): loss=19.198300587876073, w0=74.1870215691025, w1=13.344125027650682\n",
      "[ 4.46763736 -1.03608494]\n",
      "Gradient Descent(6932/9): loss=15.79389325962916, w0=71.05967541855125, w1=14.069384482976817\n",
      "[-2.41121719  1.53936457]\n",
      "Gradient Descent(6933/9): loss=18.05567332897967, w0=72.74752745273031, w1=12.991829281819125\n",
      "[-2.14043242  1.01732816]\n",
      "Gradient Descent(6934/9): loss=15.6541763561962, w0=74.24583014531012, w1=12.279699569640886\n",
      "[ 2.16794274 -1.81744093]\n",
      "Gradient Descent(6935/9): loss=16.558967863879886, w0=72.7282702271416, w1=13.551908223921869\n",
      "[-0.27595885  0.38968783]\n",
      "Gradient Descent(6936/9): loss=15.548474950058942, w0=72.92144141968798, w1=13.279126741560463\n",
      "[ 0.34773654 -1.15118749]\n",
      "Gradient Descent(6937/9): loss=15.475376071172446, w0=72.67802583934977, w1=14.084957982957043\n",
      "[ 2.66652686  2.09018061]\n",
      "Gradient Descent(6938/9): loss=15.758712997145363, w0=70.8114570373228, w1=12.621831557143848\n",
      "[-1.76290791 -0.90150784]\n",
      "Gradient Descent(6939/9): loss=18.835183819801646, w0=72.04549257653683, w1=13.252887047904965\n",
      "[-1.5181206   0.37814981]\n",
      "Gradient Descent(6940/9): loss=16.190900762254795, w0=73.10817699581347, w1=12.988182180199841\n",
      "[-1.64341024 -0.76954014]\n",
      "Gradient Descent(6941/9): loss=15.52393946819713, w0=74.2585641647858, w1=13.526860280769808\n",
      "[ 1.27570149 -0.88223312]\n",
      "Gradient Descent(6942/9): loss=15.852266579520842, w0=73.36557312087781, w1=14.144423467875463\n",
      "[ 0.2198337  -0.21056775]\n",
      "Gradient Descent(6943/9): loss=15.609375188860545, w0=73.21168953230568, w1=14.291820891506507\n",
      "[-1.45847834 -1.82143191]\n",
      "Gradient Descent(6944/9): loss=15.719029030947649, w0=74.23262436805251, w1=15.566823229003697\n",
      "[ 3.98031387  1.74754576]\n",
      "Gradient Descent(6945/9): loss=18.004484667993175, w0=71.44640465791542, w1=14.343541194293731\n",
      "[-3.58223153 -0.58581845]\n",
      "Gradient Descent(6946/9): loss=17.46564810007134, w0=73.95396673159927, w1=14.753614112534006\n",
      "[ 3.39243715  0.98817468]\n",
      "Gradient Descent(6947/9): loss=16.41513013332178, w0=71.57926072477682, w1=14.061891837563106\n",
      "[-2.39212415  0.95611838]\n",
      "Gradient Descent(6948/9): loss=17.025385945204828, w0=73.25374763262931, w1=13.39260896950723\n",
      "[ 2.12697236 -2.17647157]\n",
      "Gradient Descent(6949/9): loss=15.390488365660264, w0=71.76486698319891, w1=14.916139065666634\n",
      "[-0.58946453  0.92837883]\n",
      "Gradient Descent(6950/9): loss=17.58655322691052, w0=72.17749215097625, w1=14.266273886331625\n",
      "[ 1.21200604  1.55600022]\n",
      "Gradient Descent(6951/9): loss=16.318435133444368, w0=71.3290879247638, w1=13.177073731740478\n",
      "[-2.14944297 -1.22729447]\n",
      "Gradient Descent(6952/9): loss=17.361969436922383, w0=72.83369800481111, w1=14.036179859828628\n",
      "[ 0.58731996 -0.6542744 ]\n",
      "Gradient Descent(6953/9): loss=15.646618930125864, w0=72.42257403495078, w1=14.494171938583063\n",
      "[-0.23762246  1.66682402]\n",
      "Gradient Descent(6954/9): loss=16.280075550977564, w0=72.58890975527765, w1=13.32739512714019\n",
      "[-0.08790857  3.15381768]\n",
      "Gradient Descent(6955/9): loss=15.646009284052965, w0=72.65044575469774, w1=11.119722748213539\n",
      "[-2.05511616 -3.42226552]\n",
      "Gradient Descent(6956/9): loss=18.377694370161567, w0=74.08902706419123, w1=13.515308611022258\n",
      "[-0.15924956  1.98806174]\n",
      "Gradient Descent(6957/9): loss=15.702617442580912, w0=74.20050175780095, w1=12.12366539277553\n",
      "[-0.62421117  1.6954409 ]\n",
      "Gradient Descent(6958/9): loss=16.716263085896113, w0=74.637449577582, w1=10.936856760373827\n",
      "[ 1.18981929  0.83152507]\n",
      "Gradient Descent(6959/9): loss=19.521478532824126, w0=73.80457607571913, w1=10.354789213462421\n",
      "[ 0.95580783 -3.22906186]\n",
      "Gradient Descent(6960/9): loss=20.398844230496834, w0=73.13551059276801, w1=12.61513251338113\n",
      "[-2.30415203  0.5698942 ]\n",
      "Gradient Descent(6961/9): loss=15.772184176557273, w0=74.74841701172149, w1=12.216206573716587\n",
      "[ 6.38954458 -3.60051138]\n",
      "Gradient Descent(6962/9): loss=17.24188926606369, w0=70.27573580573932, w1=14.736564538727002\n",
      "[-4.28381673  4.95166609]\n",
      "Gradient Descent(6963/9): loss=20.730450432131413, w0=73.27440751704148, w1=11.270398276546057\n",
      "[ 0.26428366 -0.53583783]\n",
      "Gradient Descent(6964/9): loss=17.826612801741383, w0=73.08940895382527, w1=11.645484756000545\n",
      "[-0.25847413 -7.46616297]\n",
      "Gradient Descent(6965/9): loss=17.08899625147155, w0=73.27034084551168, w1=16.871798836219902\n",
      "[ 3.43565863  4.08390895]\n",
      "Gradient Descent(6966/9): loss=21.139290981010188, w0=70.86537980754244, w1=14.013062571937592\n",
      "[-4.02774226  2.33150035]\n",
      "Gradient Descent(6967/9): loss=18.47702764850675, w0=73.68479938981525, w1=12.381012326810747\n",
      "[ 1.80740921 -1.63685019]\n",
      "Gradient Descent(6968/9): loss=16.06585139879643, w0=72.41961294209766, w1=13.526807461873984\n",
      "[-3.21358395  0.19792053]\n",
      "Gradient Descent(6969/9): loss=15.769205005813673, w0=74.66912170539008, w1=13.388263091834537\n",
      "[ 0.84640856 -1.74961582]\n",
      "Gradient Descent(6970/9): loss=16.335656471968523, w0=74.07663571363604, w1=14.612994166950418\n",
      "[ 0.67069048  0.0331334 ]\n",
      "Gradient Descent(6971/9): loss=16.334371987937278, w0=73.60715237473673, w1=14.589800787686258\n",
      "[-0.86772402 -1.20215371]\n",
      "Gradient Descent(6972/9): loss=16.051092577395845, w0=74.21455919117594, w1=15.431308382128886\n",
      "[ 0.85964812  1.3122343 ]\n",
      "Gradient Descent(6973/9): loss=17.71403765622576, w0=73.61280550386896, w1=14.512744373317926\n",
      "[-0.38571792  0.21884443]\n",
      "Gradient Descent(6974/9): loss=15.970308705481719, w0=73.88280804814752, w1=14.35955327005215\n",
      "[ 2.51682115 -1.49551055]\n",
      "Gradient Descent(6975/9): loss=15.946341203963355, w0=72.12103324322698, w1=15.40641065331806\n",
      "[-0.57217787  7.91165204]\n",
      "Gradient Descent(6976/9): loss=17.929804901436935, w0=72.52155775375792, w1=9.868254227461962\n",
      "[ 1.02617647 -1.67560226]\n",
      "Gradient Descent(6977/9): loss=22.205476327249297, w0=71.8032342234675, w1=11.041175807824159\n",
      "[ 1.6412123  -0.62212114]\n",
      "Gradient Descent(6978/9): loss=19.470193336531644, w0=70.65438561342022, w1=11.476660604875764\n",
      "[ 0.05536403 -1.58443469]\n",
      "Gradient Descent(6979/9): loss=20.875572359485535, w0=70.61563079580164, w1=12.585764885660618\n",
      "[-4.30081803  0.94745763]\n",
      "Gradient Descent(6980/9): loss=19.37208087218602, w0=73.62620341342289, w1=11.922544542102312\n",
      "[-0.4663991  -0.79469243]\n",
      "Gradient Descent(6981/9): loss=16.653479260301705, w0=73.95268278608953, w1=12.478829245265512\n",
      "[ 3.397036    1.42016619]\n",
      "Gradient Descent(6982/9): loss=16.10375433382281, w0=71.57475758817513, w1=11.48471291537648\n",
      "[-4.32713801 -0.08224863]\n",
      "Gradient Descent(6983/9): loss=18.853662551518326, w0=74.60375419500642, w1=11.542286953229237\n",
      "[ 1.47455485 -0.35705679]\n",
      "Gradient Descent(6984/9): loss=18.12052680429574, w0=73.57156580035469, w1=11.792226703948193\n",
      "[ 4.23254372 -4.90699601]\n",
      "Gradient Descent(6985/9): loss=16.84823495441585, w0=70.6087851950282, w1=15.227123913996401\n",
      "[-2.93767913  4.82916859]\n",
      "Gradient Descent(6986/9): loss=20.51759114367253, w0=72.66516058658547, w1=11.846705899394232\n",
      "[-2.48426702 -1.97318437]\n",
      "Gradient Descent(6987/9): loss=16.91691350030027, w0=74.40414750009566, w1=13.22793495685346\n",
      "[ 2.54486263 -0.26015169]\n",
      "Gradient Descent(6988/9): loss=16.033884146271653, w0=72.62274365615038, w1=13.410041139366442\n",
      "[-1.53873654 -0.60263362]\n",
      "Gradient Descent(6989/9): loss=15.613555099585588, w0=73.69985923322986, w1=13.83188467498678\n",
      "[ 1.47370881 -0.83296827]\n",
      "Gradient Descent(6990/9): loss=15.53029302994849, w0=72.6682630639546, w1=14.414962464361556\n",
      "[ 0.8003717   1.46122342]\n",
      "Gradient Descent(6991/9): loss=16.0189587309939, w0=72.10800287223655, w1=13.392106071662319\n",
      "[ 0.22009081  0.35735126]\n",
      "Gradient Descent(6992/9): loss=16.09292739757126, w0=71.95393930429242, w1=13.14196019113694\n",
      "[-1.79613707  0.27557538]\n",
      "Gradient Descent(6993/9): loss=16.34070297316176, w0=73.21123525336876, w1=12.94905742832287\n",
      "[ 1.23413514 -0.5647979 ]\n",
      "Gradient Descent(6994/9): loss=15.530103786087638, w0=72.34734065551855, w1=13.344415960853693\n",
      "[-2.59996213 -0.67269576]\n",
      "Gradient Descent(6995/9): loss=15.843048559639016, w0=74.16731414660751, w1=13.815302991661818\n",
      "[ 3.70114871  0.14778685]\n",
      "Gradient Descent(6996/9): loss=15.823605298732552, w0=71.57651005140562, w1=13.711852195932387\n",
      "[ 0.76622346  0.08719065]\n",
      "Gradient Descent(6997/9): loss=16.88758420733767, w0=71.04015363149867, w1=13.6508187429687\n",
      "[-3.83932293  2.05096193]\n",
      "Gradient Descent(6998/9): loss=17.9402624873178, w0=73.72767968566917, w1=12.215145394957872\n",
      "[ 3.75184739 -2.35979019]\n",
      "Gradient Descent(6999/9): loss=16.279525632221404, w0=71.10138651133761, w1=13.866998524690997\n",
      "[-0.26015306 -0.89425841]\n",
      "Gradient Descent(7000/9): loss=17.864489065605433, w0=71.28349365575009, w1=14.492979409709342\n",
      "[-1.68389591  1.5778158 ]\n",
      "Gradient Descent(7001/9): loss=17.920153917772854, w0=72.46222079524829, w1=13.388508346475792\n",
      "[-0.78880982  0.20837243]\n",
      "Gradient Descent(7002/9): loss=15.73591041045378, w0=73.01438767014629, w1=13.242647642052724\n",
      "[ 0.99251598  0.21850781]\n",
      "Gradient Descent(7003/9): loss=15.453057448226229, w0=72.31962648233217, w1=13.08969217746981\n",
      "[-2.45059186 -0.61471196]\n",
      "Gradient Descent(7004/9): loss=15.93657164939198, w0=74.0350407818407, w1=13.519990548387742\n",
      "[-2.58730721 -1.17208381]\n",
      "Gradient Descent(7005/9): loss=15.66132755487721, w0=75.84615582699266, w1=14.340449214325066\n",
      "[ 5.261043    3.80712261]\n",
      "Gradient Descent(7006/9): loss=19.013270518930124, w0=72.16342572808577, w1=11.675463388219669\n",
      "[-2.05356492 -1.92517792]\n",
      "Gradient Descent(7007/9): loss=17.652556092999404, w0=73.60092117012606, w1=13.023087934376226\n",
      "[-0.89404783  0.46024212]\n",
      "Gradient Descent(7008/9): loss=15.537265080692107, w0=74.22675464988599, w1=12.700918447945083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.96459263 -1.14125825]\n",
      "Gradient Descent(7009/9): loss=16.124236280340188, w0=74.90196948931063, w1=13.499799226063043\n",
      "[ 0.78952239 -0.17490376]\n",
      "Gradient Descent(7010/9): loss=16.678997968971096, w0=74.34930381680807, w1=13.622231860381529\n",
      "[ 3.08776633  3.59754068]\n",
      "Gradient Descent(7011/9): loss=15.952959149539279, w0=72.18786738321474, w1=11.103953384178396\n",
      "[-3.36436998 -3.32158922]\n",
      "Gradient Descent(7012/9): loss=18.819681812568213, w0=74.54292637172121, w1=13.429065840824723\n",
      "[ 0.34474295 -1.03564691]\n",
      "Gradient Descent(7013/9): loss=16.167176365239573, w0=74.301606306096, w1=14.154018676899419\n",
      "[-1.71191237  0.22150531]\n",
      "Gradient Descent(7014/9): loss=16.12094615102377, w0=75.49994496614245, w1=13.998964958847893\n",
      "[ 1.16654552 -0.79261333]\n",
      "Gradient Descent(7015/9): loss=17.95396811952616, w0=74.68336309889082, w1=14.55379429056488\n",
      "[-1.90324128 -0.11780533]\n",
      "Gradient Descent(7016/9): loss=16.92798706578644, w0=76.01563199144029, w1=14.63625801831717\n",
      "[ 4.66582365  2.29196606]\n",
      "Gradient Descent(7017/9): loss=19.75853934501052, w0=72.74955543893267, w1=13.031881775016183\n",
      "[ 0.75266644 -2.00038688]\n",
      "Gradient Descent(7018/9): loss=15.634331496385391, w0=72.22268892950811, w1=14.43215258851022\n",
      "[-3.35211761 -1.72454781]\n",
      "Gradient Descent(7019/9): loss=16.41322913976201, w0=74.56917125338875, w1=15.639336055376893\n",
      "[ 3.63009924  2.97669853]\n",
      "Gradient Descent(7020/9): loss=18.53100528614758, w0=72.0281017860341, w1=13.555647081588686\n",
      "[-1.72292252 -0.65564759]\n",
      "Gradient Descent(7021/9): loss=16.189921313813638, w0=73.2341475521674, w1=14.014600393039627\n",
      "[ 2.38989332  0.35997449]\n",
      "Gradient Descent(7022/9): loss=15.53072692509584, w0=71.56122222770294, w1=13.762618246764706\n",
      "[-1.9820681   1.24372948]\n",
      "Gradient Descent(7023/9): loss=16.92702997210442, w0=72.94866989813237, w1=12.892007611771291\n",
      "[-0.38024605 -1.01475596]\n",
      "Gradient Descent(7024/9): loss=15.61818585609494, w0=73.21484213398735, w1=13.602336785068566\n",
      "[-0.67861048  1.81414158]\n",
      "Gradient Descent(7025/9): loss=15.39653304721638, w0=73.68986946825164, w1=12.332437681961082\n",
      "[ 0.73993983 -1.3662463 ]\n",
      "Gradient Descent(7026/9): loss=16.122394746270988, w0=73.17191158878853, w1=13.288810095324314\n",
      "[-3.95228568 -0.58939544]\n",
      "Gradient Descent(7027/9): loss=15.411552990952988, w0=75.93851156140104, w1=13.701386902437129\n",
      "[ 1.10106183  0.47574069]\n",
      "Gradient Descent(7028/9): loss=18.907384622156915, w0=75.167768277037, w1=13.368368421988674\n",
      "[ 2.46029616  0.88625204]\n",
      "Gradient Descent(7029/9): loss=17.14773654448282, w0=73.44556096704393, w1=12.747991995324067\n",
      "[-1.32004423  2.00280179]\n",
      "Gradient Descent(7030/9): loss=15.665092457585004, w0=74.36959193131722, w1=11.346030743424315\n",
      "[-1.27359379 -2.28006437]\n",
      "Gradient Descent(7031/9): loss=18.240719547594182, w0=75.26110758461971, w1=12.942075804947795\n",
      "[ 1.20788638 -2.12897612]\n",
      "Gradient Descent(7032/9): loss=17.465323999836954, w0=74.41558711807886, w1=14.432359089699544\n",
      "[ 4.70868439  2.42429403]\n",
      "Gradient Descent(7033/9): loss=16.468722009391062, w0=71.11950804543403, w1=12.73535327209356\n",
      "[-4.77918118 -1.68178067]\n",
      "Gradient Descent(7034/9): loss=18.026961178005795, w0=74.46493487471949, w1=13.912599742171231\n",
      "[-2.65751257  0.95363328]\n",
      "Gradient Descent(7035/9): loss=16.165219153103315, w0=76.32519367513082, w1=13.2450564435094\n",
      "[ 4.31062721  1.90640625]\n",
      "Gradient Descent(7036/9): loss=20.00772356384185, w0=73.3077546293653, w1=11.910572066027303\n",
      "[-1.13318393 -0.52557984]\n",
      "Gradient Descent(7037/9): loss=16.61708428837056, w0=74.1009833776265, w1=12.278477952592654\n",
      "[ 2.58250323 -1.40307282]\n",
      "Gradient Descent(7038/9): loss=16.433044041607648, w0=72.29323111935312, w1=13.260628928879333\n",
      "[-2.42988514 -4.71467655]\n",
      "Gradient Descent(7039/9): loss=15.910577781565616, w0=73.99415071697881, w1=16.560902511295087\n",
      "[ 2.13868396  0.33356865]\n",
      "Gradient Descent(7040/9): loss=20.377914138559582, w0=72.49707194592894, w1=16.327404459163688\n",
      "[-0.04083782  1.14844501]\n",
      "Gradient Descent(7041/9): loss=19.75804780711738, w0=72.52565842289874, w1=15.523492951836195\n",
      "[-0.1612735   2.56099237]\n",
      "Gradient Descent(7042/9): loss=17.769521732918946, w0=72.63854987034894, w1=13.730798294192596\n",
      "[-0.67085373  1.15096555]\n",
      "Gradient Descent(7043/9): loss=15.632166238716756, w0=73.1081474786293, w1=12.92512240889538\n",
      "[ 1.07974961  2.15745231]\n",
      "Gradient Descent(7044/9): loss=15.556929004137034, w0=72.35232275218384, w1=11.414905789326058\n",
      "[-3.5688543  -1.18896889]\n",
      "Gradient Descent(7045/9): loss=17.96090568454265, w0=74.8505207593867, w1=12.247184008876657\n",
      "[ 3.0673158  -1.39981248]\n",
      "Gradient Descent(7046/9): loss=17.356950875002113, w0=72.70339969701105, w1=13.227052745263222\n",
      "[ 0.41457677  0.88147754]\n",
      "Gradient Descent(7047/9): loss=15.592164624642423, w0=72.41319596019137, w1=12.610018465198578\n",
      "[-0.31353887  0.68671821]\n",
      "Gradient Descent(7048/9): loss=16.151910849826944, w0=72.63267316740492, w1=12.129315719070721\n",
      "[ 0.95554256 -2.54743006]\n",
      "Gradient Descent(7049/9): loss=16.516298524707135, w0=71.96379337631264, w1=13.91251675807705\n",
      "[-1.08608261 -2.54886986]\n",
      "Gradient Descent(7050/9): loss=16.364168740447624, w0=72.72405120318315, w1=15.696725663538533\n",
      "[ 3.23864841  3.46793907]\n",
      "Gradient Descent(7051/9): loss=18.005838060343127, w0=70.45699731433388, w1=13.269168311184059\n",
      "[-5.55870639  4.35787659]\n",
      "Gradient Descent(7052/9): loss=19.432123124906987, w0=74.34809178523022, w1=10.218654699368848\n",
      "[ 4.20136657 -2.55302093]\n",
      "Gradient Descent(7053/9): loss=21.25877361218052, w0=71.40713518376454, w1=12.005769351063412\n",
      "[-2.11618716 -1.06162519]\n",
      "Gradient Descent(7054/9): loss=18.25212422508752, w0=72.88846619573496, w1=12.748906986932544\n",
      "[-1.87859748  2.15567066]\n",
      "Gradient Descent(7055/9): loss=15.735123375743598, w0=74.20348443366464, w1=11.239937524783713\n",
      "[ 1.15839802 -0.42858036]\n",
      "Gradient Descent(7056/9): loss=18.307835601474235, w0=73.3926058174293, w1=11.539943775089471\n",
      "[ 0.06593483 -2.74293603]\n",
      "Gradient Descent(7057/9): loss=17.27210834349716, w0=73.346451434438, w1=13.45999899876797\n",
      "[ 0.91878062  0.53117142]\n",
      "Gradient Descent(7058/9): loss=15.387461849243826, w0=72.70330500146545, w1=13.088179001572378\n",
      "[ 0.16035219 -1.03381388]\n",
      "Gradient Descent(7059/9): loss=15.63695130429326, w0=72.59105846781287, w1=13.811848718652453\n",
      "[ 0.59783606 -1.630357  ]\n",
      "Gradient Descent(7060/9): loss=15.688053698211213, w0=72.17257322242209, w1=14.953098617291825\n",
      "[-2.70228674  0.89636157]\n",
      "Gradient Descent(7061/9): loss=17.10003283277817, w0=74.06417394174404, w1=14.325645516017294\n",
      "[ 0.07258238 -0.11832268]\n",
      "Gradient Descent(7062/9): loss=16.04033328287708, w0=74.01336627704168, w1=14.408471394677582\n",
      "[ 0.60586662 -2.31354079]\n",
      "Gradient Descent(7063/9): loss=16.075984503799713, w0=73.58925964059867, w1=16.02794995092997\n",
      "[-0.82812733  2.34957847]\n",
      "Gradient Descent(7064/9): loss=18.67625724800923, w0=74.1689487697215, w1=14.38324502202546\n",
      "[ 3.74704855  0.29381672]\n",
      "Gradient Descent(7065/9): loss=16.176909358770278, w0=71.54601478560696, w1=14.177573314738664\n",
      "[-5.03515763  0.33258122]\n",
      "Gradient Descent(7066/9): loss=17.156982591315156, w0=75.07062512942805, w1=13.94476646020108\n",
      "[-0.57803645  2.29414192]\n",
      "Gradient Descent(7067/9): loss=17.072362493331777, w0=75.47525064147509, w1=12.338867116221934\n",
      "[ 0.0854615   0.25478004]\n",
      "Gradient Descent(7068/9): loss=18.415749205973484, w0=75.41542759387647, w1=12.160521088773416\n",
      "[ 4.60583964 -1.20609029]\n",
      "Gradient Descent(7069/9): loss=18.506413760752917, w0=72.19133984728859, w1=13.004784289523789\n",
      "[ 2.64948915 -1.53748482]\n",
      "Gradient Descent(7070/9): loss=16.106509944566945, w0=70.33669744496393, w1=14.08102366072663\n",
      "[-7.21934454  0.58730692]\n",
      "Gradient Descent(7071/9): loss=19.939264004608074, w0=75.39023862148113, w1=13.669908814082183\n",
      "[ 2.52350578 -0.21578642]\n",
      "Gradient Descent(7072/9): loss=17.60124688447536, w0=73.62378457644627, w1=13.820959306901274\n",
      "[ 0.40317703 -0.9141224 ]\n",
      "Gradient Descent(7073/9): loss=15.498517241599801, w0=73.3415606536008, w1=14.460844987227333\n",
      "[-0.25512755  2.86977268]\n",
      "Gradient Descent(7074/9): loss=15.868333131918364, w0=73.52014993535079, w1=12.452004110088184\n",
      "[ 0.42157854  0.52820344]\n",
      "Gradient Descent(7075/9): loss=15.939569608254958, w0=73.22504495728822, w1=12.082261699792078\n",
      "[-0.93030035 -0.33172325]\n",
      "Gradient Descent(7076/9): loss=16.364694171132037, w0=73.87625520370459, w1=12.31446797403462\n",
      "[ 0.98963459 -2.33755223]\n",
      "Gradient Descent(7077/9): loss=16.23434117456439, w0=73.18351098951527, w1=13.950754537970582\n",
      "[-2.9365821   2.31322504]\n",
      "Gradient Descent(7078/9): loss=15.5029234960706, w0=75.23911845827968, w1=12.331497010535895\n",
      "[ 6.91935684 -2.78126573]\n",
      "Gradient Descent(7079/9): loss=17.936981825862365, w0=70.39556866891888, w1=14.278383019811791\n",
      "[-4.06665232 -1.30613886]\n",
      "Gradient Descent(7080/9): loss=19.905051242356038, w0=73.24222529284937, w1=15.19268022447973\n",
      "[ 1.83000569  7.16455797]\n",
      "Gradient Descent(7081/9): loss=16.854353467619635, w0=71.96122130746275, w1=10.177489647695761\n",
      "[ 2.18623078 -4.5232272 ]\n",
      "Gradient Descent(7082/9): loss=21.726271108039136, w0=70.43085976254724, w1=13.343748688483545\n",
      "[ 1.39832974  0.68736013]\n",
      "Gradient Descent(7083/9): loss=19.493693632802586, w0=69.45202894514927, w1=12.862596599863053\n",
      "[-1.90460088 -2.07780562]\n",
      "Gradient Descent(7084/9): loss=22.956374976354095, w0=70.78524956274265, w1=14.31706053069391\n",
      "[-0.45196652  5.06238905]\n",
      "Gradient Descent(7085/9): loss=18.88318248952828, w0=71.10162612481541, w1=10.77338819802561\n",
      "[-0.70968859 -2.91082933]\n",
      "Gradient Descent(7086/9): loss=21.45106391340814, w0=71.59840813700977, w1=12.810968731799875\n",
      "[-2.62848106 -0.82855334]\n",
      "Gradient Descent(7087/9): loss=17.04688057247239, w0=73.43834487734127, w1=13.390956066723462\n",
      "[ 1.62980869  1.88286979]\n",
      "Gradient Descent(7088/9): loss=15.400255698728975, w0=72.29747879693493, w1=12.072947215387831\n",
      "[ 5.0081007  -2.14144672]\n",
      "Gradient Descent(7089/9): loss=16.871831590934224, w0=68.79180830368479, w1=13.571959921887954\n",
      "[-7.82096563 -2.55818282]\n",
      "Gradient Descent(7090/9): loss=25.524656545001292, w0=74.26648424790997, w1=15.362687899199221\n",
      "[-0.72238763  0.21274328]\n",
      "Gradient Descent(7091/9): loss=17.631624829220574, w0=74.77215558549139, w1=15.213767601480138\n",
      "[ 2.58266593  1.32062455]\n",
      "Gradient Descent(7092/9): loss=17.98194879257202, w0=72.96428943423885, w1=14.289330416821356\n",
      "[-1.73846242  1.08625495]\n",
      "Gradient Descent(7093/9): loss=15.767957321981587, w0=74.1812131295996, w1=13.528951953450228\n",
      "[ 1.18002612 -0.04096226]\n",
      "Gradient Descent(7094/9): loss=15.780742906383695, w0=73.35519484775823, w1=13.557625535874124\n",
      "[-2.08811837  1.02086371]\n",
      "Gradient Descent(7095/9): loss=15.390800275281375, w0=74.81687770817437, w1=12.8430209377734\n",
      "[ 1.95801797 -1.23130376]\n",
      "Gradient Descent(7096/9): loss=16.7482729414671, w0=73.44626513027687, w1=13.704933570055058\n",
      "[ 0.5086539  -1.23884435]\n",
      "Gradient Descent(7097/9): loss=15.422854363020178, w0=73.09020740203755, w1=14.572124612130095\n",
      "[ 0.54966374 -0.28983586]\n",
      "Gradient Descent(7098/9): loss=16.00331987035278, w0=72.70544278331253, w1=14.775009712058539\n",
      "[-1.28886401  1.12928034]\n",
      "Gradient Descent(7099/9): loss=16.397939282296626, w0=73.60764758754813, w1=13.98451347462499\n",
      "[ 1.26395298 -1.23229433]\n",
      "Gradient Descent(7100/9): loss=15.562511785118922, w0=72.7228805001619, w1=14.847119506179205\n",
      "[-1.33328861  3.9409735 ]\n",
      "Gradient Descent(7101/9): loss=16.483833116470638, w0=73.65618252795758, w1=12.088438057825394\n",
      "[ 3.94936922 -0.07794295]\n",
      "Gradient Descent(7102/9): loss=16.419326409400885, w0=70.891624074755, w1=12.142998126064107\n",
      "[-5.8197339   0.72113423]\n",
      "Gradient Descent(7103/9): loss=19.164808106547248, w0=74.96543780258871, w1=11.63820416342236\n",
      "[ 2.26436719 -1.96236234]\n",
      "Gradient Descent(7104/9): loss=18.4784467615867, w0=73.38038077011711, w1=13.011857799328183\n",
      "[-1.02072979  0.14315096]\n",
      "Gradient Descent(7105/9): loss=15.499069408167154, w0=74.09489162521372, w1=12.911652129465628\n",
      "[ 0.5211756  -0.31394426]\n",
      "Gradient Descent(7106/9): loss=15.868010292756395, w0=73.73006870853129, w1=13.131413112443312\n",
      "[ 2.31527825  2.12898775]\n",
      "Gradient Descent(7107/9): loss=15.54165605263548, w0=72.10937393419931, w1=11.641121690637895\n",
      "[-1.33614827 -0.35602262]\n",
      "Gradient Descent(7108/9): loss=17.777672894026036, w0=73.04467772475415, w1=11.890337525787656\n",
      "[-4.60321512 -2.14826097]\n",
      "Gradient Descent(7109/9): loss=16.68000552472499, w0=76.26692830787445, w1=13.394120201684984\n",
      "[ 2.31385454 -1.1695521 ]\n",
      "Gradient Descent(7110/9): loss=19.808934131102284, w0=74.64723013310771, w1=14.212806670832796\n",
      "[ 1.84100897 -1.49682037]\n",
      "Gradient Descent(7111/9): loss=16.570322896861832, w0=73.35852385488472, w1=15.2605809309306\n",
      "[ 2.91530969  0.68499768]\n",
      "Gradient Descent(7112/9): loss=16.973720868439234, w0=71.31780707070956, w1=14.78108255305102\n",
      "[-1.33747763 -0.69991047]\n",
      "Gradient Descent(7113/9): loss=18.18518507196409, w0=72.25404141297656, w1=15.271019881668277\n",
      "[-2.01738135 -0.92127548]\n",
      "Gradient Descent(7114/9): loss=17.53095487291688, w0=73.66620835626652, w1=15.915912716622337\n",
      "[-0.9104215   1.94889309]\n",
      "Gradient Descent(7115/9): loss=18.422722339691827, w0=74.3035034083808, w1=14.551687554550234\n",
      "[-3.28195422  1.3563577 ]\n",
      "Gradient Descent(7116/9): loss=16.470080505257233, w0=76.60087135990094, w1=13.602237166741824\n",
      "[ 3.18810385 -0.88963325]\n",
      "Gradient Descent(7117/9): loss=20.86135105128779, w0=74.36919866670961, w1=14.224980439993214\n",
      "[ 1.22268809 -0.29838978]\n",
      "Gradient Descent(7118/9): loss=16.241710021192244, w0=73.51331700062485, w1=14.433853289205038\n",
      "[ 0.17610024 -0.62198905]\n",
      "Gradient Descent(7119/9): loss=15.86514733635913, w0=73.39004683199168, w1=14.869245626725498\n",
      "[ 0.05197367 -0.08419188]\n",
      "Gradient Descent(7120/9): loss=16.355909105758396, w0=73.3536652650171, w1=14.928179943212854\n",
      "[-0.07133642  2.25488018]\n",
      "Gradient Descent(7121/9): loss=16.436701558751118, w0=73.4036007609662, w1=13.349763814441657\n",
      "[ 1.35246851  0.34513141]\n",
      "Gradient Descent(7122/9): loss=15.400345905893133, w0=72.45687280086446, w1=13.108171828911392\n",
      "[-3.78921862 -0.30590404]\n",
      "Gradient Descent(7123/9): loss=15.805234762460547, w0=75.10932583206454, w1=13.322304653935882\n",
      "[ 0.16936751 -1.30396704]\n",
      "Gradient Descent(7124/9): loss=17.046122006512984, w0=74.99076857503488, w1=14.235081584541415\n",
      "[ 7.87182084  3.32035045]\n",
      "Gradient Descent(7125/9): loss=17.110823290908744, w0=69.48049398887625, w1=11.910836269434263\n",
      "[-5.29752885 -1.96714533]\n",
      "Gradient Descent(7126/9): loss=23.88769068629195, w0=73.18876418552898, w1=13.287837997236114\n",
      "[-2.38142081 -0.16187083]\n",
      "Gradient Descent(7127/9): loss=15.40982485195444, w0=74.8557587518726, w1=13.40114757681282\n",
      "[ 0.05646393 -2.05327717]\n",
      "Gradient Descent(7128/9): loss=16.60864110376154, w0=74.81623400283797, w1=14.838441598308373\n",
      "[ 3.79817016  2.87254777]\n",
      "Gradient Descent(7129/9): loss=17.46767725224414, w0=72.15751489364736, w1=12.82765815735127\n",
      "[-3.55486673  1.21045816]\n",
      "Gradient Descent(7130/9): loss=16.244185817399003, w0=74.64592160176059, w1=11.980337443202197\n",
      "[ 1.11764148 -1.03191314]\n",
      "Gradient Descent(7131/9): loss=17.4239020105615, w0=73.86357256847438, w1=12.702676642574785\n",
      "[-1.99698421 -2.6098222 ]\n",
      "Gradient Descent(7132/9): loss=15.850031064058204, w0=75.26146151837365, w1=14.529552182899867\n",
      "[ 3.43109976 -0.96895327]\n",
      "Gradient Descent(7133/9): loss=17.872575491015088, w0=72.85969168351473, w1=15.207819475115114\n",
      "[ 0.21248458  1.89194298]\n",
      "Gradient Descent(7134/9): loss=16.973342824687624, w0=72.71095247708715, w1=13.883459385777895\n",
      "[ 0.14257172  0.63599891]\n",
      "Gradient Descent(7135/9): loss=15.637320402514924, w0=72.61115226993618, w1=13.438260145504216\n",
      "[ 1.20921352 -2.34928355]\n",
      "Gradient Descent(7136/9): loss=15.619834268564233, w0=71.76470280275353, w1=15.082758628060493\n",
      "[-1.1850888   1.16476255]\n",
      "Gradient Descent(7137/9): loss=17.840022097222693, w0=72.59426496574078, w1=14.26742484128553\n",
      "[-2.72558077 -2.17252219]\n",
      "Gradient Descent(7138/9): loss=15.940893270613213, w0=74.50217150475497, w1=15.788190373552851\n",
      "[ 0.60954398  6.68904039]\n",
      "Gradient Descent(7139/9): loss=18.78035649557401, w0=74.07549071756631, w1=11.10586210198261\n",
      "[-0.0240569  -1.33827842]\n",
      "Gradient Descent(7140/9): loss=18.508895399080572, w0=74.09233054782942, w1=12.042656998082125\n",
      "[ 2.97382918 -2.88813823]\n",
      "Gradient Descent(7141/9): loss=16.737180136144016, w0=72.01065012036416, w1=14.064353759975194\n",
      "[ 0.08109852  0.77395988]\n",
      "Gradient Descent(7142/9): loss=16.380183969503758, w0=71.95388115288638, w1=13.52258184332789\n",
      "[-1.24608922 -1.27581503]\n",
      "Gradient Descent(7143/9): loss=16.28466150070259, w0=72.82614360990009, w1=14.415652367039518\n",
      "[-2.45165978  5.80873153]\n",
      "Gradient Descent(7144/9): loss=15.933287959139715, w0=74.54230545285385, w1=10.349540296844179\n",
      "[ 1.29089267 -2.6603561 ]\n",
      "Gradient Descent(7145/9): loss=21.06410729609019, w0=73.63868058158818, w1=12.211789564552387\n",
      "[ 0.30996386 -0.37468542]\n",
      "Gradient Descent(7146/9): loss=16.249131310581138, w0=73.4217058785378, w1=12.474069358625066\n",
      "[ 0.31086708 -0.39102274]\n",
      "Gradient Descent(7147/9): loss=15.899711226886877, w0=73.20409892230182, w1=12.74778527385553\n",
      "[-0.64706137  2.94750646]\n",
      "Gradient Descent(7148/9): loss=15.657780646264564, w0=73.65704187881279, w1=10.684530748818542\n",
      "[ 0.42709022 -1.81070285]\n",
      "Gradient Descent(7149/9): loss=19.358336220610973, w0=73.35807872293074, w1=11.952022744859919\n",
      "[ 0.72360628 -0.57189816]\n",
      "Gradient Descent(7150/9): loss=16.554863805906365, w0=72.85155432536605, w1=12.352351459883877\n",
      "[-2.38330079 -1.1287395 ]\n",
      "Gradient Descent(7151/9): loss=16.119203833636227, w0=74.51986487591557, w1=13.14246911291278\n",
      "[ 3.74680901 -2.35469135]\n",
      "Gradient Descent(7152/9): loss=16.194222362895154, w0=71.89709856807058, w1=14.79075305934262\n",
      "[-2.08535346  1.57637998]\n",
      "Gradient Descent(7153/9): loss=17.22085948111623, w0=73.35684599213498, w1=13.687287075168928\n",
      "[-1.66742825 -0.30536018]\n",
      "Gradient Descent(7154/9): loss=15.409411198712938, w0=74.52404576416524, w1=13.90103920022022\n",
      "[ 3.19992273  0.37566329]\n",
      "Gradient Descent(7155/9): loss=16.231248225371864, w0=72.28409985328089, w1=13.63807489431004\n",
      "[ 1.03130697 -1.21264053]\n",
      "Gradient Descent(7156/9): loss=15.908297589218558, w0=71.56218497600962, w1=14.486923268440684\n",
      "[-0.26678931  2.07613875]\n",
      "Gradient Descent(7157/9): loss=17.39258126411574, w0=71.74893749060867, w1=13.033626146878037\n",
      "[-1.13587187  0.51103057]\n",
      "Gradient Descent(7158/9): loss=16.6788729274318, w0=72.54404779975299, w1=12.675904744915641\n",
      "[ 2.49728307 -2.47495254]\n",
      "Gradient Descent(7159/9): loss=15.990096929816646, w0=70.79594964985017, w1=14.40837151995327\n",
      "[-2.49649614  0.79536467]\n",
      "Gradient Descent(7160/9): loss=18.93702465318793, w0=72.54349694834461, w1=13.851616252506012\n",
      "[-1.16494624 -0.64368231]\n",
      "Gradient Descent(7161/9): loss=15.736612974227027, w0=73.35895931885564, w1=14.302193866856802\n",
      "[-1.39337498  2.53465325]\n",
      "Gradient Descent(7162/9): loss=15.726240647998063, w0=74.33432180589517, w1=12.527936591327096\n",
      "[ 2.47189071 -0.81354538]\n",
      "Gradient Descent(7163/9): loss=16.38004237298173, w0=72.60399830677775, w1=13.097418355755822\n",
      "[ 0.92309555 -0.54141792]\n",
      "Gradient Descent(7164/9): loss=15.696959603024924, w0=71.95783141841943, w1=13.476410896728003\n",
      "[ 1.02172137  0.34026103]\n",
      "Gradient Descent(7165/9): loss=16.27846234281372, w0=71.24262645967848, w1=13.23822817235556\n",
      "[-1.3378453   0.34453231]\n",
      "Gradient Descent(7166/9): loss=17.51895189456907, w0=72.17911816803897, w1=12.997055555690219\n",
      "[-0.13679113  0.16663883]\n",
      "Gradient Descent(7167/9): loss=16.123760494621017, w0=72.27487196109116, w1=12.880408375666615\n",
      "[ 1.27240165 -1.09754751]\n",
      "Gradient Descent(7168/9): loss=16.084702039634916, w0=71.38419080603258, w1=13.648691632738103\n",
      "[-1.46364556  0.15143255]\n",
      "Gradient Descent(7169/9): loss=17.223701474091822, w0=72.40874270141482, w1=13.542688844608694\n",
      "[-3.14272763 -2.77284372]\n",
      "Gradient Descent(7170/9): loss=15.779642080099034, w0=74.60865203941626, w1=15.48367945024793\n",
      "[ 5.18666969  5.40312059]\n",
      "Gradient Descent(7171/9): loss=18.258087303456172, w0=70.97798325961725, w1=11.701495039216603\n",
      "[-4.69789569 -0.61450127]\n",
      "Gradient Descent(7172/9): loss=19.648702551621582, w0=74.26651024196019, w1=12.131645927579877\n",
      "[-3.81259547 -0.2496811 ]\n",
      "Gradient Descent(7173/9): loss=16.76749346518071, w0=76.93532707103438, w1=12.306422696545347\n",
      "[ 3.97875335 -1.33091924]\n",
      "Gradient Descent(7174/9): loss=22.704107712009648, w0=74.1501997239153, w1=13.238066166179765\n",
      "[-3.31815202 -0.35654731]\n",
      "Gradient Descent(7175/9): loss=15.78169009687829, w0=76.47290613971393, w1=13.487649285949313\n",
      "[ 4.26596846  0.53313074]\n",
      "Gradient Descent(7176/9): loss=20.438889439214968, w0=73.4867282202706, w1=13.114457767038171\n",
      "[ 0.27573884 -1.08919926]\n",
      "Gradient Descent(7177/9): loss=15.471180473940978, w0=73.29371103070623, w1=13.876897245746898\n",
      "[ 0.09568801 -1.85195453]\n",
      "Gradient Descent(7178/9): loss=15.46476577803224, w0=73.22672942509342, w1=15.173265413872906\n",
      "[ 0.34157751  1.18132791]\n",
      "Gradient Descent(7179/9): loss=16.822206136175335, w0=72.98762516501591, w1=14.346335873852016\n",
      "[-1.14545825 -0.1040298 ]\n",
      "Gradient Descent(7180/9): loss=15.808314837428185, w0=73.789445939236, w1=14.419156732829986\n",
      "[ 0.57122971  1.77768517]\n",
      "Gradient Descent(7181/9): loss=15.949937649337139, w0=73.38958513881106, w1=13.1747771165173\n",
      "[ 1.96540754 -1.18493692]\n",
      "Gradient Descent(7182/9): loss=15.436956360917335, w0=72.01379986134461, w1=14.004232958328076\n",
      "[-2.55221192  0.23964961]\n",
      "Gradient Descent(7183/9): loss=16.342805106164054, w0=73.80034820612978, w1=13.83647823393299\n",
      "[ 0.72543477 -2.99042129]\n",
      "Gradient Descent(7184/9): loss=15.577762536538835, w0=73.29254387019103, w1=15.929773133875411\n",
      "[-0.14860296  3.40564221]\n",
      "Gradient Descent(7185/9): loss=18.387287532566955, w0=73.39656594068987, w1=13.545823585496848\n",
      "[-0.41745228  0.38114661]\n",
      "Gradient Descent(7186/9): loss=15.39334110000422, w0=73.68878253376938, w1=13.279020960469504\n",
      "[ 0.41065256 -0.40177336]\n",
      "Gradient Descent(7187/9): loss=15.48398382253488, w0=73.40132574242224, w1=13.560262312441658\n",
      "[ 5.05217236 -1.37835341]\n",
      "Gradient Descent(7188/9): loss=15.39489979192526, w0=69.86480509297468, w1=14.525109698326386\n",
      "[-3.578216    2.95084035]\n",
      "Gradient Descent(7189/9): loss=21.8117369761684, w0=72.36955629416002, w1=12.4595214553483\n",
      "[ 1.40380205 -4.72472626]\n",
      "Gradient Descent(7190/9): loss=16.333508667312064, w0=71.38689485915648, w1=15.766829834803893\n",
      "[-3.20712293  6.60751255]\n",
      "Gradient Descent(7191/9): loss=19.819717131068867, w0=73.63188090854632, w1=11.141571047164152\n",
      "[ 2.49689959 -3.25218676]\n",
      "Gradient Descent(7192/9): loss=18.176448554780713, w0=71.88405119878065, w1=13.418101779991678\n",
      "[ 0.01304489 -0.06981076]\n",
      "Gradient Descent(7193/9): loss=16.38165364626749, w0=71.87491977629259, w1=13.4669693134677\n",
      "[-0.82849441 -1.94821308]\n",
      "Gradient Descent(7194/9): loss=16.392752720833013, w0=72.45486586432507, w1=14.830718472599502\n",
      "[-2.95799432  0.86173121]\n",
      "Gradient Descent(7195/9): loss=16.650504126832594, w0=74.52546188492646, w1=14.227506625989978\n",
      "[ 2.01700835 -1.0389945 ]\n",
      "Gradient Descent(7196/9): loss=16.423831186366478, w0=73.11355604082746, w1=14.954802772496482\n",
      "[-0.48056262  2.86955553]\n",
      "Gradient Descent(7197/9): loss=16.490099560727113, w0=73.44994987191828, w1=12.946113898048457\n",
      "[-0.41594486 -3.73992818]\n",
      "Gradient Descent(7198/9): loss=15.540423916221174, w0=73.74111127137311, w1=15.564063621995407\n",
      "[-2.21862145  1.68529157]\n",
      "Gradient Descent(7199/9): loss=17.658136925490997, w0=75.29414628748181, w1=14.384359523622091\n",
      "[-0.01883304  1.39288074]\n",
      "Gradient Descent(7200/9): loss=17.79552964222072, w0=75.30732941536017, w1=13.409343008470579\n",
      "[ 5.28686425  0.20042853]\n",
      "Gradient Descent(7201/9): loss=17.415268502798728, w0=71.60652444233284, w1=13.269043038288844\n",
      "[-1.04997576 -2.9199148 ]\n",
      "Gradient Descent(7202/9): loss=16.83173392854525, w0=72.34150747736882, w1=15.312983401543368\n",
      "[ 0.98429108  1.7521282 ]\n",
      "Gradient Descent(7203/9): loss=17.519875800699307, w0=71.652503719398, w1=14.086493660653442\n",
      "[ 1.00444645  0.19151479]\n",
      "Gradient Descent(7204/9): loss=16.917106586141507, w0=70.94939120444339, w1=13.952433307186277\n",
      "[-4.59336687 -1.11601359]\n",
      "Gradient Descent(7205/9): loss=18.2460327109272, w0=74.16474801537734, w1=14.733642821014843\n",
      "[ 1.18861587  3.23894372]\n",
      "Gradient Descent(7206/9): loss=16.55122754802453, w0=73.33271690690569, w1=12.466382213863078\n",
      "[ 1.073997 -1.922836]\n",
      "Gradient Descent(7207/9): loss=15.900059459672246, w0=72.58091900987128, w1=13.812367413797674\n",
      "[-2.38592838  0.8414387 ]\n",
      "Gradient Descent(7208/9): loss=15.695404169759737, w0=74.25106887618668, w1=13.223360324527874\n",
      "[-0.41083412 -0.91080821]\n",
      "Gradient Descent(7209/9): loss=15.876811140380335, w0=74.53865275918709, w1=13.860926073235374\n",
      "[-2.0194225   0.97037297]\n",
      "Gradient Descent(7210/9): loss=16.233227116634737, w0=75.95224850631858, w1=13.181664997163647\n",
      "[ 3.59574714 -0.60427646]\n",
      "Gradient Descent(7211/9): loss=18.963653907928247, w0=73.43522550848178, w1=13.604658516533636\n",
      "[ 3.46314871  1.2996597 ]\n",
      "Gradient Descent(7212/9): loss=15.403676970933232, w0=71.0110214147281, w1=12.694896729995286\n",
      "[-3.33962365 -0.91494305]\n",
      "Gradient Descent(7213/9): loss=18.299673260155146, w0=73.34875797046782, w1=13.33535686663714\n",
      "[-1.21382933 -2.59931347]\n",
      "Gradient Descent(7214/9): loss=15.397810625599636, w0=74.19843849820897, w1=15.154876297748336\n",
      "[ 2.93789988 -0.15580078]\n",
      "Gradient Descent(7215/9): loss=17.198049898238637, w0=72.14190858282473, w1=15.263936845496117\n",
      "[-2.00572485  0.43383926]\n",
      "Gradient Descent(7216/9): loss=17.64118370145518, w0=73.5459159808342, w1=14.96024936632346\n",
      "[ 1.2024535   0.28076857]\n",
      "Gradient Descent(7217/9): loss=16.513633154009796, w0=72.70419852964176, w1=14.763711365722303\n",
      "[ 3.68312408  2.93219642]\n",
      "Gradient Descent(7218/9): loss=16.384101382878637, w0=70.12601167624976, w1=12.71117387233733\n",
      "[-2.01880969 -1.01610809]\n",
      "Gradient Descent(7219/9): loss=20.699041546301512, w0=71.53917845997927, w1=13.422449535003974\n",
      "[-4.63835502  0.53371604]\n",
      "Gradient Descent(7220/9): loss=16.927089838003063, w0=74.78602697407942, w1=13.048848310104917\n",
      "[-1.28948705  0.44357065]\n",
      "Gradient Descent(7221/9): loss=16.591898439580593, w0=75.68866791075111, w1=12.738348858540832\n",
      "[ 1.50013767 -0.84975408]\n",
      "Gradient Descent(7222/9): loss=18.528101828559628, w0=74.63857154287825, w1=13.333176715605315\n",
      "[ 2.27513266 -0.2708613 ]\n",
      "Gradient Descent(7223/9): loss=16.30066542110766, w0=73.0459786834301, w1=13.522779625644437\n",
      "[ 0.37437066  3.69380724]\n",
      "Gradient Descent(7224/9): loss=15.417553204922683, w0=72.78391922385693, w1=10.937114556554334\n",
      "[-1.2666759  -2.64375612]\n",
      "Gradient Descent(7225/9): loss=18.74834127145023, w0=73.67059235428054, w1=12.787743841467663\n",
      "[-0.23724671 -1.35045555]\n",
      "Gradient Descent(7226/9): loss=15.696238413143336, w0=73.8366650510004, w1=13.733062728676144\n",
      "[ 1.67767749  0.42378152]\n",
      "Gradient Descent(7227/9): loss=15.565266063047101, w0=72.66229080520375, w1=13.436415661509887\n",
      "[-0.75018703 -2.42197603]\n",
      "Gradient Descent(7228/9): loss=15.586304158575825, w0=73.18742172626885, w1=15.131798881530102\n",
      "[ 1.51178658  0.44188064]\n",
      "Gradient Descent(7229/9): loss=16.75625383662833, w0=72.12917112014061, w1=14.822482431968643\n",
      "[-0.95658495  0.27040867]\n",
      "Gradient Descent(7230/9): loss=16.965725809742324, w0=72.79878058863721, w1=14.633196364767278\n",
      "[ 1.73952571 -0.20905948]\n",
      "Gradient Descent(7231/9): loss=16.173732966623245, w0=71.58111259065944, w1=14.77953799803388\n",
      "[-0.17094899  1.44669275]\n",
      "Gradient Descent(7232/9): loss=17.69751915597037, w0=71.70077688368269, w1=13.76685307542481\n",
      "[ 1.45113989  0.7963513 ]\n",
      "Gradient Descent(7233/9): loss=16.696168426701046, w0=70.68497895983887, w1=13.209407168446532\n",
      "[-3.14957625 -1.04305928]\n",
      "Gradient Descent(7234/9): loss=18.825712236284623, w0=72.88968233601491, w1=13.939548667100286\n",
      "[-5.15840561  0.72728233]\n",
      "Gradient Descent(7235/9): loss=15.57331740283092, w0=76.5005662637679, w1=13.430451032735197\n",
      "[ 3.68438988  4.88768115]\n",
      "Gradient Descent(7236/9): loss=20.528384922132584, w0=73.92149334473694, w1=10.009074227544891\n",
      "[ 2.11550296 -5.92689544]\n",
      "Gradient Descent(7237/9): loss=21.60547554736131, w0=72.44064127620838, w1=14.157901035305088\n",
      "[ 2.44478987  7.28164181]\n",
      "Gradient Descent(7238/9): loss=15.979901756222208, w0=70.72928836891548, w1=9.060751770043936\n",
      "[ 0.09014689 -0.52170963]\n",
      "Gradient Descent(7239/9): loss=28.438167384239396, w0=70.66618554680863, w1=9.425948513731052\n",
      "[-2.7078647  -3.04012936]\n",
      "Gradient Descent(7240/9): loss=27.054888272723147, w0=72.56169084013081, w1=11.554039064853766\n",
      "[ 1.68531527 -1.46280981]\n",
      "Gradient Descent(7241/9): loss=17.50807807033667, w0=71.38197014766025, w1=12.578005928804192\n",
      "[-0.34802027  1.33928469]\n",
      "Gradient Descent(7242/9): loss=17.620205127335158, w0=71.6255843342975, w1=11.640506647478272\n",
      "[-0.99507353 -2.90965312]\n",
      "Gradient Descent(7243/9): loss=18.468902120148964, w0=72.32213580215532, w1=13.677263830765249\n",
      "[-2.008303    1.49385519]\n",
      "Gradient Descent(7244/9): loss=15.877585355022472, w0=73.72794790178288, w1=12.631565200089666\n",
      "[-0.13872958  2.94876848]\n",
      "Gradient Descent(7245/9): loss=15.839753975658649, w0=73.82505860867619, w1=10.567427262785538\n",
      "[ 0.69545223 -1.67699283]\n",
      "Gradient Descent(7246/9): loss=19.767643378367495, w0=73.3382420485518, w1=11.741322242921528\n",
      "[-3.56803576 -2.17633899]\n",
      "Gradient Descent(7247/9): loss=16.897870232026193, w0=75.83586707961464, w1=13.264759538975209\n",
      "[ 3.26343525 -0.68418998]\n",
      "Gradient Descent(7248/9): loss=18.639732631119045, w0=73.55146240638418, w1=13.74369252461904\n",
      "[ 1.4028004  -0.54744375]\n",
      "Gradient Descent(7249/9): loss=15.453894142608018, w0=72.5695021245204, w1=14.126903146747285\n",
      "[-0.98582558  1.06150358]\n",
      "Gradient Descent(7250/9): loss=15.857707857042453, w0=73.25958002889908, w1=13.383850640251406\n",
      "[ 1.15536595 -1.17154611]\n",
      "Gradient Descent(7251/9): loss=15.391072296236404, w0=72.45082386098562, w1=14.203932919804453\n",
      "[-0.47508273 -0.68060532]\n",
      "Gradient Descent(7252/9): loss=16.00354276192217, w0=72.78338177416164, w1=14.680356640592633\n",
      "[-2.06369447  2.14262253]\n",
      "Gradient Descent(7253/9): loss=16.236986785228456, w0=74.22796790087199, w1=13.180520871321434\n",
      "[ 0.91947771 -0.28707686]\n",
      "Gradient Descent(7254/9): loss=15.866866535215875, w0=73.58433350176428, w1=13.381474672001415\n",
      "[ 2.66542626  1.05433069]\n",
      "Gradient Descent(7255/9): loss=15.43288261743493, w0=71.7185351197642, w1=12.64344318850319\n",
      "[ 0.82094616 -4.51590381]\n",
      "Gradient Descent(7256/9): loss=16.976482909664444, w0=71.14387280887964, w1=15.804575858589189\n",
      "[-2.88632342  1.56370962]\n",
      "Gradient Descent(7257/9): loss=20.39973860467122, w0=73.16429920028337, w1=14.709979121386391\n",
      "[-1.46016779  1.58625157]\n",
      "Gradient Descent(7258/9): loss=16.151066964035017, w0=74.186416655873, w1=13.599603024463514\n",
      "[ 2.39046835 -0.50172603]\n",
      "Gradient Descent(7259/9): loss=15.791348099053733, w0=72.51308881197924, w1=13.950811243990945\n",
      "[ 1.92004487  3.55961057]\n",
      "Gradient Descent(7260/9): loss=15.80170514815203, w0=71.16905740270181, w1=11.459083843766665\n",
      "[-1.59481023 -0.24147851]\n",
      "Gradient Descent(7261/9): loss=19.684882603560915, w0=72.28542456555464, w1=11.62811879869402\n",
      "[-1.70822783 -0.49633954]\n",
      "Gradient Descent(7262/9): loss=17.608620905578043, w0=73.48118404335501, w1=11.975556476247462\n",
      "[ 2.68116577 -0.81679671]\n",
      "Gradient Descent(7263/9): loss=16.534663978984835, w0=71.60436800594547, w1=12.547314172362139\n",
      "[ 1.23859956 -0.59234628]\n",
      "Gradient Descent(7264/9): loss=17.24786748187387, w0=70.73734831742155, w1=12.961956569285288\n",
      "[-3.26574409 -0.93153754]\n",
      "Gradient Descent(7265/9): loss=18.787957939673166, w0=73.02336917812995, w1=13.614032844632979\n",
      "[-1.2123006  -3.22571214]\n",
      "Gradient Descent(7266/9): loss=15.431508270333346, w0=73.87197959724627, w1=15.872031343447564\n",
      "[-1.80015944  1.30710919]\n",
      "Gradient Descent(7267/9): loss=18.414558040363612, w0=75.13209120601086, w1=14.957054911193692\n",
      "[ 1.44384605  3.1171365 ]\n",
      "Gradient Descent(7268/9): loss=18.166591275922226, w0=74.12139896762562, w1=12.77505935865703\n",
      "[-0.44202542 -0.772831  ]\n",
      "Gradient Descent(7269/9): loss=15.976514912054943, w0=74.43081676034808, w1=13.316041055585153\n",
      "[-2.12933971  0.44899496]\n",
      "Gradient Descent(7270/9): loss=16.04554687470747, w0=75.92135455454503, w1=13.001744581908355\n",
      "[ 4.85316568 -0.89066228]\n",
      "Gradient Descent(7271/9): loss=18.95181541192896, w0=72.52413858006383, w1=13.625208181332876\n",
      "[ 1.62224678  0.6364176 ]\n",
      "Gradient Descent(7272/9): loss=15.692755633356324, w0=71.38856583598299, w1=13.179715863008203\n",
      "[-2.88568879  0.04695681]\n",
      "Gradient Descent(7273/9): loss=17.246077900319477, w0=73.40854798920718, w1=13.146846097366229\n",
      "[ 3.11086513 -1.89120657]\n",
      "Gradient Descent(7274/9): loss=15.447857426650167, w0=71.23094239851306, w1=14.470690698320514\n",
      "[ 1.58513335  2.88452664]\n",
      "Gradient Descent(7275/9): loss=18.004849250445684, w0=70.121349056319, w1=12.451522047727343\n",
      "[-2.54622062 -2.21074461]\n",
      "Gradient Descent(7276/9): loss=20.9470851532253, w0=71.90370348766066, w1=13.999043274860679\n",
      "[-2.81634021  1.07329673]\n",
      "Gradient Descent(7277/9): loss=16.48709388840246, w0=73.87514163299494, w1=13.247735562062438\n",
      "[ 4.71535876  4.79476215]\n",
      "Gradient Descent(7278/9): loss=15.58170263328161, w0=70.5743905017483, w1=9.891402058248694\n",
      "[-0.62296615 -3.4414268 ]\n",
      "Gradient Descent(7279/9): loss=25.52179933945735, w0=71.01046680408821, w1=12.300400819721105\n",
      "[-3.005829   0.5841728]\n",
      "Gradient Descent(7280/9): loss=18.68835963245772, w0=73.11454710272261, w1=11.891479862407136\n",
      "[ 3.71142295 -3.5885256 ]\n",
      "Gradient Descent(7281/9): loss=16.663216898398737, w0=70.51655103933305, w1=14.40344778152939\n",
      "[-3.33624359  3.26165825]\n",
      "Gradient Descent(7282/9): loss=19.66942609647832, w0=72.85192155527425, w1=12.120287007761512\n",
      "[ 1.80493221 -1.17740453]\n",
      "Gradient Descent(7283/9): loss=16.40758881242516, w0=71.58846901045659, w1=12.944470175990062\n",
      "[-2.64087456 -0.95591582]\n",
      "Gradient Descent(7284/9): loss=16.983414960100145, w0=73.4370812034399, w1=13.613611252253495\n",
      "[ 0.25491419 -1.89870055]\n",
      "Gradient Descent(7285/9): loss=15.405099593925208, w0=73.25864126761093, w1=14.942701636644738\n",
      "[-0.79284455  1.04303048]\n",
      "Gradient Descent(7286/9): loss=16.456678936023206, w0=73.8136324560793, w1=14.212580302225613\n",
      "[ 2.33619877  1.48507252]\n",
      "Gradient Descent(7287/9): loss=15.789485002228323, w0=72.17829331566116, w1=13.173029537878044\n",
      "[ 2.38635528 -5.51256774]\n",
      "Gradient Descent(7288/9): loss=16.055228751528006, w0=70.50784462142529, w1=17.031826954554248\n",
      "[-2.162031    1.16308067]\n",
      "Gradient Descent(7289/9): loss=25.575760234450446, w0=72.0212663225499, w1=16.21767048502899\n",
      "[-4.07446557  4.59492299]\n",
      "Gradient Descent(7290/9): loss=19.943921250070822, w0=74.8733922202187, w1=13.001224395307327\n",
      "[ 3.92934741 -0.14288224]\n",
      "Gradient Descent(7291/9): loss=16.747726355842406, w0=72.12284903317003, w1=13.101241961459817\n",
      "[-1.59441625  3.61085706]\n",
      "Gradient Descent(7292/9): loss=16.14321376778143, w0=73.23894041136177, w1=10.57364201833873\n",
      "[-2.85636988 -1.46812728]\n",
      "Gradient Descent(7293/9): loss=19.61002198975482, w0=75.23839932467187, w1=11.601331115205213\n",
      "[ 1.15517552  0.51450569]\n",
      "Gradient Descent(7294/9): loss=19.040542089073888, w0=74.42977646127483, w1=11.241177130783141\n",
      "[-3.15168197 -2.18968105]\n",
      "Gradient Descent(7295/9): loss=18.536490699125295, w0=76.63595384201872, w1=12.773953867796145\n",
      "[ 0.21545783 -1.60797146]\n",
      "Gradient Descent(7296/9): loss=21.21952385591041, w0=76.48513335981514, w1=13.899533886588483\n",
      "[ 4.53802356 -2.70750708]\n",
      "Gradient Descent(7297/9): loss=20.565927859229408, w0=73.3085168667982, w1=15.794788843890103\n",
      "[ 1.60008637  1.38407132]\n",
      "Gradient Descent(7298/9): loss=18.065783763392204, w0=72.18845640993692, w1=14.825938916722995\n",
      "[-1.83414405  3.17672898]\n",
      "Gradient Descent(7299/9): loss=16.90307782662425, w0=73.4723572479619, w1=12.602228633133569\n",
      "[-3.34134109 -2.27354306]\n",
      "Gradient Descent(7300/9): loss=15.786796348570746, w0=75.81129601281181, w1=14.193708772855407\n",
      "[ 3.48685952 -0.06966136]\n",
      "Gradient Descent(7301/9): loss=18.809369208963247, w0=73.3704943514788, w1=14.24247172453445\n",
      "[-1.84843995  0.98788089]\n",
      "Gradient Descent(7302/9): loss=15.679720398067602, w0=74.66440231988902, w1=13.550955101913997\n",
      "[ 1.18808595 -0.10022577]\n",
      "Gradient Descent(7303/9): loss=16.327533778341124, w0=73.83274215259239, w1=13.621113139156275\n",
      "[ 3.94498265 -1.97666239]\n",
      "Gradient Descent(7304/9): loss=15.541048525684415, w0=71.07125429742455, w1=15.004776809908869\n",
      "[-3.16849761  1.20696651]\n",
      "Gradient Descent(7305/9): loss=19.01892440536936, w0=73.28920262679421, w1=14.159900253222782\n",
      "[-2.36235416  1.23162022]\n",
      "Gradient Descent(7306/9): loss=15.617226739117843, w0=74.94285053672583, w1=13.297766098124224\n",
      "[-0.29842309 -3.05841735]\n",
      "Gradient Descent(7307/9): loss=16.76192275972175, w0=75.15174670161115, w1=15.438658242180018\n",
      "[ 6.28525221  1.94274715]\n",
      "Gradient Descent(7308/9): loss=19.030378513632133, w0=70.75207015125905, w1=14.078735234453745\n",
      "[-6.1243362   3.46661999]\n",
      "Gradient Descent(7309/9): loss=18.79580744179364, w0=75.0391054880687, w1=11.652101241952357\n",
      "[ 1.93456433 -2.22423984]\n",
      "Gradient Descent(7310/9): loss=18.57880190512578, w0=73.6849104583581, w1=13.20906912896541\n",
      "[-0.25254165  1.0164527 ]\n",
      "Gradient Descent(7311/9): loss=15.498947754838621, w0=73.86168961052147, w1=12.497552241246463\n",
      "[ 1.04510651  0.99658853]\n",
      "Gradient Descent(7312/9): loss=16.029387220499007, w0=73.13011505006209, w1=11.799940269655428\n",
      "[-0.67519502 -2.42579086]\n",
      "Gradient Descent(7313/9): loss=16.810121491313026, w0=73.60275156251889, w1=13.497993868964084\n",
      "[ 0.49368195  0.56195238]\n",
      "Gradient Descent(7314/9): loss=15.433742822936154, w0=73.25717419714468, w1=13.104627204417213\n",
      "[ 3.07719545 -3.55054449]\n",
      "Gradient Descent(7315/9): loss=15.456907534510671, w0=71.1031373833952, w1=15.590008349421716\n",
      "[ 0.22864246  4.79891086]\n",
      "Gradient Descent(7316/9): loss=20.012330914853163, w0=70.94308766472402, w1=12.230770747530473\n",
      "[-3.4131358  -1.37136914]\n",
      "Gradient Descent(7317/9): loss=18.929026579070516, w0=73.33228272305172, w1=13.190729144132185\n",
      "[ 0.59443293  1.15920336]\n",
      "Gradient Descent(7318/9): loss=15.4283793124824, w0=72.91617966897542, w1=12.379286791597831\n",
      "[-0.96634326 -2.90575866]\n",
      "Gradient Descent(7319/9): loss=16.06270080226505, w0=73.59261995421895, w1=14.413317850765981\n",
      "[-0.07715356 -1.01385899]\n",
      "Gradient Descent(7320/9): loss=15.866307638311886, w0=73.6466274481986, w1=15.123019144195933\n",
      "[ 3.16159498  1.4628681 ]\n",
      "Gradient Descent(7321/9): loss=16.798316904943555, w0=71.43351096131428, w1=14.099011477388387\n",
      "[-2.0185101   0.13344305]\n",
      "Gradient Descent(7322/9): loss=17.308218141136123, w0=72.84646803337738, w1=14.005601343213796\n",
      "[ 1.37485257 -0.76231676]\n",
      "Gradient Descent(7323/9): loss=15.624274967791443, w0=71.88407123505402, w1=14.539223078561331\n",
      "[-2.00409852  1.40858262]\n",
      "Gradient Descent(7324/9): loss=16.941008863428262, w0=73.28694019669548, w1=13.553215242491836\n",
      "[-0.43835095 -3.70913254]\n",
      "Gradient Descent(7325/9): loss=15.388613572988188, w0=73.5937858582873, w1=16.149608021976988\n",
      "[ 0.91951484  1.1511972 ]\n",
      "Gradient Descent(7326/9): loss=18.99501825766044, w0=72.95012547253557, w1=15.343769978581264\n",
      "[-1.73183588  3.4007576 ]\n",
      "Gradient Descent(7327/9): loss=17.18234115861298, w0=74.16241058537983, w1=12.963239658852808\n",
      "[ 1.95401846  3.22484487]\n",
      "Gradient Descent(7328/9): loss=15.896396142713533, w0=72.79459766032443, w1=10.705848251172686\n",
      "[ 0.86156565 -2.87277443]\n",
      "Gradient Descent(7329/9): loss=19.357711523106406, w0=72.19150170331065, w1=12.71679035538658\n",
      "[ 1.04455713 -0.70858354]\n",
      "Gradient Descent(7330/9): loss=16.284578176198995, w0=71.4603117147274, w1=13.212798832693448\n",
      "[-1.53891841  0.78869694]\n",
      "Gradient Descent(7331/9): loss=17.102572647363445, w0=72.53755459893654, w1=12.660710977578566\n",
      "[-1.29467931  0.06255875]\n",
      "Gradient Descent(7332/9): loss=16.007315386737694, w0=73.44383011794632, w1=12.616919849647322\n",
      "[-1.51246806 -3.67814437]\n",
      "Gradient Descent(7333/9): loss=15.769329613087251, w0=74.50255775866499, w1=15.191620905387296\n",
      "[ 0.1832587   3.06432616]\n",
      "Gradient Descent(7334/9): loss=17.581603370357477, w0=74.37427666609238, w1=13.046592593200755\n",
      "[ 1.2229737   0.95516045]\n",
      "Gradient Descent(7335/9): loss=16.063267367504203, w0=73.51819507954457, w1=12.37798027950915\n",
      "[-3.48162668  2.4583627 ]\n",
      "Gradient Descent(7336/9): loss=16.01794394667066, w0=75.95533375266902, w1=10.657126387599675\n",
      "[ 5.39190744 -5.04918413]\n",
      "Gradient Descent(7337/9): loss=22.91094011930761, w0=72.18099854329968, w1=14.191555275139583\n",
      "[ 0.82841327  0.41285117]\n",
      "Gradient Descent(7338/9): loss=16.258547295946, w0=71.6011092510671, w1=13.90255945563012\n",
      "[-2.31895671  0.88981749]\n",
      "Gradient Descent(7339/9): loss=16.908095175300488, w0=73.22437894464267, w1=13.279687212056732\n",
      "[ 4.87960516 -0.50332727]\n",
      "Gradient Descent(7340/9): loss=15.40831103215458, w0=69.80865533340805, w1=13.632016299332019\n",
      "[-4.36956495  0.4655143 ]\n",
      "Gradient Descent(7341/9): loss=21.471027978341912, w0=72.86735080031526, w1=13.306156286462738\n",
      "[-0.76546896  0.62311682]\n",
      "Gradient Descent(7342/9): loss=15.491930232273296, w0=73.40317907185813, w1=12.869974515738475\n",
      "[ 0.98233086 -1.25197152]\n",
      "Gradient Descent(7343/9): loss=15.577746587560908, w0=72.71554747266036, w1=13.746354580931273\n",
      "[ 1.0467181  -0.77880635]\n",
      "Gradient Descent(7344/9): loss=15.588695433981, w0=71.98284480558654, w1=14.291519024855718\n",
      "[ 0.29621757 -0.99596342]\n",
      "Gradient Descent(7345/9): loss=16.57486454612048, w0=71.7754925075673, w1=14.988693417579016\n",
      "[ 0.07280212 -0.42390221]\n",
      "Gradient Descent(7346/9): loss=17.6772137366798, w0=71.72453102555961, w1=15.285424965490858\n",
      "[-0.08057597 -1.403489  ]\n",
      "Gradient Descent(7347/9): loss=18.247680758866366, w0=71.78093420713283, w1=16.267867263576328\n",
      "[-2.03067722  2.10999569]\n",
      "Gradient Descent(7348/9): loss=20.417357576784347, w0=73.20240826319974, w1=14.790870278912237\n",
      "[ 0.15951801  1.08981014]\n",
      "Gradient Descent(7349/9): loss=16.24964269687428, w0=73.09074565356178, w1=14.028003183313508\n",
      "[-0.38326632  1.01616543]\n",
      "Gradient Descent(7350/9): loss=15.556839555482215, w0=73.35903207415667, w1=13.316687382418637\n",
      "[ 1.76063597  0.72064924]\n",
      "Gradient Descent(7351/9): loss=15.401296113453467, w0=72.12658689463285, w1=12.812232914736617\n",
      "[ 2.17564534  1.32200152]\n",
      "Gradient Descent(7352/9): loss=16.289987950376336, w0=70.60363515714548, w1=11.886831852897084\n",
      "[-2.08007268 -0.17197202]\n",
      "Gradient Descent(7353/9): loss=20.27334379731386, w0=72.05968603455239, w1=12.007212266789933\n",
      "[-2.19078601 -0.03617596]\n",
      "Gradient Descent(7354/9): loss=17.23168545330311, w0=73.5932362400292, w1=12.032535435984425\n",
      "[ 0.04334991 -0.00859816]\n",
      "Gradient Descent(7355/9): loss=16.477843008565433, w0=73.56289130139746, w1=12.03855414746893\n",
      "[-1.33499573 -0.44831288]\n",
      "Gradient Descent(7356/9): loss=16.46052871565425, w0=74.49738831575704, w1=12.352373161865485\n",
      "[ 2.37798745  0.39972314]\n",
      "Gradient Descent(7357/9): loss=16.745500371240166, w0=72.83279709764206, w1=12.07256696327718\n",
      "[ 3.83145352 -1.65658876]\n",
      "Gradient Descent(7358/9): loss=16.482235146867072, w0=70.1507796331711, w1=13.232179096956958\n",
      "[-2.28735923  2.63352685]\n",
      "Gradient Descent(7359/9): loss=20.35619622124236, w0=71.75193109726148, w1=11.388710301443787\n",
      "[ 1.53783431 -1.67411321]\n",
      "Gradient Descent(7360/9): loss=18.760900805385184, w0=70.67544708261353, w1=12.560589551399103\n",
      "[-4.44511611  1.99748243]\n",
      "Gradient Descent(7361/9): loss=19.23648675840217, w0=73.78702836273993, w1=11.162351853507985\n",
      "[ 4.60722602 -3.00325075]\n",
      "Gradient Descent(7362/9): loss=18.19254484257975, w0=70.56197015153428, w1=13.264627375835287\n",
      "[-2.59179539 -0.63810811]\n",
      "Gradient Descent(7363/9): loss=19.140799117083887, w0=72.37622692626982, w1=13.71130304991107\n",
      "[-0.36627004  0.95886867]\n",
      "Gradient Descent(7364/9): loss=15.83378710139562, w0=72.63261595160235, w1=13.040094984124948\n",
      "[-0.17722516  1.48078193]\n",
      "Gradient Descent(7365/9): loss=15.701182466597361, w0=72.75667356214953, w1=12.003547633169692\n",
      "[-1.44856854 -1.52572917]\n",
      "Gradient Descent(7366/9): loss=16.619737073011983, w0=73.77067154210346, w1=13.071558052174982\n",
      "[ 1.0119118   0.28221948]\n",
      "Gradient Descent(7367/9): loss=15.582827930878848, w0=73.06233328052912, w1=12.87400441440063\n",
      "[-3.73370882  1.35836571]\n",
      "Gradient Descent(7368/9): loss=15.596145639912589, w0=75.67592945550665, w1=11.92314841898968\n",
      "[ 0.84964652  0.08033197]\n",
      "Gradient Descent(7369/9): loss=19.434313390811496, w0=75.08117689370756, w1=11.866916041617912\n",
      "[ 3.22252237 -0.67561168]\n",
      "Gradient Descent(7370/9): loss=18.283583995843184, w0=72.82541123787655, w1=12.339844219835935\n",
      "[ 0.21135868 -2.54316049]\n",
      "Gradient Descent(7371/9): loss=16.145288810886623, w0=72.67746016412971, w1=14.120056563040972\n",
      "[-0.19911876 -0.09690258]\n",
      "Gradient Descent(7372/9): loss=15.780920768834738, w0=72.8168432956553, w1=14.187888366540978\n",
      "[-2.29236457 -1.33836855]\n",
      "Gradient Descent(7373/9): loss=15.750446489918069, w0=74.42149849668667, w1=15.12474635247049\n",
      "[ 2.38109934  1.75079169]\n",
      "Gradient Descent(7374/9): loss=17.374670539227893, w0=72.75472895935589, w1=13.899192166907172\n",
      "[-0.71665113  1.686639  ]\n",
      "Gradient Descent(7375/9): loss=15.619234060249076, w0=73.25638475120688, w1=12.718544869567442\n",
      "[-0.4188174 -1.4870827]\n",
      "Gradient Descent(7376/9): loss=15.676280422756829, w0=73.5495569281279, w1=13.75950276261537\n",
      "[ 2.91180132  0.17241541]\n",
      "Gradient Descent(7377/9): loss=15.457703790247342, w0=71.51129600639881, w1=13.638811977338015\n",
      "[-0.33262694 -2.2369225 ]\n",
      "Gradient Descent(7378/9): loss=16.98742192130131, w0=71.74413486759312, w1=15.204657726907378\n",
      "[-3.3509039   0.84087591]\n",
      "Gradient Descent(7379/9): loss=18.07452608003458, w0=74.08976759480103, w1=14.616044588949125\n",
      "[ 1.20203382 -0.94058649]\n",
      "Gradient Descent(7380/9): loss=16.348198354597866, w0=73.24834392055018, w1=15.274455131554664\n",
      "[-1.26687321 -0.37610022]\n",
      "Gradient Descent(7381/9): loss=16.997477223026326, w0=74.13515516775394, w1=15.537725285999162\n",
      "[ 0.1436243  -0.33078946]\n",
      "Gradient Descent(7382/9): loss=17.85743293578449, w0=74.03461815732724, w1=15.769277907554986\n",
      "[ 1.06890836  0.08091108]\n",
      "Gradient Descent(7383/9): loss=18.281258292592813, w0=73.28638230255665, w1=15.712640151107275\n",
      "[-0.29571212  0.835976  ]\n",
      "Gradient Descent(7384/9): loss=17.878899385068518, w0=73.49338078397501, w1=15.127456948625914\n",
      "[-1.30667726  1.05592323]\n",
      "Gradient Descent(7385/9): loss=16.763310762772093, w0=74.40805486447546, w1=14.388310686280338\n",
      "[ 2.05918841  2.09735467]\n",
      "Gradient Descent(7386/9): loss=16.419309277460883, w0=72.9666229786078, w1=12.920162417724336\n",
      "[ 0.5954155  -1.19371952]\n",
      "Gradient Descent(7387/9): loss=15.59599830513104, w0=72.54983212906362, w1=13.755766081193027\n",
      "[-2.06459718  4.25009927]\n",
      "Gradient Descent(7388/9): loss=15.700825546202168, w0=73.99505015695117, w1=10.780696590485963\n",
      "[-2.12690885 -4.75258148]\n",
      "Gradient Descent(7389/9): loss=19.27402147802762, w0=75.48388635480366, w1=14.107503623457283\n",
      "[ 1.16055468  1.76503046]\n",
      "Gradient Descent(7390/9): loss=17.980920690033592, w0=74.67149808218232, w1=12.871982302183218\n",
      "[-0.94456519 -0.83060897]\n",
      "Gradient Descent(7391/9): loss=16.51941375418984, w0=75.33269371760738, w1=13.45340858453939\n",
      "[ 0.79089567  2.23808111]\n",
      "Gradient Descent(7392/9): loss=17.464528869069518, w0=74.77906675094847, w1=11.886751808803586\n",
      "[-1.85080146 -6.95608211]\n",
      "Gradient Descent(7393/9): loss=17.757477109626382, w0=76.07462777244017, w1=16.75600928688308\n",
      "[ 3.20124359  3.78948519]\n",
      "Gradient Descent(7394/9): loss=24.619110690281882, w0=73.83375725962891, w1=14.103369655041035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.05455253  2.05244528]\n",
      "Gradient Descent(7395/9): loss=15.72607308552374, w0=73.09557048770999, w1=12.666657959891932\n",
      "[-2.28369002 -2.47936015]\n",
      "Gradient Descent(7396/9): loss=15.736088320198556, w0=74.69415350100041, w1=14.402210065276417\n",
      "[ 0.77517142 -0.30664317]\n",
      "Gradient Descent(7397/9): loss=16.791712933021486, w0=74.15153350570573, w1=14.616860283744524\n",
      "[ 0.28968971  2.24918814]\n",
      "Gradient Descent(7398/9): loss=16.40018922934799, w0=73.94875071169645, w1=13.04242858671564\n",
      "[-1.31387671  0.35976532]\n",
      "Gradient Descent(7399/9): loss=15.695896770262276, w0=74.86846440737166, w1=12.790592864404466\n",
      "[-0.23796611  1.18596307]\n",
      "Gradient Descent(7400/9): loss=16.862922653101904, w0=75.03504068574757, w1=11.960418716205464\n",
      "[ 3.93095973 -0.25837429]\n",
      "Gradient Descent(7401/9): loss=18.055761706061325, w0=72.28336887521911, w1=12.141280719245778\n",
      "[-3.55303298 -3.74602307]\n",
      "Gradient Descent(7402/9): loss=16.792196408812753, w0=74.77049196166202, w1=14.763496871106566\n",
      "[ 2.13367687  1.95674448]\n",
      "Gradient Descent(7403/9): loss=17.30006853077101, w0=73.27691815381881, w1=13.393775736092719\n",
      "[-2.73024683 -1.98790431]\n",
      "Gradient Descent(7404/9): loss=15.389724992366276, w0=75.1880909353758, w1=14.785308750262187\n",
      "[ 1.90379714  0.93151657]\n",
      "Gradient Descent(7405/9): loss=18.03211671194055, w0=73.85543293526301, w1=14.133247148711096\n",
      "[ 2.06485688  1.50577097]\n",
      "Gradient Descent(7406/9): loss=15.75708894387716, w0=72.41003311616825, w1=13.079207472098146\n",
      "[-1.80134726  0.39884541]\n",
      "Gradient Descent(7407/9): loss=15.856719762820942, w0=73.6709761960193, w1=12.800015684598847\n",
      "[ 0.19251325 -0.90738641]\n",
      "Gradient Descent(7408/9): loss=15.687966637649009, w0=73.53621691885246, w1=13.435186170567047\n",
      "[-2.86035987  2.01584141]\n",
      "Gradient Descent(7409/9): loss=15.416232576281875, w0=75.53846882519876, w1=12.024097181662182\n",
      "[ 1.24901205 -2.35434053]\n",
      "Gradient Descent(7410/9): loss=18.96429097221805, w0=74.66416039331949, w1=13.672135551106749\n",
      "[ 1.6141946   0.05819885]\n",
      "Gradient Descent(7411/9): loss=16.343177821016404, w0=73.53422417186772, w1=13.63139635890854\n",
      "[ 0.1211268  -1.31977282]\n",
      "Gradient Descent(7412/9): loss=15.426264441613498, w0=73.44943541146199, w1=14.555237335782552\n",
      "[ 1.52523609  0.63365098]\n",
      "Gradient Descent(7413/9): loss=15.976356985187726, w0=72.38177014789983, w1=14.11168164744108\n",
      "[-0.93158085 -1.16361277]\n",
      "Gradient Descent(7414/9): loss=16.00159091413816, w0=73.03387674363617, w1=14.926210587357529\n",
      "[ 2.78964288  1.14505064]\n",
      "Gradient Descent(7415/9): loss=16.465878089458226, w0=71.0811267251365, w1=14.124675138956293\n",
      "[-3.07603169  1.15381721]\n",
      "Gradient Descent(7416/9): loss=18.042107782471245, w0=73.23434890526352, w1=13.317003093853534\n",
      "[-2.71773499  1.61753881]\n",
      "Gradient Descent(7417/9): loss=15.400899510609431, w0=75.13676339576658, w1=12.184725926167697\n",
      "[-1.51425515 -0.56201724]\n",
      "Gradient Descent(7418/9): loss=17.922415098939986, w0=76.19674200328006, w1=12.578137991589124\n",
      "[ 0.38487805 -4.31939193]\n",
      "Gradient Descent(7419/9): loss=20.005488086935795, w0=75.92732736505535, w1=15.60171234542208\n",
      "[ 2.30650917  2.90276824]\n",
      "Gradient Descent(7420/9): loss=21.104741581575613, w0=74.31277094833466, w1=13.569774579329009\n",
      "[ 2.22891942  1.75640806]\n",
      "Gradient Descent(7421/9): loss=15.908970051367406, w0=72.752527356235, w1=12.340288937824816\n",
      "[-2.31631456 -0.02354085]\n",
      "Gradient Descent(7422/9): loss=16.181584903062834, w0=74.37394754815054, w1=12.3567675348695\n",
      "[-0.71530284 -1.61324497]\n",
      "Gradient Descent(7423/9): loss=16.59961808323693, w0=74.87465953343859, w1=13.48603901114474\n",
      "[ 2.80650894 -1.94136732]\n",
      "Gradient Descent(7424/9): loss=16.635273453095333, w0=72.91010327434356, w1=14.844996136955375\n",
      "[ 2.80539019 -0.87765575]\n",
      "Gradient Descent(7425/9): loss=16.39154607014712, w0=70.94633014186981, w1=15.459355164219051\n",
      "[-0.05357746  1.15294897]\n",
      "Gradient Descent(7426/9): loss=20.100974307647725, w0=70.98383436483773, w1=14.652290885184168\n",
      "[ 0.69594367  0.98745645]\n",
      "Gradient Descent(7427/9): loss=18.741610425688368, w0=70.49667379889812, w1=13.961071368989767\n",
      "[-2.17911763  1.6527142 ]\n",
      "Gradient Descent(7428/9): loss=19.414039835673158, w0=72.02205613685595, w1=12.804171431083509\n",
      "[ 0.66370703  0.02054436]\n",
      "Gradient Descent(7429/9): loss=16.42288708240135, w0=71.55746121415277, w1=12.789790380266034\n",
      "[-3.41288834 -0.10715851]\n",
      "Gradient Descent(7430/9): loss=17.131532123674184, w0=73.94648305005394, w1=12.864801333802204\n",
      "[-0.70700197 -2.1221308 ]\n",
      "Gradient Descent(7431/9): loss=15.787863660660797, w0=74.44138443213122, w1=14.35029289608963\n",
      "[-0.07817654 -0.86486183]\n",
      "Gradient Descent(7432/9): loss=16.423178052615064, w0=74.4961080112467, w1=14.955696176104217\n",
      "[ 2.62028758  1.19718169]\n",
      "Gradient Descent(7433/9): loss=17.19777747113536, w0=72.66190670526171, w1=14.117668991384217\n",
      "[ 1.22087273  0.05050228]\n",
      "Gradient Descent(7434/9): loss=15.789103820475264, w0=71.80729579120397, w1=14.08231739415236\n",
      "[ 0.39074195  2.10278462]\n",
      "Gradient Descent(7435/9): loss=16.672482982702768, w0=71.53377642723818, w1=12.61036815889775\n",
      "[-2.46404525 -0.82553498]\n",
      "Gradient Descent(7436/9): loss=17.31282382637771, w0=73.2586080990965, w1=13.188242648271213\n",
      "[ 2.12672232 -0.17625031]\n",
      "Gradient Descent(7437/9): loss=15.428988722986922, w0=71.76990247624764, w1=13.311617868714835\n",
      "[ 0.75015865  3.13544425]\n",
      "Gradient Descent(7438/9): loss=16.5613335180324, w0=71.24479142074291, w1=11.11680689715162\n",
      "[-4.18860792 -3.13776469]\n",
      "Gradient Descent(7439/9): loss=20.27701722893784, w0=74.17681696483763, w1=13.3132421786603\n",
      "[ 0.21867929  0.95747165]\n",
      "Gradient Descent(7440/9): loss=15.789495799559635, w0=74.02374146367512, w1=12.643012024468844\n",
      "[ 0.92810425  1.76038165]\n",
      "Gradient Descent(7441/9): loss=16.002239880554853, w0=73.37406849136401, w1=11.410744866113202\n",
      "[ 1.65125329 -2.13507072]\n",
      "Gradient Descent(7442/9): loss=17.529412999229674, w0=72.21819118836383, w1=12.905294369804384\n",
      "[-2.35770527  2.48832998]\n",
      "Gradient Descent(7443/9): loss=16.129464317450967, w0=73.86858488039216, w1=11.163463384003032\n",
      "[ 0.50461909 -4.48031089]\n",
      "Gradient Descent(7444/9): loss=18.233511413766745, w0=73.5153515184771, w1=14.299681005265706\n",
      "[ 0.29391494  5.42736252]\n",
      "Gradient Descent(7445/9): loss=15.74657761231052, w0=73.30961106030492, w1=10.500527238971715\n",
      "[ 3.60512272 -3.13133382]\n",
      "Gradient Descent(7446/9): loss=19.823783158187418, w0=70.7860251592892, w1=12.692460909817683\n",
      "[-5.24456022 -4.1028418 ]\n",
      "Gradient Descent(7447/9): loss=18.840543637874934, w0=74.45721731461649, w1=15.56445016738252\n",
      "[-1.74584347  1.79179284]\n",
      "Gradient Descent(7448/9): loss=18.23558156731731, w0=75.67930774278956, w1=14.310195180518594\n",
      "[ 2.86471759  2.99277044]\n",
      "Gradient Descent(7449/9): loss=18.57577123007071, w0=73.67400542676764, w1=12.215255869448368\n",
      "[-1.24134154 -2.04655188]\n",
      "Gradient Descent(7450/9): loss=16.257544776750432, w0=74.5429445051688, w1=13.647842182673621\n",
      "[-0.39741861 -1.31146887]\n",
      "Gradient Descent(7451/9): loss=16.18005028143729, w0=74.82113753039603, w1=14.565870393604506\n",
      "[-1.53836672  1.99315802]\n",
      "Gradient Descent(7452/9): loss=17.141951059287578, w0=75.89799423213049, w1=13.170659782984954\n",
      "[ 3.15025584 -1.33416875]\n",
      "Gradient Descent(7453/9): loss=18.824240729279243, w0=73.69281514283956, w1=14.104577911044535\n",
      "[-0.93774245 -1.75405561]\n",
      "Gradient Descent(7454/9): loss=15.660674169274891, w0=74.34923486039783, w1=15.332416841023838\n",
      "[-0.16980295 -1.89512811]\n",
      "Gradient Descent(7455/9): loss=17.658987291338647, w0=74.46809692880974, w1=16.65900651659886\n",
      "[ 1.172672   -1.03722889]\n",
      "Gradient Descent(7456/9): loss=21.129186676759808, w0=73.64722653062768, w1=17.38506673784091\n",
      "[ 1.28558372  0.02407185]\n",
      "Gradient Descent(7457/9): loss=23.074196029168412, w0=72.74731792603546, w1=17.368216440674196\n",
      "[-1.10072942  0.93544166]\n",
      "Gradient Descent(7458/9): loss=23.09550757793214, w0=73.51782851688462, w1=16.713407280244223\n",
      "[ 0.69801059  2.45893703]\n",
      "Gradient Descent(7459/9): loss=20.639346108624675, w0=73.02922110044932, w1=14.992151356354682\n",
      "[-2.31753721  0.54298076]\n",
      "Gradient Descent(7460/9): loss=16.564656897928938, w0=74.65149714449406, w1=14.61206482475582\n",
      "[ 5.179454    1.05149479]\n",
      "Gradient Descent(7461/9): loss=16.948503969750735, w0=71.02587934191212, w1=13.876018472389084\n",
      "[-2.2330526  -0.70210424]\n",
      "Gradient Descent(7462/9): loss=18.036425860697094, w0=72.58901616492933, w1=14.367491443886857\n",
      "[ 0.22173988 -0.09409699]\n",
      "Gradient Descent(7463/9): loss=16.028409772791488, w0=72.43379824607945, w1=14.433359340318574\n",
      "[ 1.67529025  4.39453314]\n",
      "Gradient Descent(7464/9): loss=16.210515516691608, w0=71.26109507300446, w1=11.357186145244395\n",
      "[-2.4583481  -3.01355025]\n",
      "Gradient Descent(7465/9): loss=19.70463945599656, w0=72.98193873994292, w1=13.466671317988036\n",
      "[ 1.99847699  0.66967017]\n",
      "Gradient Descent(7466/9): loss=15.434639682130422, w0=71.58300484586059, w1=12.997902201142445\n",
      "[-2.49360364 -0.01586458]\n",
      "Gradient Descent(7467/9): loss=16.96557717731512, w0=73.32852739215413, w1=13.00900740485717\n",
      "[-0.45054927 -0.6687505 ]\n",
      "Gradient Descent(7468/9): loss=15.497268248035345, w0=73.64391187946745, w1=13.477132752139482\n",
      "[-0.1010618   2.44172765]\n",
      "Gradient Descent(7469/9): loss=15.44713765333923, w0=73.71465514119396, w1=11.767923396201073\n",
      "[ 1.82888047 -0.40708058]\n",
      "Gradient Descent(7470/9): loss=16.939506912650476, w0=72.43443881450469, w1=12.052879802420657\n",
      "[-3.31702826  0.9400231 ]\n",
      "Gradient Descent(7471/9): loss=16.773169224394383, w0=74.75635859894605, w1=11.39486363374021\n",
      "[ 2.26830846 -3.63654002]\n",
      "Gradient Descent(7472/9): loss=18.628545530753502, w0=73.16854267873386, w1=13.940441646194143\n",
      "[-1.09125928 -1.61746496]\n",
      "Gradient Descent(7473/9): loss=15.499883559222761, w0=73.93242417312499, w1=15.072667120219092\n",
      "[-0.33084245  4.15908576]\n",
      "Gradient Descent(7474/9): loss=16.85848269462607, w0=74.16401389101519, w1=12.161307087405756\n",
      "[ 2.83718382 -1.4030488 ]\n",
      "Gradient Descent(7475/9): loss=16.633514146671, w0=72.17798521447564, w1=13.143441245880728\n",
      "[-0.20419795 -2.3534579 ]\n",
      "Gradient Descent(7476/9): loss=16.06508448213404, w0=72.32092378290388, w1=14.790861773929823\n",
      "[ 0.2432673   0.06736566]\n",
      "Gradient Descent(7477/9): loss=16.718806930616278, w0=72.15063667282057, w1=14.743705811359634\n",
      "[ 0.4821346   2.34113979]\n",
      "Gradient Descent(7478/9): loss=16.83827816866248, w0=71.81314245464965, w1=13.104907960936417\n",
      "[-2.64314284 -0.3263371 ]\n",
      "Gradient Descent(7479/9): loss=16.55248109979565, w0=73.6633424441372, w1=13.333343933677398\n",
      "[ 1.17359145  0.74670311]\n",
      "Gradient Descent(7480/9): loss=15.464835469413071, w0=72.84182843200624, w1=12.810651754490916\n",
      "[-0.07338336 -1.6747225 ]\n",
      "Gradient Descent(7481/9): loss=15.711903263986116, w0=72.89319678160037, w1=13.982957504650408\n",
      "[-0.49101327 -1.04254393]\n",
      "Gradient Descent(7482/9): loss=15.592806020072953, w0=73.23690606746747, w1=14.71273825468167\n",
      "[-0.50746656  2.33542287]\n",
      "Gradient Descent(7483/9): loss=16.14768961324504, w0=73.59213265874219, w1=13.077942243987318\n",
      "[ 0.19334793  0.73649945]\n",
      "Gradient Descent(7484/9): loss=15.511062309884117, w0=73.45678910813922, w1=12.562392625558797\n",
      "[-1.66882557 -0.82586322]\n",
      "Gradient Descent(7485/9): loss=15.819888532329925, w0=74.62496700414383, w1=13.140496881495691\n",
      "[-2.2887752  -1.15481011]\n",
      "Gradient Descent(7486/9): loss=16.32926186342132, w0=76.22710964608065, w1=13.948863960027285\n",
      "[ 3.7721651  5.9996066]\n",
      "Gradient Descent(7487/9): loss=19.797734322937405, w0=73.58659407629992, w1=9.749139343011292\n",
      "[ 2.03549533 -5.72789073]\n",
      "Gradient Descent(7488/9): loss=22.387304137630366, w0=72.16174734201925, w1=13.758662856118544\n",
      "[-0.80619462 -0.14018144]\n",
      "Gradient Descent(7489/9): loss=16.065704268023914, w0=72.72608357253036, w1=13.856789863320031\n",
      "[-1.55785728  1.28222501]\n",
      "Gradient Descent(7490/9): loss=15.61820180335876, w0=73.81658366905297, w1=12.95923235744586\n",
      "[ 0.03536221 -0.19752933]\n",
      "Gradient Descent(7491/9): loss=15.6579252334374, w0=73.79183012412913, w1=13.097502889145808\n",
      "[-0.81826625 -0.5350905 ]\n",
      "Gradient Descent(7492/9): loss=15.582886186284954, w0=74.36461650038854, w1=13.472066237561664\n",
      "[-1.10762553 -0.68437455]\n",
      "Gradient Descent(7493/9): loss=15.959110455324062, w0=75.13995437212868, w1=13.951128424513257\n",
      "[ 2.9638286   2.22351584]\n",
      "Gradient Descent(7494/9): loss=17.200922142006224, w0=73.06527435095359, w1=12.394667334174967\n",
      "[-0.06809778 -0.74925605]\n",
      "Gradient Descent(7495/9): loss=16.0006891784183, w0=73.112942797811, w1=12.919146567770518\n",
      "[-1.17469463 -0.79329617]\n",
      "Gradient Descent(7496/9): loss=15.55938165076811, w0=73.93522904225175, w1=13.474453888425623\n",
      "[ 1.36092592 -0.4271221 ]\n",
      "Gradient Descent(7497/9): loss=15.591539054856149, w0=72.98258089917684, w1=13.77343935592566\n",
      "[-0.62768939  1.01090564]\n",
      "Gradient Descent(7498/9): loss=15.477492262057174, w0=73.42196347436007, w1=13.065805410122348\n",
      "[-1.42569884 -1.01566138]\n",
      "Gradient Descent(7499/9): loss=15.479744690755004, w0=74.41995266540306, w1=13.77676837699688\n",
      "[ 0.80693697 -0.87728787]\n",
      "Gradient Descent(7500/9): loss=16.063981512514, w0=73.85509678312174, w1=14.39086988822722\n",
      "[ 1.11187365  1.7267455 ]\n",
      "Gradient Descent(7501/9): loss=15.95845038854962, w0=73.07678522795102, w1=13.18214804044722\n",
      "[ 0.52916316 -1.36218343]\n",
      "Gradient Descent(7502/9): loss=15.453734342623962, w0=72.7063710145569, w1=14.13567644451554\n",
      "[ 1.05583652 -0.40660349]\n",
      "Gradient Descent(7503/9): loss=15.773640341210925, w0=71.9672854491797, w1=14.420298890124872\n",
      "[-3.3508897   2.18387742]\n",
      "Gradient Descent(7504/9): loss=16.708221580400902, w0=74.31290824253233, w1=12.891584698597383\n",
      "[-3.26796396 -0.81064794]\n",
      "Gradient Descent(7505/9): loss=16.07800146507591, w0=76.60048301793995, w1=13.45903825859533\n",
      "[ 3.84811877  2.16857419]\n",
      "Gradient Descent(7506/9): loss=20.85277445533328, w0=73.90679988101091, w1=11.941036323959363\n",
      "[-1.03042289 -2.07271028]\n",
      "Gradient Descent(7507/9): loss=16.75745960338211, w0=74.62809590397623, w1=13.391933517792904\n",
      "[-0.33046593 -0.84776381]\n",
      "Gradient Descent(7508/9): loss=16.279750438198363, w0=74.85942205518835, w1=13.985368184199784\n",
      "[ 1.78583297  1.32865487]\n",
      "Gradient Descent(7509/9): loss=16.739126945286017, w0=73.60933897493207, w1=13.055309772747783\n",
      "[ 1.63094376  0.27807903]\n",
      "Gradient Descent(7510/9): loss=15.525690612061776, w0=72.46767834247117, w1=12.8606544526333\n",
      "[ 0.05883801 -0.55850172]\n",
      "Gradient Descent(7511/9): loss=15.918843554131287, w0=72.42649173319928, w1=13.25160565810454\n",
      "[ 0.36215815  1.10647387]\n",
      "Gradient Descent(7512/9): loss=15.788121855366803, w0=72.17298102561848, w1=12.477073947922342\n",
      "[-0.57591916 -0.05107475]\n",
      "Gradient Descent(7513/9): loss=16.516784173086595, w0=72.5761244380151, w1=12.512826269546618\n",
      "[-3.22488252  0.85055871]\n",
      "Gradient Descent(7514/9): loss=16.110938968798216, w0=74.83354220103315, w1=11.917435172349201\n",
      "[-1.26594685 -0.60134011]\n",
      "Gradient Descent(7515/9): loss=17.791458169983624, w0=75.71970499262767, w1=12.338373250779984\n",
      "[ 1.94800897 -1.4638412 ]\n",
      "Gradient Descent(7516/9): loss=18.979426994089003, w0=74.35609871240763, w1=13.363062094007725\n",
      "[ 0.26558807  2.79169636]\n",
      "Gradient Descent(7517/9): loss=15.956801201809384, w0=74.17018706363066, w1=11.408874645302454\n",
      "[-1.37184057 -1.89526854]\n",
      "Gradient Descent(7518/9): loss=17.913992673451556, w0=75.13047546166494, w1=12.735562626162164\n",
      "[ 0.39438521  0.80305873]\n",
      "Gradient Descent(7519/9): loss=17.349231642728398, w0=74.85440581256509, w1=12.17342151468808\n",
      "[ 0.67849071 -2.83078036]\n",
      "Gradient Descent(7520/9): loss=17.456640714413503, w0=74.37946231641367, w1=14.154967765000698\n",
      "[-1.36768353  2.83264106]\n",
      "Gradient Descent(7521/9): loss=16.203071636178443, w0=75.33684078818771, w1=12.17211902638182\n",
      "[ 2.51328012 -0.72430267]\n",
      "Gradient Descent(7522/9): loss=18.32754671321039, w0=73.57754470365376, w1=12.679130894836385\n",
      "[ 2.11386562  0.38114816]\n",
      "Gradient Descent(7523/9): loss=15.746574188462857, w0=72.0978387707716, w1=12.412327182987811\n",
      "[-0.13374327 -2.10012553]\n",
      "Gradient Descent(7524/9): loss=16.67085105506297, w0=72.19145905738685, w1=13.882415051537416\n",
      "[-0.62045833  0.22917263]\n",
      "Gradient Descent(7525/9): loss=16.074684839755367, w0=72.62577988920845, w1=13.721994208997788\n",
      "[-1.16733272 -1.51852708]\n",
      "Gradient Descent(7526/9): loss=15.638445039350874, w0=73.44291279054784, w1=14.784963168117297\n",
      "[ 2.71321989  2.37369442]\n",
      "Gradient Descent(7527/9): loss=16.2488267345157, w0=71.54365886464828, w1=13.123377076582367\n",
      "[-2.41330924 -4.07089165]\n",
      "Gradient Descent(7528/9): loss=16.981085837825063, w0=73.2329753359552, w1=15.973001232864883\n",
      "[ 1.37589118  5.74709332]\n",
      "Gradient Descent(7529/9): loss=18.495989631693362, w0=72.269851512147, w1=11.950035910632831\n",
      "[-0.76414051 -0.34155026]\n",
      "Gradient Descent(7530/9): loss=17.08020318761426, w0=72.80474987106032, w1=12.18912109562495\n",
      "[-3.70559341  2.83720609]\n",
      "Gradient Descent(7531/9): loss=16.338345558345697, w0=75.3986652597215, w1=10.203076833400681\n",
      "[-0.38312664 -1.6808165 ]\n",
      "Gradient Descent(7532/9): loss=22.969030391868245, w0=75.66685390799792, w1=11.379648381810817\n",
      "[ 2.4764336  -3.70344736]\n",
      "Gradient Descent(7533/9): loss=20.406425297556947, w0=73.93335038648868, w1=13.972061532091873\n",
      "[ 3.23513674 -2.3962498 ]\n",
      "Gradient Descent(7534/9): loss=15.711526014916029, w0=71.66875466574685, w1=15.649436394794575\n",
      "[-1.68727661  2.35010606]\n",
      "Gradient Descent(7535/9): loss=19.060323335289528, w0=72.8498482954918, w1=14.00436215350039\n",
      "[-0.74892457  0.83561233]\n",
      "Gradient Descent(7536/9): loss=15.622117260849095, w0=73.37409549516612, w1=13.419433520848903\n",
      "[ 1.11351038 -0.19706609]\n",
      "Gradient Descent(7537/9): loss=15.390918537069153, w0=72.59463823191733, w1=13.557379780535634\n",
      "[-2.63047597 -0.11122183]\n",
      "Gradient Descent(7538/9): loss=15.6334028727356, w0=74.43597141028965, w1=13.635235064620522\n",
      "[ 6.68950892  3.98983818]\n",
      "Gradient Descent(7539/9): loss=16.05011993836039, w0=69.75331516794206, w1=10.84234833801176\n",
      "[-3.27288467 -5.39151277]\n",
      "Gradient Descent(7540/9): loss=25.13168093590312, w0=72.0443344379944, w1=14.616407277542246\n",
      "[-2.80126269  0.43906112]\n",
      "Gradient Descent(7541/9): loss=16.8126599915631, w0=74.00521831908752, w1=14.309064496637008\n",
      "[ 1.52090995 -0.4706553 ]\n",
      "Gradient Descent(7542/9): loss=15.982771515185572, w0=72.94058135719023, w1=14.638523203820812\n",
      "[-1.01319083  0.89395243]\n",
      "Gradient Descent(7543/9): loss=16.119733873484094, w0=73.64981494070445, w1=14.012756502918128\n",
      "[ 2.21054635  2.10782772]\n",
      "Gradient Descent(7544/9): loss=15.591285749878995, w0=72.10243249266661, w1=12.537277099716741\n",
      "[-3.07673113  0.11277363]\n",
      "Gradient Descent(7545/9): loss=16.539803674965405, w0=74.25614428624236, w1=12.458335559421673\n",
      "[ 0.89590385 -0.66338966]\n",
      "Gradient Descent(7546/9): loss=16.370429091846365, w0=73.62901159073213, w1=12.922708320761487\n",
      "[-0.38089772  0.73123664]\n",
      "Gradient Descent(7547/9): loss=15.597157176665705, w0=73.89563999635686, w1=12.410842670553501\n",
      "[ 2.06223762 -3.16343733]\n",
      "Gradient Descent(7548/9): loss=16.13816142779478, w0=72.45207366102679, w1=14.625248800044117\n",
      "[-1.51593912  1.174778  ]\n",
      "Gradient Descent(7549/9): loss=16.39636896534942, w0=73.51323104421441, w1=13.802904202188222\n",
      "[-1.72177039  0.60671172]\n",
      "Gradient Descent(7550/9): loss=15.462162555997493, w0=74.71847031412638, w1=13.378205995836423\n",
      "[ 2.91137646  1.46763274]\n",
      "Gradient Descent(7551/9): loss=16.40570859406533, w0=72.68050679185578, w1=12.350863079266858\n",
      "[ 1.66568099  0.40495966]\n",
      "Gradient Descent(7552/9): loss=16.211177412869265, w0=71.51453009773941, w1=12.067391314585981\n",
      "[-2.20124238  1.25023542]\n",
      "Gradient Descent(7553/9): loss=17.966331117058925, w0=73.05539976678475, w1=11.192226522491342\n",
      "[ 1.08817542 -3.38961076]\n",
      "Gradient Descent(7554/9): loss=18.030630197138265, w0=72.29367697329342, w1=13.56495405386\n",
      "[ 0.33182458 -0.14523738]\n",
      "Gradient Descent(7555/9): loss=15.889765994454603, w0=72.06139977063162, w1=13.66662022278832\n",
      "[-0.07773501  0.64514894]\n",
      "Gradient Descent(7556/9): loss=16.162910654937697, w0=72.11581427954799, w1=13.215015966274587\n",
      "[-0.44421119 -0.85439425]\n",
      "Gradient Descent(7557/9): loss=16.11488888207881, w0=72.4267621126083, w1=13.813091938232045\n",
      "[ 1.51177834 -1.14044227]\n",
      "Gradient Descent(7558/9): loss=15.817441952396802, w0=71.36851727713527, w1=14.611401527524782\n",
      "[-4.7437202  -1.03751137]\n",
      "Gradient Descent(7559/9): loss=17.879839647379832, w0=74.68912141665969, w1=15.337659487704828\n",
      "[-0.88325026  0.68881922]\n",
      "Gradient Descent(7560/9): loss=18.085162197363633, w0=75.30739660199478, w1=14.855486030986071\n",
      "[ 2.23494258  1.8739454 ]\n",
      "Gradient Descent(7561/9): loss=18.35930434475096, w0=73.74293679394782, w1=13.543724250346438\n",
      "[-1.81134736  2.88099734]\n",
      "Gradient Descent(7562/9): loss=15.488743766728811, w0=75.01087994390258, w1=11.527026110702565\n",
      "[ 2.9757758  -2.02578779]\n",
      "Gradient Descent(7563/9): loss=18.7663520963077, w0=72.92783688344055, w1=12.945077560973536\n",
      "[-0.77622941 -0.23043549]\n",
      "Gradient Descent(7564/9): loss=15.595814250140043, w0=73.47119747197462, w1=13.106382403320444\n",
      "[ 3.50357463 -1.84850879]\n",
      "Gradient Descent(7565/9): loss=15.471288821210953, w0=71.01869523391495, w1=14.400338559067368\n",
      "[-2.01866136  0.20510655]\n",
      "Gradient Descent(7566/9): loss=18.397992522341827, w0=72.43175818815374, w1=14.256763975357408\n",
      "[ 1.63643004 -1.55343869]\n",
      "Gradient Descent(7567/9): loss=16.059455638067476, w0=71.28625716274247, w1=15.344171061379669\n",
      "[ 0.06247248 -0.73904413]\n",
      "Gradient Descent(7568/9): loss=19.139349907197264, w0=71.24252642467238, w1=15.861501951227877\n",
      "[-2.39367807  2.94074389]\n",
      "Gradient Descent(7569/9): loss=20.326460426217455, w0=72.91810107653058, w1=13.802981225399481\n",
      "[ 0.82704855 -0.89580479]\n",
      "Gradient Descent(7570/9): loss=15.508759908305993, w0=72.33916709252877, w1=14.430044575747182\n",
      "[ 1.13555149  2.48938405]\n",
      "Gradient Descent(7571/9): loss=16.293231926388508, w0=71.54428104933436, w1=12.687475739211884\n",
      "[-3.46368532 -3.68635108]\n",
      "Gradient Descent(7572/9): loss=17.230329091703766, w0=73.96886077121525, w1=15.26792149458616\n",
      "[ 1.25332839  0.86930925]\n",
      "Gradient Descent(7573/9): loss=17.21250486026589, w0=73.09153090052311, w1=14.659405018938418\n",
      "[ 2.51762551 -0.12205449]\n",
      "Gradient Descent(7574/9): loss=16.102206244141772, w0=71.32919304648898, w1=14.744843160352135\n",
      "[-3.78684137  0.60690071]\n",
      "Gradient Descent(7575/9): loss=18.116245679476645, w0=73.97998200317906, w1=14.320012661833017\n",
      "[ 2.98499999 -0.61204258]\n",
      "Gradient Descent(7576/9): loss=15.974279266983151, w0=71.89048200894788, w1=14.74844246799014\n",
      "[-1.74030392  0.55799093]\n",
      "Gradient Descent(7577/9): loss=17.175547724345567, w0=73.108694754257, w1=14.357848819258177\n",
      "[-1.10469135 -0.41879769]\n",
      "Gradient Descent(7578/9): loss=15.788604190190735, w0=73.88197870092124, w1=14.651007203932682\n",
      "[ 0.1615678   0.45681629]\n",
      "Gradient Descent(7579/9): loss=16.244758927217973, w0=73.76888123758637, w1=14.331235800878508\n",
      "[ 2.23711224  0.5037597 ]\n",
      "Gradient Descent(7580/9): loss=15.86122702784169, w0=72.20290267261183, w1=13.978604009571715\n",
      "[-0.12037353  0.72703895]\n",
      "Gradient Descent(7581/9): loss=16.10549585908826, w0=72.28716414136903, w1=13.469676746925597\n",
      "[ 0.30735696 -0.5192147 ]\n",
      "Gradient Descent(7582/9): loss=15.892718921423874, w0=72.07201426675579, w1=13.833127034188779\n",
      "[-0.07914171 -1.70336448]\n",
      "Gradient Descent(7583/9): loss=16.19486806514651, w0=72.12741346615537, w1=15.025482171271438\n",
      "[ 1.36045523 -0.3016843 ]\n",
      "Gradient Descent(7584/9): loss=17.26096098985456, w0=71.17509480774486, w1=15.236661179755233\n",
      "[-2.05583959  3.99537622]\n",
      "Gradient Descent(7585/9): loss=19.174036654477582, w0=72.61418251966822, w1=12.439897828114457\n",
      "[-3.4760902  -2.05264008]\n",
      "Gradient Descent(7586/9): loss=16.15751795915612, w0=75.04744566202673, w1=13.876745882799735\n",
      "[ 0.35555837 -2.54291396]\n",
      "Gradient Descent(7587/9): loss=17.00212826112194, w0=74.79855480507784, w1=15.656785658281064\n",
      "[ 1.76972618  3.94679851]\n",
      "Gradient Descent(7588/9): loss=18.88767171450761, w0=73.55974647705128, w1=12.89402670434538\n",
      "[-0.39888126 -2.21346409]\n",
      "Gradient Descent(7589/9): loss=15.592733082109383, w0=73.83896336164904, w1=14.443451565021228\n",
      "[ 3.57608139  3.08293114]\n",
      "Gradient Descent(7590/9): loss=15.998819466013694, w0=71.33570639076675, w1=12.285399769612628\n",
      "[ 1.5099721  -1.25264049]\n",
      "Gradient Descent(7591/9): loss=18.01638343041346, w0=70.27872591799844, w1=13.162248113452485\n",
      "[-1.46859611 -1.02515386]\n",
      "Gradient Descent(7592/9): loss=19.981983379360063, w0=71.30674319677088, w1=13.879855818402921\n",
      "[-1.31330544  5.29723997]\n",
      "Gradient Descent(7593/9): loss=17.440385034659297, w0=72.22605700786012, w1=10.1717878399835\n",
      "[ 1.39115685 -3.36733013]\n",
      "Gradient Descent(7594/9): loss=21.42723825491772, w0=71.2522472102732, w1=12.528918932741899\n",
      "[-0.02375345  1.60157504]\n",
      "Gradient Descent(7595/9): loss=17.922109988588204, w0=71.26887462226645, w1=11.40781640716945\n",
      "[-0.25332539 -2.11018614]\n",
      "Gradient Descent(7596/9): loss=19.58267288917258, w0=71.44620239807158, w1=12.884946708196026\n",
      "[-1.83213983 -2.07923826]\n",
      "Gradient Descent(7597/9): loss=17.26979487127828, w0=72.72870027809466, w1=14.340413492170754\n",
      "[ 0.21662773 -0.83992315]\n",
      "Gradient Descent(7598/9): loss=15.91602882239297, w0=72.57706087004628, w1=14.928359695942962\n",
      "[-1.26953212  2.65466443]\n",
      "Gradient Descent(7599/9): loss=16.69212225349243, w0=73.46573335674397, w1=13.070094595755409\n",
      "[ 1.89830811 -1.03375968]\n",
      "Gradient Descent(7600/9): loss=15.484540826730022, w0=72.1369176792578, w1=13.793726371562377\n",
      "[-3.00709912  0.04733738]\n",
      "Gradient Descent(7601/9): loss=16.104519746554306, w0=74.24188706472229, w1=13.76059020745758\n",
      "[-0.1510419  -5.06816472]\n",
      "Gradient Descent(7602/9): loss=15.874652910334166, w0=74.34761639270941, w1=17.308305513048527\n",
      "[-1.22160608  2.32612958]\n",
      "Gradient Descent(7603/9): loss=23.270086281907272, w0=75.20274064723482, w1=15.680014805745458\n",
      "[-2.25236846 -0.26519763]\n",
      "Gradient Descent(7604/9): loss=19.62834744020479, w0=76.77939857236325, w1=15.86565314713416\n",
      "[ 2.23034824  3.80089897]\n",
      "Gradient Descent(7605/9): loss=24.306517870674096, w0=75.21815480739683, w1=13.205023867688686\n",
      "[ 1.44423045  0.19795856]\n",
      "Gradient Descent(7606/9): loss=17.27495071781243, w0=74.20719349426452, w1=13.066452878013909\n",
      "[ 1.1288779   0.91306303]\n",
      "Gradient Descent(7607/9): loss=15.888312008740508, w0=73.41697896601762, w1=12.427308753871275\n",
      "[ 2.25309314 -1.157978  ]\n",
      "Gradient Descent(7608/9): loss=15.947236131028196, w0=71.83981377079041, w1=13.237893351508673\n",
      "[ 0.55625034  2.30421397]\n",
      "Gradient Descent(7609/9): loss=16.472341477585736, w0=71.45043853068903, w1=11.624943573439637\n",
      "[-2.68643304 -4.42892372]\n",
      "Gradient Descent(7610/9): loss=18.80518728840834, w0=73.33094165814558, w1=14.725190176053932\n",
      "[-3.51928457  3.23758289]\n",
      "Gradient Descent(7611/9): loss=16.16218049804012, w0=75.79444085690918, w1=12.458882153386284\n",
      "[ 1.58346381 -2.67790144]\n",
      "Gradient Descent(7612/9): loss=19.033232372363116, w0=74.68601619195375, w1=14.333413161456312\n",
      "[ 2.25064031 -1.10070053]\n",
      "Gradient Descent(7613/9): loss=16.719253450719833, w0=73.11056797297752, w1=15.10390353287239\n",
      "[-0.81298848 -0.31386505]\n",
      "Gradient Descent(7614/9): loss=16.721695580049825, w0=73.67965990617154, w1=15.323609071280357\n",
      "[ 0.5698326   0.88184104]\n",
      "Gradient Descent(7615/9): loss=17.160262136809347, w0=73.2807770868196, w1=14.706320341167466\n",
      "[-2.08187489  0.13822222]\n",
      "Gradient Descent(7616/9): loss=16.138257740978034, w0=74.73808951204846, w1=14.60956478865149\n",
      "[ 1.35078407 -1.60429351]\n",
      "Gradient Descent(7617/9): loss=17.066980937755552, w0=73.79254066420329, w1=15.732570244825238\n",
      "[ 2.53213192  0.04872798]\n",
      "Gradient Descent(7618/9): loss=18.047882309595607, w0=72.02004832095784, w1=15.698460657494369\n",
      "[-1.33646313 -0.55361947]\n",
      "Gradient Descent(7619/9): loss=18.658686784024614, w0=72.95557251264296, w1=16.08599428709247\n",
      "[-2.46921254  3.90153989]\n",
      "Gradient Descent(7620/9): loss=18.839480603640894, w0=74.68402129254108, w1=13.354916361301289\n",
      "[ 0.68067395  2.01982145]\n",
      "Gradient Descent(7621/9): loss=16.359862917468515, w0=74.20754952992506, w1=11.941041349024681\n",
      "[ 2.52209572 -2.22042595]\n",
      "Gradient Descent(7622/9): loss=16.98699985401591, w0=72.44208252442965, w1=13.495339511998916\n",
      "[ 0.20048679  1.53792215]\n",
      "Gradient Descent(7623/9): loss=15.748825219460603, w0=72.30174177033949, w1=12.41879400378255\n",
      "[ 1.25840628 -1.09237988]\n",
      "Gradient Descent(7624/9): loss=16.44087263381955, w0=71.42085737135675, w1=13.183459921303047\n",
      "[-2.62792542  0.2672117 ]\n",
      "Gradient Descent(7625/9): loss=17.183956200242438, w0=73.2604051622399, w1=12.99641172859423\n",
      "[ 1.42441538 -0.8423857 ]\n",
      "Gradient Descent(7626/9): loss=15.503239344507543, w0=72.26331439752681, w1=13.586081715249602\n",
      "[-1.90742546  2.55893635]\n",
      "Gradient Descent(7627/9): loss=15.922621098028365, w0=73.5985122202862, w1=11.794826271279826\n",
      "[ 1.08376668 -4.59218491]\n",
      "Gradient Descent(7628/9): loss=16.851696161664567, w0=72.83987554555337, w1=15.00935570617373\n",
      "[-0.24920876  1.99774678]\n",
      "Gradient Descent(7629/9): loss=16.65887122972332, w0=73.0143216781603, w1=13.61093296021793\n",
      "[ 1.71626909 -0.410993  ]\n",
      "Gradient Descent(7630/9): loss=15.433585452525113, w0=71.81293331271283, w1=13.898628061318602\n",
      "[-1.75024849  4.40496514]\n",
      "Gradient Descent(7631/9): loss=16.57029676887499, w0=73.03810725251007, w1=10.815152459902073\n",
      "[-1.50798901 -0.79827463]\n",
      "Gradient Descent(7632/9): loss=18.968548392302356, w0=74.09369955902832, w1=11.373944701215148\n",
      "[ 2.95782166 -4.90712506]\n",
      "Gradient Descent(7633/9): loss=17.922838813410007, w0=72.02322439486979, w1=14.808932240329334\n",
      "[-0.47402076  2.06271407]\n",
      "Gradient Descent(7634/9): loss=17.076636718800728, w0=72.35503892837886, w1=13.365032387999845\n",
      "[-1.40708131 -0.44559641]\n",
      "Gradient Descent(7635/9): loss=15.833214338483021, w0=73.33999584218287, w1=13.676949876860162\n",
      "[-0.7807353  0.79182  ]\n",
      "Gradient Descent(7636/9): loss=15.406400572437082, w0=73.88651055297913, w1=13.122675876051753\n",
      "[-3.18520759  1.12008738]\n",
      "Gradient Descent(7637/9): loss=15.625206016351731, w0=76.1161558666498, w1=12.338614711328358\n",
      "[ 5.47358859 -1.94249135]\n",
      "Gradient Descent(7638/9): loss=20.019441869392427, w0=72.2846438559601, w1=13.698358655804798\n",
      "[-1.84366975 -0.49118889]\n",
      "Gradient Descent(7639/9): loss=15.919112141910944, w0=73.57521268000107, w1=14.042190881670333\n",
      "[-1.05308648  0.14439864]\n",
      "Gradient Descent(7640/9): loss=15.583641093055459, w0=74.31237321738415, w1=13.941111830727088\n",
      "[ 1.03155158  0.31829157]\n",
      "Gradient Descent(7641/9): loss=16.010954008974707, w0=73.59028711080799, w1=13.718307729439617\n",
      "[ 0.67750737 -1.63813037]\n",
      "Gradient Descent(7642/9): loss=15.458267864924588, w0=73.11603194990873, w1=14.8649989856504\n",
      "[ 0.05418225  0.62684064]\n",
      "Gradient Descent(7643/9): loss=16.361219717886243, w0=73.07810437179597, w1=14.426210537743106\n",
      "[-1.06279456  0.16003622]\n",
      "Gradient Descent(7644/9): loss=15.857105822864058, w0=73.82206056586854, w1=14.314185182108359\n",
      "[ 0.0311046  0.4498611]\n",
      "Gradient Descent(7645/9): loss=15.873525422938828, w0=73.80028734505473, w1=13.999282412179014\n",
      "[-0.25470128  0.12361701]\n",
      "Gradient Descent(7646/9): loss=15.64906727969819, w0=73.9785782389044, w1=13.912750508420073\n",
      "[ 1.71690513  0.06011611]\n",
      "Gradient Descent(7647/9): loss=15.714025936643857, w0=72.77674464516328, w1=13.870669232389567\n",
      "[ 1.75697811  1.03249865]\n",
      "Gradient Descent(7648/9): loss=15.596047686812948, w0=71.54685996785892, w1=13.147920178731018\n",
      "[-1.99686572 -0.42729063]\n",
      "Gradient Descent(7649/9): loss=16.96704379523815, w0=72.94466597501464, w1=13.447023616763445\n",
      "[-1.66240015  1.61313458]\n",
      "Gradient Descent(7650/9): loss=15.44741203447743, w0=74.1083460777132, w1=12.31782941318346\n",
      "[ 3.06210713 -2.03465904]\n",
      "Gradient Descent(7651/9): loss=16.392517234474422, w0=71.9648710855076, w1=13.742090744441283\n",
      "[-1.12209451  1.11626074]\n",
      "Gradient Descent(7652/9): loss=16.303497226919355, w0=72.75033724357465, w1=12.960708224432567\n",
      "[ 0.94630432  0.59881163]\n",
      "Gradient Descent(7653/9): loss=15.668312748970429, w0=72.08792421672372, w1=12.541540082022475\n",
      "[-3.4083794 -0.3061334]\n",
      "Gradient Descent(7654/9): loss=16.55318687993732, w0=74.47378979937533, w1=12.755833463558574\n",
      "[ 1.310151  -1.4991191]\n",
      "Gradient Descent(7655/9): loss=16.343932260986563, w0=73.55668409891939, w1=13.805216832601996\n",
      "[-0.50041053  1.84571615]\n",
      "Gradient Descent(7656/9): loss=15.473386385023181, w0=73.90697147011353, w1=12.513215526019401\n",
      "[ 0.63607    -3.85626275]\n",
      "Gradient Descent(7657/9): loss=16.040860831466, w0=73.46172247298277, w1=15.212599447576423\n",
      "[ 1.41205841  2.55885454]\n",
      "Gradient Descent(7658/9): loss=16.901415067039768, w0=72.47328158580764, w1=13.421401266497837\n",
      "[ 1.51905724 -1.37049156]\n",
      "Gradient Descent(7659/9): loss=15.72431331144531, w0=71.40994151774218, w1=14.380745360958151\n",
      "[-1.63451101  1.1025572 ]\n",
      "Gradient Descent(7660/9): loss=17.566509268399958, w0=72.55409922381436, w1=13.608955319619335\n",
      "[ 0.17156058 -0.92424478]\n",
      "Gradient Descent(7661/9): loss=15.667908602082159, w0=72.4340068148608, w1=14.25592666621246\n",
      "[-1.27433713 -0.12294516]\n",
      "Gradient Descent(7662/9): loss=16.05686919983306, w0=73.32604280386873, w1=14.341988275903756\n",
      "[-0.05676045  1.66423536]\n",
      "Gradient Descent(7663/9): loss=15.75816355469495, w0=73.36577512119406, w1=13.177023521290138\n",
      "[-1.94296586 -2.12850961]\n",
      "Gradient Descent(7664/9): loss=15.434279593428913, w0=74.72585121998553, w1=14.666980249138964\n",
      "[ 0.24796385  1.49946694]\n",
      "Gradient Descent(7665/9): loss=17.11590094259726, w0=74.55227652290282, w1=13.617353393396897\n",
      "[-3.31606778  0.06782204]\n",
      "Gradient Descent(7666/9): loss=16.187088435551026, w0=76.87352396780047, w1=13.569877964573095\n",
      "[ 1.26592105 -2.03775668]\n",
      "Gradient Descent(7667/9): loss=21.79672789659673, w0=75.9873792326063, w1=14.996307640546489\n",
      "[ 2.6421886   0.43112589]\n",
      "Gradient Descent(7668/9): loss=20.16327430385866, w0=74.13784721093693, w1=14.69451952027095\n",
      "[ 2.4555254   5.05620887]\n",
      "Gradient Descent(7669/9): loss=16.479870875105796, w0=72.41897942840218, w1=11.155173311896348\n",
      "[-2.88517481 -1.65926195]\n",
      "Gradient Descent(7670/9): loss=18.470391189862713, w0=74.43860179837412, w1=12.316656675816649\n",
      "[ 4.72853613 -2.3711671 ]\n",
      "Gradient Descent(7671/9): loss=16.71738313629459, w0=71.12862650756823, w1=13.976473645456052\n",
      "[-1.40052538 -1.30991552]\n",
      "Gradient Descent(7672/9): loss=17.853526008272752, w0=72.10899427626883, w1=14.893414511198705\n",
      "[-1.25677548  0.94175256]\n",
      "Gradient Descent(7673/9): loss=17.08719150669701, w0=72.98873711191673, w1=14.234187721967212\n",
      "[-4.27284097  3.60307756]\n",
      "Gradient Descent(7674/9): loss=15.717073256759459, w0=75.97972578963014, w1=11.712033433361752\n",
      "[ 3.11514025 -2.30741585]\n",
      "Gradient Descent(7675/9): loss=20.555003387768124, w0=73.79912761648065, w1=13.327224525065372\n",
      "[ 0.52008081  2.23297881]\n",
      "Gradient Descent(7676/9): loss=15.52513050656409, w0=73.43507105231296, w1=11.764139358571342\n",
      "[-1.00509971 -3.33456661]\n",
      "Gradient Descent(7677/9): loss=16.867444886281337, w0=74.13864084780555, w1=14.098335987617572\n",
      "[ 1.74978368 -3.85799314]\n",
      "Gradient Descent(7678/9): loss=15.934010382903438, w0=72.91379227512446, w1=16.798931187298788\n",
      "[ 3.22622876  6.34445045]\n",
      "Gradient Descent(7679/9): loss=20.966743736338934, w0=70.65543214480364, w1=12.357815869443936\n",
      "[-1.52148964 -2.0607121 ]\n",
      "Gradient Descent(7680/9): loss=19.49602818426193, w0=71.72047489250895, w1=13.800314339962524\n",
      "[-1.47101923 -2.66045035]\n",
      "Gradient Descent(7681/9): loss=16.6751485629141, w0=72.75018835324084, w1=15.662629587625831\n",
      "[-2.88954073  2.5424811 ]\n",
      "Gradient Descent(7682/9): loss=17.916274656921008, w0=74.7728668651326, w1=13.88289281443652\n",
      "[ 1.03136217  2.9279697 ]\n",
      "Gradient Descent(7683/9): loss=16.560804031952685, w0=74.05091334531907, w1=11.833314024760844\n",
      "[-0.05701241 -1.28928183]\n",
      "Gradient Descent(7684/9): loss=17.02771967828076, w0=74.09082203556399, w1=12.735811303038899\n",
      "[-0.86838131 -0.29438103]\n",
      "Gradient Descent(7685/9): loss=15.980107147551074, w0=74.69868895048403, w1=12.941878020784864\n",
      "[ 2.20747079  0.41725236]\n",
      "Gradient Descent(7686/9): loss=16.517205887009375, w0=73.15345939941496, w1=12.649801370227944\n",
      "[ 0.27763361 -0.78143743]\n",
      "Gradient Descent(7687/9): loss=15.74012892791311, w0=72.95911587132308, w1=13.196807572651828\n",
      "[ 1.94353427 -1.05529361]\n",
      "Gradient Descent(7688/9): loss=15.481953022001067, w0=71.59864188234934, w1=13.935513102273204\n",
      "[ 0.07423736  0.15893212]\n",
      "Gradient Descent(7689/9): loss=16.926752335197445, w0=71.54667572738605, w1=13.82426061525712\n",
      "[-1.25112356  2.94615172]\n",
      "Gradient Descent(7690/9): loss=16.971679365352383, w0=72.42246221987106, w1=11.761954410065528\n",
      "[-1.61383528 -1.83147228]\n",
      "Gradient Descent(7691/9): loss=17.240955260949747, w0=73.55214691270542, w1=13.043985007914756\n",
      "[ 0.56360481 -0.04771684]\n",
      "Gradient Descent(7692/9): loss=15.51415711640904, w0=73.15762354518881, w1=13.077386795657343\n",
      "[-0.378908   -2.24227634]\n",
      "Gradient Descent(7693/9): loss=15.476109463540126, w0=73.42285914718978, w1=14.646980231602763\n",
      "[-1.79309564 -0.31238153]\n",
      "Gradient Descent(7694/9): loss=16.0754573170264, w0=74.67802609532916, w1=14.865647304306103\n",
      "[ 2.6328606   2.96948623]\n",
      "Gradient Descent(7695/9): loss=17.304167670263514, w0=72.83502367194522, w1=12.787006942417541\n",
      "[ 3.82062912  0.84848857]\n",
      "Gradient Descent(7696/9): loss=15.73110215726057, w0=70.16058328806835, w1=12.193064940127408\n",
      "[-0.7137646   0.34044149]\n",
      "Gradient Descent(7697/9): loss=21.122524505287274, w0=70.66021850966025, w1=11.954755895708866\n",
      "[-5.82901895 -1.76901791]\n",
      "Gradient Descent(7698/9): loss=20.016831135234415, w0=74.74053177412873, w1=13.193068435979411\n",
      "[ 2.36113478  1.38637248]\n",
      "Gradient Descent(7699/9): loss=16.473310176170514, w0=73.08773742708391, w1=12.222607697726778\n",
      "[-1.43516574 -2.65043521]\n",
      "Gradient Descent(7700/9): loss=16.197300068541377, w0=74.09235344202843, w1=14.077912347353191\n",
      "[ 0.68995465 -0.66824974]\n",
      "Gradient Descent(7701/9): loss=15.883555818534578, w0=73.60938518643312, w1=14.545687163713678\n",
      "[-0.43633079  0.01602215]\n",
      "Gradient Descent(7702/9): loss=16.003797440302332, w0=73.91481673614041, w1=14.534471658474537\n",
      "[-0.28618835  0.55786965]\n",
      "Gradient Descent(7703/9): loss=16.134901513969588, w0=74.1151485819431, w1=14.143962903843649\n",
      "[ 3.52627918  0.23535867]\n",
      "Gradient Descent(7704/9): loss=15.943708759232313, w0=71.64675315792037, w1=13.979211836007039\n",
      "[-3.90200791  3.75021745]\n",
      "Gradient Descent(7705/9): loss=16.86722029526464, w0=74.37815869586322, w1=11.354059623130436\n",
      "[ 0.16289205 -2.88557442]\n",
      "Gradient Descent(7706/9): loss=18.23287241115629, w0=74.26413426084738, w1=13.373961716917666\n",
      "[ 0.2151951   3.66736731]\n",
      "Gradient Descent(7707/9): loss=15.862135389522521, w0=74.11349768877596, w1=10.806804603267448\n",
      "[-2.50851789 -1.49910927]\n",
      "Gradient Descent(7708/9): loss=19.293958160359676, w0=75.86946021365735, w1=11.856181091922162\n",
      "[ 0.21771938 -5.55627378]\n",
      "Gradient Descent(7709/9): loss=20.020513419372335, w0=75.71705665109043, w1=15.74557273752988\n",
      "[ 3.55382766  2.27550356]\n",
      "Gradient Descent(7710/9): loss=20.88874008770102, w0=73.22937728725772, w1=14.152720243085344\n",
      "[-0.52060005 -0.11412545]\n",
      "Gradient Descent(7711/9): loss=15.614440633816063, w0=73.5937973243189, w1=14.2326080547489\n",
      "[ 0.63234316  2.96414545]\n",
      "Gradient Descent(7712/9): loss=15.71427638039257, w0=73.15115711551212, w1=12.157706241022698\n",
      "[ 0.57755843  2.08789208]\n",
      "Gradient Descent(7713/9): loss=16.269928963694063, w0=72.7468662116043, w1=10.696181781965034\n",
      "[-1.58249711 -3.16163825]\n",
      "Gradient Descent(7714/9): loss=19.40954433595182, w0=73.85461419005848, w1=12.909328560206369\n",
      "[ 2.38078372  2.02868748]\n",
      "Gradient Descent(7715/9): loss=15.705744615951378, w0=72.18806558467563, w1=11.489247327411798\n",
      "[-1.48769152 -2.68504914]\n",
      "Gradient Descent(7716/9): loss=17.97832274905573, w0=73.22944965160157, w1=13.368781728598469\n",
      "[ 0.25357201  0.6394748 ]\n",
      "Gradient Descent(7717/9): loss=15.394119021629287, w0=73.05194924446793, w1=12.921149368497097\n",
      "[ 1.84405046 -1.94732929]\n",
      "Gradient Descent(7718/9): loss=15.571159626173134, w0=71.76111391910479, w1=14.284279869217618\n",
      "[-1.59815029 -0.62418881]\n",
      "Gradient Descent(7719/9): loss=16.884302556595657, w0=72.87981912554504, w1=14.721212035524793\n",
      "[-0.12057976 -0.01027601]\n",
      "Gradient Descent(7720/9): loss=16.242289094082302, w0=72.96422495455062, w1=14.728405240242614\n",
      "[ 2.13915795  0.14607588]\n",
      "Gradient Descent(7721/9): loss=16.219854801358508, w0=71.46681439045942, w1=14.626152126843195\n",
      "[-1.56720889  0.64839293]\n",
      "Gradient Descent(7722/9): loss=17.71221096462568, w0=72.5638606135291, w1=14.172277074085725\n",
      "[-1.72676402  1.15429947]\n",
      "Gradient Descent(7723/9): loss=15.892205574037781, w0=73.77259542761531, w1=13.364267442757818\n",
      "[ 2.19160342 -3.42112399]\n",
      "Gradient Descent(7724/9): loss=15.507115766089834, w0=72.23847303259174, w1=15.759054234409398\n",
      "[-1.77895276  1.85923931]\n",
      "Gradient Descent(7725/9): loss=18.5405736517453, w0=73.48373996609608, w1=14.457586716481705\n",
      "[ 1.8869844   3.50566939]\n",
      "Gradient Descent(7726/9): loss=15.882022353758611, w0=72.16285088396847, w1=12.003618144157176\n",
      "[ 0.17974412 -3.3080482 ]\n",
      "Gradient Descent(7727/9): loss=17.11497598368414, w0=72.03702999865665, w1=14.319251885273092\n",
      "[ 1.75571092  0.33163053]\n",
      "Gradient Descent(7728/9): loss=16.528189867287463, w0=70.80803235245781, w1=14.0871105116933\n",
      "[-0.46486457 -0.8389685 ]\n",
      "Gradient Descent(7729/9): loss=18.660177755733386, w0=71.13343755139584, w1=14.674388460060214\n",
      "[-1.89240581 -0.10245676]\n",
      "Gradient Descent(7730/9): loss=18.43335980214776, w0=72.45812161943448, w1=14.746108195456122\n",
      "[-0.10631816 -1.15115398]\n",
      "Gradient Descent(7731/9): loss=16.53704811973014, w0=72.53254433051033, w1=15.551915984011014\n",
      "[-2.19187575  0.43496727]\n",
      "Gradient Descent(7732/9): loss=17.822749622520618, w0=74.0668573520588, w1=15.247438898472325\n",
      "[-1.89822578  1.99732053]\n",
      "Gradient Descent(7733/9): loss=17.247030821282898, w0=75.39561539972627, w1=13.849314530482463\n",
      "[ 2.32640579  1.42700694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7734/9): loss=17.662748292127976, w0=73.76713134443374, w1=12.850409669964911\n",
      "[-2.15085734 -1.46387427]\n",
      "Gradient Descent(7735/9): loss=15.69586239469642, w0=75.27273148028561, w1=13.875121660166839\n",
      "[ 4.224219    0.43640787]\n",
      "Gradient Descent(7736/9): loss=17.42190557197559, w0=72.3157781802219, w1=13.569636147966683\n",
      "[-3.88988339  3.18632839]\n",
      "Gradient Descent(7737/9): loss=15.86831367405147, w0=75.03869655080246, w1=11.339206277813823\n",
      "[ 0.50314122 -3.67298888]\n",
      "Gradient Descent(7738/9): loss=19.19889028617281, w0=74.68649769547706, w1=13.910298494945962\n",
      "[ 2.69588216 -0.55364802]\n",
      "Gradient Descent(7739/9): loss=16.44822357722908, w0=72.79938018051558, w1=14.297852111498269\n",
      "[-2.6721205  1.4250207]\n",
      "Gradient Descent(7740/9): loss=15.842849940619288, w0=74.66986453251921, w1=13.300337621435254\n",
      "[ 3.27879743  0.51951308]\n",
      "Gradient Descent(7741/9): loss=16.3485844541992, w0=72.37470633328726, w1=12.93667846549586\n",
      "[-2.18989282 -0.68914721]\n",
      "Gradient Descent(7742/9): loss=15.955809537741356, w0=73.90763130641163, w1=13.419081513994973\n",
      "[ 0.55633466  0.7453979 ]\n",
      "Gradient Descent(7743/9): loss=15.576045478215843, w0=73.51819704200098, w1=12.897302982550688\n",
      "[-3.29181425  1.34232032]\n",
      "Gradient Descent(7744/9): loss=15.580637900734303, w0=75.82246701358596, w1=11.957678761388769\n",
      "[ 1.27805469 -0.74774051]\n",
      "Gradient Descent(7745/9): loss=19.74095105815814, w0=74.92782873229699, w1=12.481097116127506\n",
      "[ 3.89541995 -2.13059273]\n",
      "Gradient Descent(7746/9): loss=17.2193297478449, w0=72.20103476792075, w1=13.97251202855345\n",
      "[ 0.72527945  3.37033725]\n",
      "Gradient Descent(7747/9): loss=16.104514841859682, w0=71.6933391510111, w1=11.613275950930511\n",
      "[-2.31068795 -0.38175612]\n",
      "Gradient Descent(7748/9): loss=18.408613174950034, w0=73.31082071509074, w1=11.88050523252088\n",
      "[ 1.81904444 -2.57557085]\n",
      "Gradient Descent(7749/9): loss=16.664762490292716, w0=72.03748960688561, w1=13.683404830095204\n",
      "[-0.84657338  2.26757967]\n",
      "Gradient Descent(7750/9): loss=16.195944346620042, w0=72.63009097252244, w1=12.09609906227185\n",
      "[-3.24505416 -2.88075236]\n",
      "Gradient Descent(7751/9): loss=16.563416669328777, w0=74.9016288818215, w1=14.11262571101\n",
      "[ 1.08913655 -1.03415774]\n",
      "Gradient Descent(7752/9): loss=16.878538181854754, w0=74.13923329685429, w1=14.836536126226811\n",
      "[ 1.61883511  0.15986194]\n",
      "Gradient Descent(7753/9): loss=16.663648725896632, w0=73.00604872214822, w1=14.724632768163698\n",
      "[-2.02978286  1.34936888]\n",
      "Gradient Descent(7754/9): loss=16.20223669946183, w0=74.42689672446203, w1=13.780074550223336\n",
      "[ 1.25981713  2.52701914]\n",
      "Gradient Descent(7755/9): loss=16.072812429713185, w0=73.54502473364916, w1=12.01116115171454\n",
      "[ 1.27242561 -2.59839423]\n",
      "Gradient Descent(7756/9): loss=16.495735595527425, w0=72.65432680711795, w1=13.830037111768506\n",
      "[-0.90882112 -0.00310947]\n",
      "Gradient Descent(7757/9): loss=15.651792565135096, w0=73.29050158804004, w1=13.83221373809979\n",
      "[-0.97069482  0.20690066]\n",
      "Gradient Descent(7758/9): loss=15.448022302792978, w0=73.9699879649876, w1=13.687383272940199\n",
      "[-0.37959223  2.21494595]\n",
      "Gradient Descent(7759/9): loss=15.635984050381122, w0=74.23570252500978, w1=12.136921110334864\n",
      "[ 1.25788897  2.06645826]\n",
      "Gradient Descent(7760/9): loss=16.730907416273897, w0=73.35518024644718, w1=10.690400325759372\n",
      "[ 0.4185583 -0.9846579]\n",
      "Gradient Descent(7761/9): loss=19.277895176426984, w0=73.0621894349339, w1=11.379660854796931\n",
      "[ 1.64087992 -1.47687418]\n",
      "Gradient Descent(7762/9): loss=17.617846179907005, w0=71.91357348927792, w1=12.413472778733745\n",
      "[-1.22436906 -1.89145149]\n",
      "Gradient Descent(7763/9): loss=16.907002379547336, w0=72.77063183102365, w1=13.737488824588226\n",
      "[ 1.17983288 -1.16479914]\n",
      "Gradient Descent(7764/9): loss=15.556028503922064, w0=71.94474881296246, w1=14.552848224307171\n",
      "[-2.29090145  5.4768714 ]\n",
      "Gradient Descent(7765/9): loss=16.8718322271379, w0=73.54837982756878, w1=10.719038245078387\n",
      "[-2.21886928 -0.36178073]\n",
      "Gradient Descent(7766/9): loss=19.228923252718676, w0=75.10158832443518, w1=10.972284757099214\n",
      "[ 2.82356364 -1.36742186]\n",
      "Gradient Descent(7767/9): loss=20.163313415196434, w0=73.12509377636437, w1=11.929480060693697\n",
      "[-3.7882873  -3.55594417]\n",
      "Gradient Descent(7768/9): loss=16.6017495608895, w0=75.77689488834598, w1=14.418640982182954\n",
      "[ 2.03394304  2.19687556]\n",
      "Gradient Descent(7769/9): loss=18.909258454100684, w0=74.35313476245864, w1=12.880828093314387\n",
      "[ 2.14494092 -0.55470666]\n",
      "Gradient Descent(7770/9): loss=16.126184932028732, w0=72.8516761159306, w1=13.26912275726946\n",
      "[ 0.52862349  1.10579173]\n",
      "Gradient Descent(7771/9): loss=15.505852586929594, w0=72.4816396702195, w1=12.495068546056988\n",
      "[-0.53068783 -1.61534788]\n",
      "Gradient Descent(7772/9): loss=16.200550956181857, w0=72.85312115086214, w1=13.625812063093377\n",
      "[ 2.2185587 -0.662105 ]\n",
      "Gradient Descent(7773/9): loss=15.493713114723814, w0=71.30013006057474, w1=14.089285562294704\n",
      "[-1.34575702 -0.81989837]\n",
      "Gradient Descent(7774/9): loss=17.55928072065187, w0=72.24215997176817, w1=14.663214424631562\n",
      "[-2.48599281 -0.05917142]\n",
      "Gradient Descent(7775/9): loss=16.639328032802617, w0=73.98235493628697, w1=14.704634421254172\n",
      "[ 1.65940163  1.55072733]\n",
      "Gradient Descent(7776/9): loss=16.373074757480314, w0=72.82077379405115, w1=13.619125292391681\n",
      "[-3.48706104  0.92133758]\n",
      "Gradient Descent(7777/9): loss=15.50754045462636, w0=75.26171651995027, w1=12.974188988077305\n",
      "[ 2.89861939  1.03300359]\n",
      "Gradient Descent(7778/9): loss=17.44977247874874, w0=73.23268294846476, w1=12.251086472043381\n",
      "[ 2.60769042 -0.24703266]\n",
      "Gradient Descent(7779/9): loss=16.14252385808687, w0=71.40729965129263, w1=12.42400933258252\n",
      "[-2.00805004 -0.75711774]\n",
      "Gradient Descent(7780/9): loss=17.72281433633753, w0=72.81293467581368, w1=12.953991752591635\n",
      "[-3.06535326  1.23611878]\n",
      "Gradient Descent(7781/9): loss=15.639753390806128, w0=74.95868195442458, w1=12.088708609387746\n",
      "[ 1.70278618 -4.10234145]\n",
      "Gradient Descent(7782/9): loss=17.739046539671357, w0=73.76673162541836, w1=14.960347626900063\n",
      "[-0.44495939 -0.55171262]\n",
      "Gradient Descent(7783/9): loss=16.593802624540853, w0=74.0782031986047, w1=15.346546462568073\n",
      "[ 2.28815297  1.14748438]\n",
      "Gradient Descent(7784/9): loss=17.43597100968421, w0=72.47649611731124, w1=14.543307394839637\n",
      "[-2.1969514 -1.6205365]\n",
      "Gradient Descent(7785/9): loss=16.285597526704773, w0=74.01436209966577, w1=15.677682945542811\n",
      "[ 0.19372508 -0.22064222]\n",
      "Gradient Descent(7786/9): loss=18.060942018547934, w0=73.87875454139187, w1=15.83213249642132\n",
      "[ 0.06489922  2.47776078]\n",
      "Gradient Descent(7787/9): loss=18.323842491048154, w0=73.83332508830874, w1=14.097699948591048\n",
      "[ 0.52722078  0.53628043]\n",
      "Gradient Descent(7788/9): loss=15.72231999701635, w0=73.4642705389593, w1=13.722303645715872\n",
      "[-2.84165993  0.60644984]\n",
      "Gradient Descent(7789/9): loss=15.429822428594518, w0=75.45343248884285, w1=13.297788757970828\n",
      "[ 5.56880776  1.10462026]\n",
      "Gradient Descent(7790/9): loss=17.73417875212428, w0=71.55526705863187, w1=12.52455457629152\n",
      "[-1.18593369  0.53579983]\n",
      "Gradient Descent(7791/9): loss=17.35351164257733, w0=72.38542064086454, w1=12.149494697315891\n",
      "[-1.53677646 -0.58785785]\n",
      "Gradient Descent(7792/9): loss=16.6833148453276, w0=73.4611641620729, w1=12.560995192097769\n",
      "[-1.80710698 -0.04676791]\n",
      "Gradient Descent(7793/9): loss=15.821893525057613, w0=74.7261390477463, w1=12.593732731743211\n",
      "[ 2.49451882 -0.75832674]\n",
      "Gradient Descent(7794/9): loss=16.803990719023666, w0=72.97997587228127, w1=13.124561447377575\n",
      "[-2.20330541  1.63920565]\n",
      "Gradient Descent(7795/9): loss=15.49823506704581, w0=74.52228965711377, w1=11.97711749523318\n",
      "[-0.25307708 -2.67505226]\n",
      "Gradient Descent(7796/9): loss=17.26922719325502, w0=74.69944361496374, w1=13.849654078491364\n",
      "[ 3.94805387  2.58374163]\n",
      "Gradient Descent(7797/9): loss=16.442061780734246, w0=71.93580590821021, w1=12.041034935796997\n",
      "[-1.2591999  -1.13441973]\n",
      "Gradient Descent(7798/9): loss=17.34302400441842, w0=72.8172458409638, w1=12.835128744112664\n",
      "[-0.87765699 -0.85541539]\n",
      "Gradient Descent(7799/9): loss=15.707242017401555, w0=73.431605732072, w1=13.433919518898175\n",
      "[-0.59290032  0.24539272]\n",
      "Gradient Descent(7800/9): loss=15.396414769160238, w0=73.84663595682566, w1=13.262144615940187\n",
      "[ 0.46614824 -0.83449122]\n",
      "Gradient Descent(7801/9): loss=15.562302104643614, w0=73.5203321853344, w1=13.84628846940236\n",
      "[ 1.83544881  1.16390002]\n",
      "Gradient Descent(7802/9): loss=15.478707648867436, w0=72.23551801558712, w1=13.031558456281997\n",
      "[-1.89766426  1.39640997]\n",
      "Gradient Descent(7803/9): loss=16.04641836248355, w0=73.56388299838491, w1=12.054071479366227\n",
      "[-2.04975526 -1.94897088]\n",
      "Gradient Descent(7804/9): loss=16.438553405760146, w0=74.99871167731403, w1=13.418351093203716\n",
      "[ 2.68122883  2.17290582]\n",
      "Gradient Descent(7805/9): loss=16.840924394311585, w0=73.12185149730503, w1=11.897317018783724\n",
      "[ 0.94156023 -2.43764434]\n",
      "Gradient Descent(7806/9): loss=16.652679624754303, w0=72.46275933748892, w1=13.603668056880606\n",
      "[ 1.17190289 -1.30277517]\n",
      "Gradient Descent(7807/9): loss=15.73898605445467, w0=71.64242731229609, w1=14.51561067488339\n",
      "[ 0.97915935  3.24796267]\n",
      "Gradient Descent(7808/9): loss=17.286147805771325, w0=70.95701576553277, w1=12.242036807157646\n",
      "[-2.69348342 -0.59930642]\n",
      "Gradient Descent(7809/9): loss=18.882373727958853, w0=72.8424541622105, w1=12.661551304445934\n",
      "[-2.51012818 -2.79235556]\n",
      "Gradient Descent(7810/9): loss=15.82249329182478, w0=74.59954388717763, w1=14.616200197787851\n",
      "[-1.88578225  0.44183102]\n",
      "Gradient Descent(7811/9): loss=16.88401433971517, w0=75.9195914609989, w1=14.306918486581523\n",
      "[ 1.55723586  0.57471124]\n",
      "Gradient Descent(7812/9): loss=19.1750928484086, w0=74.82952635704673, w1=13.90462062090692\n",
      "[ 0.44528963  0.13118363]\n",
      "Gradient Descent(7813/9): loss=16.65520171951712, w0=74.51782361325591, w1=13.812792079221532\n",
      "[ 1.13074091  0.6897317 ]\n",
      "Gradient Descent(7814/9): loss=16.19032647041909, w0=73.72630497832756, w1=13.329979889905887\n",
      "[ 2.8820624   0.64882414]\n",
      "Gradient Descent(7815/9): loss=15.4905753054214, w0=71.70886130094796, w1=12.87580298990975\n",
      "[-0.45818574 -2.36060886]\n",
      "Gradient Descent(7816/9): loss=16.824449890933916, w0=72.02959131949385, w1=14.528229194073864\n",
      "[-4.1926711   1.76885207]\n",
      "Gradient Descent(7817/9): loss=16.734847603366497, w0=74.96446108628373, w1=13.290032743752468\n",
      "[ 2.15236845  1.76027233]\n",
      "Gradient Descent(7818/9): loss=16.799227477347248, w0=73.4578031717354, w1=12.057842110579761\n",
      "[ 0.77962021 -0.42774885]\n",
      "Gradient Descent(7819/9): loss=16.41017399742697, w0=72.91206902648005, w1=12.35726630382957\n",
      "[-0.90843838 -0.08971929]\n",
      "Gradient Descent(7820/9): loss=16.088736375003727, w0=73.54797589307299, w1=12.420069807000623\n",
      "[ 0.22471482 -0.50738514]\n",
      "Gradient Descent(7821/9): loss=15.979580808112447, w0=73.39067552117838, w1=12.775239405757782\n",
      "[ 1.07687008 -3.59036106]\n",
      "Gradient Descent(7822/9): loss=15.638709615013061, w0=72.63686646828641, w1=15.288492150261094\n",
      "[-1.5181137   2.02849678]\n",
      "Gradient Descent(7823/9): loss=17.237590885280152, w0=73.69954606072926, w1=13.868544402374884\n",
      "[ 2.52057622 -0.64751972]\n",
      "Gradient Descent(7824/9): loss=15.5437484567273, w0=71.9351427063909, w1=14.321808207074131\n",
      "[-3.5676342   0.71095424]\n",
      "Gradient Descent(7825/9): loss=16.663591100742092, w0=74.43248664715796, w1=13.824140240794106\n",
      "[ 4.22483809  1.91251559]\n",
      "Gradient Descent(7826/9): loss=16.093367851017312, w0=71.47509998481623, w1=12.485379325289944\n",
      "[ 0.04031423 -0.17012648]\n",
      "Gradient Descent(7827/9): loss=17.534293800638892, w0=71.44688002494675, w1=12.6044678598063\n",
      "[ 0.72403397 -2.15248462]\n",
      "Gradient Descent(7828/9): loss=17.474696434715494, w0=70.94005624821071, w1=14.111207093481394\n",
      "[-1.7816573   0.58424216]\n",
      "Gradient Descent(7829/9): loss=18.3556226143602, w0=72.18721636129312, w1=13.702237580697028\n",
      "[ 1.64741209  0.33594551]\n",
      "Gradient Descent(7830/9): loss=16.023045276768205, w0=71.03402789922973, w1=13.467075721800596\n",
      "[-1.4414775   2.27338297]\n",
      "Gradient Descent(7831/9): loss=17.93952839019515, w0=72.04306215096867, w1=11.875707644576881\n",
      "[-2.30962188 -3.64139726]\n",
      "Gradient Descent(7832/9): loss=17.45462873625463, w0=73.65979746947278, w1=14.424685725823101\n",
      "[ 0.090965   -0.42548037]\n",
      "Gradient Descent(7833/9): loss=15.899307557834996, w0=73.59612196612362, w1=14.722521982407185\n",
      "[ 2.26026012  2.6033283 ]\n",
      "Gradient Descent(7834/9): loss=16.203838063532604, w0=72.01393988481496, w1=12.900192175875084\n",
      "[ 1.66482955 -2.32080704]\n",
      "Gradient Descent(7835/9): loss=16.372986844482547, w0=70.84855919723739, w1=14.524757103826591\n",
      "[-0.69540984  3.52153234]\n",
      "Gradient Descent(7836/9): loss=18.921846672477738, w0=71.33534608250946, w1=12.059684463\n",
      "[ 0.03853504 -0.56889898]\n",
      "Gradient Descent(7837/9): loss=18.312137405855193, w0=71.30837155205971, w1=12.457913747547218\n",
      "[-2.56776559 -2.77948769]\n",
      "Gradient Descent(7838/9): loss=17.879129442496218, w0=73.10580746710359, w1=14.40355513366487\n",
      "[-1.63771193 -1.73772166]\n",
      "Gradient Descent(7839/9): loss=15.830324073917149, w0=74.25220582058476, w1=15.619960298468845\n",
      "[ 0.17737541  1.24619089]\n",
      "Gradient Descent(7840/9): loss=18.13537226577421, w0=74.12804303243652, w1=14.747626678864728\n",
      "[ 0.92581511  0.75432915]\n",
      "Gradient Descent(7841/9): loss=16.537570080361316, w0=73.4799724569672, w1=14.219596275443754\n",
      "[-0.04967772  3.2415266 ]\n",
      "Gradient Descent(7842/9): loss=15.676909303389582, w0=73.51474685976399, w1=11.95052765339194\n",
      "[-4.51692832 -3.88856731]\n",
      "Gradient Descent(7843/9): loss=16.579472725843516, w0=76.67659668082784, w1=14.672524769591547\n",
      "[ 3.16835871  4.68757737]\n",
      "Gradient Descent(7844/9): loss=21.818532492655027, w0=74.45874558482707, w1=11.39122060769427\n",
      "[ 0.53531093 -0.3295317 ]\n",
      "Gradient Descent(7845/9): loss=18.245193914600463, w0=74.08402793658564, w1=11.621892799612935\n",
      "[ 3.40867144 -2.66471211]\n",
      "Gradient Descent(7846/9): loss=17.423768461474534, w0=71.69795793194778, w1=13.487191277844325\n",
      "[-3.16017666 -2.06125698]\n",
      "Gradient Descent(7847/9): loss=16.659466491991324, w0=73.91008159304293, w1=14.93007116353985\n",
      "[ 2.2899769   0.61857942]\n",
      "Gradient Descent(7848/9): loss=16.62748441032343, w0=72.30709776588566, w1=14.497065566159122\n",
      "[-4.09197438  1.87818559]\n",
      "Gradient Descent(7849/9): loss=16.390302602175307, w0=75.17147983332961, w1=13.18233565008329\n",
      "[ 1.5854811  -1.99799749]\n",
      "Gradient Descent(7850/9): loss=17.192716049725924, w0=74.06164306635297, w1=14.580933894045343\n",
      "[ 0.80907583  4.06646013]\n",
      "Gradient Descent(7851/9): loss=16.28693003601731, w0=73.49528998406227, w1=11.734411800491673\n",
      "[-0.93159033  2.62143685]\n",
      "Gradient Descent(7852/9): loss=16.929199553296606, w0=74.14740321239199, w1=9.899406002360209\n",
      "[ 0.71470565 -4.94339585]\n",
      "Gradient Descent(7853/9): loss=22.159400032747445, w0=73.6471092544875, w1=13.3597831004082\n",
      "[-0.0475832   0.54659915]\n",
      "Gradient Descent(7854/9): loss=15.455450009098588, w0=73.6804174911043, w1=12.977163695178787\n",
      "[-0.10924375 -2.48827799]\n",
      "Gradient Descent(7855/9): loss=15.586854868280172, w0=73.75688811325271, w1=14.7189582890558\n",
      "[ 0.39940888  1.06948574]\n",
      "Gradient Descent(7856/9): loss=16.260921822275748, w0=73.47730189857683, w1=13.970318269751257\n",
      "[ 0.62222102 -0.67347165]\n",
      "Gradient Descent(7857/9): loss=15.523049004595736, w0=73.04174718642597, w1=14.441748427175371\n",
      "[-0.57519952  0.93006778]\n",
      "Gradient Descent(7858/9): loss=15.880440562791785, w0=73.44438684867742, w1=13.790700978127193\n",
      "[-4.2655126   1.25661607]\n",
      "Gradient Descent(7859/9): loss=15.445564640837995, w0=76.43024566763341, w1=12.911069726379743\n",
      "[ 6.17405944 -2.3406309 ]\n",
      "Gradient Descent(7860/9): loss=20.46582820133287, w0=72.10840406309013, w1=14.549511354388947\n",
      "[-0.84328409  1.94086177]\n",
      "Gradient Descent(7861/9): loss=16.660849124667248, w0=72.69870292851887, w1=13.190908118539895\n",
      "[ 0.70967728  0.77856802]\n",
      "Gradient Descent(7862/9): loss=15.604734708209712, w0=72.20192883220093, w1=12.64591050104656\n",
      "[-4.11539119  1.85252514]\n",
      "Gradient Descent(7863/9): loss=16.32972524291129, w0=75.0827026619721, w1=11.349142903873938\n",
      "[ 2.59056487 -4.52854434]\n",
      "Gradient Descent(7864/9): loss=19.255419256844398, w0=73.2693072509451, w1=14.519123941760846\n",
      "[-1.35699622  1.60300055]\n",
      "Gradient Descent(7865/9): loss=15.926378952021548, w0=74.21920460503053, w1=13.397023554898091\n",
      "[-0.69694508  4.45673025]\n",
      "Gradient Descent(7866/9): loss=15.817380541912891, w0=74.70706615868252, w1=10.277312381483371\n",
      "[ 1.7053185  -2.54925638]\n",
      "Gradient Descent(7867/9): loss=21.51205912381032, w0=73.51334320843327, w1=12.06179184416073\n",
      "[-0.28371341 -0.28933137]\n",
      "Gradient Descent(7868/9): loss=16.415210102670102, w0=73.71194259891197, w1=12.2643238018824\n",
      "[-1.44929224 -1.54372487]\n",
      "Gradient Descent(7869/9): loss=16.211843243249174, w0=74.72644716756317, w1=13.344931208874115\n",
      "[-0.72646135 -1.03652497]\n",
      "Gradient Descent(7870/9): loss=16.42103503312112, w0=75.23497011484719, w1=14.070498689263303\n",
      "[ 1.85228693  1.36702587]\n",
      "Gradient Descent(7871/9): loss=17.444235955938737, w0=73.93836926079074, w1=13.113580577426928\n",
      "[-1.53232106  0.35865286]\n",
      "Gradient Descent(7872/9): loss=15.660570272004005, w0=75.01099400324948, w1=12.862523574421402\n",
      "[ 2.20989953 -3.0500715 ]\n",
      "Gradient Descent(7873/9): loss=17.050517042190624, w0=73.46406433284893, w1=14.997573626508387\n",
      "[ 0.57506468 -1.76302098]\n",
      "Gradient Descent(7874/9): loss=16.552313373545108, w0=73.06151905726614, w1=16.231688312493695\n",
      "[ 1.18338973 -0.79439202]\n",
      "Gradient Descent(7875/9): loss=19.199579048398068, w0=72.23314624960464, w1=16.787762724647582\n",
      "[-0.37303531  6.44893553]\n",
      "Gradient Descent(7876/9): loss=21.42010882683091, w0=72.49427096703697, w1=12.273507850816173\n",
      "[-1.41449056 -3.89054347]\n",
      "Gradient Descent(7877/9): loss=16.433073507212068, w0=73.48441436020754, w1=14.99688828221874\n",
      "[-1.87267847  1.60490112]\n",
      "Gradient Descent(7878/9): loss=16.55494281378566, w0=74.79528928764962, w1=13.873457495206168\n",
      "[ 5.46729273 -2.57001846]\n",
      "Gradient Descent(7879/9): loss=16.59045731810362, w0=70.96818437837885, w1=15.672470417187647\n",
      "[-4.12441644  1.50140671]\n",
      "Gradient Descent(7880/9): loss=20.49450940028536, w0=73.85527588457497, w1=14.62148571861383\n",
      "[ 2.88024278  2.78296288]\n",
      "Gradient Descent(7881/9): loss=16.195270075111, w0=71.8391059392357, w1=12.673411705105607\n",
      "[ 1.00801701 -1.04470234]\n",
      "Gradient Descent(7882/9): loss=16.769193190726238, w0=71.13349403340786, w1=13.404703340541525\n",
      "[-1.1564443   0.72473929]\n",
      "Gradient Descent(7883/9): loss=17.722425554919162, w0=71.94300504246236, w1=12.89738583712878\n",
      "[-0.72249429  1.77833241]\n",
      "Gradient Descent(7884/9): loss=16.46792831804247, w0=72.4487510464686, w1=11.652553152893288\n",
      "[-1.48308912 -1.1551148 ]\n",
      "Gradient Descent(7885/9): loss=17.412300362029576, w0=73.48691343047469, w1=12.461133515836277\n",
      "[ 0.22471776 -2.32867368]\n",
      "Gradient Descent(7886/9): loss=15.923262221812664, w0=73.32961099550185, w1=14.091205089695272\n",
      "[-3.6024408   1.54006968]\n",
      "Gradient Descent(7887/9): loss=15.573486354334067, w0=75.85131955684844, w1=13.013156312429183\n",
      "[ 0.53490522 -1.09305672]\n",
      "Gradient Descent(7888/9): loss=18.76486630308182, w0=75.47688590255618, w1=13.778296016598913\n",
      "[ 0.35635582 -0.43492012]\n",
      "Gradient Descent(7889/9): loss=17.81312964176898, w0=75.22743682970818, w1=14.082740104057361\n",
      "[ 1.449976    1.88372725]\n",
      "Gradient Descent(7890/9): loss=17.436948847940695, w0=74.21245363193049, w1=12.76413102938648\n",
      "[ 2.37223236 -0.81426847]\n",
      "Gradient Descent(7891/9): loss=16.06376642034623, w0=72.55189097949119, w1=13.334118956059735\n",
      "[ 2.92280954  0.87430571]\n",
      "Gradient Descent(7892/9): loss=15.671791618643558, w0=70.50592429957621, w1=12.722104962339854\n",
      "[-0.93574011 -1.18402118]\n",
      "Gradient Descent(7893/9): loss=19.55933800478979, w0=71.16094237945964, w1=13.550919785715848\n",
      "[-3.80870794  3.69622688]\n",
      "Gradient Descent(7894/9): loss=17.663224147538735, w0=73.82703793968088, w1=10.96356097205548\n",
      "[ 2.209974   -7.83931277]\n",
      "Gradient Descent(7895/9): loss=18.693503262489372, w0=72.2800561365355, w1=16.451079911296056\n",
      "[ 0.28895118  0.76105125]\n",
      "Gradient Descent(7896/9): loss=20.314362205140622, w0=72.07779030799783, w1=15.918344036971842\n",
      "[-4.10209544  2.38527031]\n",
      "Gradient Descent(7897/9): loss=19.098838062630204, w0=74.94925711303847, w1=14.248654821805953\n",
      "[ 1.08377238  0.39644622]\n",
      "Gradient Descent(7898/9): loss=17.051591230695486, w0=74.19061644735334, w1=13.97114247032535\n",
      "[ 0.84058608 -1.14150331]\n",
      "Gradient Descent(7899/9): loss=15.908670072714157, w0=73.60220619375292, w1=14.770194786723678\n",
      "[ 1.51553767  2.41328237]\n",
      "Gradient Descent(7900/9): loss=16.266079790308616, w0=72.5413298259951, w1=13.080897127783414\n",
      "[-1.95546481  0.23697495]\n",
      "Gradient Descent(7901/9): loss=15.748612185231215, w0=73.91015519410452, w1=12.915014663720315\n",
      "[-1.54276362 -1.58660186]\n",
      "Gradient Descent(7902/9): loss=15.735201328728179, w0=74.9900897282435, w1=14.025635966726473\n",
      "[ 4.40571719  4.95580691]\n",
      "Gradient Descent(7903/9): loss=16.97339659767834, w0=71.906087694585, w1=10.556571126829429\n",
      "[-1.77521195 -2.92422349]\n",
      "Gradient Descent(7904/9): loss=20.62130745512899, w0=73.14873606051728, w1=12.603527572332538\n",
      "[-1.98225345 -0.64269888]\n",
      "Gradient Descent(7905/9): loss=15.780277304420986, w0=74.53631347320871, w1=13.053416789067368\n",
      "[ 2.3629764   3.46424703]\n",
      "Gradient Descent(7906/9): loss=16.248520141430678, w0=72.88222999547, w1=10.628443864991702\n",
      "[-0.41533117 -3.5827042 ]\n",
      "Gradient Descent(7907/9): loss=19.53549925212041, w0=73.17296181555238, w1=13.136336802155899\n",
      "[-0.33164898 -0.55841882]\n",
      "Gradient Descent(7908/9): loss=15.452156964806624, w0=73.40511610077398, w1=13.52722997447504\n",
      "[ 0.02596536 -0.67141789]\n",
      "Gradient Descent(7909/9): loss=15.393198890898185, w0=73.38694034871409, w1=13.997222496776509\n",
      "[-0.55688753  3.10832756]\n",
      "Gradient Descent(7910/9): loss=15.524122407257959, w0=73.77676162099253, w1=11.821393202705579\n",
      "[ 1.82729394  0.77713967]\n",
      "Gradient Descent(7911/9): loss=16.877466255693655, w0=72.49765586645997, w1=11.277395430995057\n",
      "[-1.44551687 -0.67981246]\n",
      "Gradient Descent(7912/9): loss=18.128007841257624, w0=73.50951767494743, w1=11.753264150650878\n",
      "[-1.53993616  0.68031571]\n",
      "Gradient Descent(7913/9): loss=16.899440455150657, w0=74.58747298668175, w1=11.277043150879274\n",
      "[ 4.46067    -2.92563441]\n",
      "Gradient Descent(7914/9): loss=18.64840093125923, w0=71.46500398796869, w1=13.324987234902482\n",
      "[-0.47248731  0.04425388]\n",
      "Gradient Descent(7915/9): loss=17.07032836381682, w0=71.79574510268318, w1=13.294009519148654\n",
      "[-3.73007034  2.06026847]\n",
      "Gradient Descent(7916/9): loss=16.52539766628608, w0=74.40679433803541, w1=11.851821591886193\n",
      "[-1.24165621 -1.07137981]\n",
      "Gradient Descent(7917/9): loss=17.33014458539786, w0=75.27595368729234, w1=12.601787458169493\n",
      "[ 3.34270879 -1.15911178]\n",
      "Gradient Descent(7918/9): loss=17.735488801834126, w0=72.93605753335024, w1=13.413165702483132\n",
      "[-3.68374383  0.90774159]\n",
      "Gradient Descent(7919/9): loss=15.452135591631638, w0=75.51467821110188, w1=12.777746590663561\n",
      "[-1.10670547 -0.12477738]\n",
      "Gradient Descent(7920/9): loss=18.098144962027874, w0=76.28937203970982, w1=12.86509075742575\n",
      "[ 0.91616629 -3.56684199]\n",
      "Gradient Descent(7921/9): loss=20.06112823598755, w0=75.64805563351194, w1=15.361880153149357\n",
      "[ 1.71587296  2.50943566]\n",
      "Gradient Descent(7922/9): loss=19.928138105731964, w0=74.44694456297044, w1=13.605275191242823\n",
      "[ 0.69763579 -0.59963089]\n",
      "Gradient Descent(7923/9): loss=16.058501384640554, w0=73.95859950834163, w1=14.025016813172543\n",
      "[-4.23685721 -0.7087229 ]\n",
      "Gradient Descent(7924/9): loss=15.755464394910794, w0=76.92439955807004, w1=14.521122841494694\n",
      "[-0.17229307  1.93983889]\n",
      "Gradient Descent(7925/9): loss=22.51833932840076, w0=77.04500470890872, w1=13.16323562198527\n",
      "[ 2.91877493  2.03962665]\n",
      "Gradient Descent(7926/9): loss=22.47127739205417, w0=75.00186225719801, w1=11.735496964438921\n",
      "[ 2.1294471  -5.29636635]\n",
      "Gradient Descent(7927/9): loss=18.365561630165875, w0=73.5112492896925, w1=15.442953407151839\n",
      "[ 1.20059289 -1.45001665]\n",
      "Gradient Descent(7928/9): loss=17.336661001183778, w0=72.67083426521303, w1=16.45796506549986\n",
      "[-0.22417318  3.92034078]\n",
      "Gradient Descent(7929/9): loss=20.01500139833438, w0=72.8277554915348, w1=13.713726519077126\n",
      "[ 0.1785328  -0.80624237]\n",
      "Gradient Descent(7930/9): loss=15.521924772393879, w0=72.7027825289367, w1=14.27809617944308\n",
      "[-0.32153441 -4.56157918]\n",
      "Gradient Descent(7931/9): loss=15.879319108902584, w0=72.92785661926813, w1=17.471201604585094\n",
      "[ 1.409905    3.13607664]\n",
      "Gradient Descent(7932/9): loss=23.41888269658649, w0=71.94092311884744, w1=15.275947956782003\n",
      "[ 0.71798006 -0.49250263]\n",
      "Gradient Descent(7933/9): loss=17.914421882753174, w0=71.43833707916279, w1=15.620699796974758\n",
      "[-0.20194877  0.1182878 ]\n",
      "Gradient Descent(7934/9): loss=19.399399014046242, w0=71.57970121526618, w1=15.537898338644567\n",
      "[ 0.1050183 -0.5449713]\n",
      "Gradient Descent(7935/9): loss=18.973228928848016, w0=71.50618840735001, w1=15.919378249809567\n",
      "[-2.05122505  2.4724907 ]\n",
      "Gradient Descent(7936/9): loss=19.959868215739476, w0=72.94204593989426, w1=14.188634760870867\n",
      "[ 3.97502376  0.15041515]\n",
      "Gradient Descent(7937/9): loss=15.69908168247478, w0=70.15952931009382, w1=14.083344157351778\n",
      "[-4.08313885 -0.98385469]\n",
      "Gradient Descent(7938/9): loss=20.480282270817852, w0=73.01772650399754, w1=14.772042437423677\n",
      "[-1.57362814  0.24686532]\n",
      "Gradient Descent(7939/9): loss=16.25908826301321, w0=74.11926619987618, w1=14.599236713168473\n",
      "[-0.88304345  1.2656749 ]\n",
      "Gradient Descent(7940/9): loss=16.3531516959431, w0=74.73739661270194, w1=13.713264281678644\n",
      "[ 1.35821074 -0.79818161]\n",
      "Gradient Descent(7941/9): loss=16.454970577094187, w0=73.78664909175247, w1=14.271991411842215\n",
      "[ 0.72575049  0.02974757]\n",
      "Gradient Descent(7942/9): loss=15.821130849847284, w0=73.27862374696961, w1=14.251168109529209\n",
      "[ 1.44372929  5.43178452]\n",
      "Gradient Descent(7943/9): loss=15.683576816024603, w0=72.26801324210048, w1=10.448918943601456\n",
      "[-2.53154254 -3.69570154]\n",
      "Gradient Descent(7944/9): loss=20.5049868544753, w0=74.040093019084, w1=13.035910024206828\n",
      "[ 1.39255582  0.67387938]\n",
      "Gradient Descent(7945/9): loss=15.76275375202705, w0=73.06530394670551, w1=12.564194456842065\n",
      "[ 1.47124888 -2.03469926]\n",
      "Gradient Descent(7946/9): loss=15.831107560611935, w0=72.03542972981924, w1=13.988483940568184\n",
      "[-1.53115013  0.59760226]\n",
      "Gradient Descent(7947/9): loss=16.30721349097575, w0=73.10723482087963, w1=13.570162359957116\n",
      "[ 0.28636024 -1.56612007]\n",
      "Gradient Descent(7948/9): loss=15.407404515109741, w0=72.90678265088351, w1=14.666446408239507\n",
      "[-3.89474265  0.34718029]\n",
      "Gradient Descent(7949/9): loss=16.16499506909498, w0=75.63310250433705, w1=14.423420206221905\n",
      "[ 2.60555835  1.7584049 ]\n",
      "Gradient Descent(7950/9): loss=18.567062758582882, w0=73.80921166010744, w1=13.192536779256455\n",
      "[ 0.37751933 -0.48195933]\n",
      "Gradient Descent(7951/9): loss=15.559884513274158, w0=73.54494812883071, w1=13.529908307985862\n",
      "[ 1.06541617  0.65612963]\n",
      "Gradient Descent(7952/9): loss=15.418654739811766, w0=72.79915680858048, w1=13.070617570296484\n",
      "[-1.09672421 -1.42911738]\n",
      "Gradient Descent(7953/9): loss=15.591963471350088, w0=73.5668637575948, w1=14.070999733296288\n",
      "[ 1.46116936 -1.9140337 ]\n",
      "Gradient Descent(7954/9): loss=15.597946804344014, w0=72.54404520546194, w1=15.410823321945907\n",
      "[-3.0164065   5.13507661]\n",
      "Gradient Descent(7955/9): loss=17.531640102763024, w0=74.6555297585828, w1=11.816269693021315\n",
      "[ 3.37922752 -4.43316669]\n",
      "Gradient Descent(7956/9): loss=17.696396587981965, w0=72.29007049295942, w1=14.919486378786146\n",
      "[-2.15718539  4.02148716]\n",
      "Gradient Descent(7957/9): loss=16.926221299655055, w0=73.80010026402509, w1=12.104445366176682\n",
      "[-0.76879219 -0.56752531]\n",
      "Gradient Descent(7958/9): loss=16.459675840529403, w0=74.3382547988122, w1=12.50171308623494\n",
      "[ 4.54166559 -2.80767584]\n",
      "Gradient Descent(7959/9): loss=16.409444727050076, w0=71.15908888618812, w1=14.46708617410811\n",
      "[-3.32576905  2.38017223]\n",
      "Gradient Descent(7960/9): loss=18.15209753558848, w0=73.48712722293925, w1=12.800965610376714\n",
      "[-0.72339024 -0.98809703]\n",
      "Gradient Descent(7961/9): loss=15.634900623468834, w0=73.99350039142026, w1=13.492633532676107\n",
      "[ 1.54031344 -0.48860775]\n",
      "Gradient Descent(7962/9): loss=15.630676307610452, w0=72.91528098591158, w1=13.834658959131549\n",
      "[-3.82805858  0.09736949]\n",
      "Gradient Descent(7963/9): loss=15.520565895901886, w0=75.59492199466624, w1=13.76650031575828\n",
      "[ 0.53716148  0.60718886]\n",
      "Gradient Descent(7964/9): loss=18.074311995990424, w0=75.21890895780186, w1=13.341468113071187\n",
      "[ 2.52937948  1.4874296 ]\n",
      "Gradient Descent(7965/9): loss=17.24823100490183, w0=73.44834332415111, w1=12.300267392752223\n",
      "[-0.31556161 -2.76277117]\n",
      "Gradient Descent(7966/9): loss=16.093356145009118, w0=73.66923644957912, w1=14.234207213823565\n",
      "[ 1.84615854  3.17335877]\n",
      "Gradient Descent(7967/9): loss=15.740949521715002, w0=72.37692547042886, w1=12.01285607154512\n",
      "[-3.00774971 -3.8408781 ]\n",
      "Gradient Descent(7968/9): loss=16.882162983870582, w0=74.48235026752664, w1=14.701470742226018\n",
      "[ 0.76199654 -1.53646996]\n",
      "Gradient Descent(7969/9): loss=16.83841542050699, w0=73.94895269078559, w1=15.776999714872073\n",
      "[ 2.78721019 -0.3308158 ]\n",
      "Gradient Descent(7970/9): loss=18.23918489354213, w0=71.99790555768466, w1=16.008570776448764\n",
      "[-4.66440914  5.19341612]\n",
      "Gradient Descent(7971/9): loss=19.42327943651881, w0=75.26299195798413, w1=12.37317948973081\n",
      "[ 3.19060945 -0.21747725]\n",
      "Gradient Descent(7972/9): loss=17.93671369387288, w0=73.0295653438467, w1=12.525413564598754\n",
      "[-1.07466015 -2.54707475]\n",
      "Gradient Descent(7973/9): loss=15.876173257226293, w0=73.78182744991352, w1=14.308365886313014\n",
      "[-0.23738368  0.32483417]\n",
      "Gradient Descent(7974/9): loss=15.848247003025483, w0=73.94799602405635, w1=14.080981964570222\n",
      "[-0.43452338 -0.31174447]\n",
      "Gradient Descent(7975/9): loss=15.780556805526468, w0=74.25216239094532, w1=14.2992030921108\n",
      "[-2.34824741 -0.53250736]\n",
      "Gradient Descent(7976/9): loss=16.180782658786555, w0=75.89593558083561, w1=14.671958247389338\n",
      "[ 0.00421725  3.70349253]\n",
      "Gradient Descent(7977/9): loss=19.48185023937116, w0=75.8929835038735, w1=12.07951347949005\n",
      "[ 0.63332311 -0.61957688]\n",
      "Gradient Descent(7978/9): loss=19.743726771306708, w0=75.44965732374057, w1=12.513217292725056\n",
      "[ 3.32495626  2.32897225]\n",
      "Gradient Descent(7979/9): loss=18.176541687312543, w0=73.12218793941038, w1=10.882936714571603\n",
      "[-2.50806738 -2.11150013]\n",
      "Gradient Descent(7980/9): loss=18.772256234049006, w0=74.8778351062396, w1=12.36098680575948\n",
      "[ 0.53100934 -3.40103548]\n",
      "Gradient Descent(7981/9): loss=17.266051746301308, w0=74.50612856609258, w1=14.7417116426721\n",
      "[ 0.00861062  0.72558533]\n",
      "Gradient Descent(7982/9): loss=16.916931245812787, w0=74.50010113027534, w1=14.233801912298855\n",
      "[-0.22936866 -0.46974194]\n",
      "Gradient Descent(7983/9): loss=16.39764738334074, w0=74.66065919515529, w1=14.562621272287364\n",
      "[ 0.26473885 -1.5368656 ]\n",
      "Gradient Descent(7984/9): loss=16.906218921212027, w0=74.47534200006734, w1=15.63842718937359\n",
      "[ 2.18790363  2.2862794 ]\n",
      "Gradient Descent(7985/9): loss=18.4137891700206, w0=72.9438094618562, w1=14.038031611794821\n",
      "[ 0.0043559   0.53337614]\n",
      "Gradient Descent(7986/9): loss=15.603037415843737, w0=72.94076032944346, w1=13.664668316389173\n",
      "[ 0.47199209  3.74886429]\n",
      "Gradient Descent(7987/9): loss=15.465353791380267, w0=72.6103658681285, w1=11.040463316792787\n",
      "[-0.08217686 -1.82438188]\n",
      "Gradient Descent(7988/9): loss=18.594480493288597, w0=72.66788967361836, w1=12.31753063584041\n",
      "[ 0.67605866 -2.42664035]\n",
      "Gradient Descent(7989/9): loss=16.257179374120902, w0=72.19464861367808, w1=14.016178880559789\n",
      "[-2.94145255  1.6038043 ]\n",
      "Gradient Descent(7990/9): loss=16.133986983693063, w0=74.25366540206578, w1=12.893515870200485\n",
      "[ 2.31612691  0.71741781]\n",
      "Gradient Descent(7991/9): loss=16.01825477199831, w0=72.63237656215463, w1=12.391323403128238\n",
      "[-1.26237597 -3.10273436]\n",
      "Gradient Descent(7992/9): loss=16.19700439572654, w0=73.5160397430079, w1=14.563237455861195\n",
      "[-0.80894937 -2.5499811 ]\n",
      "Gradient Descent(7993/9): loss=15.997569249669253, w0=74.08230430012152, w1=16.348224222896782\n",
      "[ 3.00136225  3.29493517]\n",
      "Gradient Descent(7994/9): loss=19.810841131424965, w0=71.98135072587617, w1=14.041769603938413\n",
      "[ 0.21568959  1.80859019]\n",
      "Gradient Descent(7995/9): loss=16.405263677003827, w0=71.83036801635723, w1=12.775756471789744\n",
      "[-1.84200757 -0.75035962]\n",
      "Gradient Descent(7996/9): loss=16.704660002490705, w0=73.11977331330705, w1=13.30100820555095\n",
      "[ 0.5960368  0.766711 ]\n",
      "Gradient Descent(7997/9): loss=15.417019352543988, w0=72.7025475549929, w1=12.764310505298692\n",
      "[-2.31847823 -1.81625727]\n",
      "Gradient Descent(7998/9): loss=15.816649697680422, w0=74.32548231326795, w1=14.035690593763384\n",
      "[ 0.11346754 -1.31602038]\n",
      "Gradient Descent(7999/9): loss=16.07250206312956, w0=74.2460550363015, w1=14.95690486071175\n",
      "[-0.78991922  4.31443856]\n",
      "Gradient Descent(8000/9): loss=16.930215257539594, w0=74.79899848887952, w1=11.93679786988648\n",
      "[ 6.68873743 -1.55831741]\n",
      "Gradient Descent(8001/9): loss=17.708808161952707, w0=70.11688228695193, w1=13.027620054930088\n",
      "[-5.25709117  1.39609597]\n",
      "Gradient Descent(8002/9): loss=20.534872304713623, w0=73.79684610688227, w1=12.0503528773397\n",
      "[ 2.63749659  0.95446424]\n",
      "Gradient Descent(8003/9): loss=16.53388856893409, w0=71.95059849229155, w1=11.382227911543865\n",
      "[-3.21635295 -1.2945517 ]\n",
      "Gradient Descent(8004/9): loss=18.487867557884453, w0=74.20204555943066, w1=12.288414102432851\n",
      "[ 1.25904484  0.03781915]\n",
      "Gradient Descent(8005/9): loss=16.50782792508972, w0=73.32071417321592, w1=12.261940695779225\n",
      "[ 2.43693779 -1.77772578]\n",
      "Gradient Descent(8006/9): loss=16.127730783454876, w0=71.61485771853965, w1=13.50634874443101\n",
      "[-2.58312236 -0.46827475]\n",
      "Gradient Descent(8007/9): loss=16.79587104949247, w0=73.42304337339048, w1=13.834141070105241\n",
      "[ 1.01778598  2.56451016]\n",
      "Gradient Descent(8008/9): loss=15.457033861785861, w0=72.71059319080801, w1=12.038983958911842\n",
      "[-5.37541559 -2.36150028]\n",
      "Gradient Descent(8009/9): loss=16.593873390763964, w0=76.47338410505996, w1=13.692034154004727\n",
      "[ 0.7620363  1.0792902]\n",
      "Gradient Descent(8010/9): loss=20.462917757075083, w0=75.93995869831292, w1=12.936531012397946\n",
      "[ 1.62121335 -1.38940788]\n",
      "Gradient Descent(8011/9): loss=19.03416599659241, w0=74.80510935507094, w1=13.9091165269321\n",
      "[ 1.73378981 -0.83211412]\n",
      "Gradient Descent(8012/9): loss=16.619925413799933, w0=73.59145648650508, w1=14.491596408352766\n",
      "[-1.12784347  1.02468682]\n",
      "Gradient Descent(8013/9): loss=15.942105841308129, w0=74.38094691878665, w1=13.774315635119423\n",
      "[ 1.94571445 -0.22746753]\n",
      "Gradient Descent(8014/9): loss=16.020094976336097, w0=73.01894680066808, w1=13.933542903310927\n",
      "[ 0.03534197  2.71908803]\n",
      "Gradient Descent(8015/9): loss=15.52667459652072, w0=72.99420742378511, w1=12.030181280038114\n",
      "[-0.29957511 -1.84445138]\n",
      "Gradient Descent(8016/9): loss=16.481372567644883, w0=73.20391000309961, w1=13.321297243513103\n",
      "[ 0.24335415  1.55044352]\n",
      "Gradient Descent(8017/9): loss=15.402486635257072, w0=73.0335620980301, w1=12.23598677761266\n",
      "[-0.16811209 -2.03013066]\n",
      "Gradient Descent(8018/9): loss=16.19320826406256, w0=73.15124055926293, w1=13.657078238745726\n",
      "[ 2.85157431 -2.52951303]\n",
      "Gradient Descent(8019/9): loss=15.4117961800663, w0=71.15513854332241, w1=15.427737362846262\n",
      "[-3.66143438  7.15159316]\n",
      "Gradient Descent(8020/9): loss=19.57048577038737, w0=73.71814260947889, w1=10.421622151755653\n",
      "[ 3.57123525 -6.03147254]\n",
      "Gradient Descent(8021/9): loss=20.151827520892915, w0=71.21827793748686, w1=14.643652929910742\n",
      "[-2.98281667  2.5509839 ]\n",
      "Gradient Descent(8022/9): loss=18.21741574818133, w0=73.30624960336655, w1=12.857964200168626\n",
      "[-0.73466913  1.12251055]\n",
      "Gradient Descent(8023/9): loss=15.579249287456987, w0=73.82051799504035, w1=12.07220681722587\n",
      "[ 0.55190621  1.4076624 ]\n",
      "Gradient Descent(8024/9): loss=16.51507557073454, w0=73.43418364514157, w1=11.086843137181095\n",
      "[-3.28209429 -2.14949345]\n",
      "Gradient Descent(8025/9): loss=18.258636271279, w0=75.73164964554266, w1=12.591488549282182\n",
      "[-0.12546169 -0.11322875]\n",
      "Gradient Descent(8026/9): loss=18.751616736189106, w0=75.81947283037293, w1=12.670748675739706\n",
      "[ 1.50480416 -0.75026228]\n",
      "Gradient Descent(8027/9): loss=18.90230254380075, w0=74.76610992031232, w1=13.195932274992781\n",
      "[ 3.44934248  0.77296808]\n",
      "Gradient Descent(8028/9): loss=16.509822091690673, w0=72.35157018280822, w1=12.654854620864814\n",
      "[-1.35234616 -0.9335055 ]\n",
      "Gradient Descent(8029/9): loss=16.170096551256464, w0=73.29821249197711, w1=13.308308469833051\n",
      "[-0.31865781 -2.32237326]\n",
      "Gradient Descent(8030/9): loss=15.400586732616672, w0=73.52127295593725, w1=14.933969752947732\n",
      "[ 1.34564922 -0.17256478]\n",
      "Gradient Descent(8031/9): loss=16.46916427035176, w0=72.57931850375786, w1=15.054765097246333\n",
      "[-1.36321224  1.08610208]\n",
      "Gradient Descent(8032/9): loss=16.881612393196402, w0=73.53356707378414, w1=14.294493639388575\n",
      "[ 2.92412389  3.22347607]\n",
      "Gradient Descent(8033/9): loss=15.746536954540776, w0=71.48668035309264, w1=12.038060389202347\n",
      "[-0.0019281 -1.4927453]\n",
      "Gradient Descent(8034/9): loss=18.05812936835269, w0=71.4880300228005, w1=13.08298209635309\n",
      "[-1.04479921 -2.30987797]\n",
      "Gradient Descent(8035/9): loss=17.095208270085003, w0=72.2193894721173, w1=14.699896677903325\n",
      "[-3.75668091  2.40977622]\n",
      "Gradient Descent(8036/9): loss=16.707622741158634, w0=74.84906610824778, w1=13.013053322303909\n",
      "[ 2.22358846 -1.65473392]\n",
      "Gradient Descent(8037/9): loss=16.704009827990465, w0=73.29255418529563, w1=14.17136706353823\n",
      "[ 1.46426116  0.78076148]\n",
      "Gradient Descent(8038/9): loss=15.625081866887568, w0=72.2675713754189, w1=13.624834030772066\n",
      "[-5.28247976 -1.68904743]\n",
      "Gradient Descent(8039/9): loss=15.923115812060377, w0=75.96530720578757, w1=14.807167233954196\n",
      "[ 3.82586227  4.99922677]\n",
      "Gradient Descent(8040/9): loss=19.83510544370377, w0=73.28720361391909, w1=11.307708494129976\n",
      "[ 0.44317244 -1.82004429]\n",
      "Gradient Descent(8041/9): loss=17.744710996752982, w0=72.97698290863366, w1=12.581739495186417\n",
      "[-1.04477058 -0.12494665]\n",
      "Gradient Descent(8042/9): loss=15.83929076362357, w0=73.7083223148077, w1=12.669202148430527\n",
      "[ 1.8457905  -1.52822873]\n",
      "Gradient Descent(8043/9): loss=15.800215140721958, w0=72.4162689629096, w1=13.738962260861484\n",
      "[ 3.05058121 -0.46884802]\n",
      "Gradient Descent(8044/9): loss=15.804630533541472, w0=70.28086211362107, w1=14.067155872955604\n",
      "[-1.91100217 -1.11144279]\n",
      "Gradient Descent(8045/9): loss=20.097697711030357, w0=71.61856363427944, w1=14.845165827681472\n",
      "[-0.75855459  1.28379946]\n",
      "Gradient Descent(8046/9): loss=17.721532182958907, w0=72.1495518456804, w1=13.946506205878974\n",
      "[ 0.47567347  1.64089344]\n",
      "Gradient Descent(8047/9): loss=16.14962760855806, w0=71.81658041552119, w1=12.797880800611006\n",
      "[ 1.55485003 -1.9984574 ]\n",
      "Gradient Descent(8048/9): loss=16.709604139373983, w0=70.72818539404653, w1=14.196800981382937\n",
      "[-2.36980142 -0.37351446]\n",
      "Gradient Descent(8049/9): loss=18.93449803148022, w0=72.38704638737258, w1=14.458261101303751\n",
      "[-2.27880618  1.70438254]\n",
      "Gradient Descent(8050/9): loss=16.275878305300868, w0=73.98221071197936, w1=13.26519332623441\n",
      "[ 0.71094804  1.03921077]\n",
      "Gradient Descent(8051/9): loss=15.645767766909968, w0=73.48454708737222, w1=12.537745785075407\n",
      "[-1.74892074 -1.24020372]\n",
      "Gradient Descent(8052/9): loss=15.8477074151707, w0=74.70879160418104, w1=13.405888390948574\n",
      "[ 1.98945679  0.03164581]\n",
      "Gradient Descent(8053/9): loss=16.389540859007788, w0=73.31617185097475, w1=13.383736324395976\n",
      "[ 0.85325264  0.55895225]\n",
      "Gradient Descent(8054/9): loss=15.390741103619046, w0=72.71889500580838, w1=12.992469748340907\n",
      "[-3.32793431  0.6359515 ]\n",
      "Gradient Descent(8055/9): loss=15.669918609910514, w0=75.04844902384542, w1=12.547303697976751\n",
      "[ 2.25187475  0.2403931 ]\n",
      "Gradient Descent(8056/9): loss=17.35976343026616, w0=73.47213669769047, w1=12.37902852970294\n",
      "[-3.74671461 -0.44411444]\n",
      "Gradient Descent(8057/9): loss=16.007520637368618, w0=76.09483692496336, w1=12.689908640895043\n",
      "[-1.43651808 -1.00292068]\n",
      "Gradient Descent(8058/9): loss=19.620345087956842, w0=77.10039957999705, w1=13.391953117762736\n",
      "[ 0.47177996  2.99719944]\n",
      "Gradient Descent(8059/9): loss=22.63437449320614, w0=76.77015360582554, w1=11.293913506911547\n",
      "[ 0.65918978 -0.78072076]\n",
      "Gradient Descent(8060/9): loss=23.81683942717387, w0=76.30872075870474, w1=11.840418035499475\n",
      "[ 5.55275385 -2.70080828]\n",
      "Gradient Descent(8061/9): loss=21.274036704325436, w0=72.42179306600933, w1=13.730983829472542\n",
      "[ 0.88236448  0.7589805 ]\n",
      "Gradient Descent(8062/9): loss=15.797760966260087, w0=71.80413792807438, w1=13.199697477126811\n",
      "[-1.14848449 -1.22432567]\n",
      "Gradient Descent(8063/9): loss=16.53482035076061, w0=72.60807707287394, w1=14.056725446444652\n",
      "[-3.12536719  0.78641336]\n",
      "Gradient Descent(8064/9): loss=15.78755151000004, w0=74.79583410257901, w1=13.506236094925345\n",
      "[ 1.1212619  -0.05066229]\n",
      "Gradient Descent(8065/9): loss=16.514109599872445, w0=74.0109507727264, w1=13.541699697452783\n",
      "[-0.59031863  1.31504718]\n",
      "Gradient Descent(8066/9): loss=15.644874208132562, w0=74.42417381441464, w1=12.621166672860342\n",
      "[ 3.50509915  0.13452976]\n",
      "Gradient Descent(8067/9): loss=16.393172861278384, w0=71.97060441128629, w1=12.526995844296305\n",
      "[ 1.79275002 -1.54753395]\n",
      "Gradient Descent(8068/9): loss=16.71530704300537, w0=70.71567939656151, w1=13.61026961178764\n",
      "[-3.34311295 -1.04716112]\n",
      "Gradient Descent(8069/9): loss=18.718077923556535, w0=73.05585845810518, w1=14.343282396068302\n",
      "[-1.26006432  1.7523642 ]\n",
      "Gradient Descent(8070/9): loss=15.787101533159532, w0=73.93790348514737, w1=13.11662745741478\n",
      "[-0.02185389  1.12231343]\n",
      "Gradient Descent(8071/9): loss=15.65915929455006, w0=73.95320120891625, w1=12.331008057181027\n",
      "[ 0.99300961 -2.56908323]\n",
      "Gradient Descent(8072/9): loss=16.26297327889377, w0=73.25809448490612, w1=14.129366315805015\n",
      "[ 1.12410793  0.0272203 ]\n",
      "Gradient Descent(8073/9): loss=15.597554756753347, w0=72.47121893292817, w1=14.110312103378922\n",
      "[-2.9467131  -0.01716814]\n",
      "Gradient Descent(8074/9): loss=15.923136009732756, w0=74.53391810173785, w1=14.12232980133448\n",
      "[ 0.83863726 -0.42967618]\n",
      "Gradient Descent(8075/9): loss=16.36116157214588, w0=73.94687202063773, w1=14.423103128999783\n",
      "[-0.45074568 -0.07485824]\n",
      "Gradient Descent(8076/9): loss=16.044052732953247, w0=74.26239399913325, w1=14.475503894181376\n",
      "[ 0.32820297  3.58619308]\n",
      "Gradient Descent(8077/9): loss=16.350657188443353, w0=74.03265192183797, w1=11.965168739967057\n",
      "[ 4.61565298  1.52797046]\n",
      "Gradient Descent(8078/9): loss=16.805670118049033, w0=70.80169483367538, w1=10.895589421035224\n",
      "[-4.29774655  0.52934114]\n",
      "Gradient Descent(8079/9): loss=21.83033187398212, w0=73.81011741936003, w1=10.52505062103449\n",
      "[-1.86517217 -4.32744359]\n",
      "Gradient Descent(8080/9): loss=19.88412994064747, w0=75.11573793636873, w1=13.554261133516714\n",
      "[ 0.14355438  2.06886792]\n",
      "Gradient Descent(8081/9): loss=17.048173272223753, w0=75.01524987049572, w1=12.106053586907638\n",
      "[ 5.15762802 -3.10402873]\n",
      "Gradient Descent(8082/9): loss=17.810841999534524, w0=71.40491025750458, w1=14.27887369851556\n",
      "[-0.35769459  0.06411111]\n",
      "Gradient Descent(8083/9): loss=17.48939991700956, w0=71.65529647365727, w1=14.233995923797124\n",
      "[-2.74557657  1.65526988]\n",
      "Gradient Descent(8084/9): loss=17.012906470814247, w0=73.57720007414429, w1=13.075307009282579\n",
      "[ 0.34663045 -0.61193712]\n",
      "Gradient Descent(8085/9): loss=15.507782976048912, w0=73.33455875732089, w1=13.503662992311547\n",
      "[ 1.50999821 -1.27707644]\n",
      "Gradient Descent(8086/9): loss=15.387000356364661, w0=72.27756001016793, w1=14.397616498656665\n",
      "[-3.34918186 -0.55337896]\n",
      "Gradient Descent(8087/9): loss=16.323657653205498, w0=74.62198731360742, w1=14.784981772194467\n",
      "[ 1.36737142  4.61122664]\n",
      "Gradient Descent(8088/9): loss=17.11963062596149, w0=73.66482731901066, w1=11.557123125284653\n",
      "[-1.69044052 -1.12126025]\n",
      "Gradient Descent(8089/9): loss=17.302848072778584, w0=74.84813568443036, w1=12.34200529718532\n",
      "[ 0.15294515  0.26739255]\n",
      "Gradient Descent(8090/9): loss=17.24086671969756, w0=74.74107407917519, w1=12.154830515396844\n",
      "[ 1.24458249 -0.18620789]\n",
      "Gradient Descent(8091/9): loss=17.31066848634458, w0=73.86986633433347, w1=12.28517603963908\n",
      "[-0.87071272 -0.0617066 ]\n",
      "Gradient Descent(8092/9): loss=16.265202405650186, w0=74.47936523818368, w1=12.328370659496796\n",
      "[ 3.10096938 -3.85983955]\n",
      "Gradient Descent(8093/9): loss=16.751319643808344, w0=72.30868666903642, w1=15.030258342340412\n",
      "[ 1.38114478  1.44408144]\n",
      "Gradient Descent(8094/9): loss=17.073328504995, w0=71.34188532544144, w1=14.019401332790096\n",
      "[-0.92386806  0.84826818]\n",
      "Gradient Descent(8095/9): loss=17.436743515554486, w0=71.98859296589985, w1=13.425613604898256\n",
      "[-0.85905679 -0.89100073]\n",
      "Gradient Descent(8096/9): loss=16.23929315691837, w0=72.58993272190978, w1=14.049314118425738\n",
      "[-1.30911597  2.0936515 ]\n",
      "Gradient Descent(8097/9): loss=15.795911361031386, w0=73.50631390366178, w1=12.5837580660234\n",
      "[-0.5854605   0.23785774]\n",
      "Gradient Descent(8098/9): loss=15.80981014438713, w0=73.9161362565097, w1=12.41725764790818\n",
      "[ 2.72975696 -2.47113098]\n",
      "Gradient Descent(8099/9): loss=16.14386824531701, w0=72.00530638665182, w1=14.147049332014086\n",
      "[-0.58366384  1.52679163]\n",
      "Gradient Descent(8100/9): loss=16.438822238090033, w0=72.41387107708073, w1=13.078295194017528\n",
      "[-3.24342982 -0.27158875]\n",
      "Gradient Descent(8101/9): loss=15.853700584822201, w0=74.68427195362136, w1=13.268407322358746\n",
      "[ 2.93598787 -1.52141949]\n",
      "Gradient Descent(8102/9): loss=16.374749287981764, w0=72.6290804447274, w1=14.333400963147442\n",
      "[-3.48350013 -0.60189948]\n",
      "Gradient Descent(8103/9): loss=15.97128706859228, w0=75.06753053501389, w1=14.754730599364812\n",
      "[ 1.20721766 -1.13805939]\n",
      "Gradient Descent(8104/9): loss=17.77156714257674, w0=74.22247817186346, w1=15.551372169667587\n",
      "[-1.39618813  1.75843895]\n",
      "Gradient Descent(8105/9): loss=17.96288317717165, w0=75.19980986000793, w1=14.320464907780208\n",
      "[ 3.23634386 -0.53868529]\n",
      "Gradient Descent(8106/9): loss=17.55552449253217, w0=72.93436916024692, w1=14.697544608363394\n",
      "[ 2.24301444  1.22346339]\n",
      "Gradient Descent(8107/9): loss=16.192084593126424, w0=71.36425905494883, w1=13.84112023402377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.46191258 -0.18556383]\n",
      "Gradient Descent(8108/9): loss=17.312995212245042, w0=72.38759785781116, w1=13.97101491646985\n",
      "[-1.99981904 -1.6343847 ]\n",
      "Gradient Descent(8109/9): loss=15.917288660249149, w0=73.78747118256256, w1=15.115084209660273\n",
      "[ 1.1652559   1.63500671]\n",
      "Gradient Descent(8110/9): loss=16.844903686290124, w0=72.97179205554596, w1=13.9705795110374\n",
      "[-2.12003669  0.26751684]\n",
      "Gradient Descent(8111/9): loss=15.558246963238657, w0=74.45581773851963, w1=13.783317723660062\n",
      "[ 2.43943419 -1.08631162]\n",
      "Gradient Descent(8112/9): loss=16.10697680563293, w0=72.7482138081173, w1=14.543735854501925\n",
      "[ 0.83000086  2.29152685]\n",
      "Gradient Descent(8113/9): loss=16.1008595039581, w0=72.16721320541774, w1=12.939667057914134\n",
      "[-3.33726649 -1.3921839 ]\n",
      "Gradient Descent(8114/9): loss=16.166448729745838, w0=74.50329974908009, w1=13.914195785062125\n",
      "[ 1.0366427   1.20262925]\n",
      "Gradient Descent(8115/9): loss=16.2115730270138, w0=73.7776498590515, w1=13.072355312779473\n",
      "[ 1.75595087 -0.01902209]\n",
      "Gradient Descent(8116/9): loss=15.585854101129767, w0=72.54848425231758, w1=13.085670774206335\n",
      "[-1.46729133  0.09126276]\n",
      "Gradient Descent(8117/9): loss=15.741361003449807, w0=73.57558818281025, w1=13.021786842362182\n",
      "[ 1.02308963 -2.50248594]\n",
      "Gradient Descent(8118/9): loss=15.53040371169722, w0=72.85942544373616, w1=14.773527000229686\n",
      "[ 1.24068424  1.62091013]\n",
      "Gradient Descent(8119/9): loss=16.31725956306108, w0=71.99094647685489, w1=13.638889910913115\n",
      "[-2.93129473 -0.30693455]\n",
      "Gradient Descent(8120/9): loss=16.247429212950827, w0=74.04285278583225, w1=13.853744094120263\n",
      "[ 1.63097163 -0.89444551]\n",
      "Gradient Descent(8121/9): loss=15.73628636925264, w0=72.90117264149646, w1=14.47985595079339\n",
      "[ 2.0984626  -2.42140221]\n",
      "Gradient Descent(8122/9): loss=15.963157425061416, w0=71.43224882136386, w1=16.17483749773856\n",
      "[-2.9782805  2.6197713]\n",
      "Gradient Descent(8123/9): loss=20.750650936705465, w0=73.51704516815762, w1=14.340997585180327\n",
      "[-2.53839797  0.16813987]\n",
      "Gradient Descent(8124/9): loss=15.781685897414038, w0=75.2939237443659, w1=14.223299674048755\n",
      "[ 3.94087427 -0.95807657]\n",
      "Gradient Descent(8125/9): loss=17.66235234439855, w0=72.5353117554893, w1=14.893953273013134\n",
      "[ 1.32192716  1.27598429]\n",
      "Gradient Descent(8126/9): loss=16.673671195932243, w0=71.60996274051911, w1=14.000764270041742\n",
      "[-2.74829854  3.30469624]\n",
      "Gradient Descent(8127/9): loss=16.939494773576058, w0=73.53377171540116, w1=11.687476899976401\n",
      "[ 1.59886689 -5.59115033]\n",
      "Gradient Descent(8128/9): loss=17.020705917794515, w0=72.41456489398146, w1=15.601282130874024\n",
      "[-3.07059484  2.88982452]\n",
      "Gradient Descent(8129/9): loss=18.023051317882, w0=74.56398127948583, w1=13.57840496840268\n",
      "[ 1.53252245 -0.17486164]\n",
      "Gradient Descent(8130/9): loss=16.197283260935517, w0=73.49121556254012, w1=13.700808113655777\n",
      "[ 0.46256295 -0.37344238]\n",
      "Gradient Descent(8131/9): loss=15.429791892886497, w0=73.16742149939795, w1=13.962217782687063\n",
      "[ 0.95655021  1.39507634]\n",
      "Gradient Descent(8132/9): loss=15.510294762700584, w0=72.49783635515615, w1=12.985664345725835\n",
      "[-0.25307656  1.58375298]\n",
      "Gradient Descent(8133/9): loss=15.82480580472085, w0=72.67498994695846, w1=11.877037257319975\n",
      "[ 0.32718874 -0.42152923]\n",
      "Gradient Descent(8134/9): loss=16.861710175831774, w0=72.44595782605451, w1=12.17210771972472\n",
      "[-0.9848722 -1.524115 ]\n",
      "Gradient Descent(8135/9): loss=16.60032453645281, w0=73.13536836421596, w1=13.238988218512176\n",
      "[-1.72619557 -1.36178202]\n",
      "Gradient Descent(8136/9): loss=15.427431571072555, w0=74.343705260795, w1=14.192235630173101\n",
      "[-0.44391472  1.70618984]\n",
      "Gradient Descent(8137/9): loss=16.19075496577975, w0=74.65444556750639, w1=12.99790274160408\n",
      "[ 1.2942179  -0.47258062]\n",
      "Gradient Descent(8138/9): loss=16.427470345155253, w0=73.74849304040713, w1=13.328709173082522\n",
      "[-1.1449042  -0.64222286]\n",
      "Gradient Descent(8139/9): loss=15.500606275814055, w0=74.54992598126714, w1=13.778265178542508\n",
      "[-1.34131078 -0.72788546]\n",
      "Gradient Descent(8140/9): loss=16.21922773700637, w0=75.48884352626523, w1=14.287785003554358\n",
      "[ 3.98401527 -3.18646304]\n",
      "Gradient Descent(8141/9): loss=18.12121875547378, w0=72.70003283850346, w1=16.5183091309935\n",
      "[ 1.58073554 -0.00333782]\n",
      "Gradient Descent(8142/9): loss=20.17877497863577, w0=71.59351795814844, w1=16.52064560155312\n",
      "[-0.59648341  4.57440675]\n",
      "Gradient Descent(8143/9): loss=21.455212086936335, w0=72.01105634165917, w1=13.31856087832671\n",
      "[-0.01895506  2.8954346 ]\n",
      "Gradient Descent(8144/9): loss=16.221744932312546, w0=72.02432488616775, w1=11.291756661380564\n",
      "[-2.5662525  -3.99168419]\n",
      "Gradient Descent(8145/9): loss=18.585401520861083, w0=73.82070163590996, w1=14.085935594174263\n",
      "[-0.93889087 -0.01813672]\n",
      "Gradient Descent(8146/9): loss=15.708389519491393, w0=74.47792524668537, w1=14.098631297447408\n",
      "[ 2.98434232  3.16204875]\n",
      "Gradient Descent(8147/9): loss=16.278349989570977, w0=72.3888856201212, w1=11.885197175107994\n",
      "[-0.0742099  -0.61771926]\n",
      "Gradient Descent(8148/9): loss=17.066672752183504, w0=72.44083254850993, w1=12.31760066021221\n",
      "[-0.65001497 -0.1676296 ]\n",
      "Gradient Descent(8149/9): loss=16.425020565284616, w0=72.89584302798684, w1=12.43494137696765\n",
      "[ 0.61689907 -4.82930492]\n",
      "Gradient Descent(8150/9): loss=16.01089458548653, w0=72.46401367882538, w1=15.815454818626158\n",
      "[-0.35523689  0.67660078]\n",
      "Gradient Descent(8151/9): loss=18.458108022713336, w0=72.71267950343542, w1=15.341834276055472\n",
      "[ 0.49925468  0.51975996]\n",
      "Gradient Descent(8152/9): loss=17.28855816544764, w0=72.36320122594273, w1=14.97800230073293\n",
      "[-0.11061621  3.5053274 ]\n",
      "Gradient Descent(8153/9): loss=16.94144471131504, w0=72.44063257046615, w1=12.52427312043341\n",
      "[ 0.47418817 -3.75957452]\n",
      "Gradient Descent(8154/9): loss=16.20637143780211, w0=72.10870085157369, w1=15.155975284474845\n",
      "[ 0.58011733  1.7409906 ]\n",
      "Gradient Descent(8155/9): loss=17.493191026946132, w0=71.70261871834073, w1=13.937281866155923\n",
      "[ 0.46332687  2.58051346]\n",
      "Gradient Descent(8156/9): loss=16.756695831458366, w0=71.37828990700875, w1=12.130922442867666\n",
      "[-1.41415925 -0.76510554]\n",
      "Gradient Descent(8157/9): loss=18.130328252134586, w0=72.36820138163245, w1=12.666496322977977\n",
      "[-0.15219241 -1.32198375]\n",
      "Gradient Descent(8158/9): loss=16.145027424830822, w0=72.4747360675714, w1=13.591884949609977\n",
      "[-0.38186078  2.91865509]\n",
      "Gradient Descent(8159/9): loss=15.727712003016588, w0=72.74203861168141, w1=11.548826384922313\n",
      "[-1.78172978 -1.52626361]\n",
      "Gradient Descent(8160/9): loss=17.40233597631338, w0=73.98924945601624, w1=12.617210909092101\n",
      "[-0.87300056 -2.11047707]\n",
      "Gradient Descent(8161/9): loss=15.999582443997893, w0=74.60034984626867, w1=14.094544857656048\n",
      "[ 0.15328833  3.59170015]\n",
      "Gradient Descent(8162/9): loss=16.428274178813506, w0=74.49304801666041, w1=11.580354752666095\n",
      "[-1.26509719 -3.73710558]\n",
      "Gradient Descent(8163/9): loss=17.908619270920557, w0=75.37861604755686, w1=14.19632865825351\n",
      "[ 1.23358732  1.78918975]\n",
      "Gradient Descent(8164/9): loss=17.815631906123137, w0=74.51510492271773, w1=12.943895834000037\n",
      "[ 0.76087651 -0.25940564]\n",
      "Gradient Descent(8165/9): loss=16.27508144657499, w0=73.98249136552765, w1=13.125479782339294\n",
      "[ 0.55915839 -0.26695782]\n",
      "Gradient Descent(8166/9): loss=15.685692139053048, w0=73.59108049535723, w1=13.312350256995424\n",
      "[ 3.16368038 -2.25508184]\n",
      "Gradient Descent(8167/9): loss=15.444044503196695, w0=71.37650422631793, w1=14.890907547100696\n",
      "[-1.5941919  1.1287115]\n",
      "Gradient Descent(8168/9): loss=18.219869154505776, w0=72.49243855810724, w1=14.10080949971025\n",
      "[-0.41305145 -0.85687571]\n",
      "Gradient Descent(8169/9): loss=15.899956506233456, w0=72.78157457375292, w1=14.700622498044911\n",
      "[ 2.20518161 -2.28989195]\n",
      "Gradient Descent(8170/9): loss=16.26244850353453, w0=71.23794744995217, w1=16.303546860385573\n",
      "[-1.47913023  6.25791041]\n",
      "Gradient Descent(8171/9): loss=21.486423979407064, w0=72.27333860847207, w1=11.923009572054116\n",
      "[-0.59194156 -2.61053364]\n",
      "Gradient Descent(8172/9): loss=17.118345002244148, w0=72.68769770047707, w1=13.750383117292413\n",
      "[ 1.25032692 -0.32187247]\n",
      "Gradient Descent(8173/9): loss=15.606273129900933, w0=71.81246885805267, w1=13.97569384859561\n",
      "[-1.42908561 -0.28569568]\n",
      "Gradient Descent(8174/9): loss=16.606238359162536, w0=72.81282878245233, w1=14.175680827715047\n",
      "[ 3.13236364  2.65947378]\n",
      "Gradient Descent(8175/9): loss=15.74379921366418, w0=70.6201742333143, w1=12.314049178963467\n",
      "[-5.92985778  0.57380218]\n",
      "Gradient Descent(8176/9): loss=19.639736847610603, w0=74.77107467989929, w1=11.912387652358982\n",
      "[ 2.58996182 -0.43714644]\n",
      "Gradient Descent(8177/9): loss=17.70513137270973, w0=72.95810140786993, w1=12.218390161514698\n",
      "[ 1.57358805 -1.86273376]\n",
      "Gradient Descent(8178/9): loss=16.237742543366917, w0=71.85658976997281, w1=13.52230379191259\n",
      "[ 0.89876142 -1.24928224]\n",
      "Gradient Descent(8179/9): loss=16.41975685343501, w0=71.22745677718818, w1=14.396801357865337\n",
      "[-0.8459532   0.79211413]\n",
      "Gradient Descent(8180/9): loss=17.94155317795625, w0=71.8196240197425, w1=13.842321469890193\n",
      "[-2.53418751 -1.25198581]\n",
      "Gradient Descent(8181/9): loss=16.538407795324705, w0=73.59355527705759, w1=14.718711533963173\n",
      "[ 0.94038398 -0.78838098]\n",
      "Gradient Descent(8182/9): loss=16.198337302188097, w0=72.93528648971883, w1=15.270578216929216\n",
      "[-1.00243351  1.44576603]\n",
      "Gradient Descent(8183/9): loss=17.053797708663794, w0=73.63698994473671, w1=14.258541997899842\n",
      "[ 1.82170503  0.63405567]\n",
      "Gradient Descent(8184/9): loss=15.748023419492023, w0=72.3617964211454, w1=13.814703026281299\n",
      "[-1.31742638  0.33025793]\n",
      "Gradient Descent(8185/9): loss=15.876426266296379, w0=73.28399488599263, w1=13.583522472684496\n",
      "[-0.20933454 -0.37561395]\n",
      "Gradient Descent(8186/9): loss=15.39132540460972, w0=73.43052906294692, w1=13.846452235708867\n",
      "[-1.00152727 -1.30024479]\n",
      "Gradient Descent(8187/9): loss=15.462467654081316, w0=74.13159814850223, w1=14.756623588168786\n",
      "[ 2.70532009  0.73553355]\n",
      "Gradient Descent(8188/9): loss=16.551989578508106, w0=72.23787408273171, w1=14.241750103319855\n",
      "[ 0.28183621  1.93572057]\n",
      "Gradient Descent(8189/9): loss=16.233857176813455, w0=72.04058873886105, w1=12.88674570559309\n",
      "[ 0.50846648 -0.56870682]\n",
      "Gradient Descent(8190/9): loss=16.34711477429177, w0=71.68466220131248, w1=13.284840478462206\n",
      "[-0.38017962 -1.87541463]\n",
      "Gradient Descent(8191/9): loss=16.6997339617734, w0=71.95078793647029, w1=14.597630722368612\n",
      "[-0.32978889  1.24434422]\n",
      "Gradient Descent(8192/9): loss=16.912763076592697, w0=72.18164015784926, w1=13.726589765204325\n",
      "[-0.53830759 -1.0187803 ]\n",
      "Gradient Descent(8193/9): loss=16.034947527447198, w0=72.55845547026833, w1=14.439735974449146\n",
      "[ 0.62589985 -0.57481959]\n",
      "Gradient Descent(8194/9): loss=16.11716597671422, w0=72.12032557637592, w1=14.842109683992092\n",
      "[-2.21549415  2.3438894 ]\n",
      "Gradient Descent(8195/9): loss=17.00261528611719, w0=73.67117148331721, w1=13.201387103395755\n",
      "[-2.23859243 -1.49255624]\n",
      "Gradient Descent(8196/9): loss=15.495778949470026, w0=75.23818618332939, w1=14.24617647256998\n",
      "[ 3.95785992  2.57394674]\n",
      "Gradient Descent(8197/9): loss=17.569703032477534, w0=72.46768423915972, w1=12.444413753775383\n",
      "[-0.25425031 -3.58675216]\n",
      "Gradient Descent(8198/9): loss=16.26314396894934, w0=72.64565945342481, w1=14.955140268242491\n",
      "[-1.55091851  2.27585456]\n",
      "Gradient Descent(8199/9): loss=16.684453680409664, w0=73.73130240934951, w1=13.362042074058564\n",
      "[ 1.37106008  2.56895601]\n",
      "Gradient Descent(8200/9): loss=15.48846183607076, w0=72.77156035511628, w1=11.56377286406164\n",
      "[-0.39454595 -5.30189356]\n",
      "Gradient Descent(8201/9): loss=17.357730933674635, w0=73.047742517027, w1=15.27509835633639\n",
      "[-1.39467836  3.9939455 ]\n",
      "Gradient Descent(8202/9): loss=17.0278953415522, w0=74.02401736994359, w1=12.479336504890668\n",
      "[ 1.19797952 -0.26409172]\n",
      "Gradient Descent(8203/9): loss=16.15278349265905, w0=73.18543170882106, w1=12.664200706300281\n",
      "[-0.6215204  -0.23534532]\n",
      "Gradient Descent(8204/9): loss=15.724302630512312, w0=73.62049599221521, w1=12.828942433555948\n",
      "[ 1.22420025 -0.7893922 ]\n",
      "Gradient Descent(8205/9): loss=15.65096395172021, w0=72.76355581873295, w1=13.381516976825234\n",
      "[-2.45682908 -3.34209688]\n",
      "Gradient Descent(8206/9): loss=15.531353187063816, w0=74.4833361721739, w1=15.720984792613168\n",
      "[ 0.74386611 -1.86569155]\n",
      "Gradient Descent(8207/9): loss=18.60489179333451, w0=73.96262989412023, w1=17.026968878071152\n",
      "[-0.79311737  1.68164457]\n",
      "Gradient Descent(8208/9): loss=21.900987127744745, w0=74.5178120549864, w1=15.849817680678404\n",
      "[-1.05194138  3.21881477]\n",
      "Gradient Descent(8209/9): loss=18.943540737422275, w0=75.25417102154215, w1=13.596647339323212\n",
      "[ 3.89892184 -1.21935906]\n",
      "Gradient Descent(8210/9): loss=17.314012863857002, w0=72.52492573606868, w1=14.450198682836673\n",
      "[ 3.29027571 -0.13610403]\n",
      "Gradient Descent(8211/9): loss=16.152487276049126, w0=70.22173273559159, w1=14.545471504031523\n",
      "[-3.08973514  1.20169537]\n",
      "Gradient Descent(8212/9): loss=20.672982510093238, w0=72.38454733444428, w1=13.70428474424934\n",
      "[-0.63801729 -0.47836835]\n",
      "Gradient Descent(8213/9): loss=15.824585372964345, w0=72.8311594353708, w1=14.039142592602367\n",
      "[ 0.68163921 -1.80895645]\n",
      "Gradient Descent(8214/9): loss=15.649443516038337, w0=72.35401198507734, w1=15.305412105636695\n",
      "[-1.65489097 -0.55919463]\n",
      "Gradient Descent(8215/9): loss=17.494192932585516, w0=73.51243566178888, w1=15.696848343739807\n",
      "[ 3.34877521  1.84581478]\n",
      "Gradient Descent(8216/9): loss=17.867607797499605, w0=71.16829301374361, w1=14.404777997231225\n",
      "[-0.94350986  0.51474253]\n",
      "Gradient Descent(8217/9): loss=18.07291031413416, w0=71.82874991616815, w1=14.04445822385437\n",
      "[-2.17938214 -0.32631361]\n",
      "Gradient Descent(8218/9): loss=16.618721392554505, w0=73.35431741482377, w1=14.272877747454801\n",
      "[-1.52148485  1.11823573]\n",
      "Gradient Descent(8219/9): loss=15.70226727821757, w0=74.41935681004422, w1=13.49011273958131\n",
      "[ 1.68322778  0.89120695]\n",
      "Gradient Descent(8220/9): loss=16.019243705457587, w0=73.24109736751855, w1=12.866267876746866\n",
      "[-3.48515451 -2.28992301]\n",
      "Gradient Descent(8221/9): loss=15.57544020285748, w0=75.68070552281485, w1=14.469213980463492\n",
      "[ 4.86214607 -1.6439702 ]\n",
      "Gradient Descent(8222/9): loss=18.723812310443147, w0=72.27720327371344, w1=15.619993118827312\n",
      "[-1.24295614  2.84062186]\n",
      "Gradient Descent(8223/9): loss=18.193147057966264, w0=73.14727256836515, w1=13.631557815082353\n",
      "[-3.16292667  0.61057437]\n",
      "Gradient Descent(8224/9): loss=15.40816940676538, w0=75.36132124005907, w1=13.204155758266062\n",
      "[ 2.68504272  1.46606316]\n",
      "Gradient Descent(8225/9): loss=17.560923414418852, w0=73.48179133569282, w1=12.177911548260957\n",
      "[-1.95310004  0.72723648]\n",
      "Gradient Descent(8226/9): loss=16.250878086423754, w0=74.84896136253717, w1=11.668846014627857\n",
      "[ 0.04183135 -2.47245858]\n",
      "Gradient Descent(8227/9): loss=18.234580171271627, w0=74.81967941881128, w1=13.399567022037589\n",
      "[ 4.64698604 -0.76162223]\n",
      "Gradient Descent(8228/9): loss=16.553067359754795, w0=71.5667891877227, w1=13.932702583492855\n",
      "[-0.59392564 -0.21575005]\n",
      "Gradient Descent(8229/9): loss=16.979981785408533, w0=71.98253713238921, w1=14.08372761870311\n",
      "[ 0.28165154  1.86267542]\n",
      "Gradient Descent(8230/9): loss=16.42817017816796, w0=71.78538105555393, w1=12.779854826537177\n",
      "[-1.88096652  0.24085595]\n",
      "Gradient Descent(8231/9): loss=16.76863609859428, w0=73.10205762076117, w1=12.611255664841202\n",
      "[ 1.00535156 -1.33559438]\n",
      "Gradient Descent(8232/9): loss=15.781402420051474, w0=72.39831152884346, w1=13.546171728127788\n",
      "[-0.20958465  0.54864626]\n",
      "Gradient Descent(8233/9): loss=15.789155347559694, w0=72.54502078235416, w1=13.162119344127149\n",
      "[-0.03911559 -1.1339349 ]\n",
      "Gradient Descent(8234/9): loss=15.716747072983301, w0=72.57240169599964, w1=13.9558737766793\n",
      "[-1.49223552 -0.4328399 ]\n",
      "Gradient Descent(8235/9): loss=15.75954845655085, w0=73.61696655743971, w1=14.258861706731064\n",
      "[ 1.9412465  -0.83740167]\n",
      "Gradient Descent(8236/9): loss=15.741603555023097, w0=72.25809400741839, w1=14.845042875145763\n",
      "[-1.12512519  4.24797394]\n",
      "Gradient Descent(8237/9): loss=16.8544212915271, w0=73.04568163813757, w1=11.871461120401998\n",
      "[ 0.93542982 -0.09352812]\n",
      "Gradient Descent(8238/9): loss=16.709935653416277, w0=72.39088076117062, w1=11.936930804271705\n",
      "[ 0.18709863 -2.54683546]\n",
      "Gradient Descent(8239/9): loss=16.98371719028315, w0=72.25991172010933, w1=13.719715625624492\n",
      "[-1.20096477  2.17408997]\n",
      "Gradient Descent(8240/9): loss=15.949277266223575, w0=73.10058706010844, w1=12.197852643365419\n",
      "[-0.9983771  -0.53729631]\n",
      "Gradient Descent(8241/9): loss=16.226159331418533, w0=73.79945103169291, w1=12.573960060920266\n",
      "[ 2.23214431  0.70253265]\n",
      "Gradient Descent(8242/9): loss=15.923861350272968, w0=72.23695001540013, w1=12.082187206147731\n",
      "[-1.12417667 -0.54816627]\n",
      "Gradient Descent(8243/9): loss=16.921021141793013, w0=73.02387368282716, w1=12.465903597709287\n",
      "[-1.89410699  1.07409831]\n",
      "Gradient Descent(8244/9): loss=15.936255095475115, w0=74.34974857567953, w1=11.714034778154428\n",
      "[ 3.63629196 -0.29130952]\n",
      "Gradient Descent(8245/9): loss=17.502081539484767, w0=71.80434420365764, w1=11.917951439176228\n",
      "[-2.14155365 -2.84963841]\n",
      "Gradient Descent(8246/9): loss=17.714857581664454, w0=73.30343175670976, w1=13.912698323253649\n",
      "[-2.78049126  0.19447841]\n",
      "Gradient Descent(8247/9): loss=15.479671476263858, w0=75.24977563653393, w1=13.776563438524423\n",
      "[ 2.21128947  1.48212546]\n",
      "Gradient Descent(8248/9): loss=17.34262984763343, w0=73.70187300694936, w1=12.73907561763451\n",
      "[ 2.46939396 -2.13763536]\n",
      "Gradient Descent(8249/9): loss=15.743371327616613, w0=71.97329723295685, w1=14.235420369745073\n",
      "[-2.66737132 -1.83033991]\n",
      "Gradient Descent(8250/9): loss=16.54346000060006, w0=73.84045715703436, w1=15.516658310149483\n",
      "[ 1.35531933  3.59081338]\n",
      "Gradient Descent(8251/9): loss=17.609812455782684, w0=72.89173362915952, w1=13.003088941861341\n",
      "[-2.4647945   0.71336968]\n",
      "Gradient Descent(8252/9): loss=15.58035058959637, w0=74.61708977876862, w1=12.503730168479578\n",
      "[ 2.55631581 -1.06558823]\n",
      "Gradient Descent(8253/9): loss=16.737545043700102, w0=72.82766870919062, w1=13.2496419289952\n",
      "[-0.3729839   0.61110839]\n",
      "Gradient Descent(8254/9): loss=15.521050154270371, w0=73.08875743662348, w1=12.82186605644813\n",
      "[-1.14298936 -0.55512632]\n",
      "Gradient Descent(8255/9): loss=15.62331504717375, w0=73.88884998943436, w1=13.210454483254512\n",
      "[ 0.29706716 -0.12657691]\n",
      "Gradient Descent(8256/9): loss=15.59910744616931, w0=73.68090297723504, w1=13.29905832373147\n",
      "[ 0.43862629 -1.44259866]\n",
      "Gradient Descent(8257/9): loss=15.47708296034276, w0=73.37386457275505, w1=14.308877383904875\n",
      "[-0.60070572 -0.44576003]\n",
      "Gradient Descent(8258/9): loss=15.732840532385747, w0=73.79435857991172, w1=14.620909406040992\n",
      "[-0.27571998  3.74437718]\n",
      "Gradient Descent(8259/9): loss=16.16227151640182, w0=73.9873625640544, w1=11.99984537756223\n",
      "[ 3.74969107 -2.88789422]\n",
      "Gradient Descent(8260/9): loss=16.721321029136174, w0=71.36257881784482, w1=14.021371333471723\n",
      "[-2.76299302 -0.65626664]\n",
      "Gradient Descent(8261/9): loss=17.397628297676622, w0=73.2966739286869, w1=14.480757983425722\n",
      "[-1.00343329  2.39107111]\n",
      "Gradient Descent(8262/9): loss=15.886937750401795, w0=73.99907723453526, w1=12.807008209174434\n",
      "[ 2.11443216 -1.93334005]\n",
      "Gradient Descent(8263/9): loss=15.860775307455574, w0=72.51897472595974, w1=14.160346246728414\n",
      "[-1.79330539  2.82607924]\n",
      "Gradient Descent(8264/9): loss=15.917790702073457, w0=73.774288500841, w1=12.182090778439882\n",
      "[ 1.26996892  1.20988709]\n",
      "Gradient Descent(8265/9): loss=16.343174837155953, w0=72.88531025880481, w1=11.33516981548449\n",
      "[-1.98871607 -2.32080986]\n",
      "Gradient Descent(8266/9): loss=17.768901170646625, w0=74.27741150842341, w1=12.9597367169211\n",
      "[ 3.28281639  2.29810665]\n",
      "Gradient Descent(8267/9): loss=16.004701047038573, w0=71.97944003439738, w1=11.351062063669222\n",
      "[ 0.61161968 -3.58264852]\n",
      "Gradient Descent(8268/9): loss=18.515395492203908, w0=71.5513062563853, w1=13.858916027172501\n",
      "[-0.86566165  1.01818605]\n",
      "Gradient Descent(8269/9): loss=16.976140369607254, w0=72.15726941205645, w1=13.146185794145216\n",
      "[-3.23935909 -0.77420502]\n",
      "Gradient Descent(8270/9): loss=16.08749743413794, w0=74.4248207738965, w1=13.68812931074925\n",
      "[ 0.53608996 -0.6790454 ]\n",
      "Gradient Descent(8271/9): loss=16.047072681899767, w0=74.04955779977678, w1=14.163461091176467\n",
      "[-2.89459066 -3.0773362 ]\n",
      "Gradient Descent(8272/9): loss=15.905136710609838, w0=76.07577126272325, w1=16.31759642935806\n",
      "[ 3.01798156  4.86462838]\n",
      "Gradient Descent(8273/9): loss=23.282023305977923, w0=73.96318417281478, w1=12.912356563261952\n",
      "[ 0.9660491  -0.32478408]\n",
      "Gradient Descent(8274/9): loss=15.770790137992465, w0=73.28694980273886, w1=13.1397054224793\n",
      "[-1.86298982  0.67885073]\n",
      "Gradient Descent(8275/9): loss=15.443714558889303, w0=74.59104267532055, w1=12.664509912582812\n",
      "[ 0.8255194  0.2254787]\n",
      "Gradient Descent(8276/9): loss=16.559426465539484, w0=74.01317909810922, w1=12.50667481979135\n",
      "[-0.63956287 -1.15669101]\n",
      "Gradient Descent(8277/9): loss=16.117954354200286, w0=74.4608731055368, w1=13.3163585276755\n",
      "[ 2.88290847 -1.45514565]\n",
      "Gradient Descent(8278/9): loss=16.080117557246833, w0=72.44283717993176, w1=14.334960485976204\n",
      "[ 2.43706335  1.90922725]\n",
      "Gradient Descent(8279/9): loss=16.113785170455056, w0=70.73689283259, w1=12.998501411959005\n",
      "[-6.31189607  1.83503715]\n",
      "Gradient Descent(8280/9): loss=18.770868980047975, w0=75.15522008126327, w1=11.713975408169679\n",
      "[ 1.75047125 -1.03491601]\n",
      "Gradient Descent(8281/9): loss=18.677016762508632, w0=73.92989020638241, w1=12.438416615761655\n",
      "[ 4.84177538 -4.70169651]\n",
      "Gradient Descent(8282/9): loss=16.13026413882542, w0=70.5406474380495, w1=15.729604175142091\n",
      "[-4.91368673  2.751747  ]\n",
      "Gradient Descent(8283/9): loss=21.707154702571867, w0=73.98022814982657, w1=13.803381272969734\n",
      "[-1.06009622 -0.21772017]\n",
      "Gradient Descent(8284/9): loss=15.673776691369369, w0=74.72229550369345, w1=13.955785390928261\n",
      "[-0.45663004  0.18209122]\n",
      "Gradient Descent(8285/9): loss=16.5193360285375, w0=75.04193653065872, w1=13.828321536808948\n",
      "[ 0.68218483 -0.78200981]\n",
      "Gradient Descent(8286/9): loss=16.974429417782343, w0=74.56440714731816, w1=14.375728401699716\n",
      "[ 2.93791312  0.61882307]\n",
      "Gradient Descent(8287/9): loss=16.59437642723304, w0=72.50786796091704, w1=13.942552253772853\n",
      "[ 0.09578974 -0.06010748]\n",
      "Gradient Descent(8288/9): loss=15.801938695589424, w0=72.4408151454139, w1=13.984627492130569\n",
      "[-1.22138658  1.34125801]\n",
      "Gradient Descent(8289/9): loss=15.877253130760364, w0=73.29578574993582, w1=13.045746886504128\n",
      "[-1.20585738 -1.37430624]\n",
      "Gradient Descent(8290/9): loss=15.480052654243295, w0=74.13988591809438, w1=14.007761257840663\n",
      "[ 4.02832684 -0.89864319]\n",
      "Gradient Descent(8291/9): loss=15.88313312206477, w0=71.32005712927234, w1=14.63681149281041\n",
      "[-1.00638804  0.67588151]\n",
      "Gradient Descent(8292/9): loss=18.00339825173666, w0=72.02452875693355, w1=14.163694439106033\n",
      "[-3.14854211  2.74942992]\n",
      "Gradient Descent(8293/9): loss=16.425483165251038, w0=74.2285082349411, w1=12.23909349265934\n",
      "[ 0.0158886   4.52746363]\n",
      "Gradient Descent(8294/9): loss=16.59218126216626, w0=74.21738621308037, w1=9.069868955043658\n",
      "[ 2.36649258 -1.61835436]\n",
      "Gradient Descent(8295/9): loss=25.535640702113856, w0=72.56084140691435, w1=10.202717007553428\n",
      "[-0.52954569 -1.85905001]\n",
      "Gradient Descent(8296/9): loss=21.02394096406906, w0=72.93152338669522, w1=11.504052016874361\n",
      "[-0.64163463  0.14778274]\n",
      "Gradient Descent(8297/9): loss=17.403171290907483, w0=73.38066762705188, w1=11.400604096115014\n",
      "[ 0.23215046  0.34563617]\n",
      "Gradient Descent(8298/9): loss=17.55099601294087, w0=73.21816230670264, w1=11.158658775158937\n",
      "[ 2.04552005 -1.20402114]\n",
      "Gradient Descent(8299/9): loss=18.082402680458518, w0=71.78629827443605, w1=12.001473575248\n",
      "[ 1.26156706  1.06813882]\n",
      "Gradient Descent(8300/9): loss=17.614947584169155, w0=70.90320133313784, w1=11.253776401750265\n",
      "[-4.36643385 -2.50774252]\n",
      "Gradient Descent(8301/9): loss=20.72105613937864, w0=73.95970503069289, w1=13.009196163907665\n",
      "[-2.44843413 -0.29533451]\n",
      "Gradient Descent(8302/9): loss=15.71821417008327, w0=75.67360891923984, w1=13.21593032059938\n",
      "[ 1.70699549  0.81098333]\n",
      "Gradient Descent(8303/9): loss=18.25213328255625, w0=74.47871207919013, w1=12.648241986754085\n",
      "[-0.49092141 -1.11723656]\n",
      "Gradient Descent(8304/9): loss=16.4334231853529, w0=74.82235706877206, w1=13.430307578756931\n",
      "[ 1.97515918 -0.53025313]\n",
      "Gradient Descent(8305/9): loss=16.555165165247544, w0=73.43974564059631, w1=13.801484771150017\n",
      "[-2.77733759 -1.83028338]\n",
      "Gradient Descent(8306/9): loss=15.44828885376004, w0=75.38388195133116, w1=15.082683136174657\n",
      "[ 2.38174446 -0.91276336]\n",
      "Gradient Descent(8307/9): loss=18.85461169794345, w0=73.71666083042193, w1=15.721617487923739\n",
      "[-1.82552477  2.76675457]\n",
      "Gradient Descent(8308/9): loss=17.98831106049975, w0=74.99452816878623, w1=13.78488928727226\n",
      "[-0.39308538  3.72619994]\n",
      "Gradient Descent(8309/9): loss=16.87848499149094, w0=75.26968793764239, w1=11.176549329017455\n",
      "[ 2.32740649 -3.84425161]\n",
      "Gradient Descent(8310/9): loss=19.98999353119836, w0=73.64050339241133, w1=13.867525454404845\n",
      "[-3.27776059  1.81298356]\n",
      "Gradient Descent(8311/9): loss=15.521146667896868, w0=75.93493580643337, w1=12.598436961973142\n",
      "[ 3.7835861  -3.29297972]\n",
      "Gradient Descent(8312/9): loss=19.26168805582512, w0=73.28642553929697, w1=14.903522768232913\n",
      "[-0.88541735  2.45565269]\n",
      "Gradient Descent(8313/9): loss=16.399533899832722, w0=73.90621768416257, w1=13.184565887197493\n",
      "[ 1.16616764 -1.9144222 ]\n",
      "Gradient Descent(8314/9): loss=15.616896612299094, w0=73.08990033378834, w1=14.524661427429741\n",
      "[-3.55560807  6.41184638]\n",
      "Gradient Descent(8315/9): loss=15.952659487802206, w0=75.57882598607058, w1=10.036368959114462\n",
      "[ 1.94868686  0.25369678]\n",
      "Gradient Descent(8316/9): loss=23.924588123223913, w0=74.21474518319647, w1=9.858781215601413\n",
      "[ 0.56880074 -3.91195661]\n",
      "Gradient Descent(8317/9): loss=22.365416982014956, w0=73.81658466224525, w1=12.597150845217971\n",
      "[ 2.13059723 -0.86890605]\n",
      "Gradient Descent(8318/9): loss=15.911933476851368, w0=72.32516660466986, w1=13.20538508034322\n",
      "[ 1.88305433 -1.80687445]\n",
      "Gradient Descent(8319/9): loss=15.89275912761293, w0=71.00702857068028, w1=14.470197197339232\n",
      "[-0.67824276 -1.60970004]\n",
      "Gradient Descent(8320/9): loss=18.49135868440056, w0=71.48179850159865, w1=15.596987222503445\n",
      "[-1.13609505  2.58435691]\n",
      "Gradient Descent(8321/9): loss=19.269209922295516, w0=72.27706503414832, w1=13.787937382493407\n",
      "[-1.69346718 -1.62825977]\n",
      "Gradient Descent(8322/9): loss=15.950388224602651, w0=73.46249206017895, w1=14.927719218545288\n",
      "[-1.09209833  2.82295512]\n",
      "Gradient Descent(8323/9): loss=16.44845762368134, w0=74.22696089185065, w1=12.951650637679986\n",
      "[-1.3431146  1.9213857]\n",
      "Gradient Descent(8324/9): loss=15.960593284606764, w0=75.16714111354362, w1=11.606680644325985\n",
      "[ 3.24075702 -1.46956753]\n",
      "Gradient Descent(8325/9): loss=18.894486832975737, w0=72.89861120218197, w1=12.635377917529357\n",
      "[ 1.75429316 -1.05563379]\n",
      "Gradient Descent(8326/9): loss=15.820473571784314, w0=71.67060598708653, w1=13.374321572889079\n",
      "[-1.25164652  0.36657033]\n",
      "Gradient Descent(8327/9): loss=16.709018928044518, w0=72.54675855410551, w1=13.117722343528062\n",
      "[-1.04576768 -0.17389723]\n",
      "Gradient Descent(8328/9): loss=15.730532891000754, w0=73.27879593358188, w1=13.239450401143747\n",
      "[-0.88952979  2.27624867]\n",
      "Gradient Descent(8329/9): loss=15.414865190257627, w0=73.90146678953894, w1=11.646076331269523\n",
      "[ 0.21478138 -2.94986054]\n",
      "Gradient Descent(8330/9): loss=17.251553883630226, w0=73.75111982026192, w1=13.710978707321411\n",
      "[-0.07873586  0.0839404 ]\n",
      "Gradient Descent(8331/9): loss=15.517144835652292, w0=73.80623492010189, w1=13.6522204238742\n",
      "[-0.42438465  0.33752742]\n",
      "Gradient Descent(8332/9): loss=15.531999634917144, w0=74.10330417342136, w1=13.415951227045413\n",
      "[ 1.06074668  1.31043396]\n",
      "Gradient Descent(8333/9): loss=15.71547036427086, w0=73.36078149752339, w1=12.498647453350824\n",
      "[ 0.25166148  0.55037878]\n",
      "Gradient Descent(8334/9): loss=15.869367213991593, w0=73.18461846213299, w1=12.11338230476375\n",
      "[-1.64742708 -2.62843373]\n",
      "Gradient Descent(8335/9): loss=16.325290513135368, w0=74.33781741581093, w1=13.953285916394417\n",
      "[ 1.24447664  0.01612574]\n",
      "Gradient Descent(8336/9): loss=16.042882607352542, w0=73.46668376485596, w1=13.941997897203258\n",
      "[ 3.41385337  2.08176264]\n",
      "Gradient Descent(8337/9): loss=15.50766510645108, w0=71.07698640828917, w1=12.484764048170431\n",
      "[-3.47401405 -2.1153914 ]\n",
      "Gradient Descent(8338/9): loss=18.33825072861007, w0=73.50879624062165, w1=13.96553803005676\n",
      "[ 0.70793596 -0.00812528]\n",
      "Gradient Descent(8339/9): loss=15.526986592429862, w0=73.01324107027057, w1=13.971225722906299\n",
      "[-2.20455488  0.80812482]\n",
      "Gradient Descent(8340/9): loss=15.546071417676789, w0=74.55642948612359, w1=13.405538348266324\n",
      "[-0.25333566 -0.52302547]\n",
      "Gradient Descent(8341/9): loss=16.18560134000121, w0=74.73376445004698, w1=13.771656176764763\n",
      "[ 1.19817262  0.4630325 ]\n",
      "Gradient Descent(8342/9): loss=16.4650765804579, w0=73.89504361777902, w1=13.447533427801229\n",
      "[ 0.34425714  2.06895901]\n",
      "Gradient Descent(8343/9): loss=15.567079211496358, w0=73.65406361638824, w1=11.999262118973421\n",
      "[ 1.31620886  0.95128401]\n",
      "Gradient Descent(8344/9): loss=16.546605429093987, w0=72.73271741702922, w1=11.333363312815814\n",
      "[-1.65376362 -3.27845712]\n",
      "Gradient Descent(8345/9): loss=17.84677043911145, w0=73.8903519544494, w1=13.628283299972608\n",
      "[-2.9433851  -1.23572846]\n",
      "Gradient Descent(8346/9): loss=15.57478886381704, w0=75.95072152255241, w1=14.49329322172948\n",
      "[ 0.0289462   0.20355897]\n",
      "Gradient Descent(8347/9): loss=19.428852720378373, w0=75.93045918132302, w1=14.350801945347065\n",
      "[ 6.0012082  -1.87655642]\n",
      "Gradient Descent(8348/9): loss=19.24095048505623, w0=71.72961343790661, w1=15.664391442573935\n",
      "[-0.53855817  0.280015  ]\n",
      "Gradient Descent(8349/9): loss=18.995829693932958, w0=72.10660415440802, w1=15.468380944867949\n",
      "[-1.43619203  4.0701593 ]\n",
      "Gradient Descent(8350/9): loss=18.06815092565151, w0=73.11193857401145, w1=12.619269436073534\n",
      "[-3.6543666  -1.07462576]\n",
      "Gradient Descent(8351/9): loss=15.772627930071138, w0=75.66999519210242, w1=13.371507469548293\n",
      "[-0.10032502  1.05802935]\n",
      "Gradient Descent(8352/9): loss=18.214603928214224, w0=75.74022270389099, w1=12.630886923443528\n",
      "[ 0.69459159 -1.19921522]\n",
      "Gradient Descent(8353/9): loss=18.738333805133518, w0=75.25400858866095, w1=13.470337577463\n",
      "[ 3.11226871  0.55357784]\n",
      "Gradient Descent(8354/9): loss=17.306901526204125, w0=73.07542049386856, w1=13.082833089820921\n",
      "[ 1.06498872 -0.78121722]\n",
      "Gradient Descent(8355/9): loss=15.48851593069078, w0=72.32992838800946, w1=13.629685145756161\n",
      "[ 2.29040705 -3.78957632]\n",
      "Gradient Descent(8356/9): loss=15.861775619825488, w0=70.72664345526955, w1=16.282388568555636\n",
      "[ 1.87218432 -0.739059  ]\n",
      "Gradient Descent(8357/9): loss=22.608844192182534, w0=69.41611443411438, w1=16.7997298702178\n",
      "[-3.76417115  3.45224639]\n",
      "Gradient Descent(8358/9): loss=28.415841521124275, w0=72.05103424031132, w1=14.383157398852019\n",
      "[-1.76472644 -0.56788499]\n",
      "Gradient Descent(8359/9): loss=16.566379264402563, w0=73.28634274719144, w1=14.780676895225787\n",
      "[ 2.6073691  0.6163021]\n",
      "Gradient Descent(8360/9): loss=16.232170854781458, w0=71.46118437425102, w1=14.34926542414264\n",
      "[-1.77403171  2.56946503]\n",
      "Gradient Descent(8361/9): loss=17.443412675578642, w0=72.7030065720738, w1=12.550639900539034\n",
      "[-0.81552366  0.66329074]\n",
      "Gradient Descent(8362/9): loss=15.992066278688677, w0=73.27387313679289, w1=12.086336379722082\n",
      "[ 0.33747323 -2.80881845]\n",
      "Gradient Descent(8363/9): loss=16.35683726302522, w0=73.03764187656665, w1=14.052509295506903\n",
      "[ 0.62834422 -0.30143892]\n",
      "Gradient Descent(8364/9): loss=15.582775741911979, w0=72.5978009239072, w1=14.263516536653219\n",
      "[ 1.64055944  0.74261114]\n",
      "Gradient Descent(8365/9): loss=15.935354581477952, w0=71.44940931487787, w1=13.743688737585252\n",
      "[-2.72557259 -2.22156519]\n",
      "Gradient Descent(8366/9): loss=17.121843139666858, w0=73.35731013098328, w1=15.29878437181688\n",
      "[ 1.8096411   4.78825287]\n",
      "Gradient Descent(8367/9): loss=17.042408251948014, w0=72.09056136199519, w1=11.947007361645188\n",
      "[-3.61536051 -5.11687647]\n",
      "Gradient Descent(8368/9): loss=17.284518704839382, w0=74.62131371662551, w1=15.528820888875826\n",
      "[-0.91210777 -0.43766118]\n",
      "Gradient Descent(8369/9): loss=18.366294978613123, w0=75.25978915529939, w1=15.835183716991713\n",
      "[-1.9625271   1.18251642]\n",
      "Gradient Descent(8370/9): loss=20.092327181002958, w0=76.63355812823828, w1=15.0074222220247\n",
      "[ 1.83142573  0.19225263]\n",
      "Gradient Descent(8371/9): loss=22.129421193018263, w0=75.35156011690633, w1=14.87284537854026\n",
      "[ 2.61838514  2.73730311]\n",
      "Gradient Descent(8372/9): loss=18.47323487377442, w0=73.51869051864934, w1=12.956733201696284\n",
      "[-0.38491575 -3.26665393]\n",
      "Gradient Descent(8373/9): loss=15.54790195107187, w0=73.7881315421975, w1=15.24339095615142\n",
      "[-1.46522905 -0.70171799]\n",
      "Gradient Descent(8374/9): loss=17.06329036659327, w0=74.81379187532227, w1=15.734593547572647\n",
      "[ 4.02415472  8.46368338]\n",
      "Gradient Descent(8375/9): loss=19.083134500528963, w0=71.99688357158855, w1=9.810015184988924\n",
      "[ 0.99414414 -3.46201524]\n",
      "Gradient Descent(8376/9): loss=22.960381167277163, w0=71.3009826739109, w1=12.23342585119358\n",
      "[-1.84610324 -1.33651318]\n",
      "Gradient Descent(8377/9): loss=18.14840657623545, w0=72.59325493859725, w1=13.168985076454677\n",
      "[-0.26178629 -0.04129079]\n",
      "Gradient Descent(8378/9): loss=15.67963078144269, w0=72.77650533963187, w1=13.197888629145133\n",
      "[-2.53668577 -2.22864627]\n",
      "Gradient Descent(8379/9): loss=15.559460198902089, w0=74.55218537670679, w1=14.757941015542016\n",
      "[ 4.63283776  1.26416964]\n",
      "Gradient Descent(8380/9): loss=16.994435380832527, w0=71.30919894363687, w1=13.873022270153141\n",
      "[ 1.46584766  6.11418112]\n",
      "Gradient Descent(8381/9): loss=17.432796991455717, w0=70.28310557844212, w1=9.593095488904893\n",
      "[-0.68161035 -4.40520104]\n",
      "Gradient Descent(8382/9): loss=27.471291280123296, w0=70.76023282433502, w1=12.676736215955856\n",
      "[-0.91527472 -0.85725012]\n",
      "Gradient Descent(8383/9): loss=18.918063697770656, w0=71.4009251261828, w1=13.276811300834913\n",
      "[-2.78282018  0.42468441]\n",
      "Gradient Descent(8384/9): loss=17.19819089007588, w0=73.348899251046, w1=12.979532215518878\n",
      "[ 0.2834813   0.41211968]\n",
      "Gradient Descent(8385/9): loss=15.512489243754564, w0=73.15046233777193, w1=12.691048436650858\n",
      "[-0.94594406  0.66189186]\n",
      "Gradient Descent(8386/9): loss=15.707173657612095, w0=73.81262317915903, w1=12.227724132118258\n",
      "[-0.98219778 -2.37205505]\n",
      "Gradient Descent(8387/9): loss=16.304150679630563, w0=74.50016162682073, w1=13.888162667614683\n",
      "[ 1.28269662  1.4717261 ]\n",
      "Gradient Descent(8388/9): loss=16.196810681212305, w0=73.60227399348823, w1=12.857954394652898\n",
      "[-2.2829514  -1.26290948]\n",
      "Gradient Descent(8389/9): loss=15.626719874485666, w0=75.20033997321185, w1=13.741991032353198\n",
      "[ 1.20027489  3.40396854]\n",
      "Gradient Descent(8390/9): loss=17.237497640426277, w0=74.36014754753676, w1=11.359213055327762\n",
      "[ 4.1398579  -1.28206141]\n",
      "Gradient Descent(8391/9): loss=18.20256513526677, w0=71.46224701798977, w1=12.256656042603495\n",
      "[-5.89856051 -4.72822362]\n",
      "Gradient Descent(8392/9): loss=17.811337962024094, w0=75.59123937848867, w1=15.566412574078567\n",
      "[ 0.84451025  2.21675662]\n",
      "Gradient Descent(8393/9): loss=20.201880167984235, w0=75.00008220442517, w1=14.014682942045418\n",
      "[ 1.6648874   2.14543086]\n",
      "Gradient Descent(8394/9): loss=16.98447590852975, w0=73.83466102463133, w1=12.512881342429703\n",
      "[ 1.44613948 -1.3062893 ]\n",
      "Gradient Descent(8395/9): loss=15.999468394840413, w0=72.82236338923963, w1=13.427283855247504\n",
      "[-0.54481224  0.49821475]\n",
      "Gradient Descent(8396/9): loss=15.498446009500107, w0=73.2037319544474, w1=13.07853353081256\n",
      "[ 0.52752909 -3.26930906]\n",
      "Gradient Descent(8397/9): loss=15.47042724775578, w0=72.83446158803605, w1=15.367049873428906\n",
      "[ 0.00832138  0.14306953]\n",
      "Gradient Descent(8398/9): loss=17.272461108146054, w0=72.82863662402048, w1=15.266901201685727\n",
      "[-1.07086396  3.11535239]\n",
      "Gradient Descent(8399/9): loss=17.091154954262514, w0=73.57824139708305, w1=13.086154529192633\n",
      "[-2.30048618 -0.43651422]\n",
      "Gradient Descent(8400/9): loss=15.50375054061712, w0=75.18858172242868, w1=13.391714484945465\n",
      "[ 0.11757314  4.01867549]\n",
      "Gradient Descent(8401/9): loss=17.18462741634348, w0=75.1062805210285, w1=10.578641641869384\n",
      "[ 3.80801305 -2.45616173]\n",
      "Gradient Descent(8402/9): loss=21.23631544273233, w0=72.44067138281112, w1=12.297954856322006\n",
      "[ 1.15373236 -0.98849973]\n",
      "Gradient Descent(8403/9): loss=16.448181665860755, w0=71.63305873060476, w1=12.989904664735795\n",
      "[-0.40838401 -1.950115  ]\n",
      "Gradient Descent(8404/9): loss=16.885077098039186, w0=71.91892753462399, w1=14.35498516712671\n",
      "[ 0.13953812  2.59525038]\n",
      "Gradient Descent(8405/9): loss=16.714243939443218, w0=71.82125084979597, w1=12.5383099001674\n",
      "[-1.7453092  -6.04695176]\n",
      "Gradient Descent(8406/9): loss=16.91338739653559, w0=73.04296729246788, w1=16.771176129414915\n",
      "[-0.05425091  1.9337245 ]\n",
      "Gradient Descent(8407/9): loss=20.83424362783576, w0=73.08094293292291, w1=15.417568978768035\n",
      "[ 2.28011516  0.3224265 ]\n",
      "Gradient Descent(8408/9): loss=17.286211902917792, w0=71.48486231889802, w1=15.191870426764556\n",
      "[-0.20380733  1.47328133]\n",
      "Gradient Descent(8409/9): loss=18.487978831932576, w0=71.62752744711764, w1=14.160573494784565\n",
      "[-3.38033517 -0.69405058]\n",
      "Gradient Descent(8410/9): loss=17.00610916664841, w0=73.99376206300131, w1=14.646408899738743\n",
      "[ 0.12106838  1.78804794]\n",
      "Gradient Descent(8411/9): loss=16.311366244676663, w0=73.90901419691701, w1=13.394775341741822\n",
      "[-2.8824643   1.99044739]\n",
      "Gradient Descent(8412/9): loss=15.578664227793258, w0=75.92673920650876, w1=12.001462168436966\n",
      "[ 5.58370684 -5.21684888]\n",
      "Gradient Descent(8413/9): loss=19.944363010011763, w0=72.01814442109061, w1=15.653256387663026\n",
      "[-0.71260553  1.73865485]\n",
      "Gradient Descent(8414/9): loss=18.561838744041914, w0=72.5169682930706, w1=14.436197989171983\n",
      "[-1.33311162  1.95990972]\n",
      "Gradient Descent(8415/9): loss=16.14514870950102, w0=73.45014642630443, w1=13.064261182727801\n",
      "[-0.23544252  0.31419737]\n",
      "Gradient Descent(8416/9): loss=15.484390775690311, w0=73.6149561914689, w1=12.844323025382748\n",
      "[-0.81533058 -1.61497397]\n",
      "Gradient Descent(8417/9): loss=15.639279195119531, w0=74.18568759576576, w1=13.974804806613013\n",
      "[ 1.76181623  0.88381883]\n",
      "Gradient Descent(8418/9): loss=15.906069034067912, w0=72.95241623600381, w1=13.35613162215678\n",
      "[ 2.72772538  0.01870193]\n",
      "Gradient Descent(8419/9): loss=15.451837071619785, w0=71.04300846943711, w1=13.34304026931632\n",
      "[-4.44968495 -0.49922138]\n",
      "Gradient Descent(8420/9): loss=17.92853337503838, w0=74.15778793270253, w1=13.692495233666898\n",
      "[ 0.54448087  0.34970118]\n",
      "Gradient Descent(8421/9): loss=15.7816583015594, w0=73.77665132265768, w1=13.447704408592251\n",
      "[ 1.88409979  0.06648524]\n",
      "Gradient Descent(8422/9): loss=15.502913924166846, w0=72.45778147293915, w1=13.4011647435738\n",
      "[ 1.64686203 -1.05051106]\n",
      "Gradient Descent(8423/9): loss=15.738538230999769, w0=71.30497804912181, w1=14.136522484528241\n",
      "[-4.19344091  1.22037016]\n",
      "Gradient Descent(8424/9): loss=17.579536613471806, w0=74.24038668786315, w1=13.282263371823998\n",
      "[ 2.07333405 -2.79998901]\n",
      "Gradient Descent(8425/9): loss=15.853278635795236, w0=72.78905285515364, w1=15.242255677233356\n",
      "[-1.30869303  0.19981016]\n",
      "Gradient Descent(8426/9): loss=17.066613636991733, w0=73.70513797659278, w1=15.10238856688961\n",
      "[ 0.18256656  2.85219034]\n",
      "Gradient Descent(8427/9): loss=16.786976072186174, w0=73.57734138648523, w1=13.10585532541344\n",
      "[ 1.60141263 -0.57666148]\n",
      "Gradient Descent(8428/9): loss=15.495935711740696, w0=72.45635254857714, w1=13.509518359970444\n",
      "[ 0.76856684  2.76524326]\n",
      "Gradient Descent(8429/9): loss=15.737093360153036, w0=71.91835575828833, w1=11.57384807723122\n",
      "[-1.32247381 -0.85093946]\n",
      "Gradient Descent(8430/9): loss=18.14813858947944, w0=72.84408742812226, w1=12.16950570072736\n",
      "[-0.34370979 -3.24246135]\n",
      "Gradient Descent(8431/9): loss=16.345384284056937, w0=73.08468427826217, w1=14.43922864493521\n",
      "[ 0.45096954  1.19943624]\n",
      "Gradient Descent(8432/9): loss=15.868113759943625, w0=72.76900559942317, w1=13.5996232797813\n",
      "[ 0.6363646   0.59997807]\n",
      "Gradient Descent(8433/9): loss=15.530845789081111, w0=72.32355037839396, w1=13.179638632861378\n",
      "[-2.35143638  0.70778785]\n",
      "Gradient Descent(8434/9): loss=15.901720556243061, w0=73.9695558418341, w1=12.684187134944386\n",
      "[ 0.79767697 -3.41236291]\n",
      "Gradient Descent(8435/9): loss=15.930558663028386, w0=73.41118196106189, w1=15.07284117385995\n",
      "[-3.28976138  3.40977299]\n",
      "Gradient Descent(8436/9): loss=16.661792407124857, w0=75.71401492360317, w1=12.686000079105625\n",
      "[ 1.31538091 -0.14846853]\n",
      "Gradient Descent(8437/9): loss=18.62930239511273, w0=74.79324828992468, w1=12.789928051148834\n",
      "[ 4.78628465 -1.34065074]\n",
      "Gradient Descent(8438/9): loss=16.747778775597595, w0=71.44284903145454, w1=13.728383568035454\n",
      "[-2.34280885  2.67765545]\n",
      "Gradient Descent(8439/9): loss=17.1300421063714, w0=73.08281522510613, w1=11.854024751728303\n",
      "[-1.65520631 -3.20844073]\n",
      "Gradient Descent(8440/9): loss=16.729601126229706, w0=74.24145964033404, w1=14.09993326502919\n",
      "[ 1.64264904 -2.08469463]\n",
      "Gradient Descent(8441/9): loss=16.027138595767397, w0=73.09160531233829, w1=15.559219504280318\n",
      "[-1.02712608  0.41527256]\n",
      "Gradient Descent(8442/9): loss=17.568528715924703, w0=73.81059357067998, w1=15.268528712603832\n",
      "[ 1.07396224  0.63312636]\n",
      "Gradient Descent(8443/9): loss=17.11929446124597, w0=73.05881999955191, w1=14.825340264028862\n",
      "[ 3.061953    2.75949684]\n",
      "Gradient Descent(8444/9): loss=16.31888147177488, w0=70.91545290205667, w1=12.893692474413626\n",
      "[-3.16646637  1.78541479]\n",
      "Gradient Descent(8445/9): loss=18.38615519586861, w0=73.13197935911836, w1=11.643902123824429\n",
      "[ -8.75768186e-04  -1.07445448e+00]\n",
      "Gradient Descent(8446/9): loss=17.08410032792735, w0=73.13259239684827, w1=12.396020257795517\n",
      "[ 1.43110202 -1.37919906]\n",
      "Gradient Descent(8447/9): loss=15.986095857050806, w0=72.13082098490338, w1=13.361459597036164\n",
      "[-0.83505273 -0.93581862]\n",
      "Gradient Descent(8448/9): loss=16.069281723779298, w0=72.71535789620069, w1=14.016532632237348\n",
      "[-1.41086851  0.12447063]\n",
      "Gradient Descent(8449/9): loss=15.697344043236791, w0=73.70296585109027, w1=13.929403187927466\n",
      "[-0.50752845  1.3803379 ]\n",
      "Gradient Descent(8450/9): loss=15.570657190664825, w0=74.05823576651166, w1=12.963166659764605\n",
      "[ 2.56222209  0.14078175]\n",
      "Gradient Descent(8451/9): loss=15.811385403011105, w0=72.26468030460579, w1=12.864619432739664\n",
      "[ 0.18937136 -0.65676699]\n",
      "Gradient Descent(8452/9): loss=16.104726805473202, w0=72.13212035543971, w1=13.324356327466566\n",
      "[-1.60482359 -0.32345306]\n",
      "Gradient Descent(8453/9): loss=16.07284716199898, w0=73.2554968703283, w1=13.550773467026232\n",
      "[-0.48123826 -0.22190811]\n",
      "Gradient Descent(8454/9): loss=15.38915094934253, w0=73.59236365169595, w1=13.706109142570842\n",
      "[ 1.25031266 -0.19250249]\n",
      "Gradient Descent(8455/9): loss=15.456049312536564, w0=72.71714479083732, w1=13.840860884133447\n",
      "[-2.15376731  0.07763891]\n",
      "Gradient Descent(8456/9): loss=15.61743794570807, w0=74.2247819061352, w1=13.786513645849002\n",
      "[ 2.99874345  1.8492543 ]\n",
      "Gradient Descent(8457/9): loss=15.86620144078735, w0=72.12566149151569, w1=12.49203563355538\n",
      "[-1.39842134 -0.7214145 ]\n",
      "Gradient Descent(8458/9): loss=16.556056911175943, w0=73.10455643096303, w1=12.997025785142764\n",
      "[ 1.2490627  -1.61230709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8459/9): loss=15.520310729566312, w0=72.23021254148786, w1=14.125640749685473\n",
      "[ 1.64974623  3.09887776]\n",
      "Gradient Descent(8460/9): loss=16.160238470996088, w0=71.07539017952742, w1=11.956426318331614\n",
      "[ 1.02681039 -0.91732143]\n",
      "Gradient Descent(8461/9): loss=19.007029889325167, w0=70.35662290528221, w1=12.598551319336105\n",
      "[-2.04757444  0.36650094]\n",
      "Gradient Descent(8462/9): loss=20.087973316797317, w0=71.78992501344712, w1=12.342000660794957\n",
      "[ 0.04855193 -0.73736684]\n",
      "Gradient Descent(8463/9): loss=17.164085380345607, w0=71.75593866300096, w1=12.858157451536202\n",
      "[ 0.48131167 -1.86187123]\n",
      "Gradient Descent(8464/9): loss=16.76174954323803, w0=71.41902049106343, w1=14.16146731485387\n",
      "[-0.41233008  1.52548863]\n",
      "Gradient Descent(8465/9): loss=17.375910564992477, w0=71.70765154531612, w1=13.093625276932098\n",
      "[-3.38815721  0.23103923]\n",
      "Gradient Descent(8466/9): loss=16.71854649667854, w0=74.07936159394404, w1=12.931897817025497\n",
      "[ 2.13382979 -1.7366973 ]\n",
      "Gradient Descent(8467/9): loss=15.844395972870668, w0=72.58568074358436, w1=14.14758592705236\n",
      "[ 0.18897544 -0.70643768]\n",
      "Gradient Descent(8468/9): loss=15.859718209665402, w0=72.45339793524803, w1=14.642092303117868\n",
      "[-1.91477245  0.46783039]\n",
      "Gradient Descent(8469/9): loss=16.414691701228033, w0=73.7937386476833, w1=14.31461103350767\n",
      "[ 0.19793045 -0.22157416]\n",
      "Gradient Descent(8470/9): loss=15.859324043332057, w0=73.65518733444807, w1=14.469712942182623\n",
      "[-1.71634572 -0.81321126]\n",
      "Gradient Descent(8471/9): loss=15.941194691127574, w0=74.85662934012036, w1=15.03896082192554\n",
      "[ 0.86319792 -0.05938795]\n",
      "Gradient Descent(8472/9): loss=17.822542747054662, w0=74.25239079842629, w1=15.080532389573644\n",
      "[ 3.16853243  2.4598291 ]\n",
      "Gradient Descent(8473/9): loss=17.126531349088122, w0=72.0344180947462, w1=13.358652019784884\n",
      "[-0.60031885  0.13212682]\n",
      "Gradient Descent(8474/9): loss=16.18639072722038, w0=72.45464129022317, w1=13.26616324424267\n",
      "[ 2.27397384 -1.54913438]\n",
      "Gradient Descent(8475/9): loss=15.760885553932214, w0=70.86285960328219, w1=14.35055731199555\n",
      "[-3.44057756  1.77863729]\n",
      "Gradient Descent(8476/9): loss=18.72010546221916, w0=73.27126389468226, w1=13.105511211147638\n",
      "[ 1.9030888  -2.88484846]\n",
      "Gradient Descent(8477/9): loss=15.4561578417076, w0=71.93910173158888, w1=15.124905132567035\n",
      "[ 2.00098592  1.13178926]\n",
      "Gradient Descent(8478/9): loss=17.656986357612315, w0=70.53841158421241, w1=14.332652651272586\n",
      "[-2.54145161 -2.10767995]\n",
      "Gradient Descent(8479/9): loss=19.546060206664123, w0=72.31742770994735, w1=15.808028618824833\n",
      "[-4.71257256  0.70674738]\n",
      "Gradient Descent(8480/9): loss=18.573186546093634, w0=75.61622850255502, w1=15.313305450363933\n",
      "[ 4.12462872  8.1825694 ]\n",
      "Gradient Descent(8481/9): loss=19.76347328286094, w0=72.72898840176192, w1=9.585506868728874\n",
      "[-1.41532916 -5.32260092]\n",
      "Gradient Descent(8482/9): loss=23.127881351373667, w0=73.7197188115993, w1=13.311327511857932\n",
      "[ 0.4344205  -1.36858662]\n",
      "Gradient Descent(8483/9): loss=15.490716071486016, w0=73.4156244588694, w1=14.2693381443494\n",
      "[ 0.00841343  6.61092073]\n",
      "Gradient Descent(8484/9): loss=15.705047993262042, w0=73.40973505580618, w1=9.6416936333001\n",
      "[-1.82563169 -4.60587727]\n",
      "Gradient Descent(8485/9): loss=22.757788361592112, w0=74.68767723883808, w1=12.865807723986388\n",
      "[ 2.82623599 -1.07665785]\n",
      "Gradient Descent(8486/9): loss=16.545604195885204, w0=72.70931204468525, w1=13.619468216661751\n",
      "[-0.5401167   1.34974966]\n",
      "Gradient Descent(8487/9): loss=15.566538109242147, w0=73.08739373793811, w1=12.67464345800331\n",
      "[-3.09015782 -3.33447513]\n",
      "Gradient Descent(8488/9): loss=15.731282859631765, w0=75.25050421236305, w1=15.00877604567568\n",
      "[ 1.51685668  0.81819835]\n",
      "Gradient Descent(8489/9): loss=18.46901260434119, w0=74.18870453795031, w1=14.436037197441744\n",
      "[-2.05555955 -0.32674277]\n",
      "Gradient Descent(8490/9): loss=16.24348428769622, w0=75.6275962231271, w1=14.664757138482248\n",
      "[-0.41109937  2.1551821 ]\n",
      "Gradient Descent(8491/9): loss=18.811071028399144, w0=75.91536578533294, w1=13.156129667281634\n",
      "[ 3.48022926 -2.2909576 ]\n",
      "Gradient Descent(8492/9): loss=18.8742245269197, w0=73.47920530033959, w1=14.759799985692247\n",
      "[-1.2056639   3.22293815]\n",
      "Gradient Descent(8493/9): loss=16.222364887864366, w0=74.32317002798852, w1=12.50374328227208\n",
      "[ 0.40391566 -0.6549173 ]\n",
      "Gradient Descent(8494/9): loss=16.391821511749313, w0=74.04042906395912, w1=12.96218539345133\n",
      "[ 0.11594775 -0.09844178]\n",
      "Gradient Descent(8495/9): loss=15.798441384889689, w0=73.9592656423613, w1=13.03109464013855\n",
      "[-0.29263765  5.31743538]\n",
      "Gradient Descent(8496/9): loss=15.707857911572289, w0=74.16411199474231, w1=9.308889875202443\n",
      "[ 5.82406625 -6.84834165]\n",
      "Gradient Descent(8497/9): loss=24.462383593084752, w0=70.08726562289927, w1=14.102729033289085\n",
      "[-0.80976632  2.80796995]\n",
      "Gradient Descent(8498/9): loss=20.721285276859096, w0=70.6541020468518, w1=12.137150067874655\n",
      "[-1.12667047 -3.35744852]\n",
      "Gradient Descent(8499/9): loss=19.77144942170229, w0=71.44277137316082, w1=14.48736402976508\n",
      "[-2.14159094 -1.20085708]\n",
      "Gradient Descent(8500/9): loss=17.606948062577302, w0=72.94188502983698, w1=15.327963986067097\n",
      "[-3.27206417  3.52646564]\n",
      "Gradient Descent(8501/9): loss=17.15586978178249, w0=75.23232994621627, w1=12.859438037149403\n",
      "[ 2.2973897  0.1698689]\n",
      "Gradient Descent(8502/9): loss=17.45697071203354, w0=73.62415715757284, w1=12.740529805241149\n",
      "[-0.62060545 -1.28052916]\n",
      "Gradient Descent(8503/9): loss=15.713610977843285, w0=74.05858097211858, w1=13.636900215759146\n",
      "[-0.67043436 -0.51078781]\n",
      "Gradient Descent(8504/9): loss=15.690593538252081, w0=74.52788502322628, w1=13.99445168316045\n",
      "[-0.20830636  1.21622357]\n",
      "Gradient Descent(8505/9): loss=16.27969848438057, w0=74.67369947801416, w1=13.143095181641996\n",
      "[ 1.12353755  1.54732922]\n",
      "Gradient Descent(8506/9): loss=16.39443639796772, w0=73.88722319088852, w1=12.05996472797248\n",
      "[-1.46426811 -2.05839954]\n",
      "Gradient Descent(8507/9): loss=16.56973279492466, w0=74.91221086537145, w1=13.500844407666804\n",
      "[ 1.04114132  3.41478605]\n",
      "Gradient Descent(8508/9): loss=16.69554057144983, w0=74.18341194441287, w1=11.110494170791746\n",
      "[ 1.11814234 -2.41565369]\n",
      "Gradient Descent(8509/9): loss=18.5880816392657, w0=73.40071230567763, w1=12.801451753615327\n",
      "[-2.79952056 -1.74219639]\n",
      "Gradient Descent(8510/9): loss=15.621608729246718, w0=75.36037669959308, w1=14.020989229642327\n",
      "[ 1.69033581  0.33981733]\n",
      "Gradient Descent(8511/9): loss=17.66749566142934, w0=74.17714163016407, w1=13.783117101376561\n",
      "[-0.62988672 -0.94959726]\n",
      "Gradient Descent(8512/9): loss=15.821953520316494, w0=74.61806233446617, w1=14.447835181794218\n",
      "[ 3.93972361 -0.24659034]\n",
      "Gradient Descent(8513/9): loss=16.73119250516272, w0=71.8602558108034, w1=14.620448417252934\n",
      "[-0.99439913  3.83978285]\n",
      "Gradient Descent(8514/9): loss=17.064226533486078, w0=72.55633520288211, w1=11.932600419203709\n",
      "[ 1.55562963 -1.5355134 ]\n",
      "Gradient Descent(8515/9): loss=16.85468280671716, w0=71.46739446035781, w1=13.007459797169155\n",
      "[-0.64289905 -1.06910197]\n",
      "Gradient Descent(8516/9): loss=17.165500576174185, w0=71.91742379523428, w1=13.755831174446195\n",
      "[-4.30686695 -0.74881978]\n",
      "Gradient Descent(8517/9): loss=16.37138230472851, w0=74.93223066120527, w1=14.280005018468902\n",
      "[ 1.67727105  1.8613092 ]\n",
      "Gradient Descent(8518/9): loss=17.048149609656974, w0=73.75814092924773, w1=12.97708858180753\n",
      "[ 3.40572719 -0.97627426]\n",
      "Gradient Descent(8519/9): loss=15.619952843881604, w0=71.37413189389, w1=13.660480562614868\n",
      "[-0.94641393 -0.88837044]\n",
      "Gradient Descent(8520/9): loss=17.24502345661253, w0=72.03662164200287, w1=14.282339872256017\n",
      "[ 3.07998138  6.58740791]\n",
      "Gradient Descent(8521/9): loss=16.49839536811298, w0=69.88063467466993, w1=9.671154333421514\n",
      "[ 0.92170026 -3.15362465]\n",
      "Gradient Descent(8522/9): loss=28.46371046515229, w0=69.23544449045569, w1=11.878691586014511\n",
      "[-8.20915261 -3.71837205]\n",
      "Gradient Descent(8523/9): loss=24.90314160453732, w0=74.98185131669067, w1=14.481552019280718\n",
      "[ 2.02328109  1.7403922 ]\n",
      "Gradient Descent(8524/9): loss=17.31228183067475, w0=73.56555455394991, w1=13.26327748109425\n",
      "[ 3.48107002 -0.35666814]\n",
      "Gradient Descent(8525/9): loss=15.44620203507396, w0=71.1288055376465, w1=13.51294518009709\n",
      "[-3.53314267  1.93894847]\n",
      "Gradient Descent(8526/9): loss=17.73030472883816, w0=73.60200540509032, w1=12.155681251114476\n",
      "[-4.12884197 -0.38056194]\n",
      "Gradient Descent(8527/9): loss=16.309874848363002, w0=76.49219478134812, w1=12.422074610615363\n",
      "[ 4.52452585 -4.13131634]\n",
      "Gradient Descent(8528/9): loss=21.0596611378255, w0=73.32502668561143, w1=15.313996049686384\n",
      "[ 0.70099762  4.64198018]\n",
      "Gradient Descent(8529/9): loss=17.068669809070975, w0=72.83432835453299, w1=12.064609921826245\n",
      "[ 0.23155499 -2.26295893]\n",
      "Gradient Descent(8530/9): loss=16.492758590653605, w0=72.67223986100525, w1=13.64868117629258\n",
      "[ 1.50349942 -1.4821127 ]\n",
      "Gradient Descent(8531/9): loss=15.593407428879553, w0=71.61979026738456, w1=14.68616006438703\n",
      "[ 1.47876699  1.8051019 ]\n",
      "Gradient Descent(8532/9): loss=17.51500434266876, w0=70.58465337249136, w1=13.422588735572782\n",
      "[-2.45079781  1.06950892]\n",
      "Gradient Descent(8533/9): loss=19.0575876810517, w0=72.30021183756845, w1=12.673932494634752\n",
      "[-0.54613686 -1.61637556]\n",
      "Gradient Descent(8534/9): loss=16.2042584705199, w0=72.6825076395021, w1=13.805395383892208\n",
      "[-3.88046035 -1.27961515]\n",
      "Gradient Descent(8535/9): loss=15.625836321831198, w0=75.39882988341259, w1=14.701125990929382\n",
      "[ 2.06317221  1.41599394]\n",
      "Gradient Descent(8536/9): loss=18.347132000541805, w0=73.95460933719356, w1=13.70993023515046\n",
      "[ 0.92648496 -2.07485238]\n",
      "Gradient Descent(8537/9): loss=15.630641863958067, w0=73.30606986475021, w1=15.16232690050182\n",
      "[ 0.67033093  4.23264434]\n",
      "Gradient Descent(8538/9): loss=16.80155737388924, w0=72.83683821438859, w1=12.199475859677786\n",
      "[-2.55674778  3.11994987]\n",
      "Gradient Descent(8539/9): loss=16.30985350770843, w0=74.6265616590281, w1=10.015510951079932\n",
      "[-0.07259591 -3.78904378]\n",
      "Gradient Descent(8540/9): loss=22.274198056990276, w0=74.67737879621627, w1=12.667841599997539\n",
      "[ 0.37346245 -1.44156968]\n",
      "Gradient Descent(8541/9): loss=16.672431345770367, w0=74.41595508430382, w1=13.67694037919967\n",
      "[ 0.47284931  0.69492873]\n",
      "Gradient Descent(8542/9): loss=16.03481641859225, w0=74.0849605664695, w1=13.190490268911057\n",
      "[ 1.55820055  0.0815781 ]\n",
      "Gradient Descent(8543/9): loss=15.740583604660602, w0=72.99422017869783, w1=13.133385599347973\n",
      "[-0.87568905 -0.41194587]\n",
      "Gradient Descent(8544/9): loss=15.490769598848827, w0=73.6072025107226, w1=13.421747706441952\n",
      "[ 0.01805978 -0.69351825]\n",
      "Gradient Descent(8545/9): loss=15.43664016224696, w0=73.59456066304905, w1=13.907210483493614\n",
      "[-0.02209108 -3.10715835]\n",
      "Gradient Descent(8546/9): loss=15.522456961794067, w0=73.6100244167666, w1=16.082221326144357\n",
      "[ 3.24420827  4.61746268]\n",
      "Gradient Descent(8547/9): loss=18.822374501378007, w0=71.33907862996634, w1=12.849997453631866\n",
      "[ 0.88840303 -0.78933387]\n",
      "Gradient Descent(8548/9): loss=17.494864652499835, w0=70.71719650678021, w1=13.402531164822745\n",
      "[-2.49060963 -1.4621301 ]\n",
      "Gradient Descent(8549/9): loss=18.708623482190514, w0=72.46062324794117, w1=14.426022233365778\n",
      "[-0.43448501 -1.89193361]\n",
      "Gradient Descent(8550/9): loss=16.180832392926956, w0=72.76476275790374, w1=15.750375758307353\n",
      "[ 0.75782599  0.91361741]\n",
      "Gradient Descent(8551/9): loss=18.10384858562279, w0=72.23428456188347, w1=15.110843573294762\n",
      "[-3.36043166  0.93156061]\n",
      "Gradient Descent(8552/9): loss=17.277598016364465, w0=74.58658672248453, w1=14.458751142883113\n",
      "[ 2.07863857  1.8722604 ]\n",
      "Gradient Descent(8553/9): loss=16.700637304263534, w0=73.13153972239043, w1=13.148168864151396\n",
      "[-2.00592945  1.08004682]\n",
      "Gradient Descent(8554/9): loss=15.454032440893972, w0=74.5356903367795, w1=12.392136088255752\n",
      "[-1.34022048  0.04296073]\n",
      "Gradient Descent(8555/9): loss=16.748293322316172, w0=75.47384467092735, w1=12.3620635748885\n",
      "[ 1.71393102 -1.92584681]\n",
      "Gradient Descent(8556/9): loss=18.386488777093792, w0=74.27409295529223, w1=13.71015633981902\n",
      "[-0.04541238  0.05724087]\n",
      "Gradient Descent(8557/9): loss=15.892807614201843, w0=74.30588162037041, w1=13.670087728603376\n",
      "[ 4.6562998   0.50794443]\n",
      "Gradient Descent(8558/9): loss=15.916040379538519, w0=71.04647176346023, w1=13.314526625425609\n",
      "[-1.55383347 -0.97983932]\n",
      "Gradient Descent(8559/9): loss=17.925047332262604, w0=72.13415519338959, w1=14.000414151600891\n",
      "[-1.7317237   0.51127021]\n",
      "Gradient Descent(8560/9): loss=16.193982532969844, w0=73.34636178236299, w1=13.642525002554109\n",
      "[-1.98332306 -1.57011437]\n",
      "Gradient Descent(8561/9): loss=15.400516800184706, w0=74.73468792648612, w1=14.741605060586382\n",
      "[ 2.36168427  5.57074315]\n",
      "Gradient Descent(8562/9): loss=17.21997759252648, w0=73.08150893408019, w1=10.842084853982534\n",
      "[ 4.03985156 -0.45484683]\n",
      "Gradient Descent(8563/9): loss=18.886987152606434, w0=70.2536128393787, w1=11.16047763248306\n",
      "[-4.94583    -3.36646023]\n",
      "Gradient Descent(8564/9): loss=22.6970528058862, w0=73.71569384238855, w1=13.51699979638554\n",
      "[ 3.4634504  -1.24703913]\n",
      "Gradient Descent(8565/9): loss=15.47552878511736, w0=71.29127856566502, w1=14.389927188213539\n",
      "[-4.15215715  3.33764913]\n",
      "Gradient Descent(8566/9): loss=17.8054236840816, w0=74.19778857203876, w1=12.053572798600914\n",
      "[-2.19883468 -2.10725262]\n",
      "Gradient Descent(8567/9): loss=16.811312388189783, w0=75.7369728472824, w1=13.528649634889831\n",
      "[ 2.42347151  1.45976346]\n",
      "Gradient Descent(8568/9): loss=18.371334009657012, w0=74.04054278887256, w1=12.506815209602578\n",
      "[ 0.47874243 -2.31793388]\n",
      "Gradient Descent(8569/9): loss=16.137873674028313, w0=73.70542309092373, w1=14.129368924010048\n",
      "[ 2.42473419  2.50934946]\n",
      "Gradient Descent(8570/9): loss=15.681581218742366, w0=72.00810916020741, w1=12.372824300498305\n",
      "[-1.26838789 -0.19621713]\n",
      "Gradient Descent(8571/9): loss=16.825145872162217, w0=72.89598068292545, w1=12.510176292765237\n",
      "[ 2.07688503 -0.56296809]\n",
      "Gradient Descent(8572/9): loss=15.93506668112377, w0=71.44216115863911, w1=12.904253952914381\n",
      "[-4.77482179  0.76682978]\n",
      "Gradient Descent(8573/9): loss=17.265973211822345, w0=74.78453641044365, w1=12.367473109592657\n",
      "[ 1.10869852  0.78431615]\n",
      "Gradient Descent(8574/9): loss=17.11539168448162, w0=74.00844744823138, w1=11.818451806138697\n",
      "[ 0.89102582 -1.93024882]\n",
      "Gradient Descent(8575/9): loss=17.02105461389445, w0=73.38472937410603, w1=13.16962597954097\n",
      "[-0.55107509  0.71685585]\n",
      "Gradient Descent(8576/9): loss=15.438087663160427, w0=73.77048193513725, w1=12.667826887308882\n",
      "[ 4.23271407 -0.79511421]\n",
      "Gradient Descent(8577/9): loss=15.829021624981124, w0=70.80758208473547, w1=13.224406831995008\n",
      "[-0.24707642  1.7568722 ]\n",
      "Gradient Descent(8578/9): loss=18.5094214366425, w0=70.98053557791654, w1=11.994596295380344\n",
      "[-2.51778836 -0.90554285]\n",
      "Gradient Descent(8579/9): loss=19.164551216702694, w0=72.74298743271338, w1=12.628476290223716\n",
      "[-2.60193303 -0.54534325]\n",
      "Gradient Descent(8580/9): loss=15.899953805782344, w0=74.56434055197712, w1=13.010216563888717\n",
      "[-2.7411831  -1.25490085]\n",
      "Gradient Descent(8581/9): loss=16.303082701248886, w0=76.48316872033435, w1=13.888647160753269\n",
      "[ 2.77856198  1.44458046]\n",
      "Gradient Descent(8582/9): loss=20.55514898866506, w0=74.53817533196474, w1=12.87744084205143\n",
      "[-0.43925453 -1.71950415]\n",
      "Gradient Descent(8583/9): loss=16.341336579092495, w0=74.84565350058122, w1=14.081093749371004\n",
      "[ 3.77849722 -1.37455892]\n",
      "Gradient Descent(8584/9): loss=16.77065293315462, w0=72.20070544524775, w1=15.043284995510115\n",
      "[-0.93191851  1.23965641]\n",
      "Gradient Descent(8585/9): loss=17.205828664930223, w0=72.85304840512238, w1=14.175525506522517\n",
      "[-4.2429142   0.54768107]\n",
      "Gradient Descent(8586/9): loss=15.725150548346098, w0=75.82308834329075, w1=13.792148754710658\n",
      "[ 1.9020828  1.5099941]\n",
      "Gradient Descent(8587/9): loss=18.633037286462965, w0=74.49163038190878, w1=12.735152881300282\n",
      "[ 2.44979941 -0.39828482]\n",
      "Gradient Descent(8588/9): loss=16.380325014849877, w0=72.7767707958151, w1=13.013952253562566\n",
      "[-1.48962736  0.31515541]\n",
      "Gradient Descent(8589/9): loss=15.628076827214262, w0=73.81950994478622, w1=12.793343466908041\n",
      "[ 4.71773902 -0.74608045]\n",
      "Gradient Descent(8590/9): loss=15.759560391747533, w0=70.51709263097703, w1=13.315599781596942\n",
      "[-2.09991661  2.99360048]\n",
      "Gradient Descent(8591/9): loss=19.254745028511095, w0=71.9870342608081, w1=11.220079447160744\n",
      "[-0.08330104 -2.98954815]\n",
      "Gradient Descent(8592/9): loss=18.792836272846635, w0=72.04534498943462, w1=13.31276315019051\n",
      "[-1.71431212 -0.22139065]\n",
      "Gradient Descent(8593/9): loss=16.179296178961447, w0=73.24536347340546, w1=13.46773660848862\n",
      "[-0.33879539  1.44032422]\n",
      "Gradient Descent(8594/9): loss=15.387138544394325, w0=73.48252024314554, w1=12.459509655608095\n",
      "[ 0.6166075  -1.98578644]\n",
      "Gradient Descent(8595/9): loss=15.92407937261947, w0=73.05089499394478, w1=13.84956016509967\n",
      "[-0.20367338 -0.57702672]\n",
      "Gradient Descent(8596/9): loss=15.483812603911089, w0=73.19346635748899, w1=14.253478866740902\n",
      "[ 2.76551205  1.98534735]\n",
      "Gradient Descent(8597/9): loss=15.690290782550024, w0=71.25760792211663, w1=12.863735724231299\n",
      "[-1.36123477  0.90251568]\n",
      "Gradient Descent(8598/9): loss=17.648889039107186, w0=72.21047226095367, w1=12.231974744835048\n",
      "[ 0.13018097  1.85703373]\n",
      "Gradient Descent(8599/9): loss=16.751244211345472, w0=72.11934557875163, w1=10.93205113342342\n",
      "[-2.49909287 -6.433805  ]\n",
      "Gradient Descent(8600/9): loss=19.320991809725946, w0=73.86871058716365, w1=15.435714631967809\n",
      "[-1.01262405  2.87493723]\n",
      "Gradient Descent(8601/9): loss=17.46405112487902, w0=74.57754741946198, w1=13.42325856938615\n",
      "[ 2.04251442 -0.48431874]\n",
      "Gradient Descent(8602/9): loss=16.211328494342354, w0=73.14778732421856, w1=13.762281688163819\n",
      "[-0.63117502  0.75279868]\n",
      "Gradient Descent(8603/9): loss=15.436488232289788, w0=73.58960983976179, w1=13.23532261142189\n",
      "[ 0.54053098 -1.56464363]\n",
      "Gradient Descent(8604/9): loss=15.459466710430014, w0=73.21123815577292, w1=14.330573154158609\n",
      "[-1.05847776  3.71763608]\n",
      "Gradient Descent(8605/9): loss=15.75128815976442, w0=73.95217258451063, w1=11.728227896504468\n",
      "[ 0.38789674 -0.47818127]\n",
      "Gradient Descent(8606/9): loss=17.136383827723233, w0=73.6806448647585, w1=12.062954787071073\n",
      "[-1.85913739 -6.35069065]\n",
      "Gradient Descent(8607/9): loss=16.464266271545828, w0=74.98204103577267, w1=16.50843824546758\n",
      "[ 3.60807361  1.72962909]\n",
      "Gradient Descent(8608/9): loss=21.397350822274024, w0=72.45638950621576, w1=15.297697881010167\n",
      "[ 1.3739238  -0.32409599]\n",
      "Gradient Descent(8609/9): loss=17.389153750637096, w0=71.49464284361846, w1=15.524565077070875\n",
      "[-1.2506045   2.91682441]\n",
      "Gradient Descent(8610/9): loss=19.09530177782627, w0=72.3700659961771, w1=13.482787992868708\n",
      "[-4.34406118 -0.5046475 ]\n",
      "Gradient Descent(8611/9): loss=15.812647558202233, w0=75.41090881872213, w1=13.836041244832877\n",
      "[ 2.30523637  2.69556275]\n",
      "Gradient Descent(8612/9): loss=17.690189570056734, w0=73.79724335793433, w1=11.949147316468364\n",
      "[ 0.31297343  1.21565165]\n",
      "Gradient Descent(8613/9): loss=16.68386885346236, w0=73.57816195659912, w1=11.098191158981559\n",
      "[ 1.14142003 -3.75574427]\n",
      "Gradient Descent(8614/9): loss=18.26210583873293, w0=72.77916793822797, w1=13.727212151007357\n",
      "[-3.35822268  1.27469699]\n",
      "Gradient Descent(8615/9): loss=15.54900179668303, w0=75.12992381596133, w1=12.834924260071684\n",
      "[-0.24406121  1.62958782]\n",
      "Gradient Descent(8616/9): loss=17.27921509432756, w0=75.30076666370252, w1=11.694212784916719\n",
      "[-0.48610799 -1.42743967]\n",
      "Gradient Descent(8617/9): loss=18.993605116924456, w0=75.64104225511615, w1=12.693420555631494\n",
      "[ 0.02523394 -0.44904796]\n",
      "Gradient Descent(8618/9): loss=18.449502069648332, w0=75.62337849453932, w1=13.007754127298819\n",
      "[-0.59312057  0.44488775]\n",
      "Gradient Descent(8619/9): loss=18.210443966000085, w0=76.03856289010916, w1=12.696332698944724\n",
      "[ 3.59965246  2.07297093]\n",
      "Gradient Descent(8620/9): loss=19.45925657630346, w0=73.51880617020684, w1=11.245253047167758\n",
      "[ 1.27160296 -1.51617154]\n",
      "Gradient Descent(8621/9): loss=17.90757869127214, w0=72.62868409634515, w1=12.30657312230857\n",
      "[ 0.26805054 -3.65708862]\n",
      "Gradient Descent(8622/9): loss=16.29528652793761, w0=72.44104871904739, w1=14.866535159497262\n",
      "[ 2.37160727 -0.28249914]\n",
      "Gradient Descent(8623/9): loss=16.71122292191249, w0=70.78092362697247, w1=15.064284556769612\n",
      "[-4.24553762  1.53622872]\n",
      "Gradient Descent(8624/9): loss=19.79890269010134, w0=73.75279995856356, w1=13.988924450596441\n",
      "[ 1.17577709  0.46422459]\n",
      "Gradient Descent(8625/9): loss=15.620820796710575, w0=72.9297559928697, w1=13.66396723881697\n",
      "[-2.16379504 -1.68903927]\n",
      "Gradient Descent(8626/9): loss=15.469171226337481, w0=74.44441252362023, w1=14.846294730760317\n",
      "[-0.04136589 -0.35443607]\n",
      "Gradient Descent(8627/9): loss=16.981475674435107, w0=74.47336864522556, w1=15.09439997867116\n",
      "[-0.97469667  1.64064385]\n",
      "Gradient Descent(8628/9): loss=17.385042992674443, w0=75.15565631674465, w1=13.945949281801317\n",
      "[-0.07785125  0.79164124]\n",
      "Gradient Descent(8629/9): loss=17.227603596645164, w0=75.21015219088541, w1=13.391800411475527\n",
      "[-0.09763851  1.03161401]\n",
      "Gradient Descent(8630/9): loss=17.22572119896486, w0=75.27849915004542, w1=12.669670606270957\n",
      "[-1.98403161  1.76239694]\n",
      "Gradient Descent(8631/9): loss=17.68324497902917, w0=76.66732127729728, w1=11.435992751764571\n",
      "[ 3.74958605  0.76453207]\n",
      "Gradient Descent(8632/9): loss=23.164194275562235, w0=74.04261104310956, w1=10.900820303063975\n",
      "[-4.86608881 -5.01457341]\n",
      "Gradient Descent(8633/9): loss=18.991497822941948, w0=77.44887320851377, w1=14.41102169002135\n",
      "[ 7.74525269  1.83270769]\n",
      "Gradient Descent(8634/9): loss=24.451366096901882, w0=72.02719632540418, w1=13.128126305770557\n",
      "[-0.9157369  -0.82568743]\n",
      "Gradient Descent(8635/9): loss=16.249991241965642, w0=72.66821215301877, w1=13.706107505927083\n",
      "[-0.69569723 -0.13749521]\n",
      "Gradient Descent(8636/9): loss=15.607271640523791, w0=73.15520021118435, w1=13.802354153476792\n",
      "[ 0.26393119  0.02907892]\n",
      "Gradient Descent(8637/9): loss=15.447558575721906, w0=72.97044837819801, w1=13.781998911244656\n",
      "[ 1.36530429  1.35642632]\n",
      "Gradient Descent(8638/9): loss=15.483894018374736, w0=72.01473537757698, w1=12.832500490438402\n",
      "[-0.12398194  0.76450095]\n",
      "Gradient Descent(8639/9): loss=16.413488729599845, w0=72.10152273732551, w1=12.29734982659483\n",
      "[-2.13967013 -1.3799105 ]\n",
      "Gradient Descent(8640/9): loss=16.795786541017346, w0=73.59929183142799, w1=13.263287179054137\n",
      "[-1.46254866  1.14461299]\n",
      "Gradient Descent(8641/9): loss=15.455933180862962, w0=74.62307589007993, w1=12.462058087417345\n",
      "[ 1.07777488 -2.20900751]\n",
      "Gradient Descent(8642/9): loss=16.78702308335438, w0=73.86863347266988, w1=14.00836334498132\n",
      "[-0.87946065  2.17721668]\n",
      "Gradient Descent(8643/9): loss=15.690770398346544, w0=74.48425592943956, w1=12.484311666243473\n",
      "[ 1.14032635 -0.61211637]\n",
      "Gradient Descent(8644/9): loss=16.589746643320673, w0=73.68602748482522, w1=12.912793123605022\n",
      "[-2.94921355  1.2483902 ]\n",
      "Gradient Descent(8645/9): loss=15.623459976429022, w0=75.75047697144764, w1=12.038919987099115\n",
      "[ 1.88228832 -3.25816378]\n",
      "Gradient Descent(8646/9): loss=19.441160466478284, w0=74.43287514744372, w1=14.319634633113576\n",
      "[ 1.72145528  3.57711735]\n",
      "Gradient Descent(8647/9): loss=16.387229651918837, w0=73.22785645008157, w1=11.815652487392715\n",
      "[-1.69248072  0.707458  ]\n",
      "Gradient Descent(8648/9): loss=16.77261795200865, w0=74.4125929551155, w1=11.320431890604349\n",
      "[ 1.07760786 -2.22769485]\n",
      "Gradient Descent(8649/9): loss=18.342846454063043, w0=73.65826745206972, w1=12.879818283115082\n",
      "[-0.4681403 -0.1533167]\n",
      "Gradient Descent(8650/9): loss=15.632198169010621, w0=73.98596566291542, w1=12.98713997594447\n",
      "[-1.46821345  0.35769463]\n",
      "Gradient Descent(8651/9): loss=15.746663896767824, w0=75.01371507880398, w1=12.736753733019286\n",
      "[ 2.27789548 -1.26465269]\n",
      "Gradient Descent(8652/9): loss=17.14072579857625, w0=73.41918824095235, w1=13.622010616280198\n",
      "[ 1.17591841 -0.7508363 ]\n",
      "Gradient Descent(8653/9): loss=15.403858070326244, w0=72.5960453516021, w1=14.147596029111451\n",
      "[ 0.16892779  1.60892626]\n",
      "Gradient Descent(8654/9): loss=15.852438026137033, w0=72.47779590122941, w1=13.021347649458074\n",
      "[-1.75329032 -0.99483649]\n",
      "Gradient Descent(8655/9): loss=15.823967913402177, w0=73.70509912389113, w1=13.717733192780113\n",
      "[-2.0122744  0.9750271]\n",
      "Gradient Descent(8656/9): loss=15.498748122139203, w0=75.11369120176194, w1=13.03521422250353\n",
      "[ 3.01400479  0.46637659]\n",
      "Gradient Descent(8657/9): loss=17.140457169290492, w0=73.00388785113122, w1=12.708750612673516\n",
      "[ 3.71731169 -0.77167614]\n",
      "Gradient Descent(8658/9): loss=15.725138838929038, w0=70.40176966637446, w1=13.248923913871748\n",
      "[-5.14101155 -1.62497298]\n",
      "Gradient Descent(8659/9): loss=19.5947921061055, w0=74.00047775070797, w1=14.386405002795243\n",
      "[ 4.29863564 -0.12196078]\n",
      "Gradient Descent(8660/9): loss=16.04654408802871, w0=70.99143280115355, w1=14.471777547297686\n",
      "[-1.73065348  0.75222387]\n",
      "Gradient Descent(8661/9): loss=18.528712722608837, w0=72.20289023554676, w1=13.945220840348103\n",
      "[-1.19331784  1.51851391]\n",
      "Gradient Descent(8662/9): loss=16.089412064379168, w0=73.03821272474809, w1=12.882261105840264\n",
      "[-1.66278439 -0.57072981]\n",
      "Gradient Descent(8663/9): loss=15.59705553144347, w0=74.20216179610043, w1=13.281771970671103\n",
      "[ 1.16170568 -1.52862778]\n",
      "Gradient Descent(8664/9): loss=15.817927844234859, w0=73.38896781688018, w1=14.351811419997754\n",
      "[ 0.35008501  2.31618974]\n",
      "Gradient Descent(8665/9): loss=15.770683042109129, w0=73.14390831048655, w1=12.730478604950187\n",
      "[ 3.36248917  0.17008747]\n",
      "Gradient Descent(8666/9): loss=15.677815588703277, w0=70.79016588819105, w1=12.611417372826121\n",
      "[-1.37810298 -2.73401169]\n",
      "Gradient Descent(8667/9): loss=18.897253365298862, w0=71.75483797371692, w1=14.525225553014618\n",
      "[-2.25241144  2.33983421]\n",
      "Gradient Descent(8668/9): loss=17.116826532031112, w0=73.33152598502046, w1=12.887341608393292\n",
      "[-2.61711955  1.02692123]\n",
      "Gradient Descent(8669/9): loss=15.562046496695816, w0=75.16350966690933, w1=12.168496750311837\n",
      "[ 1.64576382 -2.61103035]\n",
      "Gradient Descent(8670/9): loss=17.993210172895076, w0=74.01147499257198, w1=13.996217995838817\n",
      "[ 4.88749031  2.58677711]\n",
      "Gradient Descent(8671/9): loss=15.776718013087681, w0=70.59023177264596, w1=12.185474019824182\n",
      "[-3.66845893 -1.63026659]\n",
      "Gradient Descent(8672/9): loss=19.878384834910264, w0=73.1581530250899, w1=13.326660634436179\n",
      "[-0.2713857  -0.01955891]\n",
      "Gradient Descent(8673/9): loss=15.406816903215525, w0=73.34812301345289, w1=13.340351873658744\n",
      "[ 1.69671967  0.95619501]\n",
      "Gradient Descent(8674/9): loss=15.397067426672107, w0=72.16041924318556, w1=12.67101536620061\n",
      "[-1.72167402 -1.09518367]\n",
      "Gradient Descent(8675/9): loss=16.355297595602114, w0=73.36559106051914, w1=13.437643932774648\n",
      "[ 2.08885122 -2.89821786]\n",
      "Gradient Descent(8676/9): loss=15.389340975235653, w0=71.90339520418766, w1=15.46639643720803\n",
      "[-2.35842483  0.53618456]\n",
      "Gradient Descent(8677/9): loss=18.326126919029203, w0=73.55429258213614, w1=15.091067244131787\n",
      "[ 0.21208791  1.38400807]\n",
      "Gradient Descent(8678/9): loss=16.718016448775945, w0=73.40583104558522, w1=14.122261596947748\n",
      "[ 3.39395587  2.37660183]\n",
      "Gradient Descent(8679/9): loss=15.598584398602622, w0=71.03006193790542, w1=12.458640318379212\n",
      "[-1.86779905  0.85820299]\n",
      "Gradient Descent(8680/9): loss=18.469713197627744, w0=72.33752127440675, w1=11.857898222477223\n",
      "[-2.70671173 -2.83872444]\n",
      "Gradient Descent(8681/9): loss=17.158379714753124, w0=74.23221948399623, w1=13.845005330726062\n",
      "[ 1.71349381  1.31887843]\n",
      "Gradient Descent(8682/9): loss=15.892808400928901, w0=73.03277381817051, w1=12.921790430226526\n",
      "[ 1.40060962  0.62158463]\n",
      "Gradient Descent(8683/9): loss=15.575625537514707, w0=72.05234708153459, w1=12.486681189959016\n",
      "[-0.11942696  1.25997642]\n",
      "Gradient Descent(8684/9): loss=16.64969753732729, w0=72.13594595389476, w1=11.604697698459939\n",
      "[-3.46478243 -0.90919049]\n",
      "Gradient Descent(8685/9): loss=17.814182264044582, w0=74.56129365398897, w1=12.24113104381133\n",
      "[ 4.0872379  -0.40184609]\n",
      "Gradient Descent(8686/9): loss=16.956045252114574, w0=71.70022712474609, w1=12.52242330786711\n",
      "[ 0.62391559  1.28230288]\n",
      "Gradient Descent(8687/9): loss=17.114020786342657, w0=71.26348621222286, w1=11.62481128884577\n",
      "[-4.87998366 -0.01022373]\n",
      "Gradient Descent(8688/9): loss=19.16755174822876, w0=74.67947477135516, w1=11.631967900948183\n",
      "[ 1.28763951 -3.17303345]\n",
      "Gradient Descent(8689/9): loss=18.052846038556485, w0=73.77812711146026, w1=13.853091313108472\n",
      "[ 5.70668602  2.30191123]\n",
      "Gradient Descent(8690/9): loss=15.57282105610504, w0=69.78344689847813, w1=12.241753449424285\n",
      "[-4.65932213 -0.53039356]\n",
      "Gradient Descent(8691/9): loss=22.313876820392363, w0=73.04497239221915, w1=12.613028941759703\n",
      "[-2.28001429 -0.5857645 ]\n",
      "Gradient Descent(8692/9): loss=15.792445961678714, w0=74.64098239231988, w1=13.02306409412913\n",
      "[ 3.51699378  0.1092525 ]\n",
      "Gradient Descent(8693/9): loss=16.397437569877145, w0=72.17908674939146, w1=12.946587342844982\n",
      "[-1.63187039  0.1012032 ]\n",
      "Gradient Descent(8694/9): loss=16.149427871112852, w0=73.32139602295813, w1=12.875745099527\n",
      "[ 2.48042112 -1.94954537]\n",
      "Gradient Descent(8695/9): loss=15.568653550892876, w0=71.5851012413845, w1=14.240426855859385\n",
      "[-0.06686328  3.79317283]\n",
      "Gradient Descent(8696/9): loss=17.135265280024456, w0=71.63190553702992, w1=11.585205877600284\n",
      "[-4.72759304 -4.34411573]\n",
      "Gradient Descent(8697/9): loss=18.561614781914553, w0=74.94122066261177, w1=14.62608688863699\n",
      "[-2.3995037   0.45002551]\n",
      "Gradient Descent(8698/9): loss=17.399771501271093, w0=76.62087324926814, w1=14.311069030780821\n",
      "[ 3.27529921  0.08140393]\n",
      "Gradient Descent(8699/9): loss=21.265767064012202, w0=74.32816380320129, w1=14.254086276497134\n",
      "[ 2.64697806  1.62038426]\n",
      "Gradient Descent(8700/9): loss=16.220543343602646, w0=72.47527916027694, w1=13.11981729231142\n",
      "[-1.25562665  0.79101194]\n",
      "Gradient Descent(8701/9): loss=15.785738176929188, w0=73.35421781224143, w1=12.566108936184818\n",
      "[-1.25223736  0.40985746]\n",
      "Gradient Descent(8702/9): loss=15.805041337703058, w0=74.23078396415586, w1=12.279208716542417\n",
      "[ 2.74407692 -3.67111901]\n",
      "Gradient Descent(8703/9): loss=16.545347625800215, w0=72.30993012361373, w1=14.848992021194466\n",
      "[-0.8497732  -0.84994502]\n",
      "Gradient Descent(8704/9): loss=16.807471169897422, w0=72.90477136016611, w1=15.44395353434688\n",
      "[ 1.50893884  2.53142836]\n",
      "Gradient Descent(8705/9): loss=17.390728528093433, w0=71.84851417188969, w1=13.671953685590367\n",
      "[ 0.3857103   0.21654102]\n",
      "Gradient Descent(8706/9): loss=16.448968115869917, w0=71.57851695882677, w1=13.520374973551426\n",
      "[-3.79215772 -1.90334118]\n",
      "Gradient Descent(8707/9): loss=16.858021821103094, w0=74.2330273629276, w1=14.85271379879499\n",
      "[ 0.67458351  0.62742463]\n",
      "Gradient Descent(8708/9): loss=16.769413680698584, w0=73.76081890714455, w1=14.413516557291201\n",
      "[ 0.21135268 -0.10818465]\n",
      "Gradient Descent(8709/9): loss=15.930879298211316, w0=73.61287203389371, w1=14.489245812926418\n",
      "[-0.42033198 -0.43380613]\n",
      "Gradient Descent(8710/9): loss=15.946331250803167, w0=73.90710441986445, w1=14.792910102022232\n",
      "[ 4.92328834  2.40307295]\n",
      "Gradient Descent(8711/9): loss=16.436128263904646, w0=70.46080258298625, w1=13.11075903881222\n",
      "[-0.38918534  1.90716696]\n",
      "Gradient Descent(8712/9): loss=19.467233994599034, w0=70.73323231814409, w1=11.775742164090653\n",
      "[-3.82203074  1.3392808 ]\n",
      "Gradient Descent(8713/9): loss=20.116211039654573, w0=73.40865383456284, w1=10.838245601555341\n",
      "[-0.31191251 -3.36871931]\n",
      "Gradient Descent(8714/9): loss=18.881143081584092, w0=73.62699259470124, w1=13.196349116730493\n",
      "[ 2.39333235  1.93258286]\n",
      "Gradient Descent(8715/9): loss=15.481503263722791, w0=71.95165995039328, w1=11.843541113818146\n",
      "[-1.39412929 -3.27754229]\n",
      "Gradient Descent(8716/9): loss=17.625249872673397, w0=72.92755045211658, w1=14.137820715562167\n",
      "[-0.61693056  1.41967245]\n",
      "Gradient Descent(8717/9): loss=15.669555179629382, w0=73.35940184667238, w1=13.144050002694279\n",
      "[ 0.96624527 -2.51523676]\n",
      "Gradient Descent(8718/9): loss=15.444366308078694, w0=72.68303016054142, w1=14.904715731480543\n",
      "[-1.74479915  0.36583038]\n",
      "Gradient Descent(8719/9): loss=16.5877994873798, w0=73.90438956749709, w1=14.648634462376442\n",
      "[-1.85185337  2.11260786]\n",
      "Gradient Descent(8720/9): loss=16.255412546082887, w0=75.20068692437185, w1=13.169808957127463\n",
      "[ 5.35058806 -1.5984716 ]\n",
      "Gradient Descent(8721/9): loss=17.25178418601805, w0=71.45527527888015, w1=14.288739079000036\n",
      "[-2.94558162 -3.64594957]\n",
      "Gradient Descent(8722/9): loss=17.40346081060233, w0=73.51718241096394, w1=16.840903778704664\n",
      "[-0.00833656  5.08113235]\n",
      "Gradient Descent(8723/9): loss=21.05961409844568, w0=73.5230180025897, w1=13.284111134612111\n",
      "[ 1.74576116  0.20174704]\n",
      "Gradient Descent(8724/9): loss=15.431260291902975, w0=72.30098519153123, w1=13.142888203771118\n",
      "[-0.04292421  0.1792272 ]\n",
      "Gradient Descent(8725/9): loss=15.935574905093572, w0=72.33103213634654, w1=13.017429165962831\n",
      "[-2.4833727   0.84435796]\n",
      "Gradient Descent(8726/9): loss=15.956319226030539, w0=74.0693930265456, w1=12.426378592392755\n",
      "[-2.33839765 -2.43049641]\n",
      "Gradient Descent(8727/9): loss=16.241321615682065, w0=75.70627137974152, w1=14.127726082239247\n",
      "[ 2.52282272  1.00920298]\n",
      "Gradient Descent(8728/9): loss=18.505563472231852, w0=73.94029547281248, w1=13.42128399669656\n",
      "[-0.78378474 -1.78903028]\n",
      "Gradient Descent(8729/9): loss=15.596494141847142, w0=74.48894479307039, w1=14.673605191789726\n",
      "[ 2.09961522  0.19665058]\n",
      "Gradient Descent(8730/9): loss=16.81261756166309, w0=73.01921413695486, w1=14.535949783329837\n",
      "[-1.02241627  1.84694983]\n",
      "Gradient Descent(8731/9): loss=15.981438742432118, w0=73.7349055232708, w1=13.243084901514852\n",
      "[-0.76036731 -0.21153662]\n",
      "Gradient Descent(8732/9): loss=15.511117396598252, w0=74.26716263714806, w1=13.39116053609295\n",
      "[ 2.00209836 -1.89464425]\n",
      "Gradient Descent(8733/9): loss=15.863407255077778, w0=72.86569378584055, w1=14.71741151199575\n",
      "[-0.70143768 -0.40572808]\n",
      "Gradient Descent(8734/9): loss=16.243527074043616, w0=73.35670015923192, w1=15.001421165613332\n",
      "[ 0.29216296  2.80190304]\n",
      "Gradient Descent(8735/9): loss=16.545657147764597, w0=73.15218608792736, w1=13.040089037546304\n",
      "[-0.23386416 -1.04891647]\n",
      "Gradient Descent(8736/9): loss=15.492566769302863, w0=73.31589099672595, w1=13.774330565167068\n",
      "[-0.70397878  1.58442067]\n",
      "Gradient Descent(8737/9): loss=15.42952910850652, w0=73.8086761396325, w1=12.665236097636695\n",
      "[-0.31183339 -1.90089468]\n",
      "Gradient Descent(8738/9): loss=15.850059631933592, w0=74.0269595111601, w1=13.995862372469434\n",
      "[ 4.37442701 -0.93974604]\n",
      "Gradient Descent(8739/9): loss=15.787765242650616, w0=70.96486060696263, w1=14.65368459989933\n",
      "[-3.37946043  2.60541462]\n",
      "Gradient Descent(8740/9): loss=18.78725668199317, w0=73.3304829082717, w1=12.829894364583879\n",
      "[-0.52670111 -3.66840003]\n",
      "Gradient Descent(8741/9): loss=15.597687981071806, w0=73.69917368693532, w1=15.39777438863032\n",
      "[ 1.03487728  4.50608653]\n",
      "Gradient Descent(8742/9): loss=17.307483161861317, w0=72.97475959020231, w1=12.243513814967041\n",
      "[-2.54328511 -1.56020843]\n",
      "Gradient Descent(8743/9): loss=16.200913705487388, w0=74.7550591650783, w1=13.335659713546374\n",
      "[-2.02479513 -2.15365284]\n",
      "Gradient Descent(8744/9): loss=16.463724366617466, w0=76.172415753797, w1=14.843216701499372\n",
      "[ 4.39733186  1.41700392]\n",
      "Gradient Descent(8745/9): loss=20.458322950489713, w0=73.09428344851331, w1=13.851313958800398\n",
      "[ 0.57775477  2.57090841]\n",
      "Gradient Descent(8746/9): loss=15.474859491118986, w0=72.68985510690592, w1=12.051678068423026\n",
      "[ 0.8116144   0.54304843]\n",
      "Gradient Descent(8747/9): loss=16.58797735181405, w0=72.12172502792112, w1=11.671544165722853\n",
      "[-4.05963947 -3.44647508]\n",
      "Gradient Descent(8748/9): loss=17.7076469869631, w0=74.96347265645359, w1=14.084076722490906\n",
      "[ 5.80066065  3.42709391]\n",
      "Gradient Descent(8749/9): loss=16.962215658550797, w0=70.90301019870702, w1=11.685110987449406\n",
      "[-2.65736278  0.16345891]\n",
      "Gradient Descent(8750/9): loss=19.854414672399425, w0=72.76316414597466, w1=11.57068974782344\n",
      "[ 0.09589789 -0.92979377]\n",
      "Gradient Descent(8751/9): loss=17.348923629808038, w0=72.69603562235052, w1=12.221545388439322\n",
      "[-0.37532815 -1.89955282]\n",
      "Gradient Descent(8752/9): loss=16.3561140888893, w0=72.95876532858752, w1=13.551232363079997\n",
      "[ 0.82077839  1.69630719]\n",
      "Gradient Descent(8753/9): loss=15.444610416788183, w0=72.38422045224257, w1=12.363817329355847\n",
      "[-3.17247408 -2.31567095]\n",
      "Gradient Descent(8754/9): loss=16.422277267128692, w0=74.6049523083324, w1=13.984786994606987\n",
      "[-0.76169213  5.12024928]\n",
      "Gradient Descent(8755/9): loss=16.372838256139133, w0=75.13813680026179, w1=10.40061250127804\n",
      "[-0.22300862 -2.43804333]\n",
      "Gradient Descent(8756/9): loss=21.82688018058887, w0=75.2942428346239, w1=12.107242834043186\n",
      "[-0.18379164 -1.12568515]\n",
      "Gradient Descent(8757/9): loss=18.32836598809383, w0=75.42289698427014, w1=12.895222441410251\n",
      "[ 4.04079548  2.43978894]\n",
      "Gradient Descent(8758/9): loss=17.822969382468397, w0=72.59434014770551, w1=11.187370185146934\n",
      "[-2.38534808 -3.12099459]\n",
      "Gradient Descent(8759/9): loss=18.25801174953765, w0=74.26408380035961, w1=13.372066401611436\n",
      "[ 0.90188154  0.19322581]\n",
      "Gradient Descent(8760/9): loss=15.862288660476496, w0=73.63276672558028, w1=13.236808333543\n",
      "[ 0.31004176  0.97751652]\n",
      "Gradient Descent(8761/9): loss=15.472796943392515, w0=73.4157374961783, w1=12.55254677053084\n",
      "[-2.26882193 -0.8900194 ]\n",
      "Gradient Descent(8762/9): loss=15.823125460802654, w0=75.00391284993412, w1=13.17556035101186\n",
      "[ 1.50525366 -1.70098114]\n",
      "Gradient Descent(8763/9): loss=16.894176463752594, w0=73.95023529103707, w1=14.36624715247074\n",
      "[ 1.4697351  -1.52430058]\n",
      "Gradient Descent(8764/9): loss=15.994233338093865, w0=72.92142072021353, w1=15.433257557329549\n",
      "[ 0.86459735 -0.57061722]\n",
      "Gradient Descent(8765/9): loss=17.363435743845045, w0=72.3162025762693, w1=15.832689613781652\n",
      "[-1.1136429   4.23739704]\n",
      "Gradient Descent(8766/9): loss=18.632106308617228, w0=73.09575260420236, w1=12.86651168898487\n",
      "[-1.02442159  1.98251895]\n",
      "Gradient Descent(8767/9): loss=15.593531001412025, w0=73.81284771458627, w1=11.478748427430341\n",
      "[ 1.29148335 -3.55692932]\n",
      "Gradient Descent(8768/9): loss=17.5224582961391, w0=72.90880936858052, w1=13.96859895400939\n",
      "[-0.90521363  0.32862011]\n",
      "Gradient Descent(8769/9): loss=15.579548753319468, w0=73.54245891060684, w1=13.738564877502505\n",
      "[ 3.47595516  1.66648565]\n",
      "Gradient Descent(8770/9): loss=15.450275459770769, w0=71.10929030159488, w1=12.572024922322129\n",
      "[-4.20973269 -2.81124784]\n",
      "Gradient Descent(8771/9): loss=18.184144012592405, w0=74.05610318632037, w1=14.539898410110986\n",
      "[ 2.49079712  0.4358095 ]\n",
      "Gradient Descent(8772/9): loss=16.238345098537852, w0=72.3125452049563, w1=14.234831759791188\n",
      "[-0.64882998  0.55229574]\n",
      "Gradient Descent(8773/9): loss=16.152540675165334, w0=72.7667261933464, w1=13.84822474341894\n",
      "[-0.015945    2.37738258]\n",
      "Gradient Descent(8774/9): loss=15.59275623994798, w0=72.77788769455738, w1=12.184056940838781\n",
      "[ 1.90176706  0.17835252]\n",
      "Gradient Descent(8775/9): loss=16.358395151873463, w0=71.44665075191106, w1=12.059210178956711\n",
      "[-5.50839304 -0.32833662]\n",
      "Gradient Descent(8776/9): loss=18.10100673442276, w0=75.30252587859532, w1=12.289045814578074\n",
      "[ 2.40212272 -4.26879432]\n",
      "Gradient Descent(8777/9): loss=18.111976135635434, w0=73.62103997666765, w1=15.277201837873967\n",
      "[ 0.49254146  2.19145892]\n",
      "Gradient Descent(8778/9): loss=17.05487503021211, w0=73.27626095230171, w1=13.743180591149434\n",
      "[-0.77541252 -1.18916701]\n",
      "Gradient Descent(8779/9): loss=15.420751559824758, w0=73.81904971978739, w1=14.575597495126564\n",
      "[-0.56851324  3.47233284]\n",
      "Gradient Descent(8780/9): loss=16.12424946128476, w0=74.21700898479507, w1=12.144964506916093\n",
      "[ 0.32585112 -0.60398158]\n",
      "Gradient Descent(8781/9): loss=16.70270867338268, w0=73.98891319768683, w1=12.567751612650056\n",
      "[-0.75007382 -2.14725111]\n",
      "Gradient Descent(8782/9): loss=16.043230520538003, w0=74.51396487203571, w1=14.070827387629476\n",
      "[ 3.60158892 -0.9002489 ]\n",
      "Gradient Descent(8783/9): loss=16.304848614681106, w0=71.99285262702736, w1=14.701001619343995\n",
      "[-2.70698476  0.19915389]\n",
      "Gradient Descent(8784/9): loss=16.978052264123292, w0=73.88774195756287, w1=14.561593894262568\n",
      "[ 1.43388663  3.20778278]\n",
      "Gradient Descent(8785/9): loss=16.14743268453918, w0=72.88402131503126, w1=12.316145946671405\n",
      "[ 1.73090563 -1.49384833]\n",
      "Gradient Descent(8786/9): loss=16.14684064182917, w0=71.67238737350395, w1=13.361839779310856\n",
      "[-2.95626222  0.33049879]\n",
      "Gradient Descent(8787/9): loss=16.70752212618419, w0=73.7417709299584, w1=13.130490624128004\n",
      "[-0.51082164 -0.0889176 ]\n",
      "Gradient Descent(8788/9): loss=15.547150136509572, w0=74.0993460748471, w1=13.192732941201808\n",
      "[ 0.51083722  1.20743658]\n",
      "Gradient Descent(8789/9): loss=15.751420452232676, w0=73.7417600175595, w1=12.347527336360157\n",
      "[ 0.1115982   3.52648916]\n",
      "Gradient Descent(8790/9): loss=16.127088861651085, w0=73.66364127464848, w1=9.878984922145811\n",
      "[-1.58419934 -3.87133724]\n",
      "Gradient Descent(8791/9): loss=21.936853349947494, w0=74.7725808108169, w1=12.588920989123833\n",
      "[ 5.48314346  1.07461363]\n",
      "Gradient Descent(8792/9): loss=16.875858505133095, w0=70.93438039225819, w1=11.836691451574543\n",
      "[-4.89250583 -0.8046208 ]\n",
      "Gradient Descent(8793/9): loss=19.519365149099265, w0=74.35913447096749, w1=12.399926010379902\n",
      "[ 3.22916241 -1.93908923]\n",
      "Gradient Descent(8794/9): loss=16.536196032124362, w0=72.09872078618405, w1=13.75728846957985\n",
      "[-2.27331689 -0.50955277]\n",
      "Gradient Descent(8795/9): loss=16.13866506958867, w0=73.69004261170548, w1=14.113975409434193\n",
      "[ 2.10964091 -2.2964336 ]\n",
      "Gradient Descent(8796/9): loss=15.665488397880452, w0=72.21329397385591, w1=15.72147892631398\n",
      "[-1.98884861  2.39406069]\n",
      "Gradient Descent(8797/9): loss=18.48252483736201, w0=73.60548799816551, w1=14.04563644485576\n",
      "[ 0.03752475  2.69715998]\n",
      "Gradient Descent(8798/9): loss=15.594559546251741, w0=73.57922067419078, w1=12.157624461413084\n",
      "[-1.09129479 -0.00420346]\n",
      "Gradient Descent(8799/9): loss=16.300543839913402, w0=74.34312702495419, w1=12.160566883334436\n",
      "[ 1.92164542 -1.73742034]\n",
      "Gradient Descent(8800/9): loss=16.80637595204026, w0=72.99797523317793, w1=13.376761118171508\n",
      "[-1.36500627  0.29009453]\n",
      "Gradient Descent(8801/9): loss=15.434979600665875, w0=73.95347962331932, w1=13.173694947052944\n",
      "[ 1.59498309  0.89773172]\n",
      "Gradient Descent(8802/9): loss=15.650219348141588, w0=72.83699145886561, w1=12.545282745042904\n",
      "[ 0.38493385 -0.76503068]\n",
      "Gradient Descent(8803/9): loss=15.926860052228431, w0=72.56753776232114, w1=13.080804221819886\n",
      "[-0.88509978  0.11896891]\n",
      "Gradient Descent(8804/9): loss=15.729268781999636, w0=73.18710760576718, w1=12.997525987647922\n",
      "[-1.88808247  1.0975738 ]\n",
      "Gradient Descent(8805/9): loss=15.507844411461654, w0=74.50876533400218, w1=12.229224325001324\n",
      "[ 0.4049614  -2.11857433]\n",
      "Gradient Descent(8806/9): loss=16.905670285967023, w0=74.22529235369353, w1=13.71222635924046\n",
      "[ 0.58477627 -0.38292081]\n",
      "Gradient Descent(8807/9): loss=15.846644597223692, w0=73.81594896532646, w1=13.98027092328284\n",
      "[-0.24669801  4.10916586]\n",
      "Gradient Descent(8808/9): loss=15.64742334409589, w0=73.98863757425706, w1=11.103854818926752\n",
      "[ 2.27375805 -1.35223158]\n",
      "Gradient Descent(8809/9): loss=18.44955243782516, w0=72.39700694068793, w1=12.050416925378844\n",
      "[-0.46946137 -1.6883996 ]\n",
      "Gradient Descent(8810/9): loss=16.80955900942391, w0=72.72562989724247, w1=13.232296647258455\n",
      "[ 0.60624894  0.6806071 ]\n",
      "Gradient Descent(8811/9): loss=15.577973113063225, w0=72.30125564004749, w1=12.75587167501179\n",
      "[-0.29355173  0.87947395]\n",
      "Gradient Descent(8812/9): loss=16.140553844912066, w0=72.50674184929458, w1=12.140239911004942\n",
      "[ 1.81750084 -1.67891962]\n",
      "Gradient Descent(8813/9): loss=16.592807486573037, w0=71.23449126300497, w1=13.3154836438905\n",
      "[ 2.46489486 -0.75186223]\n",
      "Gradient Descent(8814/9): loss=17.520000901317683, w0=69.50906486364511, w1=13.841787202436635\n",
      "[-4.36302046 -1.20898603]\n",
      "Gradient Descent(8815/9): loss=22.614008716716636, w0=72.56317918249583, w1=14.68807742684422\n",
      "[ 1.88795821  2.13385624]\n",
      "Gradient Descent(8816/9): loss=16.382953379805297, w0=71.24160843664608, w1=13.194378059702203\n",
      "[-1.21988159 -2.23550824]\n",
      "Gradient Descent(8817/9): loss=17.53259120717332, w0=72.09552554833218, w1=14.75923382646405\n",
      "[-4.90518101 -0.55146118]\n",
      "Gradient Descent(8818/9): loss=16.92255239465833, w0=75.52915225393859, w1=15.145256654033027\n",
      "[ 1.27370822 -0.60919929]\n",
      "Gradient Descent(8819/9): loss=19.271033780980407, w0=74.63755650214769, w1=15.571696154638083\n",
      "[ 2.97063607  2.08732636]\n",
      "Gradient Descent(8820/9): loss=18.47676264531993, w0=72.55811125435977, w1=14.110567699462303\n",
      "[ 2.29064243  0.33931519]\n",
      "Gradient Descent(8821/9): loss=15.855585779435044, w0=70.95466155239528, w1=13.873047066630834\n",
      "[-2.43143537 -0.59356106]\n",
      "Gradient Descent(8822/9): loss=18.199313660842293, w0=72.65666631270017, w1=14.288539810776408\n",
      "[-2.46006325  0.99874726]\n",
      "Gradient Descent(8823/9): loss=15.916036137580466, w0=74.37871058597271, w1=13.58941672558147\n",
      "[ 2.16704819 -0.85621377]\n",
      "Gradient Descent(8824/9): loss=15.980288520361244, w0=72.86177685103975, w1=14.188766367259193\n",
      "[-2.27859535  3.64700288]\n",
      "Gradient Descent(8825/9): loss=15.730641324057965, w0=74.45679359391511, w1=11.635864352253769\n",
      "[-0.45317312  0.62242109]\n",
      "Gradient Descent(8826/9): loss=17.761910914451953, w0=74.77401477450985, w1=11.200169590284212\n",
      "[-0.54722459 -4.09672618]\n",
      "Gradient Descent(8827/9): loss=19.07938296671417, w0=75.15707198545886, w1=14.06787791925982\n",
      "[ 4.95697441 -0.34891294]\n",
      "Gradient Descent(8828/9): loss=17.294521117508516, w0=71.68718990103378, w1=14.312116976255997\n",
      "[-1.79311254  2.63621927]\n",
      "Gradient Descent(8829/9): loss=17.023130551297, w0=72.94236868226497, w1=12.466763488280364\n",
      "[ 1.72711785 -0.33060278]\n",
      "Gradient Descent(8830/9): loss=15.960715521493857, w0=71.73338619011582, w1=12.69818543686903\n",
      "[ 0.90007195 -1.21123366]\n",
      "Gradient Descent(8831/9): loss=16.908916103475296, w0=71.10333582858564, w1=13.546049001597385\n",
      "[-3.86455462 -1.19752271]\n",
      "Gradient Descent(8832/9): loss=17.787422030671596, w0=73.80852406310531, w1=14.384314901719428\n",
      "[ 4.81741712  0.88511707]\n",
      "Gradient Descent(8833/9): loss=15.92744832082953, w0=70.43633207755565, w1=13.764732954034283\n",
      "[-4.34317791  4.39647872]\n",
      "Gradient Descent(8834/9): loss=19.509416305411335, w0=73.47655661146817, w1=10.687197851269621\n",
      "[ 1.77222477 -4.38306554]\n",
      "Gradient Descent(8835/9): loss=19.301634419240827, w0=72.23599927548231, w1=13.755343727676072\n",
      "[ 1.28068937 -0.81650037]\n",
      "Gradient Descent(8836/9): loss=15.983474421336153, w0=71.3395167157451, w1=14.32689398802363\n",
      "[-4.47037405 -0.7852965 ]\n",
      "Gradient Descent(8837/9): loss=17.654596172406563, w0=74.46877854881996, w1=14.876601538679854\n",
      "[ 3.4810937   3.52705814]\n",
      "Gradient Descent(8838/9): loss=17.051681405513722, w0=72.03201295647365, w1=12.407660837575882\n",
      "[-0.28392527 -2.49090738]\n",
      "Gradient Descent(8839/9): loss=16.75674240231081, w0=72.23076064748422, w1=14.151296003997555\n",
      "[-2.85985831  3.41957341]\n",
      "Gradient Descent(8840/9): loss=16.176556146890253, w0=74.23266146319116, w1=11.757594616900075\n",
      "[ 2.71612033  1.07683034]\n",
      "Gradient Descent(8841/9): loss=17.309348646419156, w0=72.3313772313931, w1=11.003813379116083\n",
      "[-0.43286247 -2.79896698]\n",
      "Gradient Descent(8842/9): loss=18.914172154078308, w0=72.63438096274992, w1=12.963090263273722\n",
      "[-1.14718282 -1.57516567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8843/9): loss=15.736834294280246, w0=73.43740893619342, w1=14.0657062314928\n",
      "[-0.75904624  2.21058992]\n",
      "Gradient Descent(8844/9): loss=15.567876483726863, w0=73.96874130293665, w1=12.518293288125673\n",
      "[-1.72660404  0.63447595]\n",
      "Gradient Descent(8845/9): loss=16.075741801194482, w0=75.17736413110823, w1=12.07416012490927\n",
      "[ 1.59763275 -3.24643666]\n",
      "Gradient Descent(8846/9): loss=18.147353643666467, w0=74.05902120523655, w1=14.346665784603287\n",
      "[-1.20368469  3.55290344]\n",
      "Gradient Descent(8847/9): loss=16.054380319349193, w0=74.90160049104162, w1=11.859633379804336\n",
      "[-0.15675721 -1.02501512]\n",
      "Gradient Descent(8848/9): loss=17.990531003247963, w0=75.01133053637028, w1=12.577143965653732\n",
      "[ 2.66283497 -0.56992119]\n",
      "Gradient Descent(8849/9): loss=17.267948826531832, w0=73.14734605553255, w1=12.976088801513248\n",
      "[ 2.7287609  -2.78647697]\n",
      "Gradient Descent(8850/9): loss=15.523448504983916, w0=71.23721342752833, w1=14.926622681159424\n",
      "[-2.47580465  1.62807863]\n",
      "Gradient Descent(8851/9): loss=18.547687579434797, w0=72.97027667949827, w1=13.786967642353433\n",
      "[ 3.14252692  0.35006642]\n",
      "Gradient Descent(8852/9): loss=15.485463897478333, w0=70.77050783584934, w1=13.54192115038778\n",
      "[-2.97790709  3.38504037]\n",
      "Gradient Descent(8853/9): loss=18.571632358195522, w0=72.8550427969067, w1=11.172392889853509\n",
      "[ 2.28424456 -2.70653693]\n",
      "Gradient Descent(8854/9): loss=18.144057088889465, w0=71.25607160415802, w1=13.066968737563872\n",
      "[-2.52620264 -1.11566233]\n",
      "Gradient Descent(8855/9): loss=17.547483670918215, w0=73.02441345443434, w1=13.847932370108477\n",
      "[-0.16310042 -1.68262908]\n",
      "Gradient Descent(8856/9): loss=15.489998257772903, w0=73.13858375013676, w1=15.025772728137227\n",
      "[-1.05803993  1.31627307]\n",
      "Gradient Descent(8857/9): loss=16.593104070116418, w0=73.87921169872061, w1=14.104381580110203\n",
      "[ 3.06397418 -0.24862791]\n",
      "Gradient Descent(8858/9): loss=15.75227565374468, w0=71.73442976953321, w1=14.278421113784908\n",
      "[-3.30062461  0.0482224 ]\n",
      "Gradient Descent(8859/9): loss=16.920863657347486, w0=74.04486699698089, w1=14.244665430523908\n",
      "[-1.72347366  2.13862466]\n",
      "Gradient Descent(8860/9): loss=15.960423604182706, w0=75.2512985568419, w1=12.747628169222795\n",
      "[ 0.7822371  -2.60494492]\n",
      "Gradient Descent(8861/9): loss=17.56952304343714, w0=74.70373258441482, w1=14.571089614232083\n",
      "[ 3.63477856 -2.91031073]\n",
      "Gradient Descent(8862/9): loss=16.975222881511755, w0=72.15938759263159, w1=16.608307123134303\n",
      "[-0.25910282 -0.39962458]\n",
      "Gradient Descent(8863/9): loss=20.92352439331456, w0=72.3407595657626, w1=16.888044325976413\n",
      "[-0.5338407   2.00407497]\n",
      "Gradient Descent(8864/9): loss=21.64851032341743, w0=72.71444805292657, w1=15.48519184743406\n",
      "[ 2.96372384  1.51510698]\n",
      "Gradient Descent(8865/9): loss=17.564756734588126, w0=70.63984136651472, w1=14.42461696212551\n",
      "[-2.69249414  0.97504016]\n",
      "Gradient Descent(8866/9): loss=19.35438216163906, w0=72.52458726110301, w1=13.742088852109639\n",
      "[-5.02701668  3.27369866]\n",
      "Gradient Descent(8867/9): loss=15.716246532816367, w0=76.04349893438793, w1=11.45049979355096\n",
      "[ 1.52582289 -0.52684281]\n",
      "Gradient Descent(8868/9): loss=21.22482649418615, w0=74.97542291259516, w1=11.819289763607024\n",
      "[ 1.18487753  0.94375853]\n",
      "Gradient Descent(8869/9): loss=18.178112248638417, w0=74.14600863869764, w1=11.158658791700343\n",
      "[-0.93995636 -2.59307999]\n",
      "Gradient Descent(8870/9): loss=18.442558694471007, w0=74.80397808834093, w1=12.973814786743102\n",
      "[ 0.84843089  2.75507067]\n",
      "Gradient Descent(8871/9): loss=16.653988775868587, w0=74.21007646687362, w1=11.045265318326777\n",
      "[-1.75630267 -4.78351057]\n",
      "Gradient Descent(8872/9): loss=18.768823752399587, w0=75.43948833642642, w1=14.393722715705634\n",
      "[ 3.5819287  -0.97607537]\n",
      "Gradient Descent(8873/9): loss=18.10532271294353, w0=72.9321382431096, w1=15.07697547513185\n",
      "[ 0.85984016  3.95826925]\n",
      "Gradient Descent(8874/9): loss=16.726956222669, w0=72.33025012916794, w1=12.306186997078333\n",
      "[-0.00700403 -1.38374624]\n",
      "Gradient Descent(8875/9): loss=16.538800584886364, w0=72.33515294695813, w1=13.274809368255344\n",
      "[-1.18624804 -1.84403488]\n",
      "Gradient Descent(8876/9): loss=15.866499552761626, w0=73.16552657403207, w1=14.565633784696136\n",
      "[-1.56786512  0.5134174 ]\n",
      "Gradient Descent(8877/9): loss=15.983743150679272, w0=74.26303216080339, w1=14.20624160764009\n",
      "[ 2.62301434  0.75599377]\n",
      "Gradient Descent(8878/9): loss=16.11939743803193, w0=72.42692212006672, w1=13.677045969683043\n",
      "[-0.59989621  0.99819267]\n",
      "Gradient Descent(8879/9): loss=15.781202528514171, w0=72.84684946948867, w1=12.978311100258468\n",
      "[-1.65182028 -2.44291355]\n",
      "Gradient Descent(8880/9): loss=15.611526442774275, w0=74.00312366329804, w1=14.688350587710845\n",
      "[ 2.95598622  4.98981827]\n",
      "Gradient Descent(8881/9): loss=16.367774459056125, w0=71.93393331100489, w1=11.195477796415313\n",
      "[ 0.50035858 -1.71977608]\n",
      "Gradient Descent(8882/9): loss=18.91953643081979, w0=71.58368230767991, w1=12.399321055879602\n",
      "[-0.3341301 -0.3166423]\n",
      "Gradient Descent(8883/9): loss=17.431970541050344, w0=71.81757337514763, w1=12.620970668269209\n",
      "[-2.1507435  -2.50118831]\n",
      "Gradient Descent(8884/9): loss=16.84440921394369, w0=73.32309382688976, w1=14.371802488118302\n",
      "[ 1.69449043  1.21299835]\n",
      "Gradient Descent(8885/9): loss=15.784225697956106, w0=72.13695052855128, w1=13.522703644922757\n",
      "[-2.06901124 -1.33487017]\n",
      "Gradient Descent(8886/9): loss=16.05610348620393, w0=73.5852583981514, w1=14.457112767276783\n",
      "[-0.4668125  -0.73422137]\n",
      "Gradient Descent(8887/9): loss=15.90598202143808, w0=73.91202714976332, w1=14.971067723342143\n",
      "[ 1.19330616  1.52626234]\n",
      "Gradient Descent(8888/9): loss=16.68898515365951, w0=73.07671284020323, w1=13.90268408546581\n",
      "[-2.30386313  5.53673917]\n",
      "Gradient Descent(8889/9): loss=15.498930287389992, w0=74.68941703342874, w1=10.026966666460046\n",
      "[-0.45212676 -1.37165578]\n",
      "Gradient Descent(8890/9): loss=22.32031773110123, w0=75.00590576253866, w1=10.987125713501364\n",
      "[ 3.14431539 -3.02563014]\n",
      "Gradient Descent(8891/9): loss=19.957826348891732, w0=72.80488498768686, w1=13.105066809125569\n",
      "[-0.69099836 -4.79568461]\n",
      "Gradient Descent(8892/9): loss=15.575646142054314, w0=73.28858384107187, w1=16.462046033552163\n",
      "[-2.03988384  0.70969816]\n",
      "Gradient Descent(8893/9): loss=19.833058963370217, w0=74.7165025300109, w1=15.96525732229739\n",
      "[ 0.4442813  -0.60295366]\n",
      "Gradient Descent(8894/9): loss=19.486722241429963, w0=74.40550562004147, w1=16.38732488520703\n",
      "[ 2.70323469  3.08695489]\n",
      "Gradient Descent(8895/9): loss=20.230802018992854, w0=72.51324133864296, w1=14.226456464404865\n",
      "[ 0.4099025  -1.65276977]\n",
      "Gradient Descent(8896/9): loss=15.9694323407154, w0=72.22630958912846, w1=15.383395302006765\n",
      "[ 2.6867081   3.01522719]\n",
      "Gradient Descent(8897/9): loss=17.767790230088796, w0=70.34561391599118, w1=13.272736271179056\n",
      "[-4.25920934  4.57857165]\n",
      "Gradient Descent(8898/9): loss=19.75356772034478, w0=73.32706045571925, w1=10.067736115670515\n",
      "[-0.39907312 -5.46503567]\n",
      "Gradient Descent(8899/9): loss=21.20722814917859, w0=73.60641163793919, w1=13.893261083025925\n",
      "[-2.15194398 -0.44207502]\n",
      "Gradient Descent(8900/9): loss=15.520223997227799, w0=75.11277242455493, w1=14.202713595630742\n",
      "[ 4.42045058  0.48919853]\n",
      "Gradient Descent(8901/9): loss=17.30136163759682, w0=72.01845701849255, w1=13.86027462493088\n",
      "[-2.4685188  -0.09508542]\n",
      "Gradient Descent(8902/9): loss=16.271707121247054, w0=73.74642017843578, w1=13.926834416731198\n",
      "[-1.18421824  0.67303378]\n",
      "Gradient Descent(8903/9): loss=15.58822420189917, w0=74.57537294481043, w1=13.455710772440256\n",
      "[-1.8850033  -1.46892159]\n",
      "Gradient Descent(8904/9): loss=16.20723416801202, w0=75.89487525248045, w1=14.483955887485289\n",
      "[ 2.0302714   1.84131462]\n",
      "Gradient Descent(8905/9): loss=19.27261923008899, w0=74.47368526986956, w1=13.195035656283025\n",
      "[-1.75954981  2.9864725 ]\n",
      "Gradient Descent(8906/9): loss=16.122328986979653, w0=75.70537013873239, w1=11.104504909161298\n",
      "[-1.87296047 -4.84912344]\n",
      "Gradient Descent(8907/9): loss=21.1142343220252, w0=77.01644246451247, w1=14.498891320580753\n",
      "[ 3.73427369 -1.47052143]\n",
      "Gradient Descent(8908/9): loss=22.833829965767812, w0=74.40245088215305, w1=15.528256322528094\n",
      "[ 1.43612734  4.36002   ]\n",
      "Gradient Descent(8909/9): loss=18.098572037366267, w0=73.39716174688957, w1=12.476242322977622\n",
      "[-2.28226464  0.06414027]\n",
      "Gradient Descent(8910/9): loss=15.894693224131084, w0=74.99474699817915, w1=12.431344132718\n",
      "[-1.03687123 -1.47336804]\n",
      "Gradient Descent(8911/9): loss=17.38182875106774, w0=75.72055685796177, w1=13.462701759435442\n",
      "[ 1.18724697 -1.24238552]\n",
      "Gradient Descent(8912/9): loss=18.330310912199835, w0=74.88948398028901, w1=14.33237162481573\n",
      "[ 0.25312438  2.55568534]\n",
      "Gradient Descent(8913/9): loss=17.022310728940287, w0=74.7122969117901, w1=12.543391884341803\n",
      "[ 4.99254073 -4.49027711]\n",
      "Gradient Descent(8914/9): loss=16.83012964782343, w0=71.2175183990417, w1=15.686585860558276\n",
      "[-2.45838663  0.44714211]\n",
      "Gradient Descent(8915/9): loss=19.976758988478768, w0=72.93838904340512, w1=15.37358638096583\n",
      "[ 1.0621825  -0.11648086]\n",
      "Gradient Descent(8916/9): loss=17.242468972815246, w0=72.19486129158022, w1=15.4551229799277\n",
      "[-0.50837643  1.77694207]\n",
      "Gradient Descent(8917/9): loss=17.94097850206663, w0=72.550724790939, w1=14.211263529279092\n",
      "[-2.42869794  0.49028041]\n",
      "Gradient Descent(8918/9): loss=15.92964241795048, w0=74.2508133499674, w1=13.868067244034872\n",
      "[ 0.62941085 -0.96166868]\n",
      "Gradient Descent(8919/9): loss=15.919118123490689, w0=73.8102257565472, w1=14.541235321453991\n",
      "[ 1.07140825 -1.94323912]\n",
      "Gradient Descent(8920/9): loss=16.082588071499288, w0=73.0602399847673, w1=15.901502704467985\n",
      "[ 1.12003706  2.09111239]\n",
      "Gradient Descent(8921/9): loss=18.345725566114382, w0=72.27621404376255, w1=14.437724029832093\n",
      "[-2.82727231  0.45785566]\n",
      "Gradient Descent(8922/9): loss=16.36264572099323, w0=74.25530466243022, w1=14.11722506662551\n",
      "[ 2.32235864 -0.15633343]\n",
      "Gradient Descent(8923/9): loss=16.05122735636424, w0=72.62965361486808, w1=14.226658468911928\n",
      "[-2.19846145 -0.07096948]\n",
      "Gradient Descent(8924/9): loss=15.88547830276726, w0=74.16857663035839, w1=14.276337101864616\n",
      "[ 2.06627166  1.07242453]\n",
      "Gradient Descent(8925/9): loss=16.085703658129074, w0=72.7221864674859, w1=13.525639930231291\n",
      "[-3.35232546  0.481915  ]\n",
      "Gradient Descent(8926/9): loss=15.550383297012216, w0=75.06881428598649, w1=13.188299429729458\n",
      "[ 3.66228349 -1.26118872]\n",
      "Gradient Descent(8927/9): loss=17.0034699483373, w0=72.5052158453661, w1=14.0711315356648\n",
      "[-1.69360401  1.21542825]\n",
      "Gradient Descent(8928/9): loss=15.871804845990534, w0=73.69073865432308, w1=13.22033176292222\n",
      "[-0.07340476  0.4651266 ]\n",
      "Gradient Descent(8929/9): loss=15.498258763089028, w0=73.74212198628118, w1=12.894743145621987\n",
      "[-0.22840797 -1.61253179]\n",
      "Gradient Descent(8930/9): loss=15.657424016488381, w0=73.90200756672901, w1=14.0235154009325\n",
      "[-2.68073311  0.34759716]\n",
      "Gradient Descent(8931/9): loss=15.718632728665781, w0=75.77852074140498, w1=13.780197389017717\n",
      "[ 1.45395278  1.34246107]\n",
      "Gradient Descent(8932/9): loss=18.517648920293244, w0=74.7607537934269, w1=12.840474636997675\n",
      "[ 3.08686244 -0.22681877]\n",
      "Gradient Descent(8933/9): loss=16.66599810203586, w0=72.59995008750633, w1=12.999247778565291\n",
      "[-1.83697576  0.2781337 ]\n",
      "Gradient Descent(8934/9): loss=15.742109520991601, w0=73.88583311715948, w1=12.804554191798239\n",
      "[ 1.18419452 -2.80494615]\n",
      "Gradient Descent(8935/9): loss=15.788986579566057, w0=73.0568969505994, w1=14.768016496768071\n",
      "[-1.44226769  3.42148202]\n",
      "Gradient Descent(8936/9): loss=16.243841984148226, w0=74.066484330835, w1=12.372979082348706\n",
      "[ 4.40605092 -0.93397966]\n",
      "Gradient Descent(8937/9): loss=16.29674350163888, w0=70.98224868688192, w1=13.026764844942507\n",
      "[ 0.32430416  2.66457116]\n",
      "Gradient Descent(8938/9): loss=18.16038538665156, w0=70.75523577272953, w1=11.161565035692453\n",
      "[-2.00445933 -0.45402512]\n",
      "Gradient Descent(8939/9): loss=21.295255436872967, w0=72.15835730204498, w1=11.47938262131042\n",
      "[-0.67277055 -1.73544276]\n",
      "Gradient Descent(8940/9): loss=18.031301144586607, w0=72.62929668613333, w1=12.694192551914531\n",
      "[ 3.34316936 -1.86350531]\n",
      "Gradient Descent(8941/9): loss=15.91527201749745, w0=70.28907813271566, w1=13.998646271768973\n",
      "[-1.84827997  0.02682103]\n",
      "Gradient Descent(8942/9): loss=20.03507737201089, w0=71.5828741149961, w1=13.979871549465075\n",
      "[-1.18807562  0.31027654]\n",
      "Gradient Descent(8943/9): loss=16.97480987471636, w0=72.41452704551908, w1=13.762677971136569\n",
      "[ 0.06065981 -1.15575031]\n",
      "Gradient Descent(8944/9): loss=15.81259036098757, w0=72.37206518169972, w1=14.5717031852758\n",
      "[-0.61097096 -1.62652458]\n",
      "Gradient Descent(8945/9): loss=16.40701976684935, w0=72.79974485361376, w1=15.710270391726551\n",
      "[-0.32631644  1.32698937]\n",
      "Gradient Descent(8946/9): loss=17.995687795057453, w0=73.02816636361455, w1=14.781377831103992\n",
      "[ 1.02998646  2.79376261]\n",
      "Gradient Descent(8947/9): loss=16.26836730024572, w0=72.30717583822992, w1=12.825744003046808\n",
      "[-2.57390114 -2.24404115]\n",
      "Gradient Descent(8948/9): loss=16.08655921977917, w0=74.10890663689948, w1=14.396572809589285\n",
      "[ 1.49867099 -0.689813  ]\n",
      "Gradient Descent(8949/9): loss=16.13830431956083, w0=73.05983694336926, w1=14.879441907947363\n",
      "[ 2.33533895 -0.31280238]\n",
      "Gradient Descent(8950/9): loss=16.392907074925184, w0=71.42509968160051, w1=15.098403572394995\n",
      "[-4.11339506 -0.99411894]\n",
      "Gradient Descent(8951/9): loss=18.442216800795922, w0=74.3044762250514, w1=15.794286828006745\n",
      "[-3.1339483   6.47869069]\n",
      "Gradient Descent(8952/9): loss=18.57512509799323, w0=76.49824003715314, w1=11.259203346897928\n",
      "[ 1.92978131 -5.65756166]\n",
      "Gradient Descent(8953/9): loss=22.985045208843815, w0=75.14739312197692, w1=15.219496510936857\n",
      "[ 1.5342393   3.42320076]\n",
      "Gradient Descent(8954/9): loss=18.616989780389464, w0=74.07342561222295, w1=12.823255981316612\n",
      "[ 2.30745176 -0.63430985]\n",
      "Gradient Descent(8955/9): loss=15.905168345706809, w0=72.45820938205284, w1=13.267272876452676\n",
      "[-1.58229383 -0.24315094]\n",
      "Gradient Descent(8956/9): loss=15.757660943502342, w0=73.56581506535733, w1=13.437478531930125\n",
      "[-3.42895886 -0.83961712]\n",
      "Gradient Descent(8957/9): loss=15.42374263903551, w0=75.96608626655133, w1=14.025210518585284\n",
      "[ 1.75762953 -1.24756613]\n",
      "Gradient Descent(8958/9): loss=19.10490287652447, w0=74.73574559845625, w1=14.89850680929329\n",
      "[ 1.84831193 -0.7606589 ]\n",
      "Gradient Descent(8959/9): loss=17.43180424860543, w0=73.44192724754424, w1=15.430968037906212\n",
      "[ 1.82146711  0.19891718]\n",
      "Gradient Descent(8960/9): loss=17.300539859125948, w0=72.1669002708602, w1=15.291726014442508\n",
      "[-2.52812166  3.25821779]\n",
      "Gradient Descent(8961/9): loss=17.662673466240506, w0=73.93658543077227, w1=13.010973564772456\n",
      "[ 1.00326343  0.44333506]\n",
      "Gradient Descent(8962/9): loss=15.702254074328428, w0=73.23430103273851, w1=12.7006390260356\n",
      "[ 0.96989068  1.15729615]\n",
      "Gradient Descent(8963/9): loss=15.691142887092685, w0=72.55537755738388, w1=11.890531719645882\n",
      "[ 1.66005203 -1.32055153]\n",
      "Gradient Descent(8964/9): loss=16.921359490253064, w0=71.39334113417573, w1=12.814917790086044\n",
      "[-1.47162399 -0.184225  ]\n",
      "Gradient Descent(8965/9): loss=17.41296764654501, w0=72.42347792772422, w1=12.943875288049647\n",
      "[-4.15015184 -0.85069745]\n",
      "Gradient Descent(8966/9): loss=15.90828503616195, w0=75.32858421729554, w1=13.53936350322459\n",
      "[ 2.51616508 -2.25857877]\n",
      "Gradient Descent(8967/9): loss=17.457592158761877, w0=73.56726866180784, w1=15.120368644568174\n",
      "[ 1.89999126  3.07544623]\n",
      "Gradient Descent(8968/9): loss=16.769123466029974, w0=72.23727477933281, w1=12.967556285252105\n",
      "[-1.21699844 -1.56151602]\n",
      "Gradient Descent(8969/9): loss=16.07529150638238, w0=73.08917368553648, w1=14.060617501457195\n",
      "[ 2.19953762 -0.723331  ]\n",
      "Gradient Descent(8970/9): loss=15.57557415352244, w0=71.54949734977363, w1=14.56694920435009\n",
      "[-0.40227614 -0.79671208]\n",
      "Gradient Descent(8971/9): loss=17.49843844898576, w0=71.83109064885271, w1=15.124647663284373\n",
      "[-2.4824604 -0.0881086]\n",
      "Gradient Descent(8972/9): loss=17.80873160550214, w0=73.56881292878604, w1=15.18632368243941\n",
      "[ 2.24530944  0.24920403]\n",
      "Gradient Descent(8973/9): loss=16.87993135457717, w0=71.99709631998105, w1=15.011880863781606\n",
      "[-0.99486956 -0.48424093]\n",
      "Gradient Descent(8974/9): loss=17.400536340832094, w0=72.69350501532998, w1=15.35084951524303\n",
      "[-0.11920357  0.79304109]\n",
      "Gradient Descent(8975/9): loss=17.31671513438421, w0=72.77694751128261, w1=14.795720754879179\n",
      "[ 2.44639408  0.79349664]\n",
      "Gradient Descent(8976/9): loss=16.385458129920053, w0=71.06447165734664, w1=14.240273107663732\n",
      "[-3.51330222 -0.46526156]\n",
      "Gradient Descent(8977/9): loss=18.160338557111082, w0=73.52378321329529, w1=14.56595619717038\n",
      "[ 0.51542311 -2.37440161]\n",
      "Gradient Descent(8978/9): loss=16.002268712473217, w0=73.16298703880607, w1=16.22803732275762\n",
      "[-0.82557307  5.04438359]\n",
      "Gradient Descent(8979/9): loss=19.17110469550053, w0=73.74088818679614, w1=12.696968810995319\n",
      "[-0.49532844 -1.4247995 ]\n",
      "Gradient Descent(8980/9): loss=15.792121044409411, w0=74.08761809587057, w1=13.69432846154517\n",
      "[ 1.9079746  -0.29872693]\n",
      "Gradient Descent(8981/9): loss=15.72389463288598, w0=72.75203587474226, w1=13.903437313492821\n",
      "[-3.65139327  2.92769935]\n",
      "Gradient Descent(8982/9): loss=15.622479542675118, w0=75.30801116689892, w1=11.854047767097304\n",
      "[ 1.55353805 -2.62818992]\n",
      "Gradient Descent(8983/9): loss=18.73555825691495, w0=74.2205345345087, w1=13.693780709740036\n",
      "[-2.83557704 -0.7999449 ]\n",
      "Gradient Descent(8984/9): loss=15.838105874560457, w0=76.20543846418707, w1=14.253742139076051\n",
      "[ 2.32772612 -0.97501526]\n",
      "Gradient Descent(8985/9): loss=19.923912914720795, w0=74.57603018264433, w1=14.936252824538641\n",
      "[-1.10584801 -0.4755724 ]\n",
      "Gradient Descent(8986/9): loss=17.268543515326733, w0=75.35012378776797, w1=15.269153504784615\n",
      "[ 0.89502012  1.95265845]\n",
      "Gradient Descent(8987/9): loss=19.10092043164635, w0=74.72360970296246, w1=13.90229258824171\n",
      "[ 3.33004409  1.56132541]\n",
      "Gradient Descent(8988/9): loss=16.4971783227822, w0=72.39257884264457, w1=12.80936480256647\n",
      "[-0.61514444 -0.56523905]\n",
      "Gradient Descent(8989/9): loss=16.016780588529905, w0=72.82317994765562, w1=13.205032137763748\n",
      "[-0.92879457 -3.36419527]\n",
      "Gradient Descent(8990/9): loss=15.53441154258499, w0=73.47333614919518, w1=15.559968826518716\n",
      "[-1.88924623 -0.4543887 ]\n",
      "Gradient Descent(8991/9): loss=17.56571591416743, w0=74.79580851119336, w1=15.878040914489898\n",
      "[ 0.22856201  4.08061032]\n",
      "Gradient Descent(8992/9): loss=19.389709159712346, w0=74.63581510623345, w1=13.021613687480885\n",
      "[ 1.40337495  0.15262533]\n",
      "Gradient Descent(8993/9): loss=16.39115365151717, w0=73.65345264378306, w1=12.914775953023318\n",
      "[-0.8419477  -0.74128369]\n",
      "Gradient Descent(8994/9): loss=15.610095624309958, w0=74.2428160318341, w1=13.433674532865048\n",
      "[ 0.14260751 -0.0638867 ]\n",
      "Gradient Descent(8995/9): loss=15.83714755287297, w0=74.1429907729747, w1=13.478395221463927\n",
      "[ 1.12119468 -1.74960511]\n",
      "Gradient Descent(8996/9): loss=15.746347625188067, w0=73.35815449862667, w1=14.703118799121258\n",
      "[-0.97834256  0.46183288]\n",
      "Gradient Descent(8997/9): loss=16.136312341533685, w0=74.04299429046155, w1=14.379835784914228\n",
      "[-0.9062168   0.51693474]\n",
      "Gradient Descent(8998/9): loss=16.071553537961382, w0=74.677346048598, w1=14.017981468728689\n",
      "[ 1.96065818 -0.19929727]\n",
      "Gradient Descent(8999/9): loss=16.487685691378168, w0=73.30488532574688, w1=14.157489557783826\n",
      "[ 1.36791806  0.88134734]\n",
      "Gradient Descent(9000/9): loss=15.61563888015402, w0=72.34734268692323, w1=13.540546422008479\n",
      "[ 1.20002276 -0.0715179 ]\n",
      "Gradient Descent(9001/9): loss=15.835744455782915, w0=71.50732675832717, w1=13.590608948762759\n",
      "[-3.5756684   0.25404724]\n",
      "Gradient Descent(9002/9): loss=16.98799816975811, w0=74.0102946366008, w1=13.41277588176503\n",
      "[-1.67844727 -2.14453483]\n",
      "Gradient Descent(9003/9): loss=15.64472299563525, w0=75.18520772622549, w1=14.913950264749287\n",
      "[ 2.95207314  5.11476869]\n",
      "Gradient Descent(9004/9): loss=18.202887790117693, w0=73.11875652757286, w1=11.33361218515783\n",
      "[ 1.36418851  0.3084736 ]\n",
      "Gradient Descent(9005/9): loss=17.704102481726274, w0=72.16382456856009, w1=11.117680662042194\n",
      "[ 1.03683827 -1.25700784]\n",
      "Gradient Descent(9006/9): loss=18.814045021687246, w0=71.43803778044821, w1=11.997586149007494\n",
      "[ 0.13636669 -2.0685098 ]\n",
      "Gradient Descent(9007/9): loss=18.206390154725796, w0=71.3425810980784, w1=13.445543005656234\n",
      "[-4.76279505  0.94373959]\n",
      "Gradient Descent(9008/9): loss=17.290337305643913, w0=74.67653763031292, w1=12.784925294140548\n",
      "[ 0.70506173 -2.85171639]\n",
      "Gradient Descent(9009/9): loss=16.583065442055748, w0=74.18299441972563, w1=14.78112676414293\n",
      "[ 1.48186327  2.2846889 ]\n",
      "Gradient Descent(9010/9): loss=16.62795237877965, w0=73.1456901322032, w1=13.181844536043526\n",
      "[-0.80284582  1.36749378]\n",
      "Gradient Descent(9011/9): loss=15.441236855067832, w0=73.70768220545276, w1=12.224598892894575\n",
      "[ 3.60676295 -3.7094961 ]\n",
      "Gradient Descent(9012/9): loss=16.25914162354098, w0=71.18294814025396, w1=14.821246164430589\n",
      "[-5.02646971  5.75907499]\n",
      "Gradient Descent(9013/9): loss=18.513849565153627, w0=74.70147693800928, w1=10.789893674654785\n",
      "[ 4.479403   -2.55278198]\n",
      "Gradient Descent(9014/9): loss=19.994055799346466, w0=71.56589483591574, w1=12.576841063656786\n",
      "[-1.11161405  0.98864576]\n",
      "Gradient Descent(9015/9): loss=17.28651516895946, w0=72.34402466921225, w1=11.884789033216524\n",
      "[-3.13711941 -1.98672685]\n",
      "Gradient Descent(9016/9): loss=17.108930669108783, w0=74.54000825297877, w1=13.275497828181965\n",
      "[-0.37824749  2.32105015]\n",
      "Gradient Descent(9017/9): loss=16.18310514395417, w0=74.80478149430502, w1=11.650762721926085\n",
      "[ 2.35908865  2.32615725]\n",
      "Gradient Descent(9018/9): loss=18.199764597871113, w0=73.15341943880175, w1=10.022452646086007\n",
      "[ 3.18646972 -6.2783333 ]\n",
      "Gradient Descent(9019/9): loss=21.372080977959772, w0=70.9228906351842, w1=14.417285958402694\n",
      "[-1.33928866  2.18214309]\n",
      "Gradient Descent(9020/9): loss=18.63630479619414, w0=71.86039269453876, w1=12.889785793754408\n",
      "[-4.54481352 -1.92026508]\n",
      "Gradient Descent(9021/9): loss=16.587397727674535, w0=75.04176216014567, w1=14.233971352998838\n",
      "[ 2.81620658 -1.72939426]\n",
      "Gradient Descent(9022/9): loss=17.197813735557528, w0=73.0704175561815, w1=15.444547332655823\n",
      "[-1.78300494  2.92784378]\n",
      "Gradient Descent(9023/9): loss=17.34115307504783, w0=74.31852101568863, w1=13.395056685088132\n",
      "[ 3.00993376 -3.24636305]\n",
      "Gradient Descent(9024/9): loss=15.914372736143125, w0=72.21156738295532, w1=15.667510820011215\n",
      "[ 2.4910215   1.26964717]\n",
      "Gradient Descent(9025/9): loss=18.36486451637973, w0=70.4678523357662, w1=14.77875780311959\n",
      "[-0.96810623  3.07967073]\n",
      "Gradient Descent(9026/9): loss=20.222982182560802, w0=71.14552669837397, w1=12.622988295419109\n",
      "[-1.31000173 -1.17146464]\n",
      "Gradient Descent(9027/9): loss=18.060677185037505, w0=72.06252790679218, w1=13.443013540401427\n",
      "[-0.82969278  0.93351784]\n",
      "Gradient Descent(9028/9): loss=16.144726982247246, w0=72.64331285058685, w1=12.789551050300268\n",
      "[-0.08813312 -1.12006482]\n",
      "Gradient Descent(9029/9): loss=15.835695371306874, w0=72.70500603426352, w1=13.573596423352917\n",
      "[ 3.48503135 -0.97379734]\n",
      "Gradient Descent(9030/9): loss=15.563705979054397, w0=70.26548408807808, w1=14.255254563090086\n",
      "[-1.81561419  0.40536213]\n",
      "Gradient Descent(9031/9): loss=20.272338764617572, w0=71.53641402063776, w1=13.971501073218041\n",
      "[-1.20928047  3.27588662]\n",
      "Gradient Descent(9032/9): loss=17.051233053635816, w0=72.38291035170698, w1=11.678380439654639\n",
      "[-3.0377929  -2.08069784]\n",
      "Gradient Descent(9033/9): loss=17.423257461117753, w0=74.50936538048815, w1=13.134868924927847\n",
      "[ 5.45560723 -2.90926803]\n",
      "Gradient Descent(9034/9): loss=16.183997695072556, w0=70.69044032156977, w1=15.171356542447391\n",
      "[-2.23001926  4.16365728]\n",
      "Gradient Descent(9035/9): loss=20.205776192420444, w0=72.25145380481844, w1=12.2567964472386\n",
      "[ 1.55817424 -1.56410422]\n",
      "Gradient Descent(9036/9): loss=16.67701959655437, w0=71.16073183435479, w1=13.351669397744502\n",
      "[-2.44294228  0.88948992]\n",
      "Gradient Descent(9037/9): loss=17.669335524416287, w0=72.87079142987314, w1=12.729026456567743\n",
      "[-3.14080404 -1.27385633]\n",
      "Gradient Descent(9038/9): loss=15.757172328507286, w0=75.06935425437469, w1=13.620725884838018\n",
      "[ 1.64320027  0.55558689]\n",
      "Gradient Descent(9039/9): loss=16.971910106547924, w0=73.91911406210271, w1=13.23181506254791\n",
      "[ 1.02520318  0.49389465]\n",
      "Gradient Descent(9040/9): loss=15.612046978402981, w0=73.20147183949028, w1=12.886088808366564\n",
      "[-1.2181896   1.80821042]\n",
      "Gradient Descent(9041/9): loss=15.566355890155377, w0=74.05420456125773, w1=11.620341517511635\n",
      "[ 0.01924712 -2.73427651]\n",
      "Gradient Descent(9042/9): loss=17.40353275808557, w0=74.04073157835413, w1=13.534335075443883\n",
      "[-0.43552229  1.10660156]\n",
      "Gradient Descent(9043/9): loss=15.666241956843086, w0=74.34559718334657, w1=12.759713980470503\n",
      "[ 0.38922893 -2.2992129 ]\n",
      "Gradient Descent(9044/9): loss=16.198097099503492, w0=74.0731369314547, w1=14.369163009488707\n",
      "[ 0.14762177  1.07246035]\n",
      "Gradient Descent(9045/9): loss=16.085036984128866, w0=73.9698016922978, w1=13.61844076778526\n",
      "[-1.10507123 -1.07244225]\n",
      "Gradient Descent(9046/9): loss=15.623917321797041, w0=74.74335155269672, w1=14.369150342267348\n",
      "[-0.89858735  0.83962637]\n",
      "Gradient Descent(9047/9): loss=16.831860775345184, w0=75.3723626972804, w1=13.78141188668718\n",
      "[ 3.97179639  1.36339584]\n",
      "Gradient Descent(9048/9): loss=17.59135701008707, w0=72.59210522228568, w1=12.827034797087764\n",
      "[ 1.71656067 -0.56173484]\n",
      "Gradient Descent(9049/9): loss=15.845155314555711, w0=71.39051275629582, w1=13.220249183505334\n",
      "[-2.78055119 -0.75378379]\n",
      "Gradient Descent(9050/9): loss=17.231031836780947, w0=73.33689858693295, w1=13.747897838040222\n",
      "[-1.08139809 -0.95731249]\n",
      "Gradient Descent(9051/9): loss=15.422773067455989, w0=74.09387724711166, w1=14.418016583791518\n",
      "[ 2.06070883  1.75107187]\n",
      "Gradient Descent(9052/9): loss=16.146059403666047, w0=72.65138106526479, w1=13.192266272562534\n",
      "[-1.01320156  2.58022433]\n",
      "Gradient Descent(9053/9): loss=15.633629944734135, w0=73.36062215743367, w1=11.386109241012601\n",
      "[-0.88073821 -3.29539049]\n",
      "Gradient Descent(9054/9): loss=17.579699491104012, w0=73.97713890188521, w1=13.692882584070558\n",
      "[ 1.16644505 -2.86227262]\n",
      "Gradient Descent(9055/9): loss=15.64200129113163, w0=73.16062736770999, w1=15.696473419882295\n",
      "[-1.60439502  2.23357584]\n",
      "Gradient Descent(9056/9): loss=17.851786230681114, w0=74.28370388031941, w1=14.132970334363685\n",
      "[ 3.36434038 -3.45527265]\n",
      "Gradient Descent(9057/9): loss=16.089094893597714, w0=71.92866561311364, w1=16.55166118602222\n",
      "[-3.9168145   1.72751088]\n",
      "Gradient Descent(9058/9): loss=21.036284937157664, w0=74.6704357612526, w1=15.342403570280258\n",
      "[ 5.28636656  3.47062548]\n",
      "Gradient Descent(9059/9): loss=18.068092066136696, w0=70.9699791703191, w1=12.912965735494168\n",
      "[-1.39225237 -2.41959756]\n",
      "Gradient Descent(9060/9): loss=18.246843922228525, w0=71.94455582967873, w1=14.60668403009768\n",
      "[-0.44037696 -2.21049092]\n",
      "Gradient Descent(9061/9): loss=16.93131489056466, w0=72.2528197041993, w1=16.154027671365757\n",
      "[-2.8042658   6.35965508]\n",
      "Gradient Descent(9062/9): loss=19.50381585794017, w0=74.2158057640802, w1=11.702269114923535\n",
      "[ 2.67490353 -1.81286591]\n",
      "Gradient Descent(9063/9): loss=17.390475082148765, w0=72.34337329611448, w1=12.971275253526438\n",
      "[ 1.11786829 -2.17575709]\n",
      "Gradient Descent(9064/9): loss=15.96691347380653, w0=71.56086549012872, w1=14.494305218355883\n",
      "[-2.80622456  3.31362743]\n",
      "Gradient Descent(9065/9): loss=17.402329563711458, w0=73.52522268386174, w1=12.174766016480943\n",
      "[ 0.61133911 -2.40147977]\n",
      "Gradient Descent(9066/9): loss=16.264080449108487, w0=73.0972853082855, w1=13.8558018588455\n",
      "[-0.47755299  0.53314363]\n",
      "Gradient Descent(9067/9): loss=15.475942490875907, w0=73.43157240041164, w1=13.48260132052799\n",
      "[ 1.48980002  0.38984691]\n",
      "Gradient Descent(9068/9): loss=15.39536585773619, w0=72.38871238551539, w1=13.209708483769775\n",
      "[-0.70547295 -3.87101255]\n",
      "Gradient Descent(9069/9): loss=15.832041160649736, w0=72.88254345374664, w1=15.91941726904129\n",
      "[ 5.46127659  0.94846588]\n",
      "Gradient Descent(9070/9): loss=18.446583862503136, w0=69.05964984134978, w1=15.255491150792471\n",
      "[-5.48795666 -2.07760694]\n",
      "Gradient Descent(9071/9): loss=25.92711325825377, w0=72.90121950509365, w1=16.70981600604165\n",
      "[ 0.75609644  2.88184447]\n",
      "Gradient Descent(9072/9): loss=20.679780034272333, w0=72.37195199644925, w1=14.692524876678412\n",
      "[-2.01601515  1.0767268 ]\n",
      "Gradient Descent(9073/9): loss=16.54635922385226, w0=73.78316260170259, w1=13.938816120041754\n",
      "[-0.73080034  2.83736331]\n",
      "Gradient Descent(9074/9): loss=15.610954147791096, w0=74.29472283692887, w1=11.952661799641547\n",
      "[-0.12303569  0.32674576]\n",
      "Gradient Descent(9075/9): loss=17.052630845778896, w0=74.38084781643286, w1=11.723939770561428\n",
      "[ 4.09946471 -4.64332961]\n",
      "Gradient Descent(9076/9): loss=17.517960556330973, w0=71.51122251847022, w1=14.974270500737084\n",
      "[ 0.99735259  2.7029125 ]\n",
      "Gradient Descent(9077/9): loss=18.091748499251853, w0=70.81307570502995, w1=13.082231749931054\n",
      "[-4.2301016 -0.9121021]\n",
      "Gradient Descent(9078/9): loss=18.54218249118246, w0=73.77414682673893, w1=13.720703220636919\n",
      "[-2.12845247 -0.58412082]\n",
      "Gradient Descent(9079/9): loss=15.530234089310243, w0=75.2640635550607, w1=14.129587793971835\n",
      "[ 2.89622213 -1.86807825]\n",
      "Gradient Descent(9080/9): loss=17.537785729276877, w0=73.23670806413872, w1=15.437242568189053\n",
      "[ 3.10167377  3.75338921]\n",
      "Gradient Descent(9081/9): loss=17.303486697371227, w0=71.06553642757213, w1=12.809870120557266\n",
      "[-1.84744258 -0.90937467]\n",
      "Gradient Descent(9082/9): loss=18.093083366324585, w0=72.35874623161584, w1=13.446432392233557\n",
      "[-0.66223863  2.12997359]\n",
      "Gradient Descent(9083/9): loss=15.82371851030748, w0=72.82231326971004, w1=11.955450876307625\n",
      "[-0.07411462 -0.87136141]\n",
      "Gradient Descent(9084/9): loss=16.658781916702043, w0=72.8741935030358, w1=12.565403859816746\n",
      "[ 0.56575798 -1.70909671]\n",
      "Gradient Descent(9085/9): loss=15.891953960611724, w0=72.47816291536405, w1=13.761771555055459\n",
      "[ 1.12944214 -0.41570813]\n",
      "Gradient Descent(9086/9): loss=15.758397986236087, w0=71.68755341411348, w1=14.0527672448958\n",
      "[-5.53957839 -0.38190095]\n",
      "Gradient Descent(9087/9): loss=16.840293796651274, w0=75.56525828658624, w1=14.320097910251029\n",
      "[ 3.26124656  1.79178481]\n",
      "Gradient Descent(9088/9): loss=18.31849600094515, w0=73.28238569358041, w1=13.065848544126899\n",
      "[-1.23979555 -0.07005926]\n",
      "Gradient Descent(9089/9): loss=15.471596072116366, w0=74.1502425818404, w1=13.114890027098578\n",
      "[ 3.56033665 -0.8014614 ]\n",
      "Gradient Descent(9090/9): loss=15.819078031117925, w0=71.6580069248606, w1=13.675913004686269\n",
      "[-1.3493598   1.34587344]\n",
      "Gradient Descent(9091/9): loss=16.74324427058224, w0=72.60255878388256, w1=12.733801595867556\n",
      "[-0.62006297  0.78968155]\n",
      "Gradient Descent(9092/9): loss=15.903070908544443, w0=73.03660285954659, w1=12.181024509074467\n",
      "[-0.39508    -0.87408656]\n",
      "Gradient Descent(9093/9): loss=16.262289603851105, w0=73.31315885998205, w1=12.792885102760183\n",
      "[-0.27472753  0.68006834]\n",
      "Gradient Descent(9094/9): loss=15.6219387893282, w0=73.50546813130386, w1=12.31683726678176\n",
      "[-0.60475305  1.26991966]\n",
      "Gradient Descent(9095/9): loss=16.084403079635436, w0=73.92879526444987, w1=11.427893506409937\n",
      "[ 0.02767102 -2.81608972]\n",
      "Gradient Descent(9096/9): loss=17.69240035628726, w0=73.90942555178141, w1=13.39915631151002\n",
      "[-0.15083076 -2.11613659]\n",
      "Gradient Descent(9097/9): loss=15.578554823176397, w0=74.01500708585733, w1=14.880451927455919\n",
      "[-1.71547796  1.24542878]\n",
      "Gradient Descent(9098/9): loss=16.62690528071249, w0=75.21584166127079, w1=14.00865177859588\n",
      "[ 3.45224542 -1.71246834]\n",
      "Gradient Descent(9099/9): loss=17.372663871580617, w0=72.79926986501955, w1=15.207379613649904\n",
      "[ 0.62744575  2.37317228]\n",
      "Gradient Descent(9100/9): loss=17.000645177302076, w0=72.36005784264341, w1=13.546159018274992\n",
      "[-4.15019567 -0.33949786]\n",
      "Gradient Descent(9101/9): loss=15.824146577208209, w0=75.26519481417797, w1=13.783807517449745\n",
      "[ 2.99909232  3.14376892]\n",
      "Gradient Descent(9102/9): loss=17.37508302822646, w0=73.16583018831729, w1=11.58316927440319\n",
      "[-0.21958043 -1.308319  ]\n",
      "Gradient Descent(9103/9): loss=17.192529605191638, w0=73.31953649186843, w1=12.498992571687983\n",
      "[-1.99445642 -0.25690745]\n",
      "Gradient Descent(9104/9): loss=15.867121645008947, w0=74.71565598268663, w1=12.67882778866819\n",
      "[ 4.03072786  2.40438278]\n",
      "Gradient Descent(9105/9): loss=16.71725973295561, w0=71.89414648356157, w1=10.995759844919377\n",
      "[-3.70321124 -4.00552913]\n",
      "Gradient Descent(9106/9): loss=19.45058385484334, w0=74.48639435216677, w1=13.799630238617393\n",
      "[ 0.95838467 -0.67916921]\n",
      "Gradient Descent(9107/9): loss=16.148056722199286, w0=73.81552508496306, w1=14.27504868522934\n",
      "[-0.41135656  0.51405018]\n",
      "Gradient Descent(9108/9): loss=15.838202632325958, w0=74.10347467654886, w1=13.915213559502968\n",
      "[ 2.01640684  1.21714326]\n",
      "Gradient Descent(9109/9): loss=15.80840624990529, w0=72.6919898904741, w1=13.063213276633325\n",
      "[ 1.43308202 -3.1317853 ]\n",
      "Gradient Descent(9110/9): loss=15.653784776791241, w0=71.68883247490172, w1=15.255462984977557\n",
      "[-0.2148239  -0.02897628]\n",
      "Gradient Descent(9111/9): loss=18.250689071890786, w0=71.83920920348346, w1=15.275746382840738\n",
      "[-2.51231277  2.21107442]\n",
      "Gradient Descent(9112/9): loss=18.056851502984195, w0=73.59782813913064, w1=13.727994286385298\n",
      "[ 1.62126866 -1.10873312]\n",
      "Gradient Descent(9113/9): loss=15.462889277756641, w0=72.46294007649153, w1=14.504107472306577\n",
      "[-3.0291612   1.57043279]\n",
      "Gradient Descent(9114/9): loss=16.25584594541808, w0=74.58335291801843, w1=13.404804518786356\n",
      "[ 2.04757923  0.98919417]\n",
      "Gradient Descent(9115/9): loss=16.22000951024075, w0=73.15004745893374, w1=12.712368599997765\n",
      "[ 0.67506828 -1.98593617]\n",
      "Gradient Descent(9116/9): loss=15.69064609146536, w0=72.67749966566501, w1=14.102523918903113\n",
      "[-1.23593387 -0.16770486]\n",
      "Gradient Descent(9117/9): loss=15.769823189508205, w0=73.542653373028, w1=14.219917319684326\n",
      "[ 3.65575463  1.28121894]\n",
      "Gradient Descent(9118/9): loss=15.690773151933346, w0=70.9836251335221, w1=13.323064058595198\n",
      "[-2.39837969  0.2524158 ]\n",
      "Gradient Descent(9119/9): loss=18.06689303623524, w0=72.66249091801912, w1=13.146373002086316\n",
      "[-3.18962094 -1.67595899]\n",
      "Gradient Descent(9120/9): loss=15.640798064568411, w0=74.89522557703438, w1=14.319544298419174\n",
      "[ 3.56428867  0.81982216]\n",
      "Gradient Descent(9121/9): loss=17.020633217786163, w0=72.40022351134382, w1=13.74566878435364\n",
      "[ 2.41193369 -0.73922519]\n",
      "Gradient Descent(9122/9): loss=15.820602754907641, w0=70.71186992549197, w1=14.263126419915586\n",
      "[-0.23769208  0.37790593]\n",
      "Gradient Descent(9123/9): loss=19.026253067889968, w0=70.87825437986555, w1=13.99859227028201\n",
      "[-2.91733915  1.76032906]\n",
      "Gradient Descent(9124/9): loss=18.438231041134678, w0=72.92039178273386, w1=12.766361925529349\n",
      "[-2.29130672  0.81839065]\n",
      "Gradient Descent(9125/9): loss=15.710084755894401, w0=74.52430648855378, w1=12.193488467839352\n",
      "[-1.36691372 -0.96200617]\n",
      "Gradient Descent(9126/9): loss=16.969996907911227, w0=75.48114608916133, w1=12.86689278618471\n",
      "[-0.62183567  0.67722231]\n",
      "Gradient Descent(9127/9): loss=17.96563643330902, w0=75.91643105778822, w1=12.392837166956287\n",
      "[ 3.0462936   2.43988988]\n",
      "Gradient Descent(9128/9): loss=19.415313666529787, w0=73.78402553488857, w1=10.68491425020433\n",
      "[-0.56980925  0.01838221]\n",
      "Gradient Descent(9129/9): loss=19.411437052090758, w0=74.18289200781555, w1=10.672046701897417\n",
      "[-0.23467568 -0.01504578]\n",
      "Gradient Descent(9130/9): loss=19.72251513874423, w0=74.34716498410936, w1=10.682578749094763\n",
      "[ 0.43202831 -1.25709188]\n",
      "Gradient Descent(9131/9): loss=19.852526686782195, w0=74.04474516386385, w1=11.562543067196897\n",
      "[-1.96658235  0.14530157]\n",
      "Gradient Descent(9132/9): loss=17.50552477134632, w0=75.42135280585214, w1=11.460831965847794\n",
      "[ 1.59544784 -2.89374861]\n",
      "Gradient Descent(9133/9): loss=19.6868079555351, w0=74.30453931463592, w1=13.486455991645014\n",
      "[ 0.95981962 -0.82512181]\n",
      "Gradient Descent(9134/9): loss=15.896584282801003, w0=73.6326655828641, w1=14.064041258108935\n",
      "[ 2.02279271 -0.91698277]\n",
      "Gradient Descent(9135/9): loss=15.61398156234642, w0=72.21671068339036, w1=14.70592919982711\n",
      "[ 2.08029191  0.90704522]\n",
      "Gradient Descent(9136/9): loss=16.717883758598035, w0=70.76050634468018, w1=14.070997544529462\n",
      "[-5.25633407  1.21972237]\n",
      "Gradient Descent(9137/9): loss=18.769794355854607, w0=74.43994019150281, w1=13.217191882927041\n",
      "[ 0.19044808 -1.96526968]\n",
      "Gradient Descent(9138/9): loss=16.077025234171977, w0=74.30662653563765, w1=14.59288065978404\n",
      "[ 2.81343742  0.4491871 ]\n",
      "Gradient Descent(9139/9): loss=16.51824485329461, w0=72.33722033898297, w1=14.278449690132295\n",
      "[ 0.70588585 -0.20327796]\n",
      "Gradient Descent(9140/9): loss=16.16251750631669, w0=71.84310024329787, w1=14.420744263466098\n",
      "[-2.66630696  5.15687888]\n",
      "Gradient Descent(9141/9): loss=16.88110020784721, w0=73.70951511406625, w1=10.810929045824821\n",
      "[ 4.46372498 -3.74244168]\n",
      "Gradient Descent(9142/9): loss=19.033449075323592, w0=70.58490762603758, w1=13.430638220542559\n",
      "[-3.01841607  1.22743409]\n",
      "Gradient Descent(9143/9): loss=19.05647145296167, w0=72.69779887554147, w1=12.571434354305895\n",
      "[ 0.80167666 -0.23314878]\n",
      "Gradient Descent(9144/9): loss=15.976053795766193, w0=72.13662521025032, w1=12.734638497718313\n",
      "[ 0.25764397 -0.24689111]\n",
      "Gradient Descent(9145/9): loss=16.333123387048246, w0=71.95627443160846, w1=12.90746227730458\n",
      "[-0.25848166 -1.56488124]\n",
      "Gradient Descent(9146/9): loss=16.44427350174225, w0=72.13721159613297, w1=14.002879145421895\n",
      "[-1.86895321  6.64964203]\n",
      "Gradient Descent(9147/9): loss=16.191729053924167, w0=73.44547884113295, w1=9.348129724875543\n",
      "[ 1.53173156 -2.83171782]\n",
      "Gradient Descent(9148/9): loss=23.932360451811874, w0=72.37326675160986, w1=11.33033219950239\n",
      "[-1.2710158  -1.85848694]\n",
      "Gradient Descent(9149/9): loss=18.119608612312046, w0=73.26297780974217, w1=12.631273056084261\n",
      "[ 1.37266427 -4.68627162]\n",
      "Gradient Descent(9150/9): loss=15.74629133018807, w0=72.30211282140631, w1=15.91166319148853\n",
      "[-1.29447112  2.66852551]\n",
      "Gradient Descent(9151/9): loss=18.834922835307896, w0=73.20824260755641, w1=14.043695332008623\n",
      "[-0.70359349 -0.25016538]\n",
      "Gradient Descent(9152/9): loss=15.54859670221982, w0=73.70075804876937, w1=14.21881109781698\n",
      "[-0.48131474  0.35246402]\n",
      "Gradient Descent(9153/9): loss=15.741779069959087, w0=74.03767836375513, w1=13.97208628254512\n",
      "[-0.19159906 -0.35546053]\n",
      "Gradient Descent(9154/9): loss=15.783690634455368, w0=74.17179770759323, w1=14.220908651386328\n",
      "[-0.33539761  1.81476297]\n",
      "Gradient Descent(9155/9): loss=16.04590666157328, w0=74.40657603769827, w1=12.950574574012487\n",
      "[-1.1311029   1.67675649]\n",
      "Gradient Descent(9156/9): loss=16.144880808249614, w0=75.19834806988598, w1=11.776845034445989\n",
      "[ 2.58427665 -3.45212786]\n",
      "Gradient Descent(9157/9): loss=18.649185884567046, w0=73.38935441545159, w1=14.19333453385249\n",
      "[-1.6737526   0.34553921]\n",
      "Gradient Descent(9158/9): loss=15.645069791581088, w0=74.56098123575796, w1=13.95145708797968\n",
      "[ 1.74338799 -2.83747449]\n",
      "Gradient Descent(9159/9): loss=16.2998789284344, w0=73.34060964501154, w1=15.937689228223169\n",
      "[-1.724603    2.00537646]\n",
      "Gradient Descent(9160/9): loss=18.40780269486822, w0=74.54783174155129, w1=14.53392570625331\n",
      "[ 1.86939648  1.61068761]\n",
      "Gradient Descent(9161/9): loss=16.72771549682314, w0=73.23925420560468, w1=13.40644437872503\n",
      "[ 1.01240738  0.78374389]\n",
      "Gradient Descent(9162/9): loss=15.390066256850865, w0=72.53056903788588, w1=12.857823653445308\n",
      "[-2.49893193 -0.65874765]\n",
      "Gradient Descent(9163/9): loss=15.870614571125579, w0=74.2798213869289, w1=13.318947007118217\n",
      "[ 1.38325139 -0.69960241]\n",
      "Gradient Descent(9164/9): loss=15.884809428726532, w0=73.31154541217298, w1=13.808668697032113\n",
      "[ 1.77425822  0.69623981]\n",
      "Gradient Descent(9165/9): loss=15.440149272289279, w0=72.06956465941118, w1=13.32130082787755\n",
      "[-3.06681082  2.30218873]\n",
      "Gradient Descent(9166/9): loss=16.147960438767583, w0=74.2163322348979, w1=11.709768718087686\n",
      "[-2.70169408 -1.75042461]\n",
      "Gradient Descent(9167/9): loss=17.377658568109062, w0=76.10751809390689, w1=12.935065941993289\n",
      "[ 3.24190325 -2.52547821]\n",
      "Gradient Descent(9168/9): loss=19.492369253896594, w0=73.83818581730385, w1=14.702900689415797\n",
      "[ 1.5251327   0.13582233]\n",
      "Gradient Descent(9169/9): loss=16.28209417198048, w0=72.77059292811357, w1=14.6078250574341\n",
      "[-0.39438664 -2.45421668]\n",
      "Gradient Descent(9170/9): loss=16.15914357313179, w0=73.04666357476844, w1=16.325776736163128\n",
      "[-0.99485465  3.00661396]\n",
      "Gradient Descent(9171/9): loss=19.46649723698267, w0=73.74306183208847, w1=14.221146962236698\n",
      "[ 1.85967253  3.6290465 ]\n",
      "Gradient Descent(9172/9): loss=15.76161374136558, w0=72.44129106113445, w1=11.680814415654547\n",
      "[-0.52560008 -0.28859754]\n",
      "Gradient Descent(9173/9): loss=17.36739467156252, w0=72.80921111696667, w1=11.882832693304698\n",
      "[-2.12627646 -1.03505631]\n",
      "Gradient Descent(9174/9): loss=16.778372644616226, w0=74.29760463608018, w1=12.607372107823156\n",
      "[ 1.14700685 -0.99229522]\n",
      "Gradient Descent(9175/9): loss=16.270066106900835, w0=73.49469984269446, w1=13.301978764295466\n",
      "[-1.34731578 -0.69361853]\n",
      "Gradient Descent(9176/9): loss=15.421838368314353, w0=74.43782088768023, w1=13.787511734239635\n",
      "[ 2.51230655  0.59276057]\n",
      "Gradient Descent(9177/9): loss=16.08751040334889, w0=72.67920630381276, w1=13.37257933208297\n",
      "[-2.64069    -1.61110824]\n",
      "Gradient Descent(9178/9): loss=15.580564314562118, w0=74.52768930684705, w1=14.500355098813358\n",
      "[-0.77885243  4.17664357]\n",
      "Gradient Descent(9179/9): loss=16.66783447356359, w0=75.07288600540355, w1=11.576704601997932\n",
      "[ 1.88988272 -1.96254269]\n",
      "Gradient Descent(9180/9): loss=18.778963737557834, w0=73.74996809921839, w1=12.950484485969207\n",
      "[-0.45020875 -2.95466697]\n",
      "Gradient Descent(9181/9): loss=15.629918001187365, w0=74.065114227617, w1=15.018751364545881\n",
      "[ 1.44767389  3.69507554]\n",
      "Gradient Descent(9182/9): loss=16.86757700652005, w0=73.05174250344555, w1=12.432198485927005\n",
      "[-0.50967644 -1.85001094]\n",
      "Gradient Descent(9183/9): loss=15.963856060354697, w0=73.40851601413195, w1=13.727206143907418\n",
      "[-0.79524757  1.44887233]\n",
      "Gradient Descent(9184/9): loss=15.42308033060268, w0=73.9651893149873, w1=12.71299551615417\n",
      "[-0.37089409 -0.74029546]\n",
      "Gradient Descent(9185/9): loss=15.905115188315207, w0=74.22481517755847, w1=13.231202335499855\n",
      "[-0.35304624  0.63107878]\n",
      "Gradient Descent(9186/9): loss=15.85004755565621, w0=74.4719475459253, w1=12.789447187145536\n",
      "[ 0.76052325 -2.60616074]\n",
      "Gradient Descent(9187/9): loss=16.31799301596597, w0=73.93958127155781, w1=14.613759706462153\n",
      "[-1.47125091  1.38429435]\n",
      "Gradient Descent(9188/9): loss=16.237357421912243, w0=74.96945690980867, w1=13.64475366052971\n",
      "[-0.25143743 -0.06692567]\n",
      "Gradient Descent(9189/9): loss=16.803215785359836, w0=75.14546311273095, w1=13.691601626973567\n",
      "[ 0.09981225  0.6737474 ]\n",
      "Gradient Descent(9190/9): loss=17.12243862583797, w0=75.07559454036286, w1=13.219978446554133\n",
      "[ 5.59679029 -0.28292274]\n",
      "Gradient Descent(9191/9): loss=17.006797257994315, w0=71.15784133431227, w1=13.418024364737576\n",
      "[-2.72949019 -1.39485433]\n",
      "Gradient Descent(9192/9): loss=17.669210887494405, w0=73.06848446930609, w1=14.394422396779428\n",
      "[ 0.46148132  0.69221775]\n",
      "Gradient Descent(9193/9): loss=15.829646066525953, w0=72.74544754674872, w1=13.9098699700592\n",
      "[-1.24131598  1.21299802]\n",
      "Gradient Descent(9194/9): loss=15.628817735407502, w0=73.61436872972239, w1=13.060771353761716\n",
      "[ 3.12431925  2.16367241]\n",
      "Gradient Descent(9195/9): loss=15.52498673621965, w0=71.42734525601352, w1=11.546200670151437\n",
      "[-0.30471417 -1.77473616]\n",
      "Gradient Descent(9196/9): loss=18.997176115737197, w0=71.6406451771698, w1=12.788515983835627\n",
      "[ 0.64143152  0.79683721]\n",
      "Gradient Descent(9197/9): loss=16.99142626580716, w0=71.19164311506636, w1=12.230729934098616\n",
      "[-4.61133759 -3.01743686]\n",
      "Gradient Descent(9198/9): loss=18.375654772039276, w0=74.41957942829319, w1=14.342935735796797\n",
      "[ 2.55470795  1.4337001 ]\n",
      "Gradient Descent(9199/9): loss=16.39201742292421, w0=72.63128386384443, w1=13.339345667357335\n",
      "[ 0.09659613 -0.90014408]\n",
      "Gradient Descent(9200/9): loss=15.615283934695926, w0=72.56366656995469, w1=13.969446524085674\n",
      "[-2.67401955  0.02693299]\n",
      "Gradient Descent(9201/9): loss=15.772444105933706, w0=74.43548025250989, w1=13.950593429338442\n",
      "[ 1.13953706  1.87770069]\n",
      "Gradient Descent(9202/9): loss=16.148329943782656, w0=73.6378043113737, w1=12.63620294962861\n",
      "[ 0.36719939  0.69455462]\n",
      "Gradient Descent(9203/9): loss=15.80076951608984, w0=73.38076473915207, w1=12.1500147157599\n",
      "[ 2.87142267 -5.15384457]\n",
      "Gradient Descent(9204/9): loss=16.273706711579898, w0=71.37076886817222, w1=15.757705914708785\n",
      "[-4.41327935  1.28562034]\n",
      "Gradient Descent(9205/9): loss=19.829774003930325, w0=74.46006441367345, w1=14.857771675401903\n",
      "[ 0.64844576  2.24620243]\n",
      "Gradient Descent(9206/9): loss=17.015355565902144, w0=74.00615237907232, w1=13.28542997587426\n",
      "[-3.37146104 -1.73157646]\n",
      "Gradient Descent(9207/9): loss=15.658396760726612, w0=76.36617510938805, w1=14.497533499908624\n",
      "[ 2.09405757  1.80554869]\n",
      "Gradient Descent(9208/9): loss=20.62323730653101, w0=74.90033481235952, w1=13.233649417198619\n",
      "[ 1.47962189 -1.37224976]\n",
      "Gradient Descent(9209/9): loss=16.70644243166607, w0=73.86459948678501, w1=14.194224246150098\n",
      "[ 1.76974686  3.55304571]\n",
      "Gradient Descent(9210/9): loss=15.803987828733964, w0=72.62577668187966, w1=11.70709224571066\n",
      "[-0.01823428 -1.65156156]\n",
      "Gradient Descent(9211/9): loss=17.18018812101771, w0=72.63854067934638, w1=12.863185336003593\n",
      "[-1.90544041 -0.53823767]\n",
      "Gradient Descent(9212/9): loss=15.790703039831657, w0=73.97234896605922, w1=13.239951701564554\n",
      "[-0.53735114 -0.59462944]\n",
      "Gradient Descent(9213/9): loss=15.644762046185464, w0=74.34849476745475, w1=13.656192308590489\n",
      "[ 2.90237002 -1.47924582]\n",
      "Gradient Descent(9214/9): loss=15.957522300431103, w0=72.31683575177134, w1=14.69166438514921\n",
      "[-1.38938352  0.06347852]\n",
      "Gradient Descent(9215/9): loss=16.597650403873647, w0=73.28940421867254, w1=14.647229421584424\n",
      "[ 0.52638546  1.85927868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9216/9): loss=16.067446031007346, w0=72.92093439993009, w1=13.345734344842219\n",
      "[-1.36728129 -2.43952055]\n",
      "Gradient Descent(9217/9): loss=15.464422808837263, w0=73.87803130015388, w1=15.053398728572056\n",
      "[ 2.85097804  4.21278611]\n",
      "Gradient Descent(9218/9): loss=16.794723980168378, w0=71.88234666960724, w1=12.104448452667352\n",
      "[-0.64585761 -2.43692654]\n",
      "Gradient Descent(9219/9): loss=17.327835839023404, w0=72.33444699667665, w1=13.810297032732231\n",
      "[-4.55508406  2.26808808]\n",
      "Gradient Descent(9220/9): loss=15.900827099982964, w0=75.52300583833099, w1=12.222635379736868\n",
      "[ 3.09411008 -3.77539624]\n",
      "Gradient Descent(9221/9): loss=18.660416604711713, w0=73.35712878472204, w1=14.865412746203573\n",
      "[-0.65790127 -0.05083229]\n",
      "Gradient Descent(9222/9): loss=16.347968093763804, w0=73.81765967442978, w1=14.900995347054922\n",
      "[ 1.26288024  2.47116667]\n",
      "Gradient Descent(9223/9): loss=16.533061001600615, w0=72.9336435061562, w1=13.171178674943727\n",
      "[-0.74222242 -2.58693294]\n",
      "Gradient Descent(9224/9): loss=15.498384706694885, w0=73.45319920358874, w1=14.98203173294314\n",
      "[ 4.85040538  3.14160591]\n",
      "Gradient Descent(9225/9): loss=16.527054118788254, w0=70.05791544035081, w1=12.782907598592894\n",
      "[-1.95679817 -1.42503685]\n",
      "Gradient Descent(9226/9): loss=20.864525592700627, w0=71.42767415848942, w1=13.780433395368915\n",
      "[-4.42461042  1.63969257]\n",
      "Gradient Descent(9227/9): loss=17.172544923735543, w0=74.5249014515886, w1=12.632648597051908\n",
      "[ 1.82501608  2.83672415]\n",
      "Gradient Descent(9228/9): loss=16.502301644125044, w0=73.24739019538045, w1=10.64694169445615\n",
      "[-2.58685414 -1.63006703]\n",
      "Gradient Descent(9229/9): loss=19.399265507557594, w0=75.05818809575027, w1=11.787988612663488\n",
      "[-1.64082138 -1.30995072]\n",
      "Gradient Descent(9230/9): loss=18.373170038934138, w0=76.20676306051915, w1=12.704954115538579\n",
      "[ 6.32886717 -4.76088946]\n",
      "Gradient Descent(9231/9): loss=19.928334611399347, w0=71.7765560415192, w1=16.037576738125786\n",
      "[-3.26614417  0.1325088 ]\n",
      "Gradient Descent(9232/9): loss=19.808422494632516, w0=74.06285695856066, w1=15.944820576455463\n",
      "[ 0.32601014  1.79773428]\n",
      "Gradient Descent(9233/9): loss=18.71989742702099, w0=73.83464985831489, w1=14.686406578204261\n",
      "[-1.02179496 -1.00711255]\n",
      "Gradient Descent(9234/9): loss=16.260136553704918, w0=74.54990633068513, w1=15.391385362285968\n",
      "[-2.19214004  3.64625365]\n",
      "Gradient Descent(9235/9): loss=18.00188287612859, w0=76.08440436069576, w1=12.83900780428134\n",
      "[ 6.31183792 -1.83856703]\n",
      "Gradient Descent(9236/9): loss=19.484534977537145, w0=71.6661178156508, w1=14.12600472305529\n",
      "[-2.22090471  0.33369344]\n",
      "Gradient Descent(9237/9): loss=16.91960796435558, w0=73.2207511152818, w1=13.892419314688864\n",
      "[ 0.63406895  2.1296848 ]\n",
      "Gradient Descent(9238/9): loss=15.473728342444439, w0=72.77690284706787, w1=12.401639957533435\n",
      "[-1.65248202 -2.16899189]\n",
      "Gradient Descent(9239/9): loss=16.100662405490795, w0=73.93364025946666, w1=13.91993427813717\n",
      "[-1.04506442 -0.9231567 ]\n",
      "Gradient Descent(9240/9): loss=15.68740522882256, w0=74.66518535655177, w1=14.566143965347614\n",
      "[ 1.78281143 -0.54287726]\n",
      "Gradient Descent(9241/9): loss=16.916236197532076, w0=73.41721735782922, w1=14.946158045027664\n",
      "[ 0.38844068  4.59510615]\n",
      "Gradient Descent(9242/9): loss=16.46872010480172, w0=73.14530888235446, w1=11.729583738774448\n",
      "[-2.42262672 -0.71507361]\n",
      "Gradient Descent(9243/9): loss=16.928406025167334, w0=74.84114758544395, w1=12.230135268728759\n",
      "[ 2.75925125 -0.35682902]\n",
      "Gradient Descent(9244/9): loss=17.363562918917935, w0=72.90967171118116, w1=12.479915585555576\n",
      "[ 2.13350907  0.23683496]\n",
      "Gradient Descent(9245/9): loss=15.95950888193555, w0=71.4162153630274, w1=12.31413111158908\n",
      "[-1.94699821  2.9931886 ]\n",
      "Gradient Descent(9246/9): loss=17.8280688907772, w0=72.77911410909843, w1=10.218899092443037\n",
      "[-5.56754421 -5.67125951]\n",
      "Gradient Descent(9247/9): loss=20.834853279643475, w0=76.6763950551623, w1=14.18878075257854\n",
      "[ 2.12755222  4.37098299]\n",
      "Gradient Descent(9248/9): loss=21.357838785662747, w0=75.18710850195174, w1=11.129092662130935\n",
      "[ 3.15441765 -1.02996266]\n",
      "Gradient Descent(9249/9): loss=19.94067208870579, w0=72.97901614596637, w1=11.850066525679656\n",
      "[-3.82986176 -1.69947824]\n",
      "Gradient Descent(9250/9): loss=16.763343612809077, w0=75.659919376332, w1=13.039701291587235\n",
      "[ 3.18982533 -2.53946208]\n",
      "Gradient Descent(9251/9): loss=18.281664559412363, w0=73.4270416485595, w1=14.817324750482284\n",
      "[-0.15570941 -0.80448386]\n",
      "Gradient Descent(9252/9): loss=16.289351642245048, w0=73.53603823470077, w1=15.380463453299521\n",
      "[-0.07116854 -0.06173823]\n",
      "Gradient Descent(9253/9): loss=17.22162522067669, w0=73.58585621340333, w1=15.423680214312734\n",
      "[-3.77358205  2.70807392]\n",
      "Gradient Descent(9254/9): loss=17.318006024216867, w0=76.22736364899475, w1=13.528028466925107\n",
      "[ 3.92445331 -0.96073058]\n",
      "Gradient Descent(9255/9): loss=19.689595036153502, w0=73.48024633244874, w1=14.200539872990845\n",
      "[ 0.14523806  2.01252319]\n",
      "Gradient Descent(9256/9): loss=15.663042344556505, w0=73.37857968700914, w1=12.79177364157146\n",
      "[ 2.69617609 -1.49438767]\n",
      "Gradient Descent(9257/9): loss=15.626101222380472, w0=71.49125642389308, w1=13.837845010492387\n",
      "[-1.15454541  0.59375113]\n",
      "Gradient Descent(9258/9): loss=17.074818933083133, w0=72.29943820771311, w1=13.422219218258455\n",
      "[-1.52089011  0.25397215]\n",
      "Gradient Descent(9259/9): loss=15.882039612468652, w0=73.36406128715842, w1=13.244438710372041\n",
      "[ 1.41705624 -0.30846114]\n",
      "Gradient Descent(9260/9): loss=15.41602449123087, w0=72.37212191730228, w1=13.460361505337557\n",
      "[-2.01105344 -0.79581522]\n",
      "Gradient Descent(9261/9): loss=15.810932796239914, w0=73.77985932419368, w1=14.017432161471262\n",
      "[ 0.99375559  1.91956967]\n",
      "Gradient Descent(9262/9): loss=15.648526661452719, w0=73.08423041130754, w1=12.673733394884982\n",
      "[-0.26634698 -2.00028571]\n",
      "Gradient Descent(9263/9): loss=15.732674256998559, w0=73.27067329847641, w1=14.073933391060253\n",
      "[ 1.97506956 -1.55757487]\n",
      "Gradient Descent(9264/9): loss=15.562707392256698, w0=71.8881246085201, w1=15.16423580267746\n",
      "[-3.49749692  1.2365271 ]\n",
      "Gradient Descent(9265/9): loss=17.79283051287887, w0=74.33637245590987, w1=14.298666829623548\n",
      "[ 4.15257746  0.65466202]\n",
      "Gradient Descent(9266/9): loss=16.264582493393767, w0=71.42956823299095, w1=13.84040341448717\n",
      "[-1.77811915 -0.93521667]\n",
      "Gradient Descent(9267/9): loss=17.1888443483803, w0=72.67425164132771, w1=14.495055082576416\n",
      "[-3.98141733  1.79623573]\n",
      "Gradient Descent(9268/9): loss=16.093343892847304, w0=75.46124377229617, w1=13.237690073466684\n",
      "[-0.05077268  0.65944708]\n",
      "Gradient Descent(9269/9): loss=17.76381710833972, w0=75.49678465046131, w1=12.776077119312053\n",
      "[ 2.24202525 -3.61979644]\n",
      "Gradient Descent(9270/9): loss=18.059741121324596, w0=73.92736697313444, w1=15.309934627626888\n",
      "[ 0.40348518  0.44647359]\n",
      "Gradient Descent(9271/9): loss=17.261370771702552, w0=73.64492734378368, w1=14.99740311284737\n",
      "[ 2.54972425  5.74234757]\n",
      "Gradient Descent(9272/9): loss=16.59918274060184, w0=71.86012036647294, w1=10.97775981669107\n",
      "[-1.14275418 -1.76321144]\n",
      "Gradient Descent(9273/9): loss=19.543664886104306, w0=72.66004828941826, w1=12.212007822778979\n",
      "[-0.58422725 -1.05000457]\n",
      "Gradient Descent(9274/9): loss=16.3903233025565, w0=73.06900736410907, w1=12.947011020605864\n",
      "[-4.191926   -1.43502165]\n",
      "Gradient Descent(9275/9): loss=15.553066564464784, w0=76.00335556582678, w1=13.951526177691271\n",
      "[ 0.66313876  2.31452445]\n",
      "Gradient Descent(9276/9): loss=19.167707090841276, w0=75.53915843453485, w1=12.331359063338336\n",
      "[ 0.93387893  2.80008465]\n",
      "Gradient Descent(9277/9): loss=18.565788920674926, w0=74.8854431820341, w1=10.37129980716623\n",
      "[ 0.70867055 -2.86766807]\n",
      "Gradient Descent(9278/9): loss=21.48347223431474, w0=74.38937379435534, w1=12.378667456371057\n",
      "[ 2.0782606   0.67255597]\n",
      "Gradient Descent(9279/9): loss=16.592045205871376, w0=72.93459137473347, w1=11.907878278371683\n",
      "[ 0.8367164   0.44864316]\n",
      "Gradient Descent(9280/9): loss=16.68577842666759, w0=72.34888989529252, w1=11.593828069074892\n",
      "[-0.59154387 -3.16406095]\n",
      "Gradient Descent(9281/9): loss=17.610710631082522, w0=72.7629706040563, w1=13.80867073618221\n",
      "[-0.28730144 -0.84475731]\n",
      "Gradient Descent(9282/9): loss=15.580949344336378, w0=72.96408161002051, w1=14.40000085557905\n",
      "[-3.34808838  3.0607929 ]\n",
      "Gradient Descent(9283/9): loss=15.863750599490706, w0=75.30774347428108, w1=12.257445823524284\n",
      "[ 0.75423314 -2.10054012]\n",
      "Gradient Descent(9284/9): loss=18.16059416447852, w0=74.77978027279262, w1=13.727823906487446\n",
      "[ 2.06505502 -0.6828075 ]\n",
      "Gradient Descent(9285/9): loss=16.520554920259073, w0=73.33424176190464, w1=14.205789158655111\n",
      "[ 0.65456259  1.25604842]\n",
      "Gradient Descent(9286/9): loss=15.650294414669366, w0=72.87604795031142, w1=13.326555266254223\n",
      "[ 2.33510961  2.34062556]\n",
      "Gradient Descent(9287/9): loss=15.484925789578108, w0=71.24147122611346, w1=11.6881173725487\n",
      "[ 0.34836189 -3.82769158]\n",
      "Gradient Descent(9288/9): loss=19.09707139664424, w0=70.99761790447327, w1=14.367501481483027\n",
      "[-3.34558786  1.434915  ]\n",
      "Gradient Descent(9289/9): loss=18.41647881876731, w0=73.33952940981385, w1=13.363060978894287\n",
      "[ 1.48405624  1.15133204]\n",
      "Gradient Descent(9290/9): loss=15.393731667752867, w0=72.30069004524204, w1=12.557128548185995\n",
      "[-0.6072335  -3.79017833]\n",
      "Gradient Descent(9291/9): loss=16.30472324299082, w0=72.72575349413728, w1=15.210253376983047\n",
      "[-0.84952525  0.10972476]\n",
      "Gradient Descent(9292/9): loss=17.04468157151138, w0=73.3204211675961, w1=15.133446042343767\n",
      "[ 1.41468728 -0.28240641]\n",
      "Gradient Descent(9293/9): loss=16.75365639376248, w0=72.33014007146542, w1=15.33113052657082\n",
      "[-2.62646041  0.48194357]\n",
      "Gradient Descent(9294/9): loss=17.564200148661406, w0=74.16866235998575, w1=14.993770029895288\n",
      "[ 2.89604032 -0.65922316]\n",
      "Gradient Descent(9295/9): loss=16.914658416028537, w0=72.1414341342789, w1=15.455226241523983\n",
      "[ 1.91932197  2.3023458 ]\n",
      "Gradient Descent(9296/9): loss=18.001329411477872, w0=70.79790875178294, w1=13.843584182198361\n",
      "[-4.79859422  5.22399856]\n",
      "Gradient Descent(9297/9): loss=18.567130265930096, w0=74.15692470640258, w1=10.186785189791673\n",
      "[ 2.94588404 -3.29283422]\n",
      "Gradient Descent(9298/9): loss=21.179959623723295, w0=72.09480587869236, w1=12.491769145860783\n",
      "[-2.15155012 -0.40131134]\n",
      "Gradient Descent(9299/9): loss=16.5928435788105, w0=73.60089096017921, w1=12.77268708164967\n",
      "[-0.94996759 -0.36722285]\n",
      "Gradient Descent(9300/9): loss=15.682945264572263, w0=74.2658682757125, w1=13.029743074000688\n",
      "[ 3.65035834  0.20778279]\n",
      "Gradient Descent(9301/9): loss=15.959463861133111, w0=71.71061743766664, w1=12.884295123315853\n",
      "[ 2.12226108 -1.70242598]\n",
      "Gradient Descent(9302/9): loss=16.81657542823544, w0=70.22503468198491, w1=14.075993311660307\n",
      "[-0.48886291 -2.23118059]\n",
      "Gradient Descent(9303/9): loss=20.27269800256884, w0=70.56723871792826, w1=15.637819724109534\n",
      "[-3.19032643  2.11153935]\n",
      "Gradient Descent(9304/9): loss=21.432002270611825, w0=72.80046722141374, w1=14.159742177805592\n",
      "[-1.19332822 -1.30678672]\n",
      "Gradient Descent(9305/9): loss=15.73885690468059, w0=73.63579697660282, w1=15.074492881672484\n",
      "[ 2.06352942  0.64174335]\n",
      "Gradient Descent(9306/9): loss=16.715989454485186, w0=72.19132638362628, w1=14.625272535166953\n",
      "[-1.69705599  1.30614517]\n",
      "Gradient Descent(9307/9): loss=16.649900389333656, w0=73.3792655788582, w1=13.71097091899149\n",
      "[ 0.38070159 -1.32497165]\n",
      "Gradient Descent(9308/9): loss=15.416269875087453, w0=73.11277446433036, w1=14.63845107706384\n",
      "[ 2.37856946  3.81474237]\n",
      "Gradient Descent(9309/9): loss=16.073632704369007, w0=71.44777584340866, w1=11.968131420931005\n",
      "[ 0.45978009 -0.29138412]\n",
      "Gradient Descent(9310/9): loss=18.232454269494735, w0=71.12592977908442, w1=12.17210030149225\n",
      "[-2.4271746  -5.56798684]\n",
      "Gradient Descent(9311/9): loss=18.5909077542027, w0=72.82495199600231, w1=16.069691086867007\n",
      "[-1.12157437 -0.28039512]\n",
      "Gradient Descent(9312/9): loss=18.84984901073325, w0=73.61005405771216, w1=16.26596766939311\n",
      "[ 0.83004214 -0.38750826]\n",
      "Gradient Descent(9313/9): loss=19.317466722742562, w0=73.0290245602433, w1=16.5372234533454\n",
      "[-2.93656462  9.39160936]\n",
      "Gradient Descent(9314/9): loss=20.09516000986714, w0=75.08461979443224, w1=9.963096898445997\n",
      "[ 2.20589515 -5.43980413]\n",
      "Gradient Descent(9315/9): loss=23.172479576479887, w0=73.5404931911097, w1=13.770959790651547\n",
      "[-0.48952455  0.27875822]\n",
      "Gradient Descent(9316/9): loss=15.458699055543146, w0=73.88316037894218, w1=13.575829038696268\n",
      "[-0.40225287 -2.9716033 ]\n",
      "Gradient Descent(9317/9): loss=15.56410800195225, w0=74.16473739132827, w1=15.65595135112306\n",
      "[ 2.22229754  3.96933986]\n",
      "Gradient Descent(9318/9): loss=18.133055499931345, w0=72.60912911615071, w1=12.877413448444454\n",
      "[ 1.12411477 -0.27097068]\n",
      "Gradient Descent(9319/9): loss=15.801740551752655, w0=71.82224877929158, w1=13.067092924606115\n",
      "[ 0.27956419 -2.83234497]\n",
      "Gradient Descent(9320/9): loss=16.553926336377014, w0=71.62655384938236, w1=15.049734404174036\n",
      "[-2.10100984  0.72722292]\n",
      "Gradient Descent(9321/9): loss=18.008430639048324, w0=73.09726073977924, w1=14.540678363487531\n",
      "[ 1.0560468   1.96522019]\n",
      "Gradient Descent(9322/9): loss=15.96805004559654, w0=72.3580279810204, w1=13.165024233721196\n",
      "[ 0.31356511 -2.11019971]\n",
      "Gradient Descent(9323/9): loss=15.873351010189127, w0=72.13853240703946, w1=14.642164028283704\n",
      "[-0.25740665  3.4970207 ]\n",
      "Gradient Descent(9324/9): loss=16.728997280399113, w0=72.31871706073315, w1=12.194249535624115\n",
      "[ 0.62537429 -1.86659754]\n",
      "Gradient Descent(9325/9): loss=16.68760764048947, w0=71.88095505679469, w1=13.500867810834972\n",
      "[-1.08480835 -0.26695755]\n",
      "Gradient Descent(9326/9): loss=16.384349438063033, w0=72.64032090451403, w1=13.687738094929635\n",
      "[-2.51136772  0.06466006]\n",
      "Gradient Descent(9327/9): loss=15.621122403812443, w0=74.3982783065262, w1=13.642476055952663\n",
      "[ 3.89796385  0.09385972]\n",
      "Gradient Descent(9328/9): loss=16.008935290541203, w0=71.66970360841616, w1=13.576774249595426\n",
      "[-2.95824193  0.71259016]\n",
      "Gradient Descent(9329/9): loss=16.709641061955633, w0=73.74047295592601, w1=13.07796113437551\n",
      "[ 2.23065416  0.24818951]\n",
      "Gradient Descent(9330/9): loss=15.56629379978088, w0=72.17901504088003, w1=12.904228480330454\n",
      "[ 4.16146932 -0.96118899]\n",
      "Gradient Descent(9331/9): loss=16.172987525958305, w0=69.26598651394264, w1=13.577060773732526\n",
      "[-8.23705635 -0.46603581]\n",
      "Gradient Descent(9332/9): loss=23.50275836675711, w0=75.0319259571759, w1=13.90328583893885\n",
      "[ 2.21054664  2.38236148]\n",
      "Gradient Descent(9333/9): loss=16.985923957016926, w0=73.48454330969659, w1=12.235632801322772\n",
      "[-2.68464316 -1.4237731 ]\n",
      "Gradient Descent(9334/9): loss=16.177923177734936, w0=75.36379352342543, w1=13.232273973458591\n",
      "[ 4.89234172  1.58977502]\n",
      "Gradient Descent(9335/9): loss=17.558684822337963, w0=71.93915431946995, w1=12.119431456072707\n",
      "[-1.18082891 -4.53100835]\n",
      "Gradient Descent(9336/9): loss=17.22876777658663, w0=72.76573455832163, w1=15.291137300680376\n",
      "[ 1.96749981 -0.15645891]\n",
      "Gradient Descent(9337/9): loss=17.16600887873713, w0=71.38848468830271, w1=15.400658536947553\n",
      "[ 0.42686084  5.70049177]\n",
      "Gradient Descent(9338/9): loss=19.046250510559595, w0=71.08968209979906, w1=11.410314294457013\n",
      "[-2.44449789  0.45676302]\n",
      "Gradient Descent(9339/9): loss=19.956428974307382, w0=72.80083062229043, w1=11.0905801809581\n",
      "[-1.99305466 -1.58409913]\n",
      "Gradient Descent(9340/9): loss=18.36143388687871, w0=74.19596888339031, w1=12.199449569293812\n",
      "[ 3.50112795  0.70496914]\n",
      "Gradient Descent(9341/9): loss=16.612268659486595, w0=71.74517931553939, w1=11.705971172711575\n",
      "[-0.708506   -1.81179643]\n",
      "Gradient Descent(9342/9): loss=18.158268856177767, w0=72.24113351554608, w1=12.974228671720061\n",
      "[ 2.01561865 -0.73932172]\n",
      "Gradient Descent(9343/9): loss=16.067826585009396, w0=70.83020046147665, w1=13.491753877692362\n",
      "[-2.48573479  1.13054998]\n",
      "Gradient Descent(9344/9): loss=18.42092228187912, w0=72.57021481348555, w1=12.700368890333124\n",
      "[-0.36459326 -3.56574201]\n",
      "Gradient Descent(9345/9): loss=15.951452096557704, w0=72.82543009215233, w1=15.196388299902596\n",
      "[-3.3552984   0.76667684]\n",
      "Gradient Descent(9346/9): loss=16.96911821626338, w0=75.17413897117311, w1=14.659714512343081\n",
      "[ 0.39701419  3.12905812]\n",
      "Gradient Descent(9347/9): loss=17.84969824549479, w0=74.89622904077923, w1=12.46937383174727\n",
      "[ 4.06217011  2.89789076]\n",
      "Gradient Descent(9348/9): loss=17.179973838521867, w0=72.05270996642612, w1=10.440850297174478\n",
      "[ 1.83368272 -0.783682  ]\n",
      "Gradient Descent(9349/9): loss=20.77353307390816, w0=70.76913206314872, w1=10.989427700486987\n",
      "[-2.29698609 -0.56666162]\n",
      "Gradient Descent(9350/9): loss=21.67392901620431, w0=72.37702232953559, w1=11.386090836314125\n",
      "[-0.67743605 -2.96160885]\n",
      "Gradient Descent(9351/9): loss=17.99786607282759, w0=72.85122756410718, w1=13.459217028995305\n",
      "[-0.02446523 -0.97009366]\n",
      "Gradient Descent(9352/9): loss=15.484087082380013, w0=72.86835322530564, w1=14.138282590918585\n",
      "[ 1.83093472 -0.80017184]\n",
      "Gradient Descent(9353/9): loss=15.693299585863262, w0=71.58669891830044, w1=14.698402880735662\n",
      "[-2.55369663  4.28964274]\n",
      "Gradient Descent(9354/9): loss=17.585796399044337, w0=73.37428655678339, w1=11.69565296113675\n",
      "[-3.3085113  -1.72468355]\n",
      "Gradient Descent(9355/9): loss=16.980551202774784, w0=75.69024447013186, w1=12.90293144485471\n",
      "[ 3.18478791 -0.35342144]\n",
      "Gradient Descent(9356/9): loss=18.42340670950429, w0=73.46089292994375, w1=13.150326453436346\n",
      "[ 0.60692054  0.12449707]\n",
      "Gradient Descent(9357/9): loss=15.454075076622754, w0=73.03604855093535, w1=13.063178507430115\n",
      "[ 1.20796877 -2.64121095]\n",
      "Gradient Descent(9358/9): loss=15.505887483642358, w0=72.19047041035233, w1=14.912026172353686\n",
      "[-2.37491596 -0.02014889]\n",
      "Gradient Descent(9359/9): loss=17.020451897622085, w0=73.8529115796898, w1=14.926130398390105\n",
      "[-0.5436063   0.92098527]\n",
      "Gradient Descent(9360/9): loss=16.588185005178143, w0=74.23343599014646, w1=14.281440706243968\n",
      "[ 1.29470929  0.71868106]\n",
      "Gradient Descent(9361/9): loss=16.14861524615671, w0=73.32713948929194, w1=13.778363966727271\n",
      "[ 0.58695698 -2.65282136]\n",
      "Gradient Descent(9362/9): loss=15.431035938261694, w0=72.91626960395729, w1=15.635338917993401\n",
      "[ 1.60629948  5.14347583]\n",
      "Gradient Descent(9363/9): loss=17.780561302857688, w0=71.79185996698206, w1=12.034905836983235\n",
      "[-0.6159923  -0.30401331]\n",
      "Gradient Descent(9364/9): loss=17.557716100329085, w0=72.22305457603834, w1=12.247715151171146\n",
      "[-1.25773451  0.45080362]\n",
      "Gradient Descent(9365/9): loss=16.71817504460227, w0=73.1034687364817, w1=11.932152614720088\n",
      "[ 2.64552456 -2.0035152 ]\n",
      "Gradient Descent(9366/9): loss=16.601494790678174, w0=71.2516015435702, w1=13.334613256881136\n",
      "[-2.96806398 -1.43289802]\n",
      "Gradient Descent(9367/9): loss=17.481951182248473, w0=73.32924633142032, w1=14.337641871344106\n",
      "[-1.54422183  0.93634549]\n",
      "Gradient Descent(9368/9): loss=15.754533231832436, w0=74.41020161389734, w1=13.682200030509174\n",
      "[-0.60232709  0.77418156]\n",
      "Gradient Descent(9369/9): loss=16.02942856785058, w0=74.83183057496765, w1=13.140272935919295\n",
      "[ 1.08140937 -0.92367581]\n",
      "Gradient Descent(9370/9): loss=16.626078844835693, w0=74.07484401572545, w1=13.786846003359006\n",
      "[ 1.75804281 -0.62483309]\n",
      "Gradient Descent(9371/9): loss=15.737972978917592, w0=72.84421405018674, w1=14.224229168358463\n",
      "[ 0.77101089 -0.13297204]\n",
      "Gradient Descent(9372/9): loss=15.764159072972275, w0=72.30450642429477, w1=14.31730959864585\n",
      "[-1.38851151  1.52721172]\n",
      "Gradient Descent(9373/9): loss=16.22614396591932, w0=73.27646448182152, w1=13.248261392699169\n",
      "[ 0.14940866 -2.08355609]\n",
      "Gradient Descent(9374/9): loss=15.412825043825162, w0=73.17187842015532, w1=14.706750652858036\n",
      "[ 1.52899018 -1.01827342]\n",
      "Gradient Descent(9375/9): loss=16.14614658083253, w0=72.10158529715868, w1=15.419542046786441\n",
      "[-0.30102609  1.21115299]\n",
      "Gradient Descent(9376/9): loss=17.97819073921377, w0=72.31230356013685, w1=14.571734951246285\n",
      "[ 0.27871518  0.67579904]\n",
      "Gradient Descent(9377/9): loss=16.463931839641962, w0=72.11720293693283, w1=14.098675625770436\n",
      "[-1.08408909 -0.66110705]\n",
      "Gradient Descent(9378/9): loss=16.2697794637706, w0=72.87606529892636, w1=14.56145056206147\n",
      "[-0.51918409  1.25537334]\n",
      "Gradient Descent(9379/9): loss=16.058268668806214, w0=73.2394941587101, w1=13.682689224000649\n",
      "[-2.07692346  1.40025965]\n",
      "Gradient Descent(9380/9): loss=15.407968852336449, w0=74.69334058302638, w1=12.702507470623791\n",
      "[ 0.1261993  -3.19909237]\n",
      "Gradient Descent(9381/9): loss=16.667097829460147, w0=74.6050010762625, w1=14.9418721288373\n",
      "[ 4.6891427  1.9446472]\n",
      "Gradient Descent(9382/9): loss=17.3143075233332, w0=71.32260118631875, w1=13.580619088048314\n",
      "[-0.84236434 -1.18462127]\n",
      "Gradient Descent(9383/9): loss=17.334031824521666, w0=71.91225622634606, w1=14.409853980443616\n",
      "[-2.30401691  1.63962236]\n",
      "Gradient Descent(9384/9): loss=16.772969674071746, w0=73.52506806311102, w1=13.26211832927122\n",
      "[ 1.96496643 -0.25147147]\n",
      "Gradient Descent(9385/9): loss=15.436275717010227, w0=72.14959156218158, w1=13.438148361753822\n",
      "[-3.6791004   1.83849878]\n",
      "Gradient Descent(9386/9): loss=16.041497732789228, w0=74.72496184020031, w1=12.151199217950989\n",
      "[ 2.96661268 -1.89450516]\n",
      "Gradient Descent(9387/9): loss=17.292299061859463, w0=72.64833296148093, w1=13.477352827317212\n",
      "[-0.76969785 -0.24826256]\n",
      "Gradient Descent(9388/9): loss=15.594283257390655, w0=73.18712145820949, w1=13.651136617027843\n",
      "[ 0.28301108 -0.58584248]\n",
      "Gradient Descent(9389/9): loss=15.406284172011443, w0=72.98901370288115, w1=14.061226350789381\n",
      "[-1.78185696 -0.18435882]\n",
      "Gradient Descent(9390/9): loss=15.601451621431963, w0=74.23631357776111, w1=14.190277527251205\n",
      "[-0.51420574 -0.80608674]\n",
      "Gradient Descent(9391/9): loss=16.082390184933786, w0=74.59625759611629, w1=14.754538245920328\n",
      "[ 0.23485201  1.45533615]\n",
      "Gradient Descent(9392/9): loss=17.046517292651817, w0=74.43186118912479, w1=13.735802939463676\n",
      "[ 2.65616756  2.32990187]\n",
      "Gradient Descent(9393/9): loss=16.066131838747847, w0=72.572543899282, w1=12.104871631684777\n",
      "[-1.95072382 -2.31389879]\n",
      "Gradient Descent(9394/9): loss=16.591174669660962, w0=73.93805057443734, w1=13.724600782442657\n",
      "[-0.29374234 -1.34155302]\n",
      "Gradient Descent(9395/9): loss=15.623323829036009, w0=74.14367021437012, w1=14.663687898304776\n",
      "[ 1.2571637   2.74158412]\n",
      "Gradient Descent(9396/9): loss=16.447822829819966, w0=73.26365562498164, w1=12.744579012634837\n",
      "[ 3.00848923 -1.41683467]\n",
      "Gradient Descent(9397/9): loss=15.6565564699526, w0=71.1577131637595, w1=13.736363282548687\n",
      "[-3.2544128   1.37940184]\n",
      "Gradient Descent(9398/9): loss=17.70051679811907, w0=73.4358021206326, w1=12.7707819920558\n",
      "[-1.23804365 -1.56604014]\n",
      "Gradient Descent(9399/9): loss=15.647244039304843, w0=74.30243267502796, w1=13.867010089488408\n",
      "[ 0.73362797 -0.43196327]\n",
      "Gradient Descent(9400/9): loss=15.969434494119318, w0=73.78889309521132, w1=14.169384379409031\n",
      "[-3.03195579  2.85514323]\n",
      "Gradient Descent(9401/9): loss=15.746209755794759, w0=75.91126215067231, w1=12.170784114981268\n",
      "[ 0.49821208 -0.99149459]\n",
      "Gradient Descent(9402/9): loss=19.66776926893929, w0=75.56251369693736, w1=12.8648303314274\n",
      "[ 1.75518251 -0.18262923]\n",
      "Gradient Descent(9403/9): loss=18.148182008400344, w0=74.33388593784, w1=12.992670795113206\n",
      "[ 1.37915391 -0.12266528]\n",
      "Gradient Descent(9404/9): loss=16.04525514213039, w0=73.36847820195473, w1=13.078536494162954\n",
      "[-0.98100354 -0.68416691]\n",
      "Gradient Descent(9405/9): loss=15.469138250046253, w0=74.05518068271643, w1=13.557453330921144\n",
      "[ 1.45364031  4.47143896]\n",
      "Gradient Descent(9406/9): loss=15.67866708168254, w0=73.03763246882772, w1=10.427446059462344\n",
      "[ 1.65413088 -1.30813963]\n",
      "Gradient Descent(9407/9): loss=20.07689504484865, w0=71.87974084946735, w1=11.343143799179035\n",
      "[-6.83108307 -5.60838466]\n",
      "Gradient Descent(9408/9): loss=18.668304802830967, w0=76.66149899604699, w1=15.269013058075046\n",
      "[ 6.27212387  3.30350325]\n",
      "Gradient Descent(9409/9): loss=22.6569736337804, w0=72.27101228953968, w1=12.956560783517196\n",
      "[-1.09721247 -1.0078578 ]\n",
      "Gradient Descent(9410/9): loss=16.045903834078686, w0=73.03906101848054, w1=13.66206124467825\n",
      "[-0.32179326  1.98918544]\n",
      "Gradient Descent(9411/9): loss=15.434990473513995, w0=73.2643163011017, w1=12.269631439800476\n",
      "[ 2.92341277 -2.08151204]\n",
      "Gradient Descent(9412/9): loss=16.118474125053638, w0=71.21792735896209, w1=13.726689867097344\n",
      "[-3.02553757 -2.09101552]\n",
      "Gradient Descent(9413/9): loss=17.571263673994235, w0=73.33580365495882, w1=15.190400731114742\n",
      "[-0.33133475  2.38093998]\n",
      "Gradient Descent(9414/9): loss=16.84999212850299, w0=73.56773797871111, w1=13.523742743554015\n",
      "[ 1.15888425 -0.7563614 ]\n",
      "Gradient Descent(9415/9): loss=15.424344797387889, w0=72.75651900252088, w1=14.053195726332426\n",
      "[ 0.07648032  1.03254517]\n",
      "Gradient Descent(9416/9): loss=15.694730403535525, w0=72.70298277639947, w1=13.330414104521621\n",
      "[-0.8209634   3.38895439]\n",
      "Gradient Descent(9417/9): loss=15.57163744880842, w0=73.27765715979343, w1=10.958146033132007\n",
      "[-0.26452441 -1.9614582 ]\n",
      "Gradient Descent(9418/9): loss=18.565168700864252, w0=73.4628242460801, w1=12.331166772507427\n",
      "[-5.24714153 -2.69298886]\n",
      "Gradient Descent(9419/9): loss=16.059730422241955, w0=77.13582331750158, w1=14.216258974037954\n",
      "[ 5.83517392  3.37295788]\n",
      "Gradient Descent(9420/9): loss=23.0372411295441, w0=73.05120157641848, w1=11.855188458538658\n",
      "[-0.84876786 -0.98166347]\n",
      "Gradient Descent(9421/9): loss=16.734883546383266, w0=73.6453390801945, w1=12.542352890810697\n",
      "[ 2.32778157  0.3170612 ]\n",
      "Gradient Descent(9422/9): loss=15.886956307746942, w0=72.01589198323475, w1=12.320410053597943\n",
      "[-0.73221119  0.36404206]\n",
      "Gradient Descent(9423/9): loss=16.87455923914593, w0=72.52843981332705, w1=12.065580608538696\n",
      "[-1.50803867 -2.30867489]\n",
      "Gradient Descent(9424/9): loss=16.678753770787583, w0=73.58406688078762, w1=13.681653028792688\n",
      "[-2.70086459  0.63644054]\n",
      "Gradient Descent(9425/9): loss=15.448369895855105, w0=75.47467209077074, w1=13.236144652690921\n",
      "[ 1.80740171 -1.82173191]\n",
      "Gradient Descent(9426/9): loss=17.793385975723616, w0=74.20949089102383, w1=14.511356989947984\n",
      "[ 3.05329953 -0.04318052]\n",
      "Gradient Descent(9427/9): loss=16.33716630789547, w0=72.07218122079348, w1=14.541583353818137\n",
      "[ 0.53508447  1.35961512]\n",
      "Gradient Descent(9428/9): loss=16.69599806131694, w0=71.69762209271886, w1=13.589852770425047\n",
      "[-2.31523999  2.06456532]\n",
      "Gradient Descent(9429/9): loss=16.66604001592778, w0=73.31829008518028, w1=12.144657049890242\n",
      "[ 1.05631027 -2.09198186]\n",
      "Gradient Descent(9430/9): loss=16.27737121120644, w0=72.57887289296085, w1=13.609044353621151\n",
      "[-0.05209177 -1.48091132]\n",
      "Gradient Descent(9431/9): loss=15.649898855661991, w0=72.61533713433509, w1=14.645682276168499\n",
      "[ 0.12815906  1.39288892]\n",
      "Gradient Descent(9432/9): loss=16.2958694154827, w0=72.5256257892467, w1=13.670660035204886\n",
      "[-1.26609855 -1.61340067]\n",
      "Gradient Descent(9433/9): loss=15.699257897189842, w0=73.41189477334828, w1=14.800040504654351\n",
      "[-2.00756055 -0.7249128 ]\n",
      "Gradient Descent(9434/9): loss=16.26447976197984, w0=74.8171871616613, w1=15.30747946464421\n",
      "[ 4.72153938 -2.54262808]\n",
      "Gradient Descent(9435/9): loss=18.216422399335364, w0=71.51210959816349, w1=17.087319119721116\n",
      "[-1.33242493  4.56810048]\n",
      "Gradient Descent(9436/9): loss=23.480728586111404, w0=72.44480705019461, w1=13.889648782813111\n",
      "[ 0.65825546  0.9442042 ]\n",
      "Gradient Descent(9437/9): loss=15.830409874242116, w0=71.98402823132871, w1=13.228705844436757\n",
      "[-0.00909676  0.35224065]\n",
      "Gradient Descent(9438/9): loss=16.275300868439256, w0=71.99039596492746, w1=12.982137386129791\n",
      "[-1.11043006 -1.39149371]\n",
      "Gradient Descent(9439/9): loss=16.359268398253185, w0=72.76769700895008, w1=13.956182984661748\n",
      "[-1.03364415  0.7338719 ]\n",
      "Gradient Descent(9440/9): loss=15.637856332892651, w0=73.4912479141271, w1=13.44247265399411\n",
      "[-0.15576958  0.2279215 ]\n",
      "Gradient Descent(9441/9): loss=15.406050027251313, w0=73.60028661946953, w1=13.282927605926787\n",
      "[ 1.88074853  0.03433582]\n",
      "Gradient Descent(9442/9): loss=15.45217964269033, w0=72.2837626484252, w1=13.258892529030733\n",
      "[-3.00025999 -1.78428283]\n",
      "Gradient Descent(9443/9): loss=15.920479544176711, w0=74.38394464386218, w1=14.507890507387573\n",
      "[-0.37795852 -0.03617558]\n",
      "Gradient Descent(9444/9): loss=16.508537622881413, w0=74.64851560456017, w1=14.533213411900691\n",
      "[ 2.57660788  2.29565356]\n",
      "Gradient Descent(9445/9): loss=16.858281936912277, w0=72.84489008881039, w1=12.926255922787352\n",
      "[ 0.11459472 -1.72716807]\n",
      "Gradient Descent(9446/9): loss=15.639859753857227, w0=72.76467378558289, w1=14.135273571633036\n",
      "[ 1.07445962  0.2239549 ]\n",
      "Gradient Descent(9447/9): loss=15.7408199081144, w0=72.01255205026969, w1=13.978505144323307\n",
      "[ 0.13518665  4.23650179]\n",
      "Gradient Descent(9448/9): loss=16.331239429005365, w0=71.9179213932028, w1=11.012953888391174\n",
      "[-6.01039458 -5.91331494]\n",
      "Gradient Descent(9449/9): loss=19.3750255702861, w0=76.12519760053867, w1=15.152274348607015\n",
      "[ 0.20341151  2.59160319]\n",
      "Gradient Descent(9450/9): loss=20.792680303414524, w0=75.98280954429623, w1=13.338152114357523\n",
      "[ 3.15649636  2.35929823]\n",
      "Gradient Descent(9451/9): loss=19.010965638293143, w0=73.7732620953016, w1=11.686643350487838\n",
      "[-0.06520926 -0.68244405]\n",
      "Gradient Descent(9452/9): loss=17.10831970219918, w0=73.81890857790135, w1=12.164354184106115\n",
      "[ 1.40992023  2.39685302]\n",
      "Gradient Descent(9453/9): loss=16.388776985295397, w0=72.83196441907265, w1=10.486557071818314\n",
      "[ 0.54015803 -4.99660931]\n",
      "Gradient Descent(9454/9): loss=19.972079787128894, w0=72.45385379744762, w1=13.984183588795101\n",
      "[-1.29229496  1.89955972]\n",
      "Gradient Descent(9455/9): loss=15.865990735578901, w0=73.35846026812563, w1=12.654491784783085\n",
      "[ 0.75426033  1.09420787]\n",
      "Gradient Descent(9456/9): loss=15.72846502348304, w0=72.83047803519281, w1=11.888546272704259\n",
      "[-0.04644228 -0.80844109]\n",
      "Gradient Descent(9457/9): loss=16.75918290206319, w0=72.86298762867916, w1=12.454455036271254\n",
      "[ 1.44707629 -0.49536847]\n",
      "Gradient Descent(9458/9): loss=16.004316452742227, w0=71.85003422506048, w1=12.80121296846194\n",
      "[ 1.46280209 -1.00827379]\n",
      "Gradient Descent(9459/9): loss=16.658474588217736, w0=70.82607276109194, w1=13.507004623381366\n",
      "[-3.94326877  0.11768605]\n",
      "Gradient Descent(9460/9): loss=18.431400238787862, w0=73.58636089928213, w1=13.42462438623374\n",
      "[ 5.02125054 -0.82561477]\n",
      "Gradient Descent(9461/9): loss=15.430165469678265, w0=70.07148551934367, w1=14.002554725958818\n",
      "[-4.37277507  1.14888838]\n",
      "Gradient Descent(9462/9): loss=20.714618342158886, w0=73.1324280701318, w1=13.198332858814636\n",
      "[-0.00593356 -1.85116419]\n",
      "Gradient Descent(9463/9): loss=15.43851524680556, w0=73.13658156409296, w1=14.49414778970001\n",
      "[-0.03881635  1.96807043]\n",
      "Gradient Descent(9464/9): loss=15.912805419990114, w0=73.163753012187, w1=13.116498491892164\n",
      "[-2.49898728 -1.64010597]\n",
      "Gradient Descent(9465/9): loss=15.460322036027554, w0=74.91304410641399, w1=14.264572669603357\n",
      "[ 0.30326895  0.45951134]\n",
      "Gradient Descent(9466/9): loss=17.004668857099496, w0=74.70075584301699, w1=13.942914728930312\n",
      "[ 3.53922144  3.37601668]\n",
      "Gradient Descent(9467/9): loss=16.482756779352947, w0=72.22330083779622, w1=11.579703051572107\n",
      "[-1.37490808 -2.27582373]\n",
      "Gradient Descent(9468/9): loss=17.76402053609876, w0=73.18573649520101, w1=13.17277966382237\n",
      "[ 4.94337799 -1.52376532]\n",
      "Gradient Descent(9469/9): loss=15.438843783789483, w0=69.72537190106948, w1=14.239415387274663\n",
      "[-5.77381277 -1.71110346]\n",
      "Gradient Descent(9470/9): loss=22.041737068486132, w0=73.7670408375241, w1=15.437187810710398\n",
      "[ 0.80753771 -0.12252063]\n",
      "Gradient Descent(9471/9): loss=17.413663508321196, w0=73.20176443830387, w1=15.52295225248532\n",
      "[-1.76882769  2.6800263 ]\n",
      "Gradient Descent(9472/9): loss=17.477548853013392, w0=74.43994382010221, w1=13.646933843368\n",
      "[ 0.43380304  0.66590944]\n",
      "Gradient Descent(9473/9): loss=16.056552372202123, w0=74.13628169248562, w1=13.18079723512482\n",
      "[ 0.45330792 -0.34360884]\n",
      "Gradient Descent(9474/9): loss=15.785347941173246, w0=73.8189661457267, w1=13.421323426517253\n",
      "[ 0.68037361 -0.10791562]\n",
      "Gradient Descent(9475/9): loss=15.525428183360185, w0=73.34270461576422, w1=13.496864360866232\n",
      "[ 1.90068241  1.37667979]\n",
      "Gradient Descent(9476/9): loss=15.387224834807753, w0=72.01222692923552, w1=12.53318850913525\n",
      "[-0.90417091 -1.12086622]\n",
      "Gradient Descent(9477/9): loss=16.655212769845427, w0=72.64514656717313, w1=13.317794860342177\n",
      "[-1.55602372  1.32774213]\n",
      "Gradient Descent(9478/9): loss=15.609451301804805, w0=73.7343631723944, w1=12.388375367247207\n",
      "[ 1.24436254 -1.0690252 ]\n",
      "Gradient Descent(9479/9): loss=16.078390378785944, w0=72.86330939563416, w1=13.136693007594097\n",
      "[-1.44730119  0.0442899 ]\n",
      "Gradient Descent(9480/9): loss=15.537432641040468, w0=73.8764202300776, w1=13.105690074763512\n",
      "[ 0.55966165  2.67866944]\n",
      "Gradient Descent(9481/9): loss=15.62548632459924, w0=73.48465707268058, w1=11.230621464144482\n",
      "[-0.69217357 -1.46668688]\n",
      "Gradient Descent(9482/9): loss=17.93328289997038, w0=73.96917857218168, w1=12.25730227818398\n",
      "[-0.32166219 -2.427418  ]\n",
      "Gradient Descent(9483/9): loss=16.36101688227523, w0=74.19434210551853, w1=13.95649487753774\n",
      "[ 1.75114749 -1.64150364]\n",
      "Gradient Descent(9484/9): loss=15.904926798906192, w0=72.96853886397749, w1=15.105547422124804\n",
      "[-1.24263649  2.65885472]\n",
      "Gradient Descent(9485/9): loss=16.76049466481568, w0=73.83838440485654, w1=13.244349118334904\n",
      "[ 1.59539406 -2.33641393]\n",
      "Gradient Descent(9486/9): loss=15.561805468247508, w0=72.72160856572319, w1=14.879838869555822\n",
      "[-3.26461244  3.32365808]\n",
      "Gradient Descent(9487/9): loss=16.52983621994742, w0=75.00683727523656, w1=12.553278213278235\n",
      "[ 1.859704    0.64743845]\n",
      "Gradient Descent(9488/9): loss=17.282067418871225, w0=73.70504447836761, w1=12.100071299812615\n",
      "[ 1.8324191  -3.65661455]\n",
      "Gradient Descent(9489/9): loss=16.42210354500893, w0=72.42235110688216, w1=14.659701484863682\n",
      "[ 1.38010998  2.45707972]\n",
      "Gradient Descent(9490/9): loss=16.46189286044136, w0=71.45627412021715, w1=12.939745681855374\n",
      "[-3.62221852  0.14358823]\n",
      "Gradient Descent(9491/9): loss=17.22014478497807, w0=73.99182708581296, w1=12.839233921616573\n",
      "[ 3.66740831 -0.77422203]\n",
      "Gradient Descent(9492/9): loss=15.834529984807881, w0=71.42464127128778, w1=13.381189342434517\n",
      "[-4.51222557 -0.7797023 ]\n",
      "Gradient Descent(9493/9): loss=17.13784649401528, w0=74.58319917178996, w1=13.926980952030908\n",
      "[ 3.19347987  2.7557204 ]\n",
      "Gradient Descent(9494/9): loss=16.317030242132994, w0=72.34776326261937, w1=11.997976669854218\n",
      "[-2.9098072  -1.93644211]\n",
      "Gradient Descent(9495/9): loss=16.93126648782195, w0=74.38462829924308, w1=13.353486144270317\n",
      "[-0.11001114 -1.52530973]\n",
      "Gradient Descent(9496/9): loss=15.988674520371825, w0=74.46163609862354, w1=14.421202957357808\n",
      "[ 2.71594926  0.92715916]\n",
      "Gradient Descent(9497/9): loss=16.510868176288337, w0=72.56047161547451, w1=13.772191548428143\n",
      "[-1.59728804 -1.24828576]\n",
      "Gradient Descent(9498/9): loss=15.697634619552804, w0=73.67857324065099, w1=14.645991579409865\n",
      "[ 0.09464231  0.35409046]\n",
      "Gradient Descent(9499/9): loss=16.139969677842288, w0=73.61232362235329, w1=14.398128260863096\n",
      "[-0.6854294  -1.87366568]\n",
      "Gradient Descent(9500/9): loss=15.858321479325664, w0=74.09212420136791, w1=15.709694237664708\n",
      "[-1.97749402  3.44322821]\n",
      "Gradient Descent(9501/9): loss=18.19086066441561, w0=75.47637001570908, w1=13.299434490154466\n",
      "[ 1.45412638 -0.52478691]\n",
      "Gradient Descent(9502/9): loss=17.783677603568073, w0=74.4584815511613, w1=13.666785328074974\n",
      "[ 0.74542476  1.6461167 ]\n",
      "Gradient Descent(9503/9): loss=16.081485474142053, w0=73.93668421737476, w1=12.514503637826374\n",
      "[-1.91888191  0.4199805 ]\n",
      "Gradient Descent(9504/9): loss=16.058273512578634, w0=75.27990155641646, w1=12.220517285764423\n",
      "[ 1.612967   -1.16240597]\n",
      "Gradient Descent(9505/9): loss=18.150731475816016, w0=74.15082465570413, w1=13.034201465905326\n",
      "[ 3.54346956 -1.22286672]\n",
      "Gradient Descent(9506/9): loss=15.85226895948881, w0=71.67039596091865, w1=13.890208169477521\n",
      "[-3.53201797  3.89709489]\n",
      "Gradient Descent(9507/9): loss=16.788059646051426, w0=74.14280853990171, w1=11.162241747185227\n",
      "[ 3.76117375 -3.52529611]\n",
      "Gradient Descent(9508/9): loss=18.43152724027044, w0=71.50998691241327, w1=13.629949022381197\n",
      "[-3.12528418 -1.10966779]\n",
      "Gradient Descent(9509/9): loss=16.98838558704208, w0=73.6976858382636, w1=14.406716474837966\n",
      "[-1.38772515  0.35420062]\n",
      "Gradient Descent(9510/9): loss=15.897068731472185, w0=74.6690934460024, w1=14.158776039472547\n",
      "[ 0.38324047  0.80294112]\n",
      "Gradient Descent(9511/9): loss=16.561999808351636, w0=74.40082511579156, w1=13.596717254046194\n",
      "[ 0.86473424 -1.20963068]\n",
      "Gradient Descent(9512/9): loss=16.00535018421499, w0=73.79551114847771, w1=14.443458730986961\n",
      "[ 0.46083088  3.40604581]\n",
      "Gradient Descent(9513/9): loss=15.976087166233608, w0=73.47292952916605, w1=12.05922666534591\n",
      "[-3.29214932 -5.19223482]\n",
      "Gradient Descent(9514/9): loss=16.41079962708095, w0=75.7774340537231, w1=15.69379103588522\n",
      "[ 3.48379916  1.24198185]\n",
      "Gradient Descent(9515/9): loss=20.920875949568263, w0=73.33877464220048, w1=14.824403739646522\n",
      "[ 0.15726729  1.9995782 ]\n",
      "Gradient Descent(9516/9): loss=16.290991100901866, w0=73.2286875363893, w1=13.424699001985648\n",
      "[ 0.51272485  0.06083835]\n",
      "Gradient Descent(9517/9): loss=15.38952887549343, w0=72.86978014341307, w1=13.382112157724237\n",
      "[-2.35458817 -2.14712606]\n",
      "Gradient Descent(9518/9): loss=15.480598934037891, w0=74.51799186262919, w1=14.885100397862944\n",
      "[-0.3393035  -0.53206026]\n",
      "Gradient Descent(9519/9): loss=17.122619043646438, w0=74.75550431394942, w1=15.257542579505483\n",
      "[ 1.70299676  1.41697103]\n",
      "Gradient Descent(9520/9): loss=18.034339307353015, w0=73.56340657953858, w1=14.265662856145324\n",
      "[-2.35824198  2.97355747]\n",
      "Gradient Descent(9521/9): loss=15.731057869824488, w0=75.21417596623398, w1=12.184172628837926\n",
      "[-1.34823129 -1.19832403]\n",
      "Gradient Descent(9522/9): loss=18.068787206866617, w0=76.15793786895826, w1=13.022999451948976\n",
      "[-0.55033005  0.92276961]\n",
      "Gradient Descent(9523/9): loss=19.591474686061147, w0=76.54316890162949, w1=12.377060724797936\n",
      "[ 2.86949036  0.24121208]\n",
      "Gradient Descent(9524/9): loss=21.272610972857223, w0=74.53452565126098, w1=12.208212270266042\n",
      "[ 0.31847615 -1.43394939]\n",
      "Gradient Descent(9525/9): loss=16.96379291042405, w0=74.3115923439215, w1=13.211976846476796\n",
      "[ 1.75014238  0.06694657]\n",
      "Gradient Descent(9526/9): loss=15.939555503813668, w0=73.08649267585905, w1=13.16511424872804\n",
      "[ 0.13458589 -0.34887228]\n",
      "Gradient Descent(9527/9): loss=15.456887340922222, w0=72.99228255230902, w1=13.409324841448713\n",
      "[-0.07360343  0.51160367]\n",
      "Gradient Descent(9528/9): loss=15.433858254328266, w0=73.04380495489639, w1=13.051202269086154\n",
      "[-2.17929196 -1.19045345]\n",
      "Gradient Descent(9529/9): loss=15.508977618622687, w0=74.56930932406146, w1=13.884519683607559\n",
      "[ 1.97505307  1.40800742]\n",
      "Gradient Descent(9530/9): loss=16.281128733599843, w0=73.1867721733465, w1=12.898914486348685\n",
      "[-2.29941918 -1.0441126 ]\n",
      "Gradient Descent(9531/9): loss=15.560291540303334, w0=74.79636560173807, w1=13.629793308623128\n",
      "[-1.70308006  1.39271907]\n",
      "Gradient Descent(9532/9): loss=16.525818388183698, w0=75.98852164688836, w1=12.654889959727258\n",
      "[ 1.63836635  4.10100246]\n",
      "Gradient Descent(9533/9): loss=19.356487549510685, w0=74.84166520436891, w1=9.784188236162478\n",
      "[ 4.88211327 -5.19391785]\n",
      "Gradient Descent(9534/9): loss=23.412091930962564, w0=71.42418591419086, w1=13.419930732997926\n",
      "[ 1.64649863  0.03119689]\n",
      "Gradient Descent(9535/9): loss=17.135631314000474, w0=70.27163687125979, w1=13.39809290810332\n",
      "[-4.68883071 -1.20797945]\n",
      "Gradient Descent(9536/9): loss=19.956322448478534, w0=73.55381836858622, w1=14.243678524471038\n",
      "[ 1.7257236   4.04713536]\n",
      "Gradient Descent(9537/9): loss=15.711483022423623, w0=72.34581185206808, w1=11.41068377529118\n",
      "[-3.04847492 -2.17385733]\n",
      "Gradient Descent(9538/9): loss=17.97578409445667, w0=74.47974429263294, w1=12.932383908212541\n",
      "[ 3.01654428 -2.6641561 ]\n",
      "Gradient Descent(9539/9): loss=16.238759379297314, w0=72.36816329552619, w1=14.797293178348385\n",
      "[-2.62269436  1.14040444]\n",
      "Gradient Descent(9540/9): loss=16.682411967868482, w0=74.20404934584059, w1=13.999010072442733\n",
      "[-0.18797852 -0.71820915]\n",
      "Gradient Descent(9541/9): loss=15.934888777869311, w0=74.33563430737071, w1=14.501756476890819\n",
      "[-1.77318991  0.89387978]\n",
      "Gradient Descent(9542/9): loss=16.450757144093654, w0=75.5768672435236, w1=13.876040628442292\n",
      "[ 3.38475169 -1.11307029]\n",
      "Gradient Descent(9543/9): loss=18.070345374949824, w0=73.20754106318539, w1=14.655189828787393\n",
      "[ 3.24842009  2.94321348]\n",
      "Gradient Descent(9544/9): loss=16.080492253799207, w0=70.933646999386, w1=12.594940389445986\n",
      "[-2.9440949  -2.06017406]\n",
      "Gradient Descent(9545/9): loss=18.562747699347163, w0=72.99451343228671, w1=14.037062232773295\n",
      "[ 0.7920198   2.34207115]\n",
      "Gradient Descent(9546/9): loss=15.586030013214843, w0=72.44009957220653, w1=12.397612425225368\n",
      "[-2.02365332 -3.56132336]\n",
      "Gradient Descent(9547/9): loss=16.335864455293702, w0=73.8566568930544, w1=14.89053877412466\n",
      "[ 0.18123762  1.16543737]\n",
      "Gradient Descent(9548/9): loss=16.53943862717461, w0=73.72979055650349, w1=14.074732618114552\n",
      "[-1.84072812 -2.81926613]\n",
      "Gradient Descent(9549/9): loss=15.657903076349385, w0=75.01830024108304, w1=16.04821891236598\n",
      "[ 2.05115079  0.86313793]\n",
      "Gradient Descent(9550/9): loss=20.171240786523196, w0=73.58249469114995, w1=15.444022363955659\n",
      "[ 0.5381103   4.09121393]\n",
      "Gradient Descent(9551/9): loss=17.356781715779064, w0=73.20581748414071, w1=12.58017261107311\n",
      "[-1.25038757  0.86758056]\n",
      "Gradient Descent(9552/9): loss=15.794355019277637, w0=74.08108878599457, w1=11.97286621620437\n",
      "[ 1.87904808 -1.63454126]\n",
      "Gradient Descent(9553/9): loss=16.830996405191616, w0=72.76575512902849, w1=13.117045097970186\n",
      "[-1.15754445 -2.58459908]\n",
      "Gradient Descent(9554/9): loss=15.591131790407383, w0=73.57603624470727, w1=14.92626445607127\n",
      "[ 0.66948422  2.15299961]\n",
      "Gradient Descent(9555/9): loss=16.4719384666174, w0=73.10739728770491, w1=13.419164727999673\n",
      "[ 1.42254462 -1.11526085]\n",
      "Gradient Descent(9556/9): loss=15.405116615781287, w0=72.11161605710147, w1=14.199847325387422\n",
      "[ 1.04353753  0.08154916]\n",
      "Gradient Descent(9557/9): loss=16.344108672809508, w0=71.38113978932643, w1=14.142762915882722\n",
      "[-0.78428326 -0.30069286]\n",
      "Gradient Descent(9558/9): loss=17.43507373569748, w0=71.93013807241098, w1=14.353247915963644\n",
      "[-1.14271519 -0.72886703]\n",
      "Gradient Descent(9559/9): loss=16.697373290536245, w0=72.73003870442665, w1=14.863454839787526\n",
      "[-1.27168028  2.3951652 ]\n",
      "Gradient Descent(9560/9): loss=16.502241576948553, w0=73.62021489899318, w1=13.186839201359064\n",
      "[-0.2108765  -0.64050777]\n",
      "Gradient Descent(9561/9): loss=15.482008761597623, w0=73.7678284475613, w1=13.635194637054482\n",
      "[ 3.47024147 -1.35801661]\n",
      "Gradient Descent(9562/9): loss=15.510268885931382, w0=71.33865941644058, w1=14.585806265885141\n",
      "[-2.41731555  0.84795007]\n",
      "Gradient Descent(9563/9): loss=17.909135539652528, w0=73.03078030334088, w1=13.992241214365011\n",
      "[-1.94917221  2.5399914 ]\n",
      "Gradient Descent(9564/9): loss=15.55185252048799, w0=74.39520085320696, w1=12.214247231035511\n",
      "[ 3.55209791 -4.34072582]\n",
      "Gradient Descent(9565/9): loss=16.79299651398, w0=71.90873231486943, w1=15.252755306916272\n",
      "[ 2.57785294  2.96221679]\n",
      "Gradient Descent(9566/9): loss=17.917103616487527, w0=70.1042352558934, w1=13.179203556921816\n",
      "[-1.0271938  1.0995028]\n",
      "Gradient Descent(9567/9): loss=20.518091431207587, w0=70.82327091277429, w1=12.409551594085096\n",
      "[-6.0272288  -1.82629032]\n",
      "Gradient Descent(9568/9): loss=19.010568384137507, w0=75.042331071774, w1=13.687954820225103\n",
      "[-0.8427522  -1.73498922]\n",
      "Gradient Descent(9569/9): loss=16.936037451783882, w0=75.63225761048004, w1=14.902447273996803\n",
      "[ 3.99384281 -1.52463145]\n",
      "Gradient Descent(9570/9): loss=19.131881788589496, w0=72.8365676421948, w1=15.969689290302796\n",
      "[-1.35477231  2.27665923]\n",
      "Gradient Descent(9571/9): loss=18.590466744092993, w0=73.78490825718916, w1=14.376027830779083\n",
      "[-0.88390783  0.73057263]\n",
      "Gradient Descent(9572/9): loss=15.908112264535216, w0=74.40364374009354, w1=13.864626992241794\n",
      "[ 1.03398027 -2.97430301]\n",
      "Gradient Descent(9573/9): loss=16.075708644903884, w0=73.67985755249826, w1=15.946639100778178\n",
      "[-1.59035413  7.71221955]\n",
      "Gradient Descent(9574/9): loss=18.50322458054874, w0=74.79310544433818, w1=10.548085416129654\n",
      "[-1.77470311 -4.79146585]\n",
      "Gradient Descent(9575/9): loss=20.8068818544154, w0=76.03539762411829, w1=13.902111514387439\n",
      "[ 2.10174022  1.31851137]\n",
      "Gradient Descent(9576/9): loss=19.23294265301376, w0=74.56417946955332, w1=12.979153551928247\n",
      "[ 2.28313126  1.36492258]\n",
      "Gradient Descent(9577/9): loss=16.317944483338813, w0=72.96598758888815, w1=12.023707743194725\n",
      "[-1.8868313   0.01917149]\n",
      "Gradient Descent(9578/9): loss=16.499633189778944, w0=74.28676949976709, w1=12.010287698181441\n",
      "[ 3.57800925  0.00521161]\n",
      "Gradient Descent(9579/9): loss=16.958365474207206, w0=71.78216302185794, w1=12.006639572123833\n",
      "[-2.18892594 -1.39065246]\n",
      "Gradient Descent(9580/9): loss=17.613567305663462, w0=73.31441118012626, w1=12.98009629401619\n",
      "[ 0.81818373 -0.45216004]\n",
      "Gradient Descent(9581/9): loss=15.510905916197697, w0=72.74168257175563, w1=13.296608320151197\n",
      "[ 0.14714851  0.1714328 ]\n",
      "Gradient Descent(9582/9): loss=15.55513562148108, w0=72.63867861633463, w1=13.176605357262304\n",
      "[-1.3655729 -1.4895687]\n",
      "Gradient Descent(9583/9): loss=15.646496766411454, w0=73.59457964514276, w1=14.219303448400126\n",
      "[ 1.01276069 -0.98495371]\n",
      "Gradient Descent(9584/9): loss=15.704582811547064, w0=72.88564716043025, w1=14.908771048650967\n",
      "[ 0.28971088  1.04165581]\n",
      "Gradient Descent(9585/9): loss=16.490336302642408, w0=72.68284954347526, w1=14.179611983099036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.78377313 -0.46457105]\n",
      "Gradient Descent(9586/9): loss=15.817522332399696, w0=72.13420835473045, w1=14.504811720280916\n",
      "[-2.51607505  1.41412647]\n",
      "Gradient Descent(9587/9): loss=16.58377001313596, w0=73.8954608924998, w1=13.51492318900971\n",
      "[ 1.59631057  0.63791522]\n",
      "Gradient Descent(9588/9): loss=15.56743228575734, w0=72.7780434917329, w1=13.068382537663068\n",
      "[-0.9685276   2.23812298]\n",
      "Gradient Descent(9589/9): loss=15.603549329778467, w0=73.45601281419174, w1=11.501696453124943\n",
      "[-2.77518033 -2.67153568]\n",
      "Gradient Descent(9590/9): loss=17.35529819676575, w0=75.39863904542328, w1=13.371771429461008\n",
      "[ 2.49780254  0.27298781]\n",
      "Gradient Descent(9591/9): loss=17.606630415383428, w0=73.65017726606156, w1=13.180679960103763\n",
      "[-0.93269393 -2.20114439]\n",
      "Gradient Descent(9592/9): loss=15.494056985895723, w0=74.3030630157005, w1=14.72148103074431\n",
      "[-0.41632939  3.83524103]\n",
      "Gradient Descent(9593/9): loss=16.66606528419148, w0=74.59449358534134, w1=12.036812306564315\n",
      "[ 1.47617306 -1.92882063]\n",
      "Gradient Descent(9594/9): loss=17.272611480694152, w0=73.56117244435606, w1=13.386986744805338\n",
      "[-1.72174167  0.99044801]\n",
      "Gradient Descent(9595/9): loss=15.425898295081064, w0=74.76639161040326, w1=12.693673137334132\n",
      "[-0.10726865 -2.11791821]\n",
      "Gradient Descent(9596/9): loss=16.77890013123906, w0=74.841479665545, w1=14.176215882535299\n",
      "[ 0.60560103 -0.88623997]\n",
      "Gradient Descent(9597/9): loss=16.82591375588694, w0=74.41755894154235, w1=14.796583863195753\n",
      "[ 0.31413151  0.5915577 ]\n",
      "Gradient Descent(9598/9): loss=16.88424303387684, w0=74.19766688549866, w1=14.382493472621867\n",
      "[ 3.15203482 -0.5325409 ]\n",
      "Gradient Descent(9599/9): loss=16.201772076914036, w0=71.99124251426349, w1=14.755272104851171\n",
      "[-0.97073396  3.13610466]\n",
      "Gradient Descent(9600/9): loss=17.047901028540544, w0=72.67075628707516, w1=12.559998843272588\n",
      "[-1.87128681 -1.21082027]\n",
      "Gradient Descent(9601/9): loss=16.002992168417833, w0=73.980657057014, w1=13.407573031768491\n",
      "[ 0.46235604 -1.27493443]\n",
      "Gradient Descent(9602/9): loss=15.62429243339821, w0=73.6570078283957, w1=14.30002713615215\n",
      "[-1.01808329 -0.195047  ]\n",
      "Gradient Descent(9603/9): loss=15.788261631928087, w0=74.36966612974608, w1=14.436560035846925\n",
      "[-0.06689816 -0.37569014]\n",
      "Gradient Descent(9604/9): loss=16.422279248540065, w0=74.41649484048052, w1=14.699543133174748\n",
      "[ 0.86901358  2.57733859]\n",
      "Gradient Descent(9605/9): loss=16.759966223676532, w0=73.80818533534287, w1=12.89540611751782\n",
      "[ 0.03854717 -0.09524272]\n",
      "Gradient Descent(9606/9): loss=15.688828193104156, w0=73.78120231377326, w1=12.962076021850017\n",
      "[ 3.55370871  0.01935881]\n",
      "Gradient Descent(9607/9): loss=15.63858264800279, w0=71.29360621849911, w1=12.948524851554495\n",
      "[-2.38960038 -0.83446345]\n",
      "Gradient Descent(9608/9): loss=17.527599610298726, w0=72.96632648660929, w1=13.53264926434028\n",
      "[-0.21761397 -1.05312532]\n",
      "Gradient Descent(9609/9): loss=15.440948433666792, w0=73.11865626682398, w1=14.269836988436454\n",
      "[ 0.20688227  1.58215505]\n",
      "Gradient Descent(9610/9): loss=15.713395312791464, w0=72.97383867809175, w1=13.162328454032927\n",
      "[ 1.69592293 -0.09527166]\n",
      "Gradient Descent(9611/9): loss=15.487480831668922, w0=71.78669262427377, w1=13.229018618679083\n",
      "[-2.16406909  1.49660613]\n",
      "Gradient Descent(9612/9): loss=16.553181762296475, w0=73.30154098560641, w1=12.18139432481037\n",
      "[-1.80456153 -0.98139592]\n",
      "Gradient Descent(9613/9): loss=16.228731850893166, w0=74.5647340561203, w1=12.868371469073727\n",
      "[ 2.35633329 -0.54287955]\n",
      "Gradient Descent(9614/9): loss=16.38023839544759, w0=72.91530074987406, w1=13.248387151577154\n",
      "[ 0.53054131 -0.88236716]\n",
      "Gradient Descent(9615/9): loss=15.48432058852273, w0=72.54392183567217, w1=13.866044160153733\n",
      "[-3.27296441 -0.78418299]\n",
      "Gradient Descent(9616/9): loss=15.741764094588545, w0=74.83499692171601, w1=14.414972256584365\n",
      "[ 1.24359415  0.34785179]\n",
      "Gradient Descent(9617/9): loss=17.010699289701403, w0=73.96448101845046, w1=14.171476001001388\n",
      "[ 0.88309605  2.55389235]\n",
      "Gradient Descent(9618/9): loss=15.84998098166142, w0=73.34631378510856, w1=12.383751355352134\n",
      "[ 1.7181441   2.24855597]\n",
      "Gradient Descent(9619/9): loss=15.987825662331991, w0=72.14361291321119, w1=10.809762173369966\n",
      "[-0.35604877 -3.44721671]\n",
      "Gradient Descent(9620/9): loss=19.611810568585376, w0=72.39284705453169, w1=13.222813867676608\n",
      "[-1.9547897  -0.21074847]\n",
      "Gradient Descent(9621/9): loss=15.82485433634529, w0=73.76119984382444, w1=13.37033779494771\n",
      "[ 1.5668251  2.9224969]\n",
      "Gradient Descent(9622/9): loss=15.50104356545239, w0=72.66442227040427, w1=11.3245899655976\n",
      "[-1.54970046 -1.54032666]\n",
      "Gradient Descent(9623/9): loss=17.90629925397311, w0=73.74921258978077, w1=12.40281862728262\n",
      "[ 1.23354385 -2.30335316]\n",
      "Gradient Descent(9624/9): loss=16.069382764980613, w0=72.8857318958575, w1=14.015165837485386\n",
      "[ 1.46271035 -0.43109163]\n",
      "Gradient Descent(9625/9): loss=15.612552623371101, w0=71.86183464852763, w1=14.316929977992444\n",
      "[-4.97983198 -2.50178669]\n",
      "Gradient Descent(9626/9): loss=16.761791570124107, w0=75.34771703153481, w1=16.068180660366664\n",
      "[ 2.27859755  1.9870828 ]\n",
      "Gradient Descent(9627/9): loss=20.84500875717908, w0=73.75269874575574, w1=14.677222699919167\n",
      "[ 0.27449345  0.95595974]\n",
      "Gradient Descent(9628/9): loss=16.2081413363932, w0=73.56055332766303, w1=14.008050884411901\n",
      "[ 0.49777098  0.16709151]\n",
      "Gradient Descent(9629/9): loss=15.561004759283035, w0=73.21211364305135, w1=13.891086830619184\n",
      "[-1.09422176 -0.2326014 ]\n",
      "Gradient Descent(9630/9): loss=15.47384861932497, w0=73.97806887554643, w1=14.053907809829514\n",
      "[-0.28831058  2.1690753 ]\n",
      "Gradient Descent(9631/9): loss=15.784766505293202, w0=74.17988628243533, w1=12.53555510216638\n",
      "[ 1.7486971 -3.2364617]\n",
      "Gradient Descent(9632/9): loss=16.22407075640126, w0=72.95579830981374, w1=14.801078288872631\n",
      "[-1.1477359   3.09977295]\n",
      "Gradient Descent(9633/9): loss=16.316055544378546, w0=73.75921343661794, w1=12.631237220857727\n",
      "[-1.13535005 -1.8200077 ]\n",
      "Gradient Descent(9634/9): loss=15.854091022842464, w0=74.55395847397556, w1=13.905242611891662\n",
      "[-0.45884604 -0.30503695]\n",
      "Gradient Descent(9635/9): loss=16.27027178977854, w0=74.8751506998194, w1=14.118768479207121\n",
      "[ 3.87568394  0.28432356]\n",
      "Gradient Descent(9636/9): loss=16.840226279892715, w0=72.16217194105032, w1=13.919741988198693\n",
      "[ 1.71413174  1.8927823 ]\n",
      "Gradient Descent(9637/9): loss=16.123129973027194, w0=70.96227972494242, w1=12.594794378399177\n",
      "[-2.08026241 -1.0632613 ]\n",
      "Gradient Descent(9638/9): loss=18.49570570659519, w0=72.41846341135417, w1=13.339077287234145\n",
      "[ 0.38318082 -0.03427922]\n",
      "Gradient Descent(9639/9): loss=15.778990863281303, w0=72.15023683901696, w1=13.363072738511546\n",
      "[ 0.3692061  -2.07080992]\n",
      "Gradient Descent(9640/9): loss=16.046698154360666, w0=71.89179257241669, w1=14.812639682156103\n",
      "[ 0.02607212  0.00223996]\n",
      "Gradient Descent(9641/9): loss=17.25721886074887, w0=71.87354209031618, w1=14.811071709300414\n",
      "[-0.12310941  1.35068402]\n",
      "Gradient Descent(9642/9): loss=17.280886174383724, w0=71.95971867696022, w1=13.865592897781093\n",
      "[-2.37145152 -2.6467149 ]\n",
      "Gradient Descent(9643/9): loss=16.350388991025646, w0=73.61973474336216, w1=15.718293324362378\n",
      "[-1.75772055  1.77264414]\n",
      "Gradient Descent(9644/9): loss=17.944587039145837, w0=74.85013912713642, w1=14.477442425764762\n",
      "[ 0.35444425  1.84835852]\n",
      "Gradient Descent(9645/9): loss=17.09452630619628, w0=74.60202815196035, w1=13.18359145832874\n",
      "[-1.26461691 -1.39404076]\n",
      "Gradient Descent(9646/9): loss=16.285302534882977, w0=75.48725999070962, w1=14.159419993801023\n",
      "[ 0.45839948  0.04109566]\n",
      "Gradient Descent(9647/9): loss=18.022254817710124, w0=75.16638035792492, w1=14.13065303353659\n",
      "[-0.95734403  0.47184967]\n",
      "Gradient Descent(9648/9): loss=17.35079984738768, w0=75.83652117906155, w1=13.800358262506933\n",
      "[ 4.5090776   1.06781159]\n",
      "Gradient Descent(9649/9): loss=18.66970002951129, w0=72.68016685630691, w1=13.05289015125316\n",
      "[ 1.03482787 -0.47532043]\n",
      "Gradient Descent(9650/9): loss=15.665324189273093, w0=71.95578734873361, w1=13.385614450034447\n",
      "[-0.46595177 -0.52107529]\n",
      "Gradient Descent(9651/9): loss=16.2856172594926, w0=72.28195358791743, w1=13.750367151517617\n",
      "[-1.22493358  0.74683057]\n",
      "Gradient Descent(9652/9): loss=15.934554892275825, w0=73.1394070941904, w1=13.227585755127517\n",
      "[-0.71028455  0.52822012]\n",
      "Gradient Descent(9653/9): loss=15.429609228562358, w0=73.63660628083008, w1=12.857831668198092\n",
      "[-0.92773122 -0.78479803]\n",
      "Gradient Descent(9654/9): loss=15.637971970324253, w0=74.2860181330519, w1=13.407190290123133\n",
      "[ 2.01291566  2.48603962]\n",
      "Gradient Descent(9655/9): loss=15.880644966097089, w0=72.87697716902574, w1=11.666962557475525\n",
      "[ 0.44143064 -2.76955306]\n",
      "Gradient Descent(9656/9): loss=17.11584042495787, w0=72.56797572323987, w1=13.605649699922669\n",
      "[ 1.36786399 -0.37083991]\n",
      "Gradient Descent(9657/9): loss=15.657316966078085, w0=71.61047092858577, w1=13.865237639809452\n",
      "[-3.32152789 -1.32802715]\n",
      "Gradient Descent(9658/9): loss=16.877206469072153, w0=73.93554044821808, w1=14.794856647196177\n",
      "[ 0.53586828  2.02042136]\n",
      "Gradient Descent(9659/9): loss=16.456527133476513, w0=73.56043265314214, w1=13.380561696825106\n",
      "[ 0.43928934 -4.22167097]\n",
      "Gradient Descent(9660/9): loss=15.426317266826695, w0=73.2529301185307, w1=16.335731377224246\n",
      "[-1.25088423  1.56081313]\n",
      "Gradient Descent(9661/9): loss=19.46515013529202, w0=74.12854907709016, w1=15.243162185853139\n",
      "[-1.81972952 -1.63455479]\n",
      "Gradient Descent(9662/9): loss=17.289066557889686, w0=75.40235974365729, w1=16.387350540233722\n",
      "[-0.00830734  0.82549499]\n",
      "Gradient Descent(9663/9): loss=21.835822399365462, w0=75.40817488206474, w1=15.809504043898974\n",
      "[ 0.83612058  0.76562972]\n",
      "Gradient Descent(9664/9): loss=20.334884959511573, w0=74.82289047728868, w1=15.273563237089649\n",
      "[ 3.56243914  2.70469942]\n",
      "Gradient Descent(9665/9): loss=18.163710517980345, w0=72.32918308014925, w1=13.380273640250676\n",
      "[ 0.516928    2.01025163]\n",
      "Gradient Descent(9666/9): loss=15.856192499547266, w0=71.9673334820093, w1=11.973097500393191\n",
      "[ 1.00582747 -2.59173342]\n",
      "Gradient Descent(9667/9): loss=17.400750700228148, w0=71.2632542499277, w1=13.787310895196033\n",
      "[ 0.24486172  1.58966268]\n",
      "Gradient Descent(9668/9): loss=17.495002035057045, w0=71.09185104301008, w1=12.67454701936599\n",
      "[-1.49533722 -1.25398404]\n",
      "Gradient Descent(9669/9): loss=18.134591796532153, w0=72.1385870978306, w1=13.552335846298783\n",
      "[-4.008156    2.57620609]\n",
      "Gradient Descent(9670/9): loss=16.055924319282123, w0=74.94429629912091, w1=11.748991583768774\n",
      "[ 3.41372474 -1.53019623]\n",
      "Gradient Descent(9671/9): loss=18.24545286137878, w0=72.55468898003262, w1=12.82012894799696\n",
      "[-2.49169613 -1.04064605]\n",
      "Gradient Descent(9672/9): loss=15.876645787446993, w0=74.29887627188282, w1=13.548581182843014\n",
      "[ 0.74782687  1.15577122]\n",
      "Gradient Descent(9673/9): loss=15.89322586321703, w0=73.77539746620734, w1=12.73954133037171\n",
      "[ 2.90515445 -2.61953496]\n",
      "Gradient Descent(9674/9): loss=15.775723812150815, w0=71.74178934996756, w1=14.573215798872484\n",
      "[-2.41344335 -0.32647792]\n",
      "Gradient Descent(9675/9): loss=17.188320557157493, w0=73.43119969660918, w1=14.801750340576808\n",
      "[ 0.40742102 -1.11423824]\n",
      "Gradient Descent(9676/9): loss=16.269202563439002, w0=73.14600497940329, w1=15.581717108533212\n",
      "[-0.44865367 -0.29748533]\n",
      "Gradient Descent(9677/9): loss=17.60603941543265, w0=73.46006254860856, w1=15.789956841831025\n",
      "[ 0.96338049  4.27702419]\n",
      "Gradient Descent(9678/9): loss=18.068303819097938, w0=72.7856962043718, w1=12.796039906502912\n",
      "[-1.43898842  2.56073795]\n",
      "Gradient Descent(9679/9): loss=15.748738662673583, w0=73.79298809521116, w1=11.003523344425435\n",
      "[ 0.98406528  0.2387845 ]\n",
      "Gradient Descent(9680/9): loss=18.576177557586558, w0=73.10414239966293, w1=10.836374195772661\n",
      "[-0.89444567 -1.52484333]\n",
      "Gradient Descent(9681/9): loss=18.897514541032766, w0=73.73025436903508, w1=11.903764526482307\n",
      "[-1.10689336 -2.44184003]\n",
      "Gradient Descent(9682/9): loss=16.722886741208107, w0=74.50507972003899, w1=13.613052548805545\n",
      "[ 2.16678168  0.42345637]\n",
      "Gradient Descent(9683/9): loss=16.128229170661005, w0=72.98833254116387, w1=13.316633087994948\n",
      "[ 1.80084748 -0.77606559]\n",
      "Gradient Descent(9684/9): loss=15.445877764856611, w0=71.72773930215321, w1=13.859878998366463\n",
      "[ 1.84364504  2.43316237]\n",
      "Gradient Descent(9685/9): loss=16.684615301598935, w0=70.43718777150804, w1=12.156665337908484\n",
      "[-4.82468718  0.51618247]\n",
      "Gradient Descent(9686/9): loss=20.34157991150881, w0=73.81446879792222, w1=11.795337609216826\n",
      "[-0.2123139  -2.39763714]\n",
      "Gradient Descent(9687/9): loss=16.939931628994692, w0=73.963088530409, w1=13.473683607903933\n",
      "[ 1.49462437 -1.63765848]\n",
      "Gradient Descent(9688/9): loss=15.609797963508495, w0=72.91685146868124, w1=14.620044545065449\n",
      "[-3.71473905  0.52197532]\n",
      "Gradient Descent(9689/9): loss=16.107157623053357, w0=75.51716880041354, w1=14.254661822783744\n",
      "[ 4.18098879 -2.70324381]\n",
      "Gradient Descent(9690/9): loss=18.15757430874526, w0=72.59047664447147, w1=16.146932491290976\n",
      "[-0.60633366  1.17638586]\n",
      "Gradient Descent(9691/9): loss=19.19033696878725, w0=73.01491020984928, w1=15.32346238990432\n",
      "[ 2.53552787 -1.34786131]\n",
      "Gradient Descent(9692/9): loss=17.124518607063358, w0=71.24004070060582, w1=16.2669653074592\n",
      "[-0.6258075   0.96906702]\n",
      "Gradient Descent(9693/9): loss=21.379491356700335, w0=71.67810594884924, w1=15.588618392699333\n",
      "[ 2.96150084  2.77633143]\n",
      "Gradient Descent(9694/9): loss=18.915060797042194, w0=69.60505536302244, w1=13.645186394908144\n",
      "[-4.72410139  0.05786053]\n",
      "Gradient Descent(9695/9): loss=22.20344722500387, w0=72.91192633894356, w1=13.604684023403996\n",
      "[-0.36335671  1.26121646]\n",
      "Gradient Descent(9696/9): loss=15.466657161122026, w0=73.16627603569543, w1=12.721832502426007\n",
      "[ 1.14360838  2.95390994]\n",
      "Gradient Descent(9697/9): loss=15.681225611290621, w0=72.36575016910794, w1=10.654095546534068\n",
      "[-0.40152905 -0.06114397]\n",
      "Gradient Descent(9698/9): loss=19.80869474477514, w0=72.64682050067996, w1=10.696896325476345\n",
      "[ 0.76688489 -0.8657295 ]\n",
      "Gradient Descent(9699/9): loss=19.467290795084498, w0=72.11000107842676, w1=11.302906972697539\n",
      "[-3.00942532 -1.60725687]\n",
      "Gradient Descent(9700/9): loss=18.455963255922263, w0=74.21659879997866, w1=12.427986781215012\n",
      "[ 0.45216792 -3.77066402]\n",
      "Gradient Descent(9701/9): loss=16.36461753089963, w0=73.90008125486497, w1=15.067451598484391\n",
      "[-0.88997942  2.35356756]\n",
      "Gradient Descent(9702/9): loss=16.83006021433099, w0=74.52306684756043, w1=13.419954305133318\n",
      "[ 3.72900963 -2.12307298]\n",
      "Gradient Descent(9703/9): loss=16.14307191142592, w0=71.91276010740971, w1=14.906105391927186\n",
      "[-1.38764746  1.95010508]\n",
      "Gradient Descent(9704/9): loss=17.356990392310166, w0=72.88411332982457, w1=13.54103183718167\n",
      "[ 1.36541138 -0.83270984]\n",
      "Gradient Descent(9705/9): loss=15.47173947731023, w0=71.92832536496422, w1=14.123928728181086\n",
      "[-1.01207945  3.32547782]\n",
      "Gradient Descent(9706/9): loss=16.525822272721804, w0=72.63678097852942, w1=11.79609425435548\n",
      "[-1.5359379  -3.82105673]\n",
      "Gradient Descent(9707/9): loss=17.01909012034245, w0=73.71193751122136, w1=14.470833964439603\n",
      "[-0.59726672 -0.54954636]\n",
      "Gradient Descent(9708/9): loss=15.96441729483043, w0=74.13002421255118, w1=14.855516415855865\n",
      "[ 0.90729588  3.64090081]\n",
      "Gradient Descent(9709/9): loss=16.68183961887022, w0=73.49491709348152, w1=12.306885852307316\n",
      "[-0.58130275 -1.48786417]\n",
      "Gradient Descent(9710/9): loss=16.093848478730546, w0=73.90182901616365, w1=13.348390770099275\n",
      "[ 1.81897944  4.19180872]\n",
      "Gradient Descent(9711/9): loss=15.579286027534845, w0=72.6285434055042, w1=10.414124663930153\n",
      "[-0.45728253 -2.39236503]\n",
      "Gradient Descent(9712/9): loss=20.30616639826967, w0=72.94864117503086, w1=12.08878018550618\n",
      "[-1.52964878 -1.52262936]\n",
      "Gradient Descent(9713/9): loss=16.412843554927704, w0=74.01939532317172, w1=13.15462073952779\n",
      "[ 2.09684943  2.46444914]\n",
      "Gradient Descent(9714/9): loss=15.70188594384799, w0=72.55160072249191, w1=11.429506343126246\n",
      "[-0.97071424 -3.04235279]\n",
      "Gradient Descent(9715/9): loss=17.763080819468417, w0=73.23110069111941, w1=13.559153294735308\n",
      "[-2.6571345   0.80602955]\n",
      "Gradient Descent(9716/9): loss=15.391016552485002, w0=75.09109484289536, w1=12.994932606667222\n",
      "[ 1.59787777  0.28359496]\n",
      "Gradient Descent(9717/9): loss=17.118308719640176, w0=73.97258040440944, w1=12.796416138065604\n",
      "[ 0.82683531 -2.99263714]\n",
      "Gradient Descent(9718/9): loss=15.849623397033126, w0=73.39379568652264, w1=14.891262137154474\n",
      "[-1.43473847 -0.50140176]\n",
      "Gradient Descent(9719/9): loss=16.387111526090607, w0=74.39811261666027, w1=15.242243369716435\n",
      "[ 2.55897755  2.45834816]\n",
      "Gradient Descent(9720/9): loss=17.54876397340066, w0=72.6068283345612, w1=13.521399659442974\n",
      "[-5.6013377  -0.76070492]\n",
      "Gradient Descent(9721/9): loss=15.62280563516026, w0=76.52776472128005, w1=14.053893105821668\n",
      "[ 2.0923279 -0.5182505]\n",
      "Gradient Descent(9722/9): loss=20.779598956388426, w0=75.06313519287563, w1=14.416668457516499\n",
      "[ 0.5958915 -0.378582 ]\n",
      "Gradient Descent(9723/9): loss=17.3898888201027, w0=74.64601114530046, w1=14.681675860430381\n",
      "[-3.67818519  1.47407376]\n",
      "Gradient Descent(9724/9): loss=17.022318432451993, w0=77.22074077703782, w1=13.649824226738112\n",
      "[ 4.82285841  2.79105314]\n",
      "Gradient Descent(9725/9): loss=23.110309725257135, w0=73.84473989107398, w1=11.696087030954113\n",
      "[ 3.31485605 -0.95644563]\n",
      "Gradient Descent(9726/9): loss=17.128247833192813, w0=71.52434065387637, w1=12.365598970203475\n",
      "[-2.72320623 -0.96508929]\n",
      "Gradient Descent(9727/9): loss=17.572221349037324, w0=73.43058501182466, w1=13.041161471867659\n",
      "[ 1.10264884  0.18498231]\n",
      "Gradient Descent(9728/9): loss=15.491389731569544, w0=72.65873082472541, w1=12.911673855719615\n",
      "[ 1.73793282 -1.50770801]\n",
      "Gradient Descent(9729/9): loss=15.748955698509176, w0=71.44217784978082, w1=13.967069462909192\n",
      "[-3.59059432  0.93411075]\n",
      "Gradient Descent(9730/9): loss=17.219124507994728, w0=73.95559387268467, w1=13.31319193589609\n",
      "[-0.5529932  1.2454826]\n",
      "Gradient Descent(9731/9): loss=15.61865723929656, w0=74.34268911095324, w1=12.441354114136757\n",
      "[ 0.81029105  0.72638958]\n",
      "Gradient Descent(9732/9): loss=16.474938094371748, w0=73.77548537922537, w1=11.932881408177586\n",
      "[ 2.02348355 -2.38354927]\n",
      "Gradient Descent(9733/9): loss=16.698182624674402, w0=72.35904689607008, w1=13.6013658956456\n",
      "[-4.17147506 -2.99845106]\n",
      "Gradient Descent(9734/9): loss=15.830283383016331, w0=75.27907943900549, w1=15.700281640113687\n",
      "[-5.20771931  3.86792609]\n",
      "Gradient Descent(9735/9): loss=19.821776690843606, w0=78.92448295789818, w1=12.992733375710579\n",
      "[ 5.20877099 -4.23803335]\n",
      "Gradient Descent(9736/9): loss=31.356070509367502, w0=75.27834326223477, w1=15.959356718154815\n",
      "[ 3.60399263  5.35551945]\n",
      "Gradient Descent(9737/9): loss=20.429169623174868, w0=72.75554842358176, w1=12.21049310457304\n",
      "[-0.39196396  2.88227866]\n",
      "Gradient Descent(9738/9): loss=16.336269778206294, w0=73.02992319392236, w1=10.192898044661742\n",
      "[-3.27647707 -2.8081604 ]\n",
      "Gradient Descent(9739/9): loss=20.82230997242171, w0=75.32345714530301, w1=12.158610325973232\n",
      "[ 2.93572278 -0.5014212 ]\n",
      "Gradient Descent(9740/9): loss=18.31804970878992, w0=73.26845119732218, w1=12.509605167278112\n",
      "[ 0.69703704 -2.03417912]\n",
      "Gradient Descent(9741/9): loss=15.856766305210337, w0=72.7805252682055, w1=13.933530552992313\n",
      "[ 1.4618454   0.20215122]\n",
      "Gradient Descent(9742/9): loss=15.62065141413285, w0=71.75723348759472, w1=13.792024701228103\n",
      "[-4.77854146  0.56453073]\n",
      "Gradient Descent(9743/9): loss=16.615363139965286, w0=75.10221250733215, w1=13.39685318790519\n",
      "[ 0.31667657 -0.70120617]\n",
      "Gradient Descent(9744/9): loss=17.02427797189005, w0=74.88053890489715, w1=13.887697508282525\n",
      "[ 1.50840902 -0.84108865]\n",
      "Gradient Descent(9745/9): loss=16.727790376957117, w0=73.8246525897601, w1=14.476459560103708\n",
      "[ 2.28470731  2.63670813]\n",
      "Gradient Descent(9746/9): loss=16.023477762877835, w0=72.225357470626, w1=12.630763869315354\n",
      "[-0.05571491 -1.35579416]\n",
      "Gradient Descent(9747/9): loss=16.31715978137679, w0=72.26435790469205, w1=13.579819779256354\n",
      "[-4.17789073 -1.26644305]\n",
      "Gradient Descent(9748/9): loss=15.920899724358593, w0=75.18888141530465, w1=14.466329912640177\n",
      "[ 0.06948495  4.13396651]\n",
      "Gradient Descent(9749/9): loss=17.668030481269355, w0=75.14024194905595, w1=11.572553357942947\n",
      "[ 2.2861607 -0.3441282]\n",
      "Gradient Descent(9750/9): loss=18.9089644146632, w0=73.53992945897589, w1=11.81344309928441\n",
      "[ 1.16612704 -2.05389391]\n",
      "Gradient Descent(9751/9): loss=16.804374452802183, w0=72.723640533663, w1=13.251168833217921\n",
      "[ 1.11691329 -1.71801792]\n",
      "Gradient Descent(9752/9): loss=15.57461443440895, w0=71.94180122897716, w1=14.453781374027987\n",
      "[ 0.20629338  1.35549669]\n",
      "Gradient Descent(9753/9): loss=16.7744083103918, w0=71.79739586041947, w1=13.504933689972223\n",
      "[-2.35583042 -0.54900615]\n",
      "Gradient Descent(9754/9): loss=16.506001171055235, w0=73.44647715517556, w1=13.889237994097693\n",
      "[ 0.84490375 -0.24425591]\n",
      "Gradient Descent(9755/9): loss=15.481379997975187, w0=72.85504452851616, w1=14.060217133336062\n",
      "[ 1.31037951  2.94403937]\n",
      "Gradient Descent(9756/9): loss=15.650687439642823, w0=71.93777886868601, w1=11.999389572855979\n",
      "[-2.59139501 -3.81135559]\n",
      "Gradient Descent(9757/9): loss=17.401127856066317, w0=73.7517553764139, w1=14.66733848426755\n",
      "[ 0.70162665  0.54427729]\n",
      "Gradient Descent(9758/9): loss=16.195921384607278, w0=73.26061672039086, w1=14.286344383883598\n",
      "[ 1.35193938 -0.90535805]\n",
      "Gradient Descent(9759/9): loss=15.711770040213144, w0=72.3142591519669, w1=14.920095017575647\n",
      "[-0.80748914  1.26415216]\n",
      "Gradient Descent(9760/9): loss=16.90310851090937, w0=72.8795015475678, w1=14.03518850630654\n",
      "[-4.75636815 -0.01255212]\n",
      "Gradient Descent(9761/9): loss=15.626036858302045, w0=76.20895925190275, w1=14.04397498720261\n",
      "[ 1.37588396 -0.09759004]\n",
      "Gradient Descent(9762/9): loss=19.79380506659836, w0=75.24584047860924, w1=14.112288012920065\n",
      "[ 4.86743258 -0.17346774]\n",
      "Gradient Descent(9763/9): loss=17.490956669185724, w0=71.83863767376914, w1=14.233715429351376\n",
      "[ 2.37675921 -0.88297102]\n",
      "Gradient Descent(9764/9): loss=16.72907436473333, w0=70.17490622820763, w1=14.851795140493644\n",
      "[-3.57930434  1.16044413]\n",
      "Gradient Descent(9765/9): loss=21.1913230431127, w0=72.68041926588226, w1=14.039484248558296\n",
      "[-0.4710169   1.06647981]\n",
      "Gradient Descent(9766/9): loss=15.73075291413921, w0=73.01013109785694, w1=13.292948381928664\n",
      "[-1.32358107 -1.34804979]\n",
      "Gradient Descent(9767/9): loss=15.443596913254192, w0=73.9366378502748, w1=14.236583232121161\n",
      "[ 2.07299336 -0.39554402]\n",
      "Gradient Descent(9768/9): loss=15.878856401349292, w0=72.48554249952699, w1=14.513464047012398\n",
      "[ 1.71142387 -1.14636595]\n",
      "Gradient Descent(9769/9): loss=16.24694777660413, w0=71.28754579177672, w1=15.315920209363107\n",
      "[ 0.03120011 -1.11435177]\n",
      "Gradient Descent(9770/9): loss=19.08449011285128, w0=71.26570571744386, w1=16.09596644678878\n",
      "[-1.81886311 -0.19022419]\n",
      "Gradient Descent(9771/9): loss=20.865111044641193, w0=72.53890989458544, w1=16.229123377580038\n",
      "[-1.34652548  1.1872512 ]\n",
      "Gradient Descent(9772/9): loss=19.450539775699642, w0=73.48147773133363, w1=15.398047538864532\n",
      "[ 0.14083068  3.28747718]\n",
      "Gradient Descent(9773/9): loss=17.243481229993087, w0=73.38289625476388, w1=13.096813514927112\n",
      "[-1.18612053 -1.38129145]\n",
      "Gradient Descent(9774/9): loss=15.463151869139786, w0=74.21318062331042, w1=14.063717532467882\n",
      "[ 4.14752466  2.69533166]\n",
      "Gradient Descent(9775/9): loss=15.978937052100102, w0=71.30991335828938, w1=12.176985370238576\n",
      "[-0.9445567  -1.29176446]\n",
      "Gradient Descent(9776/9): loss=18.202581920814016, w0=71.9711030456161, w1=13.081220489763933\n",
      "[-4.84367453 -0.35420294]\n",
      "Gradient Descent(9777/9): loss=16.340210779857482, w0=75.36167521502793, w1=13.329162550094791\n",
      "[ 3.24586908 -0.68362235]\n",
      "Gradient Descent(9778/9): loss=17.535022177526397, w0=73.08956686112833, w1=13.807698192231001\n",
      "[-1.90178727 -1.89351206]\n",
      "Gradient Descent(9779/9): loss=15.460555709128023, w0=74.42081794764606, w1=15.13315663706927\n",
      "[ 0.40250378  4.95426924]\n",
      "Gradient Descent(9780/9): loss=17.38777396956397, w0=74.13906529965884, w1=11.665168166528854\n",
      "[-0.48189008  0.71957222]\n",
      "Gradient Descent(9781/9): loss=17.389306916630197, w0=74.47638835815303, w1=11.161467615292645\n",
      "[ 1.81986728 -0.85600098]\n",
      "Gradient Descent(9782/9): loss=18.772130732446538, w0=73.20248126021613, w1=11.760668303909394\n",
      "[ 0.18587967 -0.25959706]\n",
      "Gradient Descent(9783/9): loss=16.86762493576771, w0=73.07236548942105, w1=11.94238624619043\n",
      "[ 1.2739732  -1.30165119]\n",
      "Gradient Descent(9784/9): loss=16.592117418368822, w0=72.18058424650417, w1=12.853542077052804\n",
      "[-0.13193553  1.38609885]\n",
      "Gradient Descent(9785/9): loss=16.201693006431764, w0=72.27293911769469, w1=11.883272882385189\n",
      "[ 1.3834141 -1.436946 ]\n",
      "Gradient Descent(9786/9): loss=17.181400516517996, w0=71.30454924596712, w1=12.889135079107666\n",
      "[-2.13784482 -1.34291027]\n",
      "Gradient Descent(9787/9): loss=17.539080656901504, w0=72.80104062039666, w1=13.829172267687255\n",
      "[ 0.20155361  0.42363175]\n",
      "Gradient Descent(9788/9): loss=15.568414984381583, w0=72.65995309124027, w1=13.532630041051936\n",
      "[ 0.46873932  0.07401823]\n",
      "Gradient Descent(9789/9): loss=15.588246295316738, w0=72.33183556967481, w1=13.480817283095632\n",
      "[-1.10768871 -0.10849875]\n",
      "Gradient Descent(9790/9): loss=15.848693630907377, w0=73.10721766646945, w1=13.556766407039962\n",
      "[-2.86963177 -2.23457822]\n",
      "Gradient Descent(9791/9): loss=15.406285780606405, w0=75.11595990815076, w1=15.120971160593045\n",
      "[-0.27486638  5.55304681]\n",
      "Gradient Descent(9792/9): loss=18.39266403654848, w0=75.30836637343853, w1=11.23383839558326\n",
      "[ 1.9890841  -0.01044542]\n",
      "Gradient Descent(9793/9): loss=19.93685603186613, w0=73.91600750502703, w1=11.24115019144262\n",
      "[-1.99268758 -4.9140278 ]\n",
      "Gradient Descent(9794/9): loss=18.084963514417968, w0=75.31088880984714, w1=14.680969652446647\n",
      "[ 2.16032376  3.73769752]\n",
      "Gradient Descent(9795/9): loss=18.141474871842767, w0=73.79866217449522, w1=12.064581388403054\n",
      "[ 1.23596674  0.59796228]\n",
      "Gradient Descent(9796/9): loss=16.51456712914735, w0=72.9334854580829, w1=11.646007793987227\n",
      "[-2.21621225 -3.42970616]\n",
      "Gradient Descent(9797/9): loss=17.132081475178577, w0=74.4848340304612, w1=14.046802107163801\n",
      "[ 0.43941072 -1.04331356]\n",
      "Gradient Descent(9798/9): loss=16.255818946614546, w0=74.17724652429318, w1=14.77712159972384\n",
      "[ 1.30837501  5.76632178]\n",
      "Gradient Descent(9799/9): loss=16.617654244947634, w0=73.2613840150063, w1=10.740696355773684\n",
      "[ 1.51166392 -1.67545204]\n",
      "Gradient Descent(9800/9): loss=19.137521770231782, w0=72.2032192708566, w1=11.913512780660206\n",
      "[-1.68905242 -0.143028  ]\n",
      "Gradient Descent(9801/9): loss=17.207194771415868, w0=73.38555596445853, w1=12.013632382457613\n",
      "[ 0.42998576 -2.31726215]\n",
      "Gradient Descent(9802/9): loss=16.464781620572975, w0=73.08456593248457, w1=13.635715884310097\n",
      "[-2.6184807   2.45125648]\n",
      "Gradient Descent(9803/9): loss=15.41997138887293, w0=74.91750242452949, w1=11.919836348493002\n",
      "[ 2.24201084  1.64632235]\n",
      "Gradient Descent(9804/9): loss=17.920501265480247, w0=73.34809483963869, w1=10.76741070643301\n",
      "[ 7.11098483 -3.88031673]\n",
      "Gradient Descent(9805/9): loss=19.065645550356653, w0=68.37040545530553, w1=13.48363241805137\n",
      "[-3.5122508  -0.38595622]\n",
      "Gradient Descent(9806/9): loss=27.50640314526803, w0=70.82898101780478, w1=13.753801772756619\n",
      "[-3.53421676 -2.01369764]\n",
      "Gradient Descent(9807/9): loss=18.46141737941027, w0=73.30293274943365, w1=15.163390124213898\n",
      "[ 0.59368132  1.30765808]\n",
      "Gradient Descent(9808/9): loss=16.803313746209874, w0=72.88735582211164, w1=14.248029469330682\n",
      "[-4.80028575  2.15264615]\n",
      "Gradient Descent(9809/9): loss=15.763691430816433, w0=76.24755584788403, w1=12.741177161675822\n",
      "[ 4.64285516  2.53012056]\n",
      "Gradient Descent(9810/9): loss=20.020581491258476, w0=72.99755723655844, w1=10.97009276965432\n",
      "[ 0.29015772 -2.50198503]\n",
      "Gradient Descent(9811/9): loss=18.57889933827559, w0=72.79444683600217, w1=12.721482291170037\n",
      "[-0.65355709  1.98228456]\n",
      "Gradient Descent(9812/9): loss=15.79808206510412, w0=73.25193680221582, w1=11.333883100250024\n",
      "[ 4.93122647 -3.43125398]\n",
      "Gradient Descent(9813/9): loss=17.68906101424754, w0=69.80007827112549, w1=13.73576088524926\n",
      "[-3.28716129  2.79136115]\n",
      "Gradient Descent(9814/9): loss=21.52214028152282, w0=72.1010911775131, w1=11.781808083419826\n",
      "[-0.66402805 -1.6704059 ]\n",
      "Gradient Descent(9815/9): loss=17.538750150416774, w0=72.5659108148405, w1=12.9510922116657\n",
      "[ 0.63563722  1.00002117]\n",
      "Gradient Descent(9816/9): loss=15.790607683473878, w0=72.12096476022134, w1=12.251077392892197\n",
      "[-2.87749925  1.21162242]\n",
      "Gradient Descent(9817/9): loss=16.82857424780745, w0=74.13521423336837, w1=11.402941696563834\n",
      "[ 0.13639269 -2.81895267]\n",
      "Gradient Descent(9818/9): loss=17.8962625280109, w0=74.03973935250829, w1=13.37620856312812\n",
      "[ 1.04931468 -1.92481022]\n",
      "Gradient Descent(9819/9): loss=15.669366154655652, w0=73.30521907452746, w1=14.723575713974396\n",
      "[ 1.20642568  3.81823397]\n",
      "Gradient Descent(9820/9): loss=16.15954960915615, w0=72.46072110131284, w1=12.050811934145779\n",
      "[-3.25284786 -1.02838999]\n",
      "Gradient Descent(9821/9): loss=16.753878060025063, w0=74.73771460200514, w1=12.770684927383614\n",
      "[ 2.75107144 -2.3669375 ]\n",
      "Gradient Descent(9822/9): loss=16.67951640786292, w0=72.81196459742777, w1=14.427541177377833\n",
      "[ 1.87037196 -1.32566601]\n",
      "Gradient Descent(9823/9): loss=15.951219001240249, w0=71.50270422719865, w1=15.355507387739294\n",
      "[-1.62785667  1.90545515]\n",
      "Gradient Descent(9824/9): loss=18.74942177978162, w0=72.64220389441766, w1=14.021688783921183\n",
      "[-1.45285763  2.33064882]\n",
      "Gradient Descent(9825/9): loss=15.74512529617421, w0=73.65920423852005, w1=12.390234613117524\n",
      "[ 0.58853484  0.33010612]\n",
      "Gradient Descent(9826/9): loss=16.04608438712448, w0=73.24722985371913, w1=12.159160330179647\n",
      "[-0.15340185 -0.27365414]\n",
      "Gradient Descent(9827/9): loss=16.25890687794812, w0=73.3546111454953, w1=12.350718229997693\n",
      "[ 3.75006969 -4.06791904]\n",
      "Gradient Descent(9828/9): loss=16.025043412344147, w0=70.72956236373099, w1=15.198261556139519\n",
      "[-3.58757332  2.61089294]\n",
      "Gradient Descent(9829/9): loss=20.150563587194256, w0=73.24086368669785, w1=13.370636495117225\n",
      "[ 0.93158325  0.44478841]\n",
      "Gradient Descent(9830/9): loss=15.393244241575795, w0=72.58875541146116, w1=13.059284604873984\n",
      "[-0.26473534  1.09073028]\n",
      "Gradient Descent(9831/9): loss=15.722897609277288, w0=72.77407014935538, w1=12.295773405469093\n",
      "[-3.30736594 -0.66426257]\n",
      "Gradient Descent(9832/9): loss=16.22186665604343, w0=75.08922630913645, w1=12.76075720564621\n",
      "[ 1.95913836  3.38806883]\n",
      "Gradient Descent(9833/9): loss=17.255894957151604, w0=73.71782945447055, w1=10.389109023902648\n",
      "[-0.23628704 -3.93633273]\n",
      "Gradient Descent(9834/9): loss=20.251651355224293, w0=73.88323038033205, w1=13.144541934032182\n",
      "[ 0.30771506 -1.85812855]\n",
      "Gradient Descent(9835/9): loss=15.615699683509426, w0=73.66782983692163, w1=14.445231919267712\n",
      "[-1.17740956 -2.80969644]\n",
      "Gradient Descent(9836/9): loss=15.921905340558826, w0=74.4920165298634, w1=16.41201942992121\n",
      "[ 2.07296888  2.91224034]\n",
      "Gradient Descent(9837/9): loss=20.402815273815527, w0=73.04093831364214, w1=14.373451194862769\n",
      "[ 0.48486368  2.94302398]\n",
      "Gradient Descent(9838/9): loss=15.81727272759389, w0=72.70153373684249, w1=12.313334406280259\n",
      "[ 1.14374051 -2.45817177]\n",
      "Gradient Descent(9839/9): loss=16.241568650167174, w0=71.90091537725259, w1=14.034054647171537\n",
      "[-2.18936824  0.83716633]\n",
      "Gradient Descent(9840/9): loss=16.50976924137471, w0=73.43347314382436, w1=13.448038216894462\n",
      "[ 0.48208272  1.1581494 ]\n",
      "Gradient Descent(9841/9): loss=15.396126757452913, w0=73.09601524006516, w1=12.63733364034328\n",
      "[-1.42226112  1.88311717]\n",
      "Gradient Descent(9842/9): loss=15.760272428894416, w0=74.09159802620906, w1=11.319151624482352\n",
      "[ 1.10884191 -6.30871409]\n",
      "Gradient Descent(9843/9): loss=18.038042896493156, w0=73.31540868908306, w1=15.735251487972\n",
      "[-1.50082415  5.02551602]\n",
      "Gradient Descent(9844/9): loss=17.92984691745366, w0=74.36598559142446, w1=12.217390275914454\n",
      "[-0.19068983  0.12055719]\n",
      "Gradient Descent(9845/9): loss=16.757276655246834, w0=74.49946847394294, w1=12.133000242461742\n",
      "[-0.83857716 -2.37694697]\n",
      "Gradient Descent(9846/9): loss=17.019375881460476, w0=75.08647248488855, w1=13.796863121675688\n",
      "[ 0.9257615   2.51495048]\n",
      "Gradient Descent(9847/9): loss=17.042798764525728, w0=74.43843943471607, w1=12.036397788873941\n",
      "[ 2.42814109  0.15477084]\n",
      "Gradient Descent(9848/9): loss=17.082426529449695, w0=72.73874067393825, w1=11.928058203907542\n",
      "[-2.70427482 -2.40985455]\n",
      "Gradient Descent(9849/9): loss=16.74381644881857, w0=74.6317330466233, w1=13.614956387763854\n",
      "[ 0.07503191  3.68207891]\n",
      "Gradient Descent(9850/9): loss=16.289902527627792, w0=74.5792107074164, w1=11.03750114948548\n",
      "[-1.11260583 -4.32265105]\n",
      "Gradient Descent(9851/9): loss=19.194069378350175, w0=75.35803478560896, w1=14.063356886091853\n",
      "[ 0.91024177 -0.29845901]\n",
      "Gradient Descent(9852/9): loss=17.686489082992797, w0=74.72086554813124, w1=14.272278194339123\n",
      "[ 1.43662737  1.8656328 ]\n",
      "Gradient Descent(9853/9): loss=16.71805205204918, w0=73.71522639223416, w1=12.966335236118836\n",
      "[-0.66260339  0.50444973]\n",
      "Gradient Descent(9854/9): loss=15.606414637560285, w0=74.17904876651487, w1=12.613220422096964\n",
      "[ 1.52816924 -0.3850278 ]\n",
      "Gradient Descent(9855/9): loss=16.15301676756946, w0=73.10933029522003, w1=12.882739884552372\n",
      "[-2.48293797 -0.93675507]\n",
      "Gradient Descent(9856/9): loss=15.581113030942223, w0=74.84738687689973, w1=13.538468436981875\n",
      "[ 1.74108064  0.14160912]\n",
      "Gradient Descent(9857/9): loss=16.594240561324703, w0=73.62863042775494, w1=13.439342050725747\n",
      "[-0.36098064  0.54096895]\n",
      "Gradient Descent(9858/9): loss=15.44271761789265, w0=73.88131687292926, w1=13.060663782796365\n",
      "[-0.32248214 -0.27897368]\n",
      "Gradient Descent(9859/9): loss=15.646205122416868, w0=74.10705436994411, w1=13.255945360004393\n",
      "[-0.4321649  0.0413526]\n",
      "Gradient Descent(9860/9): loss=15.741515844566658, w0=74.4095697997924, w1=13.226998540921192\n",
      "[-1.80924738  4.96042513]\n",
      "Gradient Descent(9861/9): loss=16.040155029199024, w0=75.67604296545338, w1=9.754700952320876\n",
      "[  4.26532144 -10.20196192]\n",
      "Gradient Descent(9862/9): loss=25.16099328384573, w0=72.69031796093427, w1=16.89607429474504\n",
      "[ 0.52836748  2.02854338]\n",
      "Gradient Descent(9863/9): loss=21.40382096648605, w0=72.32046072381094, w1=15.476093929694159\n",
      "[-1.93646359  0.42312362]\n",
      "Gradient Descent(9864/9): loss=17.85247083519905, w0=73.67598523792961, w1=15.179907396160381\n",
      "[-2.08261879  1.21743214]\n",
      "Gradient Descent(9865/9): loss=16.90420547990991, w0=75.133818389196, w1=14.327704897785454\n",
      "[ 2.3581334   2.24440638]\n",
      "Gradient Descent(9866/9): loss=17.43804283492405, w0=73.48312500916894, w1=12.756620430523498\n",
      "[-1.4213217   0.25557312]\n",
      "Gradient Descent(9867/9): loss=15.665217781231389, w0=74.47805020115733, w1=12.577719247288861\n",
      "[ 2.68829471  1.40297152]\n",
      "Gradient Descent(9868/9): loss=16.493763520053406, w0=72.59624390595802, w1=11.595639185765018\n",
      "[-1.40741053 -1.9735574 ]\n",
      "Gradient Descent(9869/9): loss=17.404131235971967, w0=73.58143127577408, w1=12.977129368884679\n",
      "[ 0.76408572 -1.77813593]\n",
      "Gradient Descent(9870/9): loss=15.553513529219641, w0=73.04657127520429, w1=14.221824520153692\n",
      "[-1.13904429  1.96590282]\n",
      "Gradient Descent(9871/9): loss=15.691844233352313, w0=73.84390227527236, w1=12.845692544174968\n",
      "[ 2.72081661 -0.69532594]\n",
      "Gradient Descent(9872/9): loss=15.738117630239866, w0=71.93933064537255, w1=13.332420700371355\n",
      "[-3.16590995 -1.79994259]\n",
      "Gradient Descent(9873/9): loss=16.314194168240128, w0=74.1554676080905, w1=14.592380514841333\n",
      "[ 2.16774526  0.3913186 ]\n",
      "Gradient Descent(9874/9): loss=16.376033412386786, w0=72.63804592688935, w1=14.318457494814826\n",
      "[ 0.48989489 -1.0422542 ]\n",
      "Gradient Descent(9875/9): loss=15.952721219540743, w0=72.29511950221105, w1=15.048035436513423\n",
      "[-1.41695444  2.35066992]\n",
      "Gradient Descent(9876/9): loss=17.114509604282006, w0=73.28698761369989, w1=13.402566493702912\n",
      "[ 0.44748983  0.75376796]\n",
      "Gradient Descent(9877/9): loss=15.38888765982914, w0=72.97374473388133, w1=12.874928920754758\n",
      "[-1.72722209 -1.67612187]\n",
      "Gradient Descent(9878/9): loss=15.620026159917836, w0=74.18280019817236, w1=14.04821423162591\n",
      "[ 2.8422442  -0.58913369]\n",
      "Gradient Descent(9879/9): loss=15.942537238940881, w0=72.19322925502036, w1=14.460607816124325\n",
      "[-2.5551055   2.23530015]\n",
      "Gradient Descent(9880/9): loss=16.47272800493824, w0=73.98180310210032, w1=12.895897713204894\n",
      "[ 2.08632575 -1.21407445]\n",
      "Gradient Descent(9881/9): loss=15.792897887380608, w0=72.5213750757466, w1=13.745749830454118\n",
      "[ 1.66227117  0.74716727]\n",
      "Gradient Descent(9882/9): loss=15.719690193435369, w0=71.35778525734656, w1=13.222732740356367\n",
      "[-2.70711499 -2.52879937]\n",
      "Gradient Descent(9883/9): loss=17.29321989775842, w0=73.25276574883844, w1=14.99289229671874\n",
      "[-0.4521934   2.02335666]\n",
      "Gradient Descent(9884/9): loss=16.53159143439302, w0=73.56930113096894, w1=13.57654263528409\n",
      "[ 2.48557425  0.32304336]\n",
      "Gradient Descent(9885/9): loss=15.428492744980867, w0=71.82939915354932, w1=13.350412285005413\n",
      "[ 0.12956975 -0.77074949]\n",
      "Gradient Descent(9886/9): loss=16.4666607201934, w0=71.73870032976217, w1=13.889936927285778\n",
      "[-2.29558404  2.67612943]\n",
      "Gradient Descent(9887/9): loss=16.67938716093217, w0=73.3456091593596, w1=12.016646328049129\n",
      "[-0.31951702 -0.51905996]\n",
      "Gradient Descent(9888/9): loss=16.457504866580088, w0=73.56927107526971, w1=12.379988296598246\n",
      "[ 1.55142379  1.03631364]\n",
      "Gradient Descent(9889/9): loss=16.028493015155377, w0=72.48327442429535, w1=11.654568746899075\n",
      "[-0.38366303 -0.45570404]\n",
      "Gradient Descent(9890/9): loss=17.38003735762122, w0=72.7518385422329, w1=11.973561572460136\n",
      "[ 3.73479792 -3.46064959]\n",
      "Gradient Descent(9891/9): loss=16.667060317911247, w0=70.13747999815142, w1=14.396016286169274\n",
      "[-2.32061872 -3.21929172]\n",
      "Gradient Descent(9892/9): loss=20.787257304835105, w0=71.76191310250273, w1=16.649520492896475\n",
      "[-2.51438806  0.63041302]\n",
      "Gradient Descent(9893/9): loss=21.583255065047396, w0=73.52198474664834, w1=16.208231380283813\n",
      "[-0.3162881   0.16683665]\n",
      "Gradient Descent(9894/9): loss=19.134301993969903, w0=73.74338641371229, w1=16.091445724648498\n",
      "[-0.57563503  5.77594573]\n",
      "Gradient Descent(9895/9): loss=18.897472385637737, w0=74.14633093355546, w1=12.048283715823642\n",
      "[ 1.17668299  0.2443282 ]\n",
      "Gradient Descent(9896/9): loss=16.773682451063255, w0=73.32265284364844, w1=11.877253972725734\n",
      "[ 1.93237653 -1.22475998]\n",
      "Gradient Descent(9897/9): loss=16.670237161096946, w0=71.96998927155188, w1=12.734585956270795\n",
      "[-3.85716077  0.93213556]\n",
      "Gradient Descent(9898/9): loss=16.5398935409881, w0=74.67000180790197, w1=12.082091066529605\n",
      "[ 4.26666079 -4.71948034]\n",
      "Gradient Descent(9899/9): loss=17.30935842957748, w0=71.68333925273716, w1=15.385727302658715\n",
      "[-2.31914069  2.74266204]\n",
      "Gradient Descent(9900/9): loss=18.49932260299926, w0=73.3067377374954, w1=13.465863872107146\n",
      "[-2.46217197  2.32877186]\n",
      "Gradient Descent(9901/9): loss=15.386065881713142, w0=75.03025811393479, w1=11.835723572183547\n",
      "[ 2.63364365 -1.24686863]\n",
      "Gradient Descent(9902/9): loss=18.24466910596544, w0=73.18670756205528, w1=12.708531613667322\n",
      "[-2.1876738  -1.90847323]\n",
      "Gradient Descent(9903/9): loss=15.688995266494233, w0=74.71807922249089, w1=14.044462872736496\n",
      "[-0.05752511 -2.0085678 ]\n",
      "Gradient Descent(9904/9): loss=16.55947129148572, w0=74.75834679688153, w1=15.450460331554615\n",
      "[ 2.48192459  0.34744307]\n",
      "Gradient Descent(9905/9): loss=18.400081494516055, w0=73.02099958460335, w1=15.207250183310613\n",
      "[-1.45534756  1.09745829]\n",
      "Gradient Descent(9906/9): loss=16.915324527754898, w0=74.03974287968428, w1=14.4390293822705\n",
      "[ 0.43604432  1.00571684]\n",
      "Gradient Descent(9907/9): loss=16.124156762216526, w0=73.73451185699484, w1=13.735027597416043\n",
      "[-0.35278602  1.78399177]\n",
      "Gradient Descent(9908/9): loss=15.515540495027805, w0=73.98146206766971, w1=12.486233355329716\n",
      "[ 2.29578794 -1.86073066]\n",
      "Gradient Descent(9909/9): loss=16.115743880568015, w0=72.37441050959228, w1=13.788744816166204\n",
      "[-0.73727596 -0.0235708 ]\n",
      "Gradient Descent(9910/9): loss=15.856389067569069, w0=72.89050368020686, w1=13.805244374749659\n",
      "[ 0.60702926  6.6026821 ]\n",
      "Gradient Descent(9911/9): loss=15.520246561953181, w0=72.46558320128297, w1=9.183366903551414\n",
      "[-3.36469205 -4.54866106]\n",
      "Gradient Descent(9912/9): loss=24.958252916055255, w0=74.82086763904667, w1=12.367429646635257\n",
      "[ 1.78055152 -0.65055083]\n",
      "Gradient Descent(9913/9): loss=17.170255858550757, w0=73.57448157496783, w1=12.822815227476061\n",
      "[-2.3376142  1.0922738]\n",
      "Gradient Descent(9914/9): loss=15.641001676411015, w0=75.21081151607635, w1=12.058223564696336\n",
      "[ 2.96544759 -4.02757206]\n",
      "Gradient Descent(9915/9): loss=18.233435877398723, w0=73.13499820385212, w1=14.877524008715353\n",
      "[-1.76421167  3.07729328]\n",
      "Gradient Descent(9916/9): loss=16.375454853476597, w0=74.36994637089695, w1=12.72341870972964\n",
      "[-0.44149408 -0.40417386]\n",
      "Gradient Descent(9917/9): loss=16.250792189379638, w0=74.67899222590307, w1=13.006340411192665\n",
      "[ 3.26782949 -0.22746217]\n",
      "Gradient Descent(9918/9): loss=16.4571381677117, w0=72.39151157986227, w1=13.165563933377666\n",
      "[-1.89626798 -0.50432514]\n",
      "Gradient Descent(9919/9): loss=15.842404794448061, w0=73.71889916787104, w1=13.518591531017469\n",
      "[ 0.41049417 -0.69423822]\n",
      "Gradient Descent(9920/9): loss=15.476946456594584, w0=73.43155325109436, w1=14.004558284762176\n",
      "[ 0.47395369  0.63678305]\n",
      "Gradient Descent(9921/9): loss=15.533090632190596, w0=73.09978567151032, w1=13.558810151530594\n",
      "[-1.51129093 -1.2219606 ]\n",
      "Gradient Descent(9922/9): loss=15.407860550638864, w0=74.15768932088147, w1=14.414182573150239\n",
      "[ 0.03940612  0.26349178]\n",
      "Gradient Descent(9923/9): loss=16.19555207887988, w0=74.13010503987904, w1=14.229738328230667\n",
      "[ 0.7282919   0.70558466]\n",
      "Gradient Descent(9924/9): loss=16.0167583254262, w0=73.62030070863315, w1=13.735829069223259\n",
      "[-1.33848343  4.22199966]\n",
      "Gradient Descent(9925/9): loss=15.471947264032563, w0=74.55723910877816, w1=10.78042930426766\n",
      "[-1.92317739 -2.23335179]\n",
      "Gradient Descent(9926/9): loss=19.82693763473422, w0=75.90346328065279, w1=12.343775556014299\n",
      "[ 1.4544225 -1.0477789]\n",
      "Gradient Descent(9927/9): loss=19.435917007558768, w0=74.88536753016461, w1=13.077220787835854\n",
      "[ 0.95347595 -0.00909212]\n",
      "Gradient Descent(9928/9): loss=16.733237066233617, w0=74.21793436436512, w1=13.083585270110412\n",
      "[ 0.16352607  1.00477562]\n",
      "Gradient Descent(9929/9): loss=15.891245657011384, w0=74.10346611273101, w1=12.380242337538146\n",
      "[ 2.6310892   0.04136786]\n",
      "Gradient Descent(9930/9): loss=16.31798594994823, w0=72.26170367252304, w1=12.351284837515411\n",
      "[ 2.91179005 -1.11455307]\n",
      "Gradient Descent(9931/9): loss=16.55529963016214, w0=70.22345063903894, w1=13.131471987364499\n",
      "[-2.87439593  1.21875503]\n",
      "Gradient Descent(9932/9): loss=20.160420769215218, w0=72.23552779133661, w1=12.278343465590071\n",
      "[-1.87102587 -2.52941676]\n",
      "Gradient Descent(9933/9): loss=16.667630721841, w0=73.5452459034367, w1=14.048935199593094\n",
      "[-3.64457391  1.48159256]\n",
      "Gradient Descent(9934/9): loss=15.57947699839138, w0=76.09644764335344, w1=13.011820404326256\n",
      "[ 5.48692824 -0.7151133 ]\n",
      "Gradient Descent(9935/9): loss=19.422424329935247, w0=72.25559787652257, w1=13.512399715477626\n",
      "[-0.14261527 -1.18363411]\n",
      "Gradient Descent(9936/9): loss=15.925480592865723, w0=72.3554285636692, w1=14.340943590629735\n",
      "[-2.37624272  1.40421965]\n",
      "Gradient Descent(9937/9): loss=16.197132387546205, w0=74.018798471118, w1=13.357989835164364\n",
      "[ 0.38895017 -0.12428344]\n",
      "Gradient Descent(9938/9): loss=15.656019012147683, w0=73.74653335045222, w1=13.444988242796617\n",
      "[ 0.73329219  1.50336219]\n",
      "Gradient Descent(9939/9): loss=15.48891926991737, w0=73.23322881954283, w1=12.392634712851828\n",
      "[-2.46341273  0.56331616]\n",
      "Gradient Descent(9940/9): loss=15.978598687017698, w0=74.95761773364444, w1=11.998313397552723\n",
      "[ 6.55385064 -5.25463478]\n",
      "Gradient Descent(9941/9): loss=17.867101166459, w0=70.36992228706059, w1=15.676557743270664\n",
      "[-1.61871879  1.59566639]\n",
      "Gradient Descent(9942/9): loss=22.073839689879335, w0=71.50302544146437, w1=14.559591272971614\n",
      "[-3.14243019  4.5241022 ]\n",
      "Gradient Descent(9943/9): loss=17.572612266648242, w0=73.70272657263781, w1=11.392719736349878\n",
      "[ 1.41360229 -3.50631694]\n",
      "Gradient Descent(9944/9): loss=17.647217719360185, w0=72.71320496728418, w1=13.84714159669472\n",
      "[-0.28724436  1.74817237]\n",
      "Gradient Descent(9945/9): loss=15.622006100530916, w0=72.91427601714356, w1=12.623420939717775\n",
      "[-2.98106499  2.8271414 ]\n",
      "Gradient Descent(9946/9): loss=15.824570968215097, w0=75.00102150880738, w1=10.644421962688398\n",
      "[-0.534534   -2.08559174]\n",
      "Gradient Descent(9947/9): loss=20.862418262880265, w0=75.37519530822573, w1=12.104336181660464\n",
      "[ 1.58023472 -0.14069929]\n",
      "Gradient Descent(9948/9): loss=18.497567075324447, w0=74.26903100168285, w1=12.2028256823641\n",
      "[-0.79441591 -0.48039725]\n",
      "Gradient Descent(9949/9): loss=16.676526538872615, w0=74.8251221366233, w1=12.539103756029599\n",
      "[ 5.58710316 -2.34865865]\n",
      "Gradient Descent(9950/9): loss=17.000547138270473, w0=70.91414992183338, w1=14.183164808425957\n",
      "[-1.32233755 -0.31238233]\n",
      "Gradient Descent(9951/9): loss=18.464968066697008, w0=71.83978620372737, w1=14.401832435986051\n",
      "[ 1.33887769  0.57956684]\n",
      "Gradient Descent(9952/9): loss=16.868295977010707, w0=70.90257181986318, w1=13.996135650892482\n",
      "[-2.58656098  3.00166261]\n",
      "Gradient Descent(9953/9): loss=18.37851218484586, w0=72.71316450930483, w1=11.894971825501067\n",
      "[-2.59181358  1.81178362]\n",
      "Gradient Descent(9954/9): loss=16.810228901231447, w0=74.52743401285052, w1=10.626723288578017\n",
      "[-0.75294428 -3.39892755]\n",
      "Gradient Descent(9955/9): loss=20.216437343925477, w0=75.05449500719763, w1=13.005972575390768\n",
      "[ 3.23952643 -3.22109545]\n",
      "Gradient Descent(9956/9): loss=17.047911249245615, w0=72.78682650682113, w1=15.260739390152388\n",
      "[ 1.38803355  0.03235118]\n",
      "Gradient Descent(9957/9): loss=17.100489297007293, w0=71.81520302084515, w1=15.23809356424513\n",
      "[-1.01997164  2.4494252 ]\n",
      "Gradient Descent(9958/9): loss=18.02514487946072, w0=72.52918316645263, w1=13.523495925107225\n",
      "[ 0.26528658  0.75346771]\n",
      "Gradient Descent(9959/9): loss=15.679259109210484, w0=72.34348255717202, w1=12.996068525522421\n",
      "[-1.87602135 -0.53950594]\n",
      "Gradient Descent(9960/9): loss=15.954511153653913, w0=73.65669750265985, w1=13.373722681940706\n",
      "[-2.62377475  0.20146561]\n",
      "Gradient Descent(9961/9): loss=15.457307814606366, w0=75.4933398257552, w1=13.23269675808144\n",
      "[ 2.01164138 -1.51797266]\n",
      "Gradient Descent(9962/9): loss=17.835115622643137, w0=74.08519086031099, w1=14.295277622464857\n",
      "[ 0.20338129  0.30952062]\n",
      "Gradient Descent(9963/9): loss=16.03151435932378, w0=73.94282395968065, w1=14.078613185600995\n",
      "[ 3.22146733 -1.53343299]\n",
      "Gradient Descent(9964/9): loss=15.77576579864381, w0=71.68779682610145, w1=15.152016275600024\n",
      "[-2.66564439  0.15297108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9965/9): loss=18.07400697698704, w0=73.55374790174481, w1=15.04493652070942\n",
      "[ 1.44928868  1.63185219]\n",
      "Gradient Descent(9966/9): loss=16.644605837150756, w0=72.53924582800177, w1=13.902639990210819\n",
      "[ 0.74171563  1.83012666]\n",
      "Gradient Descent(9967/9): loss=15.760089791192021, w0=72.02004488826495, w1=12.621551331005548\n",
      "[-1.47963784 -0.78794716]\n",
      "Gradient Descent(9968/9): loss=16.565489559607464, w0=73.05579137452098, w1=13.17311434081672\n",
      "[-3.24027052  0.63752401]\n",
      "Gradient Descent(9969/9): loss=15.461242162401279, w0=75.32398073983182, w1=12.726847532274007\n",
      "[ 3.82329087 -0.80473563]\n",
      "Gradient Descent(9970/9): loss=17.729859889009596, w0=72.64767713301826, w1=13.290162471808825\n",
      "[-0.07879246 -1.30406041]\n",
      "Gradient Descent(9971/9): loss=15.6126686785108, w0=72.70283185820035, w1=14.203004761715706\n",
      "[-0.73002925  0.51164852]\n",
      "Gradient Descent(9972/9): loss=15.82215754289095, w0=73.21385233288079, w1=13.844850797207602\n",
      "[ 0.21285495 -1.33570141]\n",
      "Gradient Descent(9973/9): loss=15.455756456576077, w0=73.06485386707409, w1=14.779841784932797\n",
      "[ 0.6086926 -2.4603384]\n",
      "Gradient Descent(9974/9): loss=16.25729213736529, w0=72.6387690469631, w1=16.502078666722024\n",
      "[ 0.17442171  6.5210781 ]\n",
      "Gradient Descent(9975/9): loss=20.167849385504997, w0=72.5166738532528, w1=11.937323995751793\n",
      "[ 0.22805697  1.64248509]\n",
      "Gradient Descent(9976/9): loss=16.877426260023, w0=72.35703397413693, w1=10.787584430330588\n",
      "[-0.64288842 -5.12482498]\n",
      "Gradient Descent(9977/9): loss=19.4485440540377, w0=72.80705586548126, w1=14.374961916378776\n",
      "[-1.28965069  3.03140703]\n",
      "Gradient Descent(9978/9): loss=15.905143003289245, w0=73.70981134920967, w1=12.252976994233965\n",
      "[ 0.75575672 -1.81582927]\n",
      "Gradient Descent(9979/9): loss=16.22480976414918, w0=73.18078164731727, w1=13.524057485134296\n",
      "[ 0.42139402 -2.26829067]\n",
      "Gradient Descent(9980/9): loss=15.393271480506362, w0=72.88580583299678, w1=15.11186095696389\n",
      "[ 1.10300439  4.97422773]\n",
      "Gradient Descent(9981/9): loss=16.80112167146559, w0=72.11370276143965, w1=11.629901544368288\n",
      "[-0.36526596 -0.44053516]\n",
      "Gradient Descent(9982/9): loss=17.793246762377553, w0=72.36938893643186, w1=11.938276157908023\n",
      "[-1.02017505 -1.59271375]\n",
      "Gradient Descent(9983/9): loss=17.001281461741765, w0=73.08351147089836, w1=13.053175785633814\n",
      "[ 0.74548267 -0.35511826]\n",
      "Gradient Descent(9984/9): loss=15.498990921272366, w0=72.56167359953095, w1=13.301758568934023\n",
      "[-2.08676524  0.96816401]\n",
      "Gradient Descent(9985/9): loss=15.669815519587628, w0=74.02240926727733, w1=12.624043761205256\n",
      "[-0.42493596 -2.39338377]\n",
      "Gradient Descent(9986/9): loss=16.017319156235846, w0=74.31986443686085, w1=14.299412402744174\n",
      "[ 3.51495972  0.22648063]\n",
      "Gradient Descent(9987/9): loss=16.248120827114466, w0=71.85939262956114, w1=14.140875960385175\n",
      "[-0.63227687 -0.4013284 ]\n",
      "Gradient Descent(9988/9): loss=16.633393732832324, w0=72.30198644169262, w1=14.421805842567418\n",
      "[-0.64998006 -0.67375708]\n",
      "Gradient Descent(9989/9): loss=16.32162594113621, w0=72.75697248309277, w1=14.893435797558524\n",
      "[-0.38561232 -0.14877018]\n",
      "Gradient Descent(9990/9): loss=16.52935213475061, w0=73.0269011039977, w1=14.997574924944706\n",
      "[-0.55952256 -0.50849935]\n",
      "Gradient Descent(9991/9): loss=16.57349121804966, w0=73.41856689363284, w1=15.353524471688388\n",
      "[-0.3915925   5.58092037]\n",
      "Gradient Descent(9992/9): loss=17.14924181776103, w0=73.69268164299525, w1=11.446880211571584\n",
      "[ 0.78441649 -2.06865209]\n",
      "Gradient Descent(9993/9): loss=17.531595918712984, w0=73.14359010037327, w1=12.894936676247939\n",
      "[-0.91212957 -1.89846503]\n",
      "Gradient Descent(9994/9): loss=15.568169053174188, w0=73.78208079936725, w1=14.223862197319624\n",
      "[ 2.13568159  4.56129181]\n",
      "Gradient Descent(9995/9): loss=15.7819168088899, w0=72.28710368667407, w1=11.03095792802439\n",
      "[-3.12320577 -2.27616596]\n",
      "Gradient Descent(9996/9): loss=18.890928746663036, w0=74.47334772911239, w1=12.62427409787816\n",
      "[ 2.27537316  0.08772513]\n",
      "Gradient Descent(9997/9): loss=16.447297765892156, w0=72.88058651509532, w1=12.562866508106396\n",
      "[ 1.71196063 -2.26650424]\n",
      "Gradient Descent(9998/9): loss=15.891614208060899, w0=71.68221407595632, w1=14.149419477733433\n",
      "[-1.39362005 -0.04831709]\n",
      "Gradient Descent(9999/9): loss=16.908942849985657, w0=72.65774811370517, w1=14.183241439127084\n",
      "SGD: execution time=5.666 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 10\n",
    "gamma = 0.7\n",
    "batch_size = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f325dc632a914eee8403aa5d73d467df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Effect of Outliers and MAE Cost Function, and Subgradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Load and plot data containing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11410da90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAL8AAAD8CAYAAAAmJnXEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGOFJREFUeJztnX+MHOV5x7/Pru8WkyYlGAvz62qqolAKLYaT2y1/9FIc\nAsQWNFYlWrXrAuJMAiFUlVAtFMkVkl2FqjHBlPoS2/FVFf0jFyBpofhHfEB1W+AMNLg4kNAoBoKL\nYylt1YrlfPv0j+fezLuzM7OzuzM7s/M+H2m03t25mZfkO+8+7/M+P4iZoSguUsp6AIqSFSp+xVlU\n/IqzqPgVZ1HxK86i4lecRcWvOIuKX3EWFb/iLMuyuOk555zDq1evzuLWigMcOXLkp8y8stN5mYh/\n9erVmJ+fz+LWigMQ0Y/jnKdmj+IsKn7FWVT8irOo+BVnUfErzqLiV5xFxa84i4pfyRX1OrB9u7ym\nTSabXIoSRL0OXHst8OGHwOgocOgQUK2mdz+d+ZXcMDsrwl9clNfZ2XTvp+JXcsPEhMz45bK8Tkyk\nez81e5TcUK2KqTM7K8JP0+QBVPxKzqhW0xe9Qc0exVlU/IqzqPgVZ1HxK86i4lecRcWvDIxBhi7E\nQV2dSiLU69H++UGHLsRBxa/0TRxhB4UuZC1+NXuUvokTkzPo0IU49D3zE9FFAKYBnAuAAUwx80P9\nXlcZHoywzcwfJOxBhy7EIQmz5zSAP2Pml4noowCOENEBZn49gWsrQ0BcYQ8ydCEOfYufmd8D8N7S\nv/+HiI4BuACAit8h8ibsOCRq8xPRagBrALwQ8N0kEc0T0fzJkyeTvK2i9ERi4ieiXwAwA+BeZv5v\n//fMPMXM48w8vnJlxzKKipI6iYifiEYgwv97Zv5WEtdUlDCS2ixLwttDAHYDOMbMf93v9RQliiQ3\ny5KY+a8B8McAfpeIXl06bkzgukrOyEN4QpJ5vkl4e/4FAPV7HSWfmLCFFSuAe+/NPjwhzp5CXDS8\nQQnFNjGIgGZTjrTCEzrFBwHJbpap+JWf4xefbWKUShKaQJROeEI3tnxSewoqfgVAsPj8JsaOHcCp\nU+mEJ2QR+KbiVwAEi2/LlsHF4yRpy8dFxa8ACBdfJxMjjp0eB78tD4hnKc2HTsWvAOhtIZl0gop5\n0AaV+KLx/MrPqVbF1AnLxPL7+NOqrTmomp068ytt+E2ZsJk4LTt9UPa/il8B0L6Z1WiIe/ORR8TD\nE+SJSStBZVCJLyp+pWVmL5WA06cBZtnQuvtuYOfO8Jk4rTj+QeQHqPiVFhubWTaymOW7xUWZ+fOW\ngpgEKn6lzcb+wheAr3xFhF+peII3oo9ybybl+hwEKn4l0Ma++eZgEUe5IfNYmycKFb8CoN3GDrO5\no8IQ8libJwr18ytdEVV/J4+1eaLQmb9ADMLejnJD5rE2TxTEZlk/QMbHx3l+fn7g9y0yxt5uNGTm\n3bkTmJzMelTZQERHmHm803lq9hSE2VkRfrMJLCwAd92Vn2rIeUXFXxAmJmTGNzSb6fexHXZU/AWh\nWhVTZ9ky2aU1/vkw8pCMnjW64C0Qk5PAFVd0XnAOmz8+LVT8BSNOTMz0NPDBBxLCYPvji7JzGxcV\nv2PU68DevV7sTrksgg76NQDyVbYkaVT8jjE7K1GbhhtvFCFv3966Ozs9DezbN7iyJVmgC17HmJiQ\nRbHhqadk1l+xQkReKsnsDngPQ7MpvxDDsnMbFxW/Y1SrwK23itABEff0tJg1RuQ7dgC1mheqUKmI\nJ+mBB4pj8gBq9jhJreaZNPYs32zKWuCVV8RzNEyhCr2g4Q2OYntvAOCTn5QdYkAeiGG26+OGN+jM\n7yh+l+ittwK7dsnMv7g43OKPi9r8DhG1q1urAWecUbxFbRQ68zvA1BSwe7fY8s1msK9+2MKRk0DF\nX3CmpoDNm1s/azSCzRo7I8t+X1RU/AVnZqb9s2ZT/Pp+6nWZ9RcWgJGR4tv9avMXhDB7fuPG9nNL\nJSlH4md6WlyeJuZnejqdseaFRGZ+ItoDYD2A95n58iSuqcQnKkrTZHPt3g28/LII25UFbSeSmvm/\nAeD6hK6ldEmnwq6Tk8ALLwDPPRe9S1uryW4ukbyuWVPsmP9EZn5mfm6p+7qSMkGhxXELuxrf/tQU\nsHWrmER2nm+1Chw+XOxITpuBLXiJaBLAJACMjY0N6raFIsq82bRJXmu1aJHa3p/9+4G33gLOOqu1\nKltQlGcRF78DEz8zTwGYAiS8YVD3LRJh5o39QKxZE+2r37279f2DD3qRnPbDlEWboEGjrs6M6SZD\nKkiQdlZWoyFVG5hlp/a229p/Cc44o/WaJpwhKKMrzQZ0uYCZEzkArAZwNM65V199NSvMc3PMy5cz\nl8vyOjcX72+2bZPXuTnmkRFmkbBcp1Ty3hO1X/fOO73v7cOc18uY8gaAeY6hw0S8PUT0GIA6gE8Q\n0TtEdHsS1y06vbTfsVsHTU/LhpThmms8bw3QmqNrqNXEzLFZu9YzeQbVEigPJCJ+Zv4DZj6PmUeY\n+UJm3t35r5Ska1tedpmIePNmeQiCrlutAo8+6jWUrlTEvPHb+i4EuGk8f8ZMTUkIgt/tGIeocIRO\na4l63dvB9a8Lhr1SQ9x4fhV/hiRRP8eI+MQJYNWqzq7OoHuHLY6HFU1mGQKSqme/d6+XhbVnT/R1\nzKx+/Lh378VFSWTZt6+Ym1lhqPgzJAlfunmADCYgLczUMbP9smUy45u8XX8BKxfQqM4MMQkknaoi\nRGVgmQfIZs+e4HPtX5rTp8XUiVocF544/tCkD/XzxyeO331ujnnt2lZ//7Zt8a9l7x0UAcT086vZ\nk3PirAuqVXFX2otnU4LQ9tqEpSoOoudtHlHx55ygdUGQK9I8AMZtCgR7koKEPuyuzV5R8ecc/2wN\nBIu6XpcQ5EYD+O53gfXr43mSXC5XrgveDInbIMIOaQgLP7DbEp0+DXznO55HJ2oh61I4gx+d+TOi\n1xk3zD06MSExO82mvGeWQlRjY9HmjAuhy2Go+DOi1w2uqEXrI48Ad98t16xU4u3Yulivx6Diz4g0\nZty4bYn8uOrt0dieDPEXi40jWpcXqHHR2J4hwMy4cQQdFJPTbzhCmIvTFdenij8H2J6aoFKC/ghM\n01mlH3Mp7IFz6ZdFXZ05YMUKz0sTVErQXhwvLooXp98uKVEuU1dcnzrz54BTpzw3ZVApQf/iOIm4\n+yiXqSuuTxV/DpiYENekCTU+flzMD9uN2Ys7Msp2j3KZuuL6VG9PDrCzsZ56SkyOIHs7TmpipzAI\nF1BvT0Z06ykxC0yTiRWWWDI11bqBFfRg2GLftKn4Fdf6RcWfIL14SmxPj8FUUDMzeL0uxahM8+gg\nj5B/oQp4tnuQKaWotydRevGUTEyI+9JmfLz1wZmdbX04yuX2hai/5EitJte44w75Jfna1+TBLGrF\n5V5Q8SdILzVvqlVp8Gw/AK++2n7dSkV+EZYtk/PDFrAPPCBx/ebBGxvzXKRFd112i5o9CWIE2G1H\nk8lJaRZnWoEuLLQmofsTVaLq+xw/LuXHzaJ5xw53XJddEyfXMemjyDm8c3PMo6NSJ3N0NH5e7Nwc\nc6Xi5eHafxs3j3f5crmvP5e3aDm6ncAga3UqHr32tapWgRtu8N6bRtCAtyheXPQWu37MesN4rom8\nmd5OhlE8VPw5oV4Hnn7ae28vav3hDy++2L5wtdcblYqUJHHJt98LKv6E8fe1qtXazwlKX5yd9VyZ\nRMCVV3rfmfAHw5NPtntu7AXv4cNSjFaF34E4tlHSR5FtfuZoGzuqds7y5V59/VKpvWZ+kD2vtAO1\n+bMjysa29wI++MBbE5iZe906L8jN3pkNKj2+YkWxuyWmjbo6B8zEhPjqFxdlDt+zxzONZmfFlfn8\n8+2uSZP4Uqu50y0xbVT8A6ZalXh849NfXJTZf98+T8hRvbBc6paYNmr2ZECtJo3hjPkCtAr51KnO\nrkmXOqikRSIzPxFdD+AhAGUAX2fmv0ziulmQZv6qfW1/FTZ75o8bFuFK3H1qxFkVRx0Qwb8F4JcB\njAL4NwCXRf1NXr09aXUinJuTLoijo+HXDvIQubYzmxQYYJXmtQB+yMz/AQBE9A8AbgLwegLXHihJ\ndUqxMWHOplcuEO/aneL3lf5JQvwXAHjbev8OgN/0n0REkwAmAWBsbCyB2yZP0vmr9boEmTUawWEH\n9nl2HsCOHZ3j95X+GZi3h5mnAEwBksY4qPuGEVbmOyk72s7QMonpIyPi6fEnoPt/cWZmOsfvK/2T\nhPjfBXCR9f7Cpc9yS1TGVVL1642gjfDHx4GrrgLWrPEC0/y9b814jK+/0ZC/DYrfV/onCfG/BOAS\nIroYIvpbAPxhAtdNjW5s+ySqKZfLEq//0kti/gT9Cvh/cXqpual0SZxVcacDwI0A3oR4fe7vdH7W\n3p5uvDrbtsl5vcTTGG/NzTd7MTn2QZSsV0kRMMieXMz8FICnkrjWIOjGtu9nEWxMqM99Lvh7DqjS\noAwOZ8Mb4pblDnpQul0D1GrSKNpUUvjMZ1rr8+hiNhucFX832A9KN2sA+yE5fLi/B0hJHhV/l3Ra\nLBtRB0VdbtninedqQ4g84YT4k5hlbVGHrQHsX4VSSR6QZtOL21ex54vCiz+JevNBO7BBIcf2rwKz\n7OYCrXH7+gDkh8KHNPdab97Os/VfIyzk2J9EvmGD9wDY1RiUfFD4mb8XV2XQTB/nGkENo595RgtG\n5ZXCi7+XeB27eGyjITN93Epsdp+t2dnorCwlWwovfqB7z0pYmyDjq9+7V1yXQHhDN1dr4w8TTog/\niiBPUFCboOlpr4Z+owF8+cutJo2/qrLm1+afwi94ozAz9Je+1FoEylRFNgvXIFv9Jz8JX0hrfu1w\n4PTMb8/QjYYknmzdGr5O2LNHKiiPjAC33w689lrwYjbOOkN3eLPHafGbGdosbg8elDh6Y8LYoqxW\nRaxJhB3rmiAfOCN+e4fW9r4cOiSz/cGD7VXS/AQ9EGGzepS4dU2QD5wQf70uYl9Y8JJJ7KTwrVtF\ngAsLyaQMdhK3S71u84wT4jc18w1BM7zZiTWv/dBJ3FpzJx84IX4//goKpjw4s7z2a4bEEbdGdWaP\nE+L3J5PcfntrkFlSZojfg6PizjdOiL9aDU4mMemFpm1n3GSToO86LXLVtZlD4iT6Jn3kIYF9dNRL\nJK9UvCYQ27Yx79oVnuC+axfzsmWtzSOYoxPd0yqDqASDQSawDxvGs2MwjeNMsdhSyVsD2A3gpqel\nmfPiory3K6lFmU6290cTW/KDc+Kv16VXbbnslQNctgx4+WVvs8v8JgDy/mc/a6+3CchDYjeP8Icz\nb98u/zZd1k2Sy969mtiSB5wSv138dWQEWL9ePn/6aWB+3gtkK5e9B6BUko7o/jafQZ3Q7XBmfz7A\nlVd6RauS8Cgp/eNMYFu9LsVfFxY8P/+qVcDatd5npZL0xNq502seUalI+UA7UG3zZuDZZ8M7oftj\nhu66Sx4u8zDpxlY+KNzMH+ZVmZ72zBzAMz+++MXW2P0zz5SYnV7LB9pmFdCayG4eLhM8p2RMnFVx\n0kda3p6oNp+2d8du57l2bXIlBO37VyrSkCLKc6SkA1xsRRqWrD4763loAJmVjQlz/vmt17BLCPZz\n/9OngbExMY1Mc2iN3swXhTJ7jFel2WwNUPO7Ie28WkAWvCZLqx+bPMzdqbu9+aRQ4geCA9SqVRH8\nzIwsXv0LVbP76w937hYNWBsuCiV+O0DNbFwZ1+M998hnzz4ri9c4cfm9oLP88FAo8YdtJgUlnz/+\neP/3C0uQUYaEOKvipI80Y3vuvFM8NoDE31x3XXtziFJJzuvH82I8O6WSd0315uQDuOjtAYCPfUzs\nfSJZ+B44ILXwS9Z/abMJ7NrVWrEhCrt0ocHuuWWu2auXSMmGQpk9U1Ni0tgwyw7upZcCb77ZGrsT\nJ3/WToEcGWkPZLO7LerO7XDRl/iJ6PcBbAXwqwDWMvN8EoPqlZmZ4M+ZgTfekHicDRu664pip0Da\ni2jbs6M2/3DS78x/FMBnAexKYCx9s3EjsH+/996YP8wyOy8uSizPfffFd0eeOBH+Xj07w01f4mfm\nYwBASWR9J4Dx38/MACtXAidPSjTlww+3bjx102t31arW8/zvleFlYDY/EU0CmASAsbGxxK7rF+0V\nV0jP2717xef//PPtO7omzj5OCmKt1lqprVbrbjxKjunkDgJwEGLe+I+brHNmAYzHcS9xgq5OfyCb\nCSIzrk5/SmFY4FunXrsmvbGTG1PTFfMBkkpjZOZ1KT13feMPZJuZaU86GR2VBen27RJqHFRMKk6d\nnbh1/bUS2/Aw1K5Ov2g3bhQz58MPZaf3ttuANWu8rojlsnh8gPbAsyRicrQS23DRr6vz9wA8DGAl\ngH8ioleZ+dOJjAyd7ecg0fqTTrZv92ZjALjjDgk19tv8cRNVuh2PkmPi2EZJH3Fs/m7t5zC7PCrB\npVOZkn7Go2QHhr10STf2c5S3Jmg2ts83YRCdKjSrPV88civ+buzn2dnWBnJ+YfoXrLaQTbUGf/3O\nfsajDAe5FX839nNYA7kwojK7orqoaGfFYpFb8QPxXYxBDeQ6XTfug6VdVIpLrsUfF9NArpNJ0ksV\nZbX1i0shxB9VKjBuFeUw1NYvLkMr/qBZ3PbkNBqyiN2wwYvi7GUGV999cRlK8dsCL5elvKCJ6LQ9\nPwDwxBNSmuSrX+19BtfQ5WKSe/EH7araAm82pRamqchg1+4xfPihLIJ1Bldsci3+IDsdAF58sbVU\neLPpmTHVqvwSfP7zXkhDVBy/4i65Fr/fTp+eljh9U4YEELu+Umk1YyYn5ZdgelreB9XC17h7Jdfi\n93tagNaWokTApz4VXPU4apZX370C5Fz8fk/La695Jb8Byazqpdy3+u4VIOfiB1pdmPfeK7Z+uSwu\nzBtu8OrkdCNe9d0rwBCI32AXiSqXJZHcJKl0a7qo714Bhkj8YfZ/r6aLen6UoRG/v0jUK68EpyQq\nSlyGRvyAN1MbT025LGmJ2tZT6YWhK1Rre2oWFyUf1yyI/cVkFSWKoZr5gWBPjfrtlV4Yupnf2P52\ng7ewRnSKEsXQzfxAu6emW7+9hjYowJCK308nv70tdkBNJEUohPiBcL+9fz2waZOGNijC0Nn83eJf\nDwDyEJgm1Lo/4C6FmfnD8K8HajU51OZXcin+JBekYesBFb2SO/Gn4bPXOB4liFzZ/PW6xOc3Guqz\nV9InNzO/XZFBW3sqgyA3M78dr18qAevWqQ9eSZfciN94ZcplSUjvJT1RUbohN2ZPmFdGQxGUtMiN\n+IF2r4xGaypp0pfZQ0QPEtH3ieh7RPQ4EZ2V1MCA1t3ZRkNMIY3XV5KiX5v/AIDLmfnXAbwJYEv/\nQ/Iw6wBTe//gQfkl0AdASYK+xM/M+5n59NLbfwVwYf9D8jDrgHXrvAdAff9KUiTp7bkNwNNhXxLR\nJBHNE9H8yZMnY1+0WhVzp1LRYDQlWYjtiq9BJxAdBLAq4Kv7mfnJpXPuBzAO4LPc6YIAxsfHeX5+\nvquBqtdHiQsRHWHm8U7ndfT2MPO6Djf6EwDrAVwbR/i9ovE5StL024H9egD3AfgdZv6/ZIakKIOh\nX5t/J4CPAjhARK8S0d8mMCZFGQh9zfzM/CtJDURRBk1uYnsUZdDkWvxahU1Jk1zF9thoXI+SNrmd\n+bUKm5I2uRW/Hd+vu7pKGuTW7NHuKUra5Fb8gO7qKumSW7NHUdJGxa84i4pfcRYVv+IsKn7FWVT8\nirN0zORK5aZEJwH8eOA3DuYcAD/NehAd0DF2xy8x88pOJ2Ui/jxBRPNxUt6yRMeYDmr2KM6i4lec\nRcUPTGU9gBjoGFPAeZtfcRed+RVncUr8RHQWEX1zqbjuMSKqEtHZRHSAiH6w9PrxjMf4p0T070R0\nlIgeI6Iz8jBGItpDRO8T0VHrs9BxEdEWIvohEb1BRJ8e9Hjj4JT4ATwE4J+Z+VIAvwHgGIA/B3CI\nmS8BcGjpfSYQ0QUA7gEwzsyXAygDuCUnY/wGgOt9nwWOi4gug4z715b+5m+IqDy4ocaEmZ04APwi\ngB9haZ1jff4GgPOW/n0egDcyHOMFAN4GcDYk1+IfAVyXlzECWA3gaKf/7SDVurdY5z0DoJq1BvyH\nSzP/xQBOAthLRK8Q0deJ6CMAzmXm95bOOQHg3KwGyMzvAvgrAMcBvAfgv5h5P3I0Rh9h4zIPseGd\npc9yhUviXwbgKgCPMvMaAP8Ln/nAMk1l5v5asplvgjyo5wP4CBH9kX1O1mMMI6/jisIl8b8D4B1m\nfmHp/TchD8N/EtF5ALD0+n5G4wOAdQB+xMwnmXkBwLcA/HbOxmgTNq53AVxknXfh0me5whnxM/MJ\nAG8T0SeWProWwOsAvg1g09JnmwA8mcHwDMcB/BYRnUlEBBnjMeRrjDZh4/o2gFuIqEJEFwO4BMCL\nGYwvmqwXHQNesF0JYB7A9wA8AeDjAFZAPBU/AHAQwNkZj/EvAHwfwFEAfwegkocxAngMsg5ZgPyK\n3h41LgD3A3gLsii+Iev/74MO3eFVnMUZs0dR/Kj4FWdR8SvOouJXnEXFrziLil9xFhW/4iwqfsVZ\n/h/lOfF7HDrnOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117dc6c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.scatter(y, tx[:,1], marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-73.63227246 -14.47570488]\n",
      "Gradient Descent(0/499): loss=2829.272224438416, w0=51.54259072181176, w1=10.132993413506084\n",
      "[-22.08968174  -4.34271146]\n",
      "Gradient Descent(1/499): loss=267.0500258779429, w0=67.0053679383553, w1=13.172891437557825\n",
      "[-6.62690452 -1.30281344]\n",
      "Gradient Descent(2/499): loss=36.45002800750046, w0=71.64420110331838, w1=14.084860844773322\n",
      "[-1.98807136 -0.39084403]\n",
      "Gradient Descent(3/499): loss=15.696028199160635, w0=73.03585105280729, w1=14.358451666937965\n",
      "[-0.59642141 -0.11725321]\n",
      "Gradient Descent(4/499): loss=13.828168216410077, w0=73.45334603765397, w1=14.440528913587356\n",
      "[-0.17892642 -0.03517596]\n",
      "Gradient Descent(5/499): loss=13.660060817962522, w0=73.57859453310797, w1=14.46515208758217\n",
      "[-0.05367793 -0.01055279]\n",
      "Gradient Descent(6/499): loss=13.644931152102245, w0=73.61616908174418, w1=14.472539039780616\n",
      "[-0.01610338 -0.00316584]\n",
      "Gradient Descent(7/499): loss=13.643569482174817, w0=73.62744144633503, w1=14.474755125440149\n",
      "[-0.00483101 -0.00094975]\n",
      "Gradient Descent(8/499): loss=13.643446931881352, w0=73.63082315571229, w1=14.47541995113801\n",
      "[-0.0014493  -0.00028493]\n",
      "Gradient Descent(9/499): loss=13.643435902354941, w0=73.63183766852546, w1=14.475619398847368\n",
      "[ -4.34791206e-04  -8.54775897e-05]\n",
      "Gradient Descent(10/499): loss=13.64343490969756, w0=73.63214202236942, w1=14.475679233160175\n",
      "[ -1.30437362e-04  -2.56432769e-05]\n",
      "Gradient Descent(11/499): loss=13.643434820358397, w0=73.6322333285226, w1=14.475697183454017\n",
      "[ -3.91312085e-05  -7.69298308e-06]\n",
      "Gradient Descent(12/499): loss=13.643434812317876, w0=73.63226072036856, w1=14.47570256854217\n",
      "[ -1.17393626e-05  -2.30789492e-06]\n",
      "Gradient Descent(13/499): loss=13.64343481159423, w0=73.63226893792235, w1=14.475704184068615\n",
      "[ -3.52180877e-06  -6.92368477e-07]\n",
      "Gradient Descent(14/499): loss=13.643434811529096, w0=73.63227140318848, w1=14.475704668726548\n",
      "[ -1.05654263e-06  -2.07710544e-07]\n",
      "Gradient Descent(15/499): loss=13.643434811523234, w0=73.63227214276833, w1=14.47570481412393\n",
      "[ -3.16962785e-07  -6.23131625e-08]\n",
      "Gradient Descent(16/499): loss=13.643434811522706, w0=73.63227236464228, w1=14.475704857743143\n",
      "[ -9.50888375e-08  -1.86939493e-08]\n",
      "Gradient Descent(17/499): loss=13.643434811522662, w0=73.63227243120446, w1=14.475704870828908\n",
      "[ -2.85266588e-08  -5.60818392e-09]\n",
      "Gradient Descent(18/499): loss=13.643434811522656, w0=73.63227245117312, w1=14.475704874754637\n",
      "[ -8.55799193e-09  -1.68245484e-09]\n",
      "Gradient Descent(19/499): loss=13.643434811522656, w0=73.63227245716372, w1=14.475704875932356\n",
      "[ -2.56739124e-09  -5.04735668e-10]\n",
      "Gradient Descent(20/499): loss=13.643434811522653, w0=73.6322724589609, w1=14.475704876285672\n",
      "[ -7.70215465e-10  -1.51419702e-10]\n",
      "Gradient Descent(21/499): loss=13.643434811522654, w0=73.63227245950004, w1=14.475704876391665\n",
      "[ -2.31070061e-10  -4.54264274e-11]\n",
      "Gradient Descent(22/499): loss=13.643434811522656, w0=73.63227245966179, w1=14.475704876423464\n",
      "[ -6.93218283e-11  -1.36282435e-11]\n",
      "Gradient Descent(23/499): loss=13.643434811522654, w0=73.63227245971032, w1=14.475704876433003\n",
      "[ -2.07917239e-11  -4.08906529e-12]\n",
      "Gradient Descent(24/499): loss=13.643434811522656, w0=73.63227245972487, w1=14.475704876435865\n",
      "[ -6.24005736e-12  -1.22767654e-12]\n",
      "Gradient Descent(25/499): loss=13.643434811522656, w0=73.63227245972924, w1=14.475704876436724\n",
      "[ -1.87775129e-12  -3.67654194e-13]\n",
      "Gradient Descent(26/499): loss=13.643434811522654, w0=73.63227245973054, w1=14.475704876436982\n",
      "[ -5.69961855e-13  -1.10397138e-13]\n",
      "Gradient Descent(27/499): loss=13.643434811522654, w0=73.63227245973094, w1=14.47570487643706\n",
      "[ -1.72022396e-13  -3.23133842e-14]\n",
      "Gradient Descent(28/499): loss=13.643434811522656, w0=73.63227245973106, w1=14.475704876437083\n",
      "[ -5.85487214e-14  -8.77550463e-15]\n",
      "Gradient Descent(29/499): loss=13.643434811522656, w0=73.6322724597311, w1=14.475704876437089\n",
      "[ -1.65201186e-14  -3.41522446e-15]\n",
      "Gradient Descent(30/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.47570487643709\n",
      "[ -2.13162821e-15  -1.83274544e-15]\n",
      "Gradient Descent(31/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(32/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(33/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(34/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(35/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(36/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(37/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(38/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(39/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(40/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(41/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(42/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(43/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(44/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(45/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(46/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(47/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(48/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(49/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(50/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(51/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(52/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(53/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(54/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(55/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(56/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(57/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(58/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(59/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(60/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(61/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(62/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(63/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(64/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(65/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(66/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(67/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(68/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(69/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(70/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(71/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(72/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(73/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(74/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(75/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(76/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(77/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(78/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(79/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(80/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(81/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(82/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(83/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(84/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(85/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(86/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(87/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(88/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(89/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(90/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(91/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(92/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(93/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(94/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(95/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(96/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(97/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(98/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(99/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(100/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(101/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(102/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(103/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(104/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(105/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(106/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(107/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(108/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(109/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(110/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(111/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(112/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(113/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(114/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(115/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(116/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(117/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(118/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(119/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(120/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(121/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(122/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(123/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(124/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(125/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(126/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(127/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(128/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(129/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(130/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(131/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(132/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(133/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(134/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(135/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(136/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(137/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(138/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(139/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(140/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(141/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(142/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(143/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(144/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(145/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(146/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(147/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(148/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(149/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(150/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(151/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(152/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(153/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(154/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(155/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(156/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(157/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(158/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(159/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(160/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(161/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(162/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(163/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(164/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(165/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(166/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(167/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(168/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(169/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(170/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(171/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(172/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(173/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(174/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(175/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(176/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(177/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(178/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(179/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(180/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(181/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(182/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(183/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(184/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(185/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(186/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(187/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(188/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(189/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(190/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(191/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(192/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(193/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(194/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(195/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(196/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(197/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(198/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(199/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(200/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(201/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(202/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(203/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(204/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(205/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(206/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(207/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(208/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(209/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(210/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(211/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(212/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(213/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(214/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(215/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(216/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(217/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(218/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(219/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(220/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(221/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(222/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(223/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(224/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(225/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(226/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(227/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(228/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(229/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(230/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(231/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(232/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(233/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(234/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(235/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(236/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(237/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(238/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(239/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(240/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(241/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(242/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(243/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(244/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(245/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(246/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(247/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(248/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(249/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(250/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(251/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(252/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(253/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(254/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(255/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(256/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(257/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(258/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(259/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(260/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(261/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(262/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(263/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(264/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(265/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(266/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(267/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(268/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(269/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(270/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(271/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(272/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(273/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(274/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(275/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(276/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(277/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(278/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(279/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(280/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(281/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(282/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(283/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(284/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(285/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(286/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(287/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(288/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(289/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(290/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(291/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(292/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(293/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(294/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(295/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(296/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(297/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(298/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(299/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(300/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(301/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(302/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(303/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(304/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(305/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(306/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(307/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(308/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(309/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(310/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(311/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(312/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(313/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(314/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(315/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(316/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(317/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(318/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(319/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(320/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(321/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(322/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(323/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(324/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(325/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(326/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(327/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(328/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(329/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(330/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(331/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(332/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(333/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(334/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(335/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(336/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(337/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(338/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(339/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(340/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(341/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(342/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(343/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(344/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(345/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(346/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(347/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(348/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(349/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(350/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(351/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(352/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(353/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(354/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(355/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(356/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(357/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(358/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(359/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(360/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(361/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(362/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(363/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(364/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(365/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(366/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(367/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(368/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(369/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(370/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(371/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(372/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(373/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(374/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(375/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(376/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(377/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(378/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(379/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(380/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(381/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(382/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(383/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(384/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(385/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(386/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(387/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(388/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(389/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(390/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(391/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(392/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(393/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(394/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(395/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(396/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(397/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(398/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(399/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(400/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(401/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(402/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(403/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(404/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(405/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(406/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(407/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(408/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(409/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(410/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(411/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(412/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(413/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(414/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(415/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(416/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(417/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(418/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(419/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(420/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(421/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(422/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(423/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(424/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(425/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(426/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(427/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(428/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(429/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(430/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(431/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(432/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(433/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(434/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(435/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(436/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(437/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(438/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(439/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(440/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(441/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(442/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(443/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(444/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(445/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(446/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(447/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(448/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(449/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(450/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(451/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(452/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(453/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(454/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(455/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(456/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(457/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(458/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(459/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(460/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(461/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(462/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(463/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(464/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(465/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(466/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(467/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(468/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(469/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(470/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(471/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(472/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(473/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(474/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(475/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(476/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(477/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(478/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(479/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(480/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(481/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(482/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(483/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(484/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(485/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(486/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(487/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(488/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(489/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(490/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(491/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(492/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(493/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(494/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(495/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(496/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(497/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(498/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "[ -1.91846539e-15   6.99712224e-17]\n",
      "Gradient Descent(499/499): loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "Gradient Descent: execution time=0.243 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abaa43382ffe49af82dffbc787fb0d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x117a06908>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAL8AAAD8CAYAAAAmJnXEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3jeOPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1179db6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.scatter(y, tx[:,1], marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-74.06780585 -11.03489487]\n",
      "Gradient Descent(0/499): loss=2869.8351145358524, w0=51.84746409844842, w1=7.7244264061924195\n",
      "[-22.22034176  -3.31046846]\n",
      "Gradient Descent(1/499): loss=318.2821247015965, w0=67.40170332798297, w1=10.041754328050114\n",
      "[-6.66610253 -0.99314054]\n",
      "Gradient Descent(2/499): loss=88.6423556165128, w0=72.06797509684336, w1=10.736952704607411\n",
      "[-1.99983076 -0.29794216]\n",
      "Gradient Descent(3/499): loss=67.9747763988552, w0=73.46785662750146, w1=10.945512217574597\n",
      "[-0.59994923 -0.08938265]\n",
      "Gradient Descent(4/499): loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "[-0.17998477 -0.02681479]\n",
      "Gradient Descent(5/499): loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631798\n",
      "[-0.05399543 -0.00804444]\n",
      "Gradient Descent(6/499): loss=65.93222021235334, w0=74.0516072257859, w1=11.032481534481914\n",
      "[-0.01619863 -0.00241333]\n",
      "Gradient Descent(7/499): loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536945\n",
      "[-0.00485959 -0.000724  ]\n",
      "Gradient Descent(8/499): loss=65.93074217249234, w0=74.06634797830372, w1=11.034677666153454\n",
      "[-0.00145788 -0.0002172 ]\n",
      "Gradient Descent(9/499): loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038408\n",
      "[ -4.37362987e-04  -6.51599507e-05]\n",
      "Gradient Descent(10/499): loss=65.93073020036947, w0=74.06767464603033, w1=11.034875318003895\n",
      "[ -1.31208896e-04  -1.95479852e-05]\n",
      "Gradient Descent(11/499): loss=65.93073011140231, w0=74.06776649225755, w1=11.034889001593541\n",
      "[ -3.93626688e-05  -5.86439556e-06]\n",
      "Gradient Descent(12/499): loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "[ -1.18088006e-05  -1.75931867e-06]\n",
      "Gradient Descent(13/499): loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "[ -3.54264019e-06  -5.27795600e-07]\n",
      "Gradient Descent(14/499): loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "[ -1.06279205e-06  -1.58338677e-07]\n",
      "Gradient Descent(15/499): loss=65.93073010260395, w0=74.06780553608874, w1=11.034894818487496\n",
      "[ -3.18837625e-07  -4.75016064e-08]\n",
      "Gradient Descent(16/499): loss=65.93073010260342, w0=74.06780575927507, w1=11.03489485173862\n",
      "[ -9.56512909e-08  -1.42504799e-08]\n",
      "Gradient Descent(17/499): loss=65.93073010260339, w0=74.06780582623098, w1=11.034894861713957\n",
      "[ -2.86953844e-08  -4.27514346e-09]\n",
      "Gradient Descent(18/499): loss=65.93073010260336, w0=74.06780584631775, w1=11.034894864706557\n",
      "[ -8.60861196e-09  -1.28254457e-09]\n",
      "Gradient Descent(19/499): loss=65.93073010260338, w0=74.06780585234378, w1=11.034894865604338\n",
      "[ -2.58258323e-09  -3.84765554e-10]\n",
      "Gradient Descent(20/499): loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873675\n",
      "[ -7.74778754e-10  -1.15427151e-10]\n",
      "Gradient Descent(21/499): loss=65.93073010260336, w0=74.06780585469393, w1=11.034894865954474\n",
      "[ -2.32434710e-10  -3.46272893e-11]\n",
      "Gradient Descent(22/499): loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "[ -6.97352671e-11  -1.03889619e-11]\n",
      "Gradient Descent(23/499): loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "[ -2.09215439e-11  -3.11498641e-12]\n",
      "Gradient Descent(24/499): loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988166\n",
      "[ -6.26937886e-12  -9.37633898e-13]\n",
      "Gradient Descent(25/499): loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988822\n",
      "[ -1.87998352e-12  -2.76801824e-13]\n",
      "Gradient Descent(26/499): loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989015\n",
      "[ -5.57459469e-13  -8.53313291e-14]\n",
      "Gradient Descent(27/499): loss=65.93073010260338, w0=74.06780585492619, w1=11.034894865989076\n",
      "[ -1.72781481e-13  -3.18348978e-14]\n",
      "Gradient Descent(28/499): loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989099\n",
      "[ -4.57279978e-14  -2.00020327e-15]\n",
      "Gradient Descent(29/499): loss=65.93073010260339, w0=74.06780585492635, w1=11.0348948659891\n",
      "[ -1.70952361e-14  -1.15002425e-15]\n",
      "Gradient Descent(30/499): loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(31/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(32/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(33/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(34/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(35/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(36/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(37/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(38/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(39/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(40/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(41/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(42/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(43/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(44/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(45/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(46/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(47/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(48/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(49/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(50/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(51/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(52/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(53/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(54/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(55/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(56/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(57/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(58/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(59/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(60/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(61/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(62/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(63/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(64/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(65/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(66/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(67/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(68/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(69/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(70/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(71/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(72/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(73/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(74/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(75/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(76/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(77/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(78/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(79/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(80/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(81/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(82/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(83/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(84/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(85/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(86/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(87/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(88/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(89/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(90/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(91/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(92/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(93/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(94/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(95/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(96/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(97/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(98/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(99/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(100/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(101/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(102/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(103/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(104/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(105/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(106/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(107/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(108/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(109/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(110/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(111/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(112/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(113/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(114/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(115/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(116/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(117/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(118/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(119/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(120/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(121/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(122/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(123/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(124/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(125/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(126/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(127/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(128/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(129/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(130/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(131/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(132/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(133/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(134/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(135/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(136/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(137/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(138/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(139/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(140/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(141/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(142/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(143/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(144/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(145/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(146/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(147/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(148/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(149/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(150/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(151/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(152/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(153/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(154/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(155/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(156/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(157/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(158/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(159/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(160/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(161/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(162/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(163/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(164/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(165/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(166/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(167/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(168/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(169/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(170/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(171/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(172/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(173/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(174/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(175/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(176/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(177/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(178/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(179/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(180/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(181/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(182/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(183/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(184/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(185/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(186/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(187/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(188/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(189/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(190/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(191/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(192/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(193/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(194/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(195/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(196/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(197/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(198/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(199/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(200/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(201/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(202/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(203/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(204/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(205/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(206/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(207/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(208/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(209/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(210/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(211/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(212/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(213/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(214/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(215/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(216/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(217/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(218/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(219/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(220/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(221/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(222/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(223/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(224/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(225/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(226/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(227/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(228/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(229/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(230/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(231/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(232/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(233/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(234/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(235/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(236/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(237/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(238/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(239/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(240/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(241/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(242/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(243/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(244/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(245/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(246/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(247/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(248/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(249/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(250/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(251/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(252/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(253/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(254/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(255/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(256/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(257/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(258/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(259/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(260/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(261/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(262/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(263/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(264/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(265/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(266/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(267/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(268/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(269/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(270/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(271/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(272/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(273/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(274/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(275/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(276/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(277/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(278/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(279/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(280/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(281/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(282/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(283/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(284/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(285/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(286/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(287/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(288/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(289/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(290/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(291/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(292/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(293/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(294/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(295/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(296/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(297/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(298/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(299/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(300/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(301/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(302/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(303/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(304/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(305/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(306/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(307/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(308/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(309/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(310/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(311/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(312/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(313/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(314/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(315/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(316/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(317/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(318/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(319/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(320/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(321/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(322/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(323/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(324/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(325/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(326/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(327/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(328/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(329/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(330/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(331/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(332/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(333/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(334/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(335/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(336/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(337/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(338/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(339/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(340/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(341/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(342/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(343/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(344/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(345/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(346/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(347/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(348/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(349/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(350/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(351/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(352/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(353/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(354/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(355/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(356/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(357/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(358/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(359/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(360/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(361/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(362/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(363/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(364/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(365/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(366/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(367/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(368/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(369/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(370/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(371/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(372/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(373/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(374/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(375/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(376/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(377/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(378/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(379/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(380/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(381/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(382/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(383/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(384/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(385/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(386/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(387/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(388/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(389/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(390/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(391/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(392/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(393/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(394/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(395/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(396/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(397/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(398/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(399/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(400/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(401/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(402/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(403/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(404/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(405/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(406/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(407/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(408/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(409/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(410/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(411/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(412/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(413/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(414/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(415/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(416/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(417/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(418/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(419/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(420/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(421/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(422/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(423/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(424/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(425/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(426/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(427/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(428/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(429/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(430/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(431/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(432/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(433/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(434/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(435/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(436/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(437/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(438/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(439/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(440/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(441/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(442/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(443/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(444/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(445/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(446/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(447/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(448/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(449/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(450/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(451/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(452/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(453/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(454/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(455/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(456/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(457/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(458/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(459/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(460/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(461/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(462/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(463/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(464/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(465/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(466/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(467/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(468/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(469/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(470/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(471/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(472/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(473/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(474/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(475/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(476/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(477/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(478/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(479/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(480/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(481/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(482/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(483/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(484/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(485/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(486/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(487/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(488/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(489/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(490/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(491/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(492/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(493/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(494/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(495/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(496/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(497/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(498/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "[ -3.16578447e-15   5.41379045e-16]\n",
      "Gradient Descent(499/499): loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "Gradient Descent: execution time=0.189 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701e0122f139419fa188ebbea4e36530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the function `compute_loss(y, tx, w)` for the Mean Absolute Error cost function [here](#compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
