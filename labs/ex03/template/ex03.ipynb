{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Least squares and linear basis functions models\n",
    "## 1.1 Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # least squares: TODO\n",
    "    # returns mse, and optimal weights\n",
    "    # ***************************************************\n",
    "    w_star = np.dot(np.dot(np.linalg.inv(np.dot(tx.T,tx)),tx.T),y)\n",
    "    mse = np.sqrt(2*sum((y - np.dot(tx, w_star))**2)/(2*len(y)))\n",
    "    return mse, w_star\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "height, weight, gender = load_data_from_ex02(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.5472313578630299, array([ 73.293922  ,  13.47971243]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares(y,tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Here we will reuse the dataset `height_weight_genders.csv` from previous exercise section to check the correctness of your implementation. Please compare it with your previous result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient and loss\n",
    "    gradient = (-1/len(y))*np.dot(tx.T, y - np.dot(tx,w))\n",
    "    # ***************************************************\n",
    "    print(gradient)\n",
    "    return gradient\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    res = y - np.dot(tx,w)\n",
    "    mse = (np.dot(res.T,res)/(2*len(y)))\n",
    "    \n",
    "    return mse\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*gradient\n",
    "#         raise NotImplementedError\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "        \n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "def test_your_least_squares():\n",
    "    height, weight, gender = load_data_from_ex02(sub_sample=False, add_outlier=False)\n",
    "    x, mean_x, std_x = standardize(height)\n",
    "    y, tx = build_model_data(x, weight)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # least square or grid search: TODO\n",
    "    # this code should compare the optimal weights obtained \n",
    "    # by least squares vs. grid search\n",
    "    # ***************************************************\n",
    "    ls_mse, ls_w_star = least_squares(y,tx)\n",
    "    gd_losses, gd_ws = gradient_descent(y,tx, initial_w=[0,0], max_iters=500, gamma=0.7)\n",
    "    gd_mse = gd_losses[-1]\n",
    "    gd_w_star = gd_ws[-1]\n",
    "    print(ls_mse, ls_w_star)\n",
    "    print(gd_mse, gd_w_star)\n",
    "#     raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-73.293922   -13.47971243]\n",
      "Gradient Descent(0/499): loss=2792.236712759167, w0=51.305745401473644, w1=9.435798704492269\n",
      "[-21.9881766   -4.04391373]\n",
      "Gradient Descent(1/499): loss=265.3024621089598, w0=66.69746902191571, w1=12.266538315840005\n",
      "[-6.59645298 -1.21317412]\n",
      "Gradient Descent(2/499): loss=37.87837955044126, w0=71.31498610804834, w1=13.115760199244333\n",
      "[-1.97893589 -0.36395224]\n",
      "Gradient Descent(3/499): loss=17.41021212017447, w0=72.70024123388814, w1=13.370526764265632\n",
      "[-0.59368077 -0.10918567]\n",
      "Gradient Descent(4/499): loss=15.568077051450457, w0=73.11581777164007, w1=13.446956733772023\n",
      "[-0.17810423 -0.0327557 ]\n",
      "Gradient Descent(5/499): loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "[-0.05343127 -0.00982671]\n",
      "Gradient Descent(6/499): loss=15.387363601208632, w0=73.27789262136334, w1=13.476764421879516\n",
      "[-0.01602938 -0.00294801]\n",
      "Gradient Descent(7/499): loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "[-0.00480881 -0.0008844 ]\n",
      "Gradient Descent(8/499): loss=15.38589982226167, w0=73.29247935783842, w1=13.47944711380919\n",
      "[-0.00144264 -0.00026532]\n",
      "Gradient Descent(9/499): loss=15.385888944638305, w0=73.29348920882515, w1=13.47963283863509\n",
      "[ -4.32793280e-04  -7.95963540e-05]\n",
      "Gradient Descent(10/499): loss=15.3858879656522, w0=73.29379216412117, w1=13.479688556082861\n",
      "[ -1.29837984e-04  -2.38789062e-05]\n",
      "Gradient Descent(11/499): loss=15.385887877543452, w0=73.29388305070998, w1=13.479705271317192\n",
      "[ -3.89513952e-05  -7.16367186e-06]\n",
      "Gradient Descent(12/499): loss=15.385887869613665, w0=73.29391031668663, w1=13.479710285887492\n",
      "[ -1.16854186e-05  -2.14910156e-06]\n",
      "Gradient Descent(13/499): loss=15.38588786889998, w0=73.29391849647962, w1=13.479711790258582\n",
      "[ -3.50562557e-06  -6.44730466e-07]\n",
      "Gradient Descent(14/499): loss=15.385887868835754, w0=73.29392095041752, w1=13.479712241569908\n",
      "[ -1.05168767e-06  -1.93419140e-07]\n",
      "Gradient Descent(15/499): loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "[ -3.15506298e-07  -5.80257425e-08]\n",
      "Gradient Descent(16/499): loss=15.385887868829453, w0=73.2939219074533, w1=13.479712417581325\n",
      "[ -9.46518941e-08  -1.74077234e-08]\n",
      "Gradient Descent(17/499): loss=15.385887868829403, w0=73.29392197370962, w1=13.479712429766732\n",
      "[ -2.83955739e-08  -5.22231686e-09]\n",
      "Gradient Descent(18/499): loss=15.3858878688294, w0=73.29392199358652, w1=13.479712433422353\n",
      "[ -8.51866771e-09  -1.56669521e-09]\n",
      "Gradient Descent(19/499): loss=15.3858878688294, w0=73.2939219995496, w1=13.47971243451904\n",
      "[ -2.55559244e-09  -4.70008302e-10]\n",
      "Gradient Descent(20/499): loss=15.3858878688294, w0=73.29392200133852, w1=13.479712434848047\n",
      "[ -7.66675839e-10  -1.41001982e-10]\n",
      "Gradient Descent(21/499): loss=15.385887868829398, w0=73.29392200187519, w1=13.479712434946748\n",
      "[ -2.30001388e-10  -4.23007350e-11]\n",
      "Gradient Descent(22/499): loss=15.3858878688294, w0=73.29392200203618, w1=13.479712434976358\n",
      "[ -6.90057277e-11  -1.26902762e-11]\n",
      "Gradient Descent(23/499): loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "[ -2.07041921e-11  -3.80678102e-12]\n",
      "Gradient Descent(24/499): loss=15.385887868829398, w0=73.29392200209898, w1=13.479712434987906\n",
      "[ -6.20748324e-12  -1.14231398e-12]\n",
      "Gradient Descent(25/499): loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "[ -1.85864337e-12  -3.43004558e-13]\n",
      "Gradient Descent(26/499): loss=15.3858878688294, w0=73.29392200210464, w1=13.479712434988945\n",
      "[ -5.54064172e-13  -1.03244702e-13]\n",
      "Gradient Descent(27/499): loss=15.385887868829403, w0=73.29392200210502, w1=13.479712434989018\n",
      "[ -1.70530257e-13  -3.05135472e-14]\n",
      "Gradient Descent(28/499): loss=15.3858878688294, w0=73.29392200210513, w1=13.47971243498904\n",
      "[ -5.42968337e-14  -8.98694452e-15]\n",
      "Gradient Descent(29/499): loss=15.385887868829398, w0=73.29392200210518, w1=13.479712434989047\n",
      "[ -1.10958354e-14  -2.11457518e-15]\n",
      "Gradient Descent(30/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(31/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(32/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(33/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(34/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(35/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(36/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(37/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(38/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(39/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(40/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(41/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(42/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(43/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(44/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(45/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(46/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(47/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(48/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(49/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(50/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(51/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(52/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(53/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(54/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(55/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(56/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(57/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(58/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(59/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(60/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(61/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(62/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(63/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(64/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(65/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(66/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(67/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(68/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(69/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(70/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(71/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(72/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(73/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(74/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(75/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(76/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(77/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(78/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(79/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(80/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(81/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(82/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(83/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(84/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(85/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(86/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(87/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(88/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(89/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(90/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(91/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(92/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(93/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(94/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(95/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(96/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(97/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(98/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(99/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(100/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(101/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(102/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(103/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(104/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(105/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(106/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(107/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(108/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(109/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(110/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(111/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(112/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(113/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(114/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(115/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(116/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(117/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(118/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(119/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(120/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(121/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(122/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(123/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(124/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(125/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(126/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(127/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(128/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(129/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(130/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(131/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(132/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(133/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(134/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(135/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(136/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(137/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(138/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(139/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(140/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(141/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(142/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(143/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(144/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(145/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(146/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(147/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(148/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(149/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(150/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(151/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(152/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(153/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(154/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(155/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(156/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(157/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(158/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(159/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(160/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(161/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(162/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(163/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(164/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(165/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(166/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(167/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(168/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(169/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(170/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(171/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(172/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(173/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(174/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(175/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(176/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(177/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(178/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(179/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(180/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(181/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(182/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(183/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(184/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(185/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(186/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(187/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(188/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(189/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(190/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(191/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(192/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(193/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(194/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(195/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(196/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(197/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(198/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(199/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(200/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(201/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(202/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(203/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(204/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(205/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(206/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(207/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(208/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(209/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(210/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(211/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(212/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(213/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(214/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(215/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(216/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(217/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(218/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(219/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(220/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(221/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(222/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(223/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(224/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(225/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(226/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(227/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(228/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(229/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(230/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(231/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(232/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(233/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(234/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(235/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(236/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(237/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(238/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(239/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(240/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(241/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(242/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(243/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(244/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(245/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(246/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(247/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(248/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(249/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(250/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(251/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(252/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(253/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(254/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(255/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(256/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(257/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(258/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(259/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(260/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(261/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(262/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(263/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(264/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(265/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(266/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(267/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(268/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(269/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(270/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(271/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(272/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(273/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(274/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(275/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(276/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(277/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(278/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(279/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(280/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(281/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(282/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(283/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(284/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(285/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(286/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(287/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(288/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(289/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(290/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(291/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(292/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(293/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(294/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(295/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(296/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(297/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(298/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(299/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(300/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(301/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(302/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(303/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(304/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(305/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(306/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(307/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(308/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(309/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(310/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(311/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(312/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(313/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(314/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(315/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(316/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(317/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(318/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(319/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(320/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(321/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(322/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(323/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(324/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(325/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(326/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(327/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(328/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(329/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(330/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(331/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(332/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(333/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(334/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(335/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(336/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(337/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(338/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(339/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(340/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(341/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(342/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(343/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(344/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(345/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(346/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(347/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(348/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(349/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(350/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(351/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(352/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(353/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(354/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(355/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(356/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(357/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(358/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(359/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(360/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(361/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(362/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(363/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(364/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(365/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(366/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(367/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(368/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(369/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(370/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(371/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(372/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(373/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(374/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(375/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(376/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(377/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(378/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(379/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(380/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(381/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(382/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(383/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(384/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(385/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(386/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(387/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(388/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(389/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(390/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(391/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(392/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(393/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(394/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(395/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(396/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(397/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(398/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(399/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(400/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(401/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(402/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(403/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(404/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(405/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(406/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(407/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(408/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(409/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(410/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(411/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(412/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(413/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(414/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(415/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(416/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(417/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(418/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(419/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(420/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(421/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(422/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(423/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(424/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(425/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(426/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(427/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(428/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(429/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(430/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(431/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(432/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(433/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(434/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(435/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(436/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(437/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(438/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(439/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(440/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(441/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(442/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(443/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(444/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(445/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(446/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(447/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(448/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(449/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(450/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(451/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(452/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(453/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(454/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(455/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(456/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(457/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(458/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(459/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(460/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(461/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(462/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(463/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(464/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(465/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(466/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(467/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(468/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(469/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(470/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(471/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(472/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(473/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(474/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(475/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(476/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(477/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(478/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(479/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(480/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(481/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(482/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(483/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(484/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(485/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(486/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(487/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(488/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(489/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(490/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(491/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(492/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(493/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(494/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(495/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(496/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(497/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(498/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[  2.18278728e-15  -1.02318154e-16]\n",
      "Gradient Descent(499/499): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "5.54723135786 [ 73.293922    13.47971243]\n",
      "15.3858878688 [ 73.293922    13.47971243]\n"
     ]
    }
   ],
   "source": [
    "test_your_least_squares()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Least squares with a linear basis function model\n",
    "Start from this section, we will use the dataset `dataEx3.csv`.\n",
    "\n",
    "### Implement polynomial basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x (50,)\n",
      "shape of y (50,)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "x, y = load_data()\n",
    "print(\"shape of x {}\".format(x.shape))\n",
    "print(\"shape of y {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 13)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # polynomial basis function: TODO\n",
    "    # this function should return the matrix formed\n",
    "    # by applying the polynomial basis to the input data\n",
    "    # ***************************************************\n",
    "    x = np.array(x)\n",
    "    phi = np.ones([len(x), 1])\n",
    "\n",
    "    for i in range(degree):\n",
    "        phi = np.c_[phi, x**(i+1)]\n",
    "    \n",
    "    return phi\n",
    "    raise NotImplementedError\n",
    "    \n",
    "# build_poly([1,2,3], 12).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with polynomial regression. Note that we will use your implemented function `compute_mse`. Please copy and paste your implementation from exercise02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import compute_mse\n",
    "from plots import *\n",
    "\n",
    "def polynomial_regression():\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 3, 7, 12]\n",
    "    \n",
    "    # define the structure of the figure\n",
    "    num_row = 2\n",
    "    num_col = 2\n",
    "    f, axs = plt.subplots(num_row, num_col)\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # form the data to do polynomial regression.: TODO\n",
    "        # ***************************************************\n",
    "        tx = build_poly(x, degree)\n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # least square and calculate RMSE: TODO\n",
    "        # ***************************************************\n",
    "        rmse, weights = least_squares(y,tx)\n",
    "#         raise NotImplementedError\n",
    "\n",
    "        print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "              i=ind + 1, d=degree, loss=rmse))\n",
    "        # plot fit\n",
    "        plot_fitted_curve(\n",
    "            y, x, weights, degree, axs[ind // num_col][ind % num_col])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"visualize_polynomial_regression\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=0.47187607963421874\n",
      "Processing 2th experiment, degree=3, rmse=0.25858277667737484\n",
      "Processing 3th experiment, degree=7, rmse=0.2496587036090856\n",
      "Processing 4th experiment, degree=12, rmse=1.4371532813243226\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng8FgyJD8Hzqkjo4qLqgKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11320f390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Evaluating model predication performance\n",
    "\n",
    "\n",
    "Let us show the train and test splits for various polynomial degrees. First of all, please fill in the function `split_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data based on the given ratio: TODO\n",
    "    # ***************************************************\n",
    "    data_size = len(y)\n",
    "    train_size = int(np.floor(data_size*ratio))\n",
    "    \n",
    "    shuffled_ind = np.random.permutation(np.arange(data_size))\n",
    "    \n",
    "    train_x = x[shuffled_ind][0:train_size]\n",
    "    train_y = y[shuffled_ind][0:train_size]\n",
    "    test_x = x[shuffled_ind][train_size:]\n",
    "    test_y = y[shuffled_ind][train_size:]\n",
    "    return train_x, train_y, test_x, test_y\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, test your `split_data` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data, and return train and test data: TODO\n",
    "    # ***************************************************\n",
    "#     tx = build_poly(x, degree)\n",
    "    train_x, train_y, test_x, test_y = split_data(x,y, ratio, seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    # ***************************************************\n",
    "    train_tx = build_poly(train_x, degree)\n",
    "    test_tx = build_poly(test_x, degree)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calcualte weight through least square.: TODO\n",
    "    # ***************************************************\n",
    "    rmse_tr, w_star = least_squares(train_y, train_tx)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate RMSE for train and test data,\n",
    "    # and store them in rmse_tr and rmse_te respectively: TODO\n",
    "    # ***************************************************\n",
    "    \n",
    "    rmse_te = np.sqrt(2*sum((test_y - np.dot(test_tx, w_star))**2)/(2*len(test_y)))\n",
    "    \n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.9, degree=1, Training RMSE=0.494, Testing RMSE=0.181\n",
      "proportion=0.9, degree=3, Training RMSE=0.264, Testing RMSE=0.206\n",
      "proportion=0.9, degree=7, Training RMSE=0.254, Testing RMSE=0.220\n",
      "proportion=0.9, degree=12, Training RMSE=0.406, Testing RMSE=0.433\n",
      "proportion=0.5, degree=1, Training RMSE=0.455, Testing RMSE=0.531\n",
      "proportion=0.5, degree=3, Training RMSE=0.239, Testing RMSE=0.296\n",
      "proportion=0.5, degree=7, Training RMSE=0.232, Testing RMSE=0.284\n",
      "proportion=0.5, degree=12, Training RMSE=0.328, Testing RMSE=1.412\n",
      "proportion=0.1, degree=1, Training RMSE=0.428, Testing RMSE=0.534\n",
      "proportion=0.1, degree=3, Training RMSE=0.085, Testing RMSE=0.460\n",
      "proportion=0.1, degree=7, Training RMSE=4.466, Testing RMSE=3.909\n",
      "proportion=0.1, degree=12, Training RMSE=101.561, Testing RMSE=79.809\n"
     ]
    }
   ],
   "source": [
    "seed = 6\n",
    "degrees = [1, 3, 7, 12]\n",
    "split_ratios = [0.9, 0.5, 0.1]\n",
    "\n",
    "for split_ratio in split_ratios:\n",
    "    for degree in degrees:\n",
    "        train_test_split_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Ridge Regression\n",
    "Please fill in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "    w_star = np.dot(np.dot(np.linalg.inv(np.dot(tx.T,tx) + lambda_*2*tx.shape[0]*np.eye(tx.shape[1])),tx.T),y)\n",
    "    mse = np.sqrt(2*sum((y - np.dot(tx, w_star))**2)/(2*len(y)))\n",
    "    \n",
    "    return mse, w_star\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data, and return train and test data: TODO\n",
    "    # ***************************************************\n",
    "    train_x, train_y, test_x, test_y = split_data(x,y, ratio, seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    # ***************************************************\n",
    "\n",
    "    train_tx = build_poly(train_x, degree)\n",
    "    test_tx = build_poly(test_x, degree)       \n",
    "\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # ridge regression with a given lambda\n",
    "        # ***************************************************\n",
    "        rmse, w_star = ridge_regression(train_y, train_tx, lambda_)\n",
    "        \n",
    "        rmse_tr.append(rmse)\n",
    "        rmse_te.append(np.sqrt(2*sum((test_y - np.dot(test_tx, w_star))**2)/(2*len(test_y))))\n",
    "        \n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "        \n",
    "    # Plot the obtained results\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.5, degree=7, lambda=0.000, Training RMSE=0.227, Testing RMSE=0.338\n",
      "proportion=0.5, degree=7, lambda=0.000, Training RMSE=0.227, Testing RMSE=0.337\n",
      "proportion=0.5, degree=7, lambda=0.000, Training RMSE=0.227, Testing RMSE=0.336\n",
      "proportion=0.5, degree=7, lambda=0.000, Training RMSE=0.227, Testing RMSE=0.335\n",
      "proportion=0.5, degree=7, lambda=0.000, Training RMSE=0.228, Testing RMSE=0.334\n",
      "proportion=0.5, degree=7, lambda=0.001, Training RMSE=0.228, Testing RMSE=0.333\n",
      "proportion=0.5, degree=7, lambda=0.001, Training RMSE=0.229, Testing RMSE=0.329\n",
      "proportion=0.5, degree=7, lambda=0.003, Training RMSE=0.230, Testing RMSE=0.319\n",
      "proportion=0.5, degree=7, lambda=0.007, Training RMSE=0.232, Testing RMSE=0.302\n",
      "proportion=0.5, degree=7, lambda=0.016, Training RMSE=0.237, Testing RMSE=0.283\n",
      "proportion=0.5, degree=7, lambda=0.037, Training RMSE=0.246, Testing RMSE=0.276\n",
      "proportion=0.5, degree=7, lambda=0.085, Training RMSE=0.264, Testing RMSE=0.298\n",
      "proportion=0.5, degree=7, lambda=0.193, Training RMSE=0.291, Testing RMSE=0.348\n",
      "proportion=0.5, degree=7, lambda=0.439, Training RMSE=0.317, Testing RMSE=0.401\n",
      "proportion=0.5, degree=7, lambda=1.000, Training RMSE=0.336, Testing RMSE=0.441\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEaCAYAAAACBmAUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11331eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 56\n",
    "degree = 7\n",
    "split_ratio = 0.5\n",
    "ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e-05,   5.17947468e-05,   2.68269580e-04,\n",
       "         1.38949549e-03,   7.19685673e-03,   3.72759372e-02,\n",
       "         1.93069773e-01,   1.00000000e+00,   5.17947468e+00,\n",
       "         2.68269580e+01,   1.38949549e+02,   7.19685673e+02,\n",
       "         3.72759372e+03,   1.93069773e+04,   1.00000000e+05])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = np.logspace(-5, 5, 15)\n",
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
